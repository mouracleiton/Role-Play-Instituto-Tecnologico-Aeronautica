{
  "formatVersion": "1.0",
  "exportDate": "2025-12-04T22:54:24.358Z",
  "appVersion": "1.0",
  "curriculumData": {
    "metadata": {
      "baseOn": "Catálogo dos Cursos de Graduação 2025 - MC-13",
      "lastUpdated": "2025-12-04",
      "totalAtomicSkills": 319,
      "startDate": "2025-02-01",
      "duration": "5 years",
      "dailyStudyHours": 8,
      "version": "2025-MC-13",
      "institution": "Instituto Tecnológico de Aeronáutica (ITA)",
      "basedOn": "Catálogo dos Cursos de Graduação 2025 - MC-13"
    },
    "areas": [
      {
        "id": "10",
        "name": "Matemática Computacional",
        "description": "Área dedicada aos fundamentos matemáticos e computacionais, incluindo ciência de dados e estruturas discretas.",
        "disciplines": [
          {
            "id": "10.1",
            "name": "MC-13 - Introdução à Ciência de Dados",
            "description": "Ciência de Dados e suas aplicações. Conceitos de modelagem de problema e aprendizado. Ambiente independente e identicamente distribuído. Definições de dados, informação e conhecimento. Etapas da Ciência de Dados: coleta, integração e armazenamento de dados; análise exploratória e visualização de dados; limpeza de dados; ajuste e avaliação de modelos: exemplos e estudos de caso. Introdução a Aprendizado de Máquina Supervisionado e Não-supervisionado. Ética no uso e manipulação de dados.",
            "mainTopics": [
              {
                "id": "10.1.1",
                "name": "Ciência de Dados e Aplicações",
                "description": "Apresentação da Ciência de Dados e suas aplicações práticas em diversos contextos.",
                "totalSkills": 65,
                "atomicTopics": [
                  {
                    "id": "10.1.1.1",
                    "name": "Ciência de Dados e Aplicações Práticas",
                    "description": "Apresentação geral da Ciência de Dados e exemplos de aplicações em diversos contextos.",
                    "individualConcepts": [
                      {
                        "id": "10.1.1.1.1",
                        "name": "Fundamentos da Ciência de Dados",
                        "description": "Apresentação dos conceitos básicos que definem a Ciência de Dados, incluindo distinções fundamentais e premissas estatísticas essenciais.",
                        "specificSkills": [
                          {
                            "id": "10.1.1.1.1.1",
                            "name": "Diferenciar dados, informação e conhecimento",
                            "description": "Explicar as diferenças conceituais entre dados (valores brutos), informação (dados processados com contexto) e conhecimento (informação aplicada para tomada de decisão), com exemplos práticos em contextos reais.",
                            "atomicExpansion": {
                              "steps": [
                                {
                                  "stepNumber": 1,
                                  "title": "Definir e Compreender Dados como Valores Brutos",
                                  "subSteps": [
                                    "Leia a definição: Dados são valores brutos, sem contexto ou interpretação, como números, símbolos ou textos isolados.",
                                    "Colete exemplos cotidianos de dados: temperaturas registradas, cliques em um site ou respostas em uma pesquisa.",
                                    "Classifique itens aleatórios como dados ou não-dados: identifique o que é cru e sem processamento.",
                                    "Crie uma lista pessoal de 5 exemplos de dados do seu dia a dia.",
                                    "Discuta com um parceiro por que esses itens são considerados 'brutos'."
                                  ],
                                  "verification": "Liste 5 exemplos corretos de dados brutos sem contexto e explique por quê.",
                                  "estimatedTime": "20 minutos",
                                  "materials": [
                                    "Papel e caneta",
                                    "Acesso a um smartphone para registrar dados reais"
                                  ],
                                  "tips": "Foquem em itens sem significado inerente; evite adicionar interpretações prematuras.",
                                  "learningObjective": "Identificar e definir dados como elementos crus e não processados.",
                                  "commonMistakes": [
                                    "Confundir dados com informações já processadas",
                                    "Adicionar contexto desnecessário aos exemplos"
                                  ]
                                },
                                {
                                  "stepNumber": 2,
                                  "title": "Transformar Dados em Informação com Contexto",
                                  "subSteps": [
                                    "Revise dados do Step 1 e adicione contexto: organize-os em tabelas ou gráficos simples.",
                                    "Aprenda que informação é dados processados com relevância, como 'média de temperatura por dia'.",
                                    "Processe seus exemplos: calcule médias, agrupe por categorias ou adicione rótulos.",
                                    "Compare antes/depois: destaque como o contexto transforma dados em informação útil.",
                                    "Crie um fluxograma simples mostrando dados → processamento → informação."
                                  ],
                                  "verification": "Transforme 3 exemplos de dados em informações com contexto explícito.",
                                  "estimatedTime": "25 minutos",
                                  "materials": [
                                    "Planilha Excel ou Google Sheets",
                                    "Exemplos de dados do Step 1"
                                  ],
                                  "tips": "Use perguntas como 'o quê?', 'quando?' e 'onde?' para adicionar contexto.",
                                  "learningObjective": "Explicar como processamento e contexto elevam dados a informação.",
                                  "commonMistakes": [
                                    "Pular o processamento, tratando dados como informação",
                                    "Adicionar julgamentos subjetivos cedo demais"
                                  ]
                                },
                                {
                                  "stepNumber": 3,
                                  "title": "Elevar Informação a Conhecimento para Decisões",
                                  "subSteps": [
                                    "Defina conhecimento: informação aplicada, integrada com experiência para insights acionáveis.",
                                    "Pegue informações do Step 2 e aplique: 'Alta temperatura média sugere irrigação urgente'.",
                                    "Integre experiência pessoal ou regras gerais para gerar decisões.",
                                    "Documente 3 cenários onde informação vira conhecimento via aplicação prática.",
                                    "Avalie: o resultado permite uma ação ou previsão? Se sim, é conhecimento."
                                  ],
                                  "verification": "Gere uma decisão baseada em informação processada, justificando com experiência.",
                                  "estimatedTime": "25 minutos",
                                  "materials": [
                                    "Informações processadas do Step 2",
                                    "Notas de experiência pessoal"
                                  ],
                                  "tips": "Pergunte 'e agora, o que fazer?' para simular aplicação prática.",
                                  "learningObjective": "Diferenciar conhecimento como aplicação prática de informação.",
                                  "commonMistakes": [
                                    "Parar na informação sem ação",
                                    "Confundir opinião pessoal com conhecimento baseado em evidências"
                                  ]
                                },
                                {
                                  "stepNumber": 4,
                                  "title": "Comparar e Diferenciar os Três Conceitos com Exemplos Práticos",
                                  "subSteps": [
                                    "Crie uma tabela comparativa: colunas para Dados, Informação, Conhecimento; linhas para definição, exemplo, valor agregado.",
                                    "Use um caso real: vendas de uma loja (números → relatório mensal → estratégia de estoque).",
                                    "Debata diferenças em grupo: o que falha se confundir os níveis?",
                                    "Teste com novos exemplos: valide se seguem a pirâmide dados-informação-conhecimento.",
                                    "Resuma em um diagrama da pirâmide DIK (Data-Information-Knowledge)."
                                  ],
                                  "verification": "Apresente tabela ou diagrama comparativo com 3 exemplos integrados.",
                                  "estimatedTime": "30 minutos",
                                  "materials": [
                                    "Ferramenta de desenho como Draw.io ou papel",
                                    "Casos reais impressos ou online"
                                  ],
                                  "tips": "Visualize como uma escada: cada nível adiciona valor ao anterior.",
                                  "learningObjective": "Sintetizar diferenças conceituais e práticas entre dados, informação e conhecimento.",
                                  "commonMistakes": [
                                    "Sobrepor conceitos na tabela",
                                    "Usar exemplos vagos sem contexto real"
                                  ]
                                }
                              ],
                              "practicalExample": "Em uma rede de supermercados: Dados = 1500 unidades vendidas de leite; Informação = Leite é o item mais vendido na seção laticínios em julho (com gráfico de vendas); Conhecimento = Aumentar estoque de leite em 20% para julho próximo ano, baseado em padrões sazonais e margens de lucro.",
                              "finalVerifications": [
                                "Explicar verbalmente as diferenças entre os três conceitos sem hesitação.",
                                "Classificar corretamente 5 exemplos mistos como dados, informação ou conhecimento.",
                                "Criar um exemplo original da pirâmide DIK em um contexto pessoal.",
                                "Identificar erros em descrições confusas fornecidas pelo instrutor.",
                                "Desenhar um fluxograma completo de dados para conhecimento."
                              ],
                              "assessmentCriteria": [
                                "Precisão nas definições (80% de acerto nos termos chave).",
                                "Qualidade e relevância dos exemplos práticos (concretos e contextualizados).",
                                "Capacidade de diferenciação clara na tabela comparativa.",
                                "Profundidade na aplicação para decisões reais.",
                                "Criatividade e originalidade nos fluxogramas ou diagramas.",
                                "Ausência de confusões comuns entre os níveis."
                              ],
                              "crossCurricularConnections": [
                                "Filosofia: Epistemologia e níveis de saber (Platão's divided line).",
                                "Tecnologia da Informação: Modelos de dados em bancos de dados relacionais.",
                                "Administração: Tomada de decisão baseada em BI (Business Intelligence).",
                                "Estatística: Processamento de dados em análise descritiva.",
                                "Psicologia: Processamento cognitivo de informação para expertise."
                              ],
                              "realWorldApplication": "Na Ciência de Dados, profissionais usam essa distinção para limpar dados brutos, gerar dashboards informativos e modelos preditivos que geram conhecimento para estratégias empresariais, como otimização de supply chain na Amazon."
                            },
                            "estimatedTime": "0.5 horas",
                            "difficulty": "beginner",
                            "status": "not_started",
                            "prerequisites": []
                          },
                          {
                            "id": "10.1.1.1.1.2",
                            "name": "Explicar o ambiente i.i.d. (Independente e Identicamente Distribuído)",
                            "description": "Descrever o conceito de amostras i.i.d., sua importância para modelagem estatística e aprendizado de máquina, e identificar cenários onde essa premissa é válida ou violada.",
                            "atomicExpansion": {
                              "steps": [
                                {
                                  "stepNumber": 1,
                                  "title": "Compreender a Independência Estatística",
                                  "subSteps": [
                                    "Defina independência estatística: eventos A e B são independentes se P(A ∩ B) = P(A) * P(B).",
                                    "Explore a noção de que o resultado de uma observação não influencia outra.",
                                    "Calcule probabilidades condicionais para verificar independência: P(A|B) = P(A).",
                                    "Diferencie de dependência, onde eventos se afetam mutuamente.",
                                    "Pratique com exemplos simples como lançamentos de dados."
                                  ],
                                  "verification": "Explique em suas palavras a diferença entre eventos independentes e dependentes, com um exemplo numérico.",
                                  "estimatedTime": "20-30 minutos",
                                  "materials": [
                                    "Papel e caneta para cálculos",
                                    "Calculadora ou software como Python (NumPy para simulações)"
                                  ],
                                  "tips": [
                                    "Use diagramas de Venn para visualizar interseções.",
                                    "Sempre verifique com probabilidades condicionais."
                                  ],
                                  "learningObjective": "Dominar a definição e verificação de independência estatística.",
                                  "commonMistakes": [
                                    "Confundir independência com igualdade de probabilidades.",
                                    "Ignorar o papel das probabilidades condicionais."
                                  ]
                                },
                                {
                                  "stepNumber": 2,
                                  "title": "Compreender Distribuição Identicamente Distribuída",
                                  "subSteps": [
                                    "Defina que todas as amostras vêm da mesma distribuição de probabilidade.",
                                    "Entenda que cada variável aleatória Xi ~ P, para i=1 a n.",
                                    "Compare com casos onde distribuições variam (heterogêneas).",
                                    "Gere histogramas de amostras para visualizar similaridade.",
                                    "Discuta momentos (média, variância) iguais para todas as amostras."
                                  ],
                                  "verification": "Gere 3 conjuntos de amostras de uma distribuição normal e compare estatísticas descritivas (média, desvio padrão).",
                                  "estimatedTime": "25-35 minutos",
                                  "materials": [
                                    "Python com bibliotecas NumPy e Matplotlib",
                                    "Acesso a Jupyter Notebook"
                                  ],
                                  "tips": [
                                    "Use sementes aleatórias para reprodutibilidade.",
                                    "Foquem em estatísticas resumidas para comparação rápida."
                                  ],
                                  "learningObjective": "Identificar e verificar se amostras seguem a mesma distribuição.",
                                  "commonMistakes": [
                                    "Assumir identidade só pela média igual.",
                                    "Não considerar caudas da distribuição."
                                  ]
                                },
                                {
                                  "stepNumber": 3,
                                  "title": "Integrar os Conceitos em Amostras i.i.d.",
                                  "subSteps": [
                                    "Combine independência e distribuição idêntica: {X1, X2, ..., Xn} são i.i.d. se independentes e ~ mesma P.",
                                    "Escreva a densidade conjunta para variáveis contínuas i.i.d.",
                                    "Simule dados i.i.d. versus não-i.i.d. e compare propriedades.",
                                    "Discuta notação matemática: Xi ∼iid P.",
                                    "Aplique em cenários simples como amostragem de uma urna."
                                  ],
                                  "verification": "Simule 1000 pares de variáveis e teste independência via correlação e distribuição conjunta.",
                                  "estimatedTime": "30-40 minutos",
                                  "materials": [
                                    "Python (SciPy para testes estatísticos)",
                                    "Gráficos para visualização"
                                  ],
                                  "tips": [
                                    "Visualize com scatter plots para independência.",
                                    "Use testes como Kolmogorov-Smirnov para distribuições."
                                  ],
                                  "learningObjective": "Formular e reconhecer formalmente amostras i.i.d.",
                                  "commonMistakes": [
                                    "Confundir i.i.d. com normalidade.",
                                    "Esquecer que i.i.d. não implica estacionariedade em séries."
                                  ]
                                },
                                {
                                  "stepNumber": 4,
                                  "title": "Explorar Importância e Violações do i.i.d.",
                                  "subSteps": [
                                    "Explique por que i.i.d. permite teoremas como Lei dos Grandes Números e Teorema Central do Limite.",
                                    "Discuta aplicações em ML: treinamento/teste splits assumem i.i.d.",
                                    "Identifique violações: séries temporais (autocorrelação), amostras agrupadas.",
                                    "Dê exemplos: previsão de ações (dependente no tempo), dados de pacientes (agrupados por hospital).",
                                    "Sugira adaptações: modelos dependentes como ARIMA para violações."
                                  ],
                                  "verification": "Liste 3 cenários reais onde i.i.d. é válido e 3 onde é violado, justificando cada um.",
                                  "estimatedTime": "25-35 minutos",
                                  "materials": [
                                    "Artigos introdutórios sobre ML (scikit-learn docs)",
                                    "Exemplos de datasets como Iris (i.i.d.) vs. stock prices"
                                  ],
                                  "tips": [
                                    "Pense em ordem temporal ou clustering para violações.",
                                    "Relacione com validade de inferência estatística."
                                  ],
                                  "learningObjective": "Avaliar validade da premissa i.i.d. em contextos práticos.",
                                  "commonMistakes": [
                                    "Assumir i.i.d. universalmente em dados observados.",
                                    "Não reconhecer violações sutis como heterogeneidade não observada."
                                  ]
                                }
                              ],
                              "practicalExample": "Considere dados de altura de adultos em uma população: cada medição é independente (uma altura não afeta outra) e segue a mesma distribuição normal (mesma média e variância). Isso permite estimar a média populacional confiavelmente com a média amostral. Se as alturas fossem de famílias (dependentes geneticamente), i.i.d. seria violado.",
                              "finalVerifications": [
                                "Defina precisamente i.i.d. com notação matemática.",
                                "Dê um exemplo numérico de cálculo de independência.",
                                "Simule e compare propriedades de dados i.i.d. vs. não-i.i.d.",
                                "Identifique violações em 2 cenários de ML reais.",
                                "Explique impacto de violação i.i.d. em estimação de parâmetros.",
                                "Aplique Lei dos Grandes Números a um caso i.i.d."
                              ],
                              "assessmentCriteria": [
                                "Precisão e completude das definições (independência e idêntica).",
                                "Qualidade e relevância dos exemplos práticos.",
                                "Correta identificação de cenários válidos e violados.",
                                "Uso apropriado de conceitos estatísticos (LLN, TCL).",
                                "Profundidade na discussão de implicações para ML.",
                                "Clareza na comunicação e visualizações."
                              ],
                              "crossCurricularConnections": [
                                "Probabilidade e Estatística: base para inferência.",
                                "Aprendizado de Máquina: premissa para validação de modelos.",
                                "Séries Temporais: análise de violações de independência.",
                                "Econometria: assunções em regressão.",
                                "Física Computacional: simulações de partículas independentes."
                              ],
                              "realWorldApplication": "Em machine learning, a premissa i.i.d. justifica o uso de conjuntos de treino e teste independentes para avaliação de modelos; violada em dados de sensores IoT (temporais), exigindo técnicas como validação temporal ou modelos de sequência (LSTM)."
                            },
                            "estimatedTime": "0.5 horas",
                            "difficulty": "beginner",
                            "status": "not_started",
                            "prerequisites": []
                          },
                          {
                            "id": "10.1.1.1.1.3",
                            "name": "Descrever modelagem de problemas em Ciência de Dados",
                            "description": "Identificar como formular problemas reais como tarefas de Ciência de Dados, incluindo definição de objetivos, seleção de variáveis e escolha de abordagens analíticas adequadas.",
                            "atomicExpansion": {
                              "steps": [
                                {
                                  "stepNumber": 1,
                                  "title": "Compreender o Problema do Domínio",
                                  "subSteps": [
                                    "Pesquise o contexto do problema lendo documentação e relatórios disponíveis.",
                                    "Entreviste stakeholders para entender necessidades e impactos.",
                                    "Identifique restrições como tempo, recursos e ética.",
                                    "Mapeie o fluxo do processo atual afetado pelo problema.",
                                    "Documente o problema em linguagem simples e não técnica."
                                  ],
                                  "verification": "Produza um resumo escrito de 1 página descrevendo o problema em termos de negócio.",
                                  "estimatedTime": "2 horas",
                                  "materials": [
                                    "Documentos de domínio",
                                    "Ferramentas de entrevista (Zoom ou gravador)",
                                    "Notebook para anotações"
                                  ],
                                  "tips": "Use a técnica dos 5 'porquês' para aprofundar a causa raiz.",
                                  "learningObjective": "Dominar a compreensão profunda do problema real antes de traduzi-lo para termos técnicos.",
                                  "commonMistakes": "Ignorar perspectivas dos stakeholders ou focar prematuramente em soluções técnicas."
                                },
                                {
                                  "stepNumber": 2,
                                  "title": "Definir Objetivos e Métricas de Sucesso",
                                  "subSteps": [
                                    "Estabeleça objetivos claros e mensuráveis (SMART).",
                                    "Defina métricas de sucesso como precisão, recall ou ROI.",
                                    "Priorize objetivos principais vs. secundários.",
                                    "Alinhe objetivos com restrições identificadas.",
                                    "Escreva uma declaração de objetivo formal."
                                  ],
                                  "verification": "Crie um documento com objetivos e métricas aprovados por um peer review.",
                                  "estimatedTime": "1.5 horas",
                                  "materials": [
                                    "Template de objetivos SMART",
                                    "Planilha para métricas (Excel/Google Sheets)"
                                  ],
                                  "tips": "Pergunte: 'O que sucesso parece aqui?' para refinar métricas.",
                                  "learningObjective": "Aprender a traduzir necessidades de negócio em objetivos quantificáveis de Data Science.",
                                  "commonMistakes": "Definir métricas vagas ou desalinhadas com o negócio, como usar apenas accuracy."
                                },
                                {
                                  "stepNumber": 3,
                                  "title": "Selecionar Variáveis e Fontes de Dados",
                                  "subSteps": [
                                    "Liste variáveis candidatas baseadas no domínio (features de entrada e target).",
                                    "Avalie disponibilidade e qualidade das fontes de dados.",
                                    "Identifique variáveis proxy se dados ideais não existirem.",
                                    "Verifique correlações iniciais e multicolinearidade.",
                                    "Crie um diagrama de dados (data map)."
                                  ],
                                  "verification": "Gere um diagrama de variáveis com justificativas para cada seleção.",
                                  "estimatedTime": "2 horas",
                                  "materials": [
                                    "Ferramentas de diagramação (Draw.io ou Lucidchart)",
                                    "Catálogo de dados da organização"
                                  ],
                                  "tips": "Comece com domínio knowledge antes de dados; evite data dredging.",
                                  "learningObjective": "Selecionar variáveis relevantes que suportem os objetivos definidos.",
                                  "commonMistakes": "Incluir variáveis irrelevantes ou ignorar viés em fontes de dados."
                                },
                                {
                                  "stepNumber": 4,
                                  "title": "Escolher Abordagens Analíticas Adequadas",
                                  "subSteps": [
                                    "Classifique o problema (classificação, regressão, clustering etc.).",
                                    "Liste algoritmos possíveis e trade-offs (ex: interpretabilidade vs. performance).",
                                    "Considere baselines simples antes de modelos complexos.",
                                    "Avalie requisitos computacionais e éticos.",
                                    "Documente a escolha com justificativa."
                                  ],
                                  "verification": "Produza um relatório de 1 página com opções avaliadas e escolha final.",
                                  "estimatedTime": "1.5 horas",
                                  "materials": [
                                    "Tabela de algoritmos de ML (referência online)",
                                    "Papel para brainstorming"
                                  ],
                                  "tips": "Use o framework CRISP-DM para guiar a seleção.",
                                  "learningObjective": "Mapear problemas reais para tarefas e métodos de Data Science apropriados.",
                                  "commonMistakes": "Escolher modelos black-box sem considerar interpretabilidade necessária."
                                }
                              ],
                              "practicalExample": "Em uma empresa de telecomunicações, o problema de 'alta rotatividade de clientes' é modelado como tarefa de classificação binária: prever churn (saída) usando variáveis como duração de chamadas, plano tarifário e reclamações (entrada), com métrica principal de AUC-ROC e abordagem de Random Forest para balancear performance e interpretabilidade.",
                              "finalVerifications": [
                                "O aluno descreve corretamente o problema em termos de entrada/saída de DS.",
                                "Objetivos e métricas estão alinhados e mensuráveis.",
                                "Variáveis selecionadas são justificadas pelo domínio.",
                                "Abordagem analítica considera trade-offs explícitos.",
                                "O modelo completo pode ser prototipado com os elementos definidos."
                              ],
                              "assessmentCriteria": [
                                "Clareza na descrição do problema (20%)",
                                "Completude dos objetivos e métricas (25%)",
                                "Relevância e justificativa das variáveis (25%)",
                                "Adequação da abordagem analítica (20%)",
                                "Integração geral e viabilidade (10%)"
                              ],
                              "crossCurricularConnections": [
                                "Estatística: Análise exploratória e testes de hipótese para variáveis.",
                                "Programação: Scripts iniciais em Python/R para data mapping.",
                                "Gestão de Negócios: Alinhamento com KPIs empresariais.",
                                "Ética e Privacidade: Consideração de dados sensíveis (LGPD/GDPR)."
                              ],
                              "realWorldApplication": "Na saúde, modelar previsão de readmissão hospitalar usando dados EHR para otimizar alocação de recursos; em finanças, detectar fraudes em transações para reduzir perdas milionárias."
                            },
                            "estimatedTime": "1 hora",
                            "difficulty": "beginner",
                            "status": "not_started",
                            "prerequisites": []
                          }
                        ]
                      },
                      {
                        "id": "10.1.1.1.2",
                        "name": "Etapas do Processo de Ciência de Dados",
                        "description": "Visão geral das fases principais do ciclo de vida da Ciência de Dados, desde a preparação até a avaliação de modelos.",
                        "specificSkills": [
                          {
                            "id": "10.1.1.1.2.1",
                            "name": "Descrever coleta, integração e armazenamento de dados",
                            "description": "Explicar técnicas para coletar dados de múltiplas fontes, integrá-los de forma consistente e armazená-los em estruturas eficientes como bancos de dados ou data lakes.",
                            "atomicExpansion": {
                              "steps": [
                                {
                                  "stepNumber": 1,
                                  "title": "Compreender Fontes e Técnicas de Coleta de Dados",
                                  "subSteps": [
                                    "Identifique fontes comuns de dados: APIs, bancos de dados relacionais, arquivos CSV/JSON, sensores IoT e web scraping.",
                                    "Estude métodos de coleta: consultas SQL, chamadas HTTP para APIs, extração de arquivos e streaming de dados em tempo real.",
                                    "Analise formatos de dados: estruturados (tabelas), semi-estruturados (JSON/XML) e não estruturados (texto/imagens).",
                                    "Pratique mapeando cenários reais para cada tipo de fonte.",
                                    "Documente exemplos de desafios como autenticação e rate limiting em APIs."
                                  ],
                                  "verification": "Crie um diagrama ou tabela listando 5 fontes de dados com métodos de coleta correspondentes.",
                                  "estimatedTime": "1.5 horas",
                                  "materials": "Notebook, acesso à internet, ferramentas como Postman para testar APIs, diagramas em Draw.io",
                                  "tips": "Comece com fontes públicas gratuitas como datasets do Kaggle para prática sem autenticação.",
                                  "learningObjective": "Identificar e descrever fontes de dados e técnicas de coleta adequadas para diferentes cenários.",
                                  "commonMistakes": "Ignorar questões de privacidade e conformidade (ex: GDPR) ao planejar coleta."
                                },
                                {
                                  "stepNumber": 2,
                                  "title": "Explorar Técnicas de Integração de Dados",
                                  "subSteps": [
                                    "Aprenda conceitos chave: ETL (Extract, Transform, Load) e ELT (Extract, Load, Transform).",
                                    "Estude ferramentas: Pandas no Python para manipulação, Apache Airflow para orquestração.",
                                    "Pratique resolução de inconsistências: padronização de formatos, handling de valores ausentes e deduplicação.",
                                    "Implemente junções: inner, outer, left joins em dados de múltiplas fontes.",
                                    "Valide a qualidade: use métricas como completude, precisão e consistência."
                                  ],
                                  "verification": "Integre dois datasets de exemplo (ex: vendas de CSV e clientes de JSON) e gere um relatório de qualidade.",
                                  "estimatedTime": "2 horas",
                                  "materials": "Python com Pandas e Jupyter Notebook, datasets de amostra do Kaggle",
                                  "tips": "Sempre crie um esquema unificado de dados antes de integrar para evitar erros de tipo.",
                                  "learningObjective": "Descrever processos de integração que garantam consistência de dados de múltiplas fontes.",
                                  "commonMistakes": "Não tratar valores nulos ou discrepâncias de schema, levando a dados corrompidos."
                                },
                                {
                                  "stepNumber": 3,
                                  "title": "Analisar Estruturas de Armazenamento Eficientes",
                                  "subSteps": [
                                    "Compare bancos relacionais (SQL: PostgreSQL) vs. NoSQL (MongoDB) vs. data lakes (S3, Hadoop).",
                                    "Estude particionamento, indexação e compressão para otimização de performance.",
                                    "Aprenda sobre data warehousing (ex: Snowflake) e data lakes para big data.",
                                    "Planeje escalabilidade: sharding, replicação e backup strategies.",
                                    "Avalie custos e segurança: criptografia em repouso/transito e acesso controlado."
                                  ],
                                  "verification": "Desenhe um fluxograma comparando 3 estruturas de armazenamento para um caso de uso específico.",
                                  "estimatedTime": "1.5 horas",
                                  "materials": "Documentação oficial de PostgreSQL/MongoDB/AWS S3, ferramenta de diagramação",
                                  "tips": "Escolha armazenamento baseado no volume, variedade e velocidade (3Vs do Big Data).",
                                  "learningObjective": "Explicar escolhas de armazenamento que suportem eficiência e escalabilidade.",
                                  "commonMistakes": "Selecionar estrutura inadequada, como usar SQL para dados altamente não-estruturados."
                                },
                                {
                                  "stepNumber": 4,
                                  "title": "Sintetizar o Processo Completo em uma Descrição Coesa",
                                  "subSteps": [
                                    "Estruture uma narrativa: introdução, coleta, integração, armazenamento e considerações finais.",
                                    "Inclua diagramas de pipeline de dados usando ferramentas como Lucidchart.",
                                    "Adicione exemplos práticos e trade-offs de cada etapa.",
                                    "Revise para clareza: use linguagem acessível e termos técnicos definidos.",
                                    "Teste a descrição explicando para um par ou gravando um vídeo curto."
                                  ],
                                  "verification": "Escreva um relatório de 1 página descrevendo o processo completo para um cenário dado.",
                                  "estimatedTime": "1 hora",
                                  "materials": "Editor de texto, ferramenta de diagramação, gravador de tela opcional",
                                  "tips": "Use o framework CRISP-DM para contextualizar o processo de ciência de dados.",
                                  "learningObjective": "Integrar conhecimentos em uma explicação abrangente e acionável do pipeline de dados.",
                                  "commonMistakes": "Omitir considerações éticas ou de governança de dados."
                                }
                              ],
                              "practicalExample": "Em um projeto de análise de vendas para uma rede de varejo, colete dados de transações via API do Shopify e logs de website via web scraping; integre usando Pandas para alinhar timestamps e moedas; armazene em um data lake no AWS S3 particionado por data para consultas rápidas com Athena.",
                              "finalVerifications": [
                                "Pode listar e descrever pelo menos 5 fontes de dados comuns com métodos de coleta.",
                                "Demonstra integração de dois datasets de exemplo sem perda de dados.",
                                "Compara corretamente bancos relacionais, NoSQL e data lakes com prós/contras.",
                                "Escreve uma descrição completa do pipeline com diagrama.",
                                "Identifica 3 desafios comuns e soluções em cada etapa.",
                                "Explica impacto de escolhas ruins em eficiência e custo."
                              ],
                              "assessmentCriteria": [
                                "Precisão e completude na descrição de técnicas (90% cobertura dos conceitos chave).",
                                "Clareza e estrutura lógica da explicação (uso de exemplos e diagramas).",
                                "Profundidade nos sub-passos e verificações práticas.",
                                "Relevância das conexões com cenários reais.",
                                "Identificação correta de erros comuns e dicas preventivas.",
                                "Coerência geral do pipeline de coleta-integração-armazenamento."
                              ],
                              "crossCurricularConnections": [
                                "Programação: Uso de Python/Pandas/SQL para implementação prática.",
                                "Estatística: Avaliação de qualidade de dados para análises confiáveis.",
                                "Ética e Governança: Conformidade com LGPD/GDPR em coleta e armazenamento.",
                                "Gestão de Projetos: Orquestração de pipelines com ferramentas como Airflow."
                              ],
                              "realWorldApplication": "Na saúde, coletar dados de prontuários eletrônicos de hospitais diferentes, integrá-los para detectar padrões de doenças epidêmicas e armazená-los em data lakes seguros para pesquisas de IA, permitindo respostas rápidas a surtos como COVID-19."
                            },
                            "estimatedTime": "1 hora",
                            "difficulty": "beginner",
                            "status": "not_started",
                            "prerequisites": []
                          },
                          {
                            "id": "10.1.1.1.2.2",
                            "name": "Realizar análise exploratória e visualização de dados",
                            "description": "Aplicar métodos para explorar dados iniciais, identificar padrões e usar ferramentas de visualização (gráficos, histogramas) para insights preliminares.",
                            "atomicExpansion": {
                              "steps": [
                                {
                                  "stepNumber": 1,
                                  "title": "Carregar e Inspecionar Dados Iniciais",
                                  "subSteps": [
                                    "Importar bibliotecas necessárias: pandas e numpy.",
                                    "Carregar o dataset usando pd.read_csv() ou similar.",
                                    "Executar df.head(), df.tail(), df.shape e df.info() para overview.",
                                    "Verificar tipos de dados com df.dtypes.",
                                    "Identificar colunas categóricas e numéricas."
                                  ],
                                  "verification": "Comandos executados sem erros e dados exibidos corretamente no Jupyter Notebook.",
                                  "estimatedTime": "20 minutos",
                                  "materials": [
                                    "Python com pandas e numpy instalados",
                                    "Jupyter Notebook",
                                    "Dataset CSV de exemplo (ex: Iris ou Titanic)"
                                  ],
                                  "tips": "Sempre use pd.set_option('display.max_columns', None) para ver todas as colunas.",
                                  "learningObjective": "Compreender a estrutura, tamanho e tipos de dados do dataset.",
                                  "commonMistakes": [
                                    "Esquecer de importar pandas como pd",
                                    "Caminho incorreto do arquivo CSV",
                                    "Ignorar valores missing iniciais"
                                  ]
                                },
                                {
                                  "stepNumber": 2,
                                  "title": "Realizar Análise Estatística Descritiva",
                                  "subSteps": [
                                    "Calcular estatísticas com df.describe() para variáveis numéricas.",
                                    "Usar df.value_counts() para variáveis categóricas.",
                                    "Verificar missing values com df.isnull().sum().",
                                    "Calcular correlações iniciais com df.corr().",
                                    "Identificar outliers usando boxplot stats ou quartis."
                                  ],
                                  "verification": "Tabelas de describe(), value_counts() e isnull() geradas e interpretadas.",
                                  "estimatedTime": "25 minutos",
                                  "materials": [
                                    "Pandas",
                                    "Jupyter Notebook"
                                  ],
                                  "tips": "Filtre colunas específicas: df[num_cols].describe() para focar em numéricas.",
                                  "learningObjective": "Obter resumo estatístico para detectar distribuições e anomalias.",
                                  "commonMistakes": [
                                    "Aplicar describe() em categóricas sem conversão",
                                    "Ignorar NaNs nas correlações",
                                    "Não normalizar escalas em corr()"
                                  ]
                                },
                                {
                                  "stepNumber": 3,
                                  "title": "Criar Visualizações Exploratórias Básicas",
                                  "subSteps": [
                                    "Importar matplotlib.pyplot e seaborn.",
                                    "Criar histogramas: sns.histplot(df['coluna']) para distribuições.",
                                    "Gerar boxplots: sns.boxplot(x='cat', y='num', data=df) para outliers.",
                                    "Plots de scatter: sns.scatterplot(x='var1', y='var2', data=df) para relações.",
                                    "Heatmap de correlação: sns.heatmap(df.corr(), annot=True)."
                                  ],
                                  "verification": "Gráficos renderizados sem erros e salvos ou exibidos.",
                                  "estimatedTime": "30 minutos",
                                  "materials": [
                                    "Matplotlib",
                                    "Seaborn",
                                    "Jupyter Notebook"
                                  ],
                                  "tips": "Use plt.figure(figsize=(10,6)) para tamanhos adequados e plt.title() para labels.",
                                  "learningObjective": "Visualizar padrões, distribuições e relações entre variáveis.",
                                  "commonMistakes": [
                                    "Escalas diferentes nos eixos sem log scale",
                                    "Sobrecarregar plots com muitos dados",
                                    "Esquecer plt.show()"
                                  ]
                                },
                                {
                                  "stepNumber": 4,
                                  "title": "Identificar Padrões e Extrair Insights Preliminares",
                                  "subSteps": [
                                    "Analisar visualizações: notar skewness, clusters, gaps.",
                                    "Documentar observações em markdown cells ou dict.",
                                    "Hipotetizar relações baseadas em correlações >0.5 ou < -0.5.",
                                    "Verificar multicolinearidade via heatmap.",
                                    "Resumir insights em relatório curto."
                                  ],
                                  "verification": "Lista de 5+ insights documentados com referências aos gráficos.",
                                  "estimatedTime": "25 minutos",
                                  "materials": [
                                    "Jupyter Notebook para anotações"
                                  ],
                                  "tips": "Use perguntas guiadas: 'Quais variáveis mais influenciam o target?'",
                                  "learningObjective": "Traduzir análises em insights acionáveis iniciais.",
                                  "commonMistakes": [
                                    "Confundir correlação com causalidade",
                                    "Ignorar contexto do domínio",
                                    "Não priorizar insights relevantes"
                                  ]
                                }
                              ],
                              "practicalExample": "Usando o dataset Titanic (disponível no seaborn: sns.load_dataset('titanic')), carregue os dados, explore 'Age' e 'Survived' com histogramas e boxplots por classe ('Pclass'), identifique que passageiros de 1ª classe têm maior taxa de sobrevivência e mais crianças salvas, visualizando padrões de idade e gênero.",
                              "finalVerifications": [
                                "Dataset carregado e inspecionado sem erros.",
                                "Estatísticas descritivas e missing values reportados.",
                                "Pelo menos 4 visualizações (histograma, boxplot, scatter, heatmap) criadas.",
                                "5+ insights preliminares documentados.",
                                "Código reproduzível e limpo.",
                                "Nenhum erro de runtime nos scripts."
                              ],
                              "assessmentCriteria": [
                                "Precisão e completude das estatísticas descritivas (90% cobertura de variáveis).",
                                "Qualidade visual das plots (labels, títulos, legibilidade).",
                                "Profundidade dos insights (baseados em evidências, não suposições).",
                                "Eficiência do código (uso de funções vetoriais pandas).",
                                "Identificação correta de pelo menos 3 padrões ou anomalias.",
                                "Documentação clara de observações."
                              ],
                              "crossCurricularConnections": [
                                "Estatística: Medidas de tendência central e dispersão.",
                                "Programação: Manipulação de dados com Python/pandas.",
                                "Design Gráfico: Princípios de visualização eficaz (Tufte).",
                                "Negócios: Interpretação de dados para decisões.",
                                "Matemática: Correlação e distribuições probabilísticas."
                              ],
                              "realWorldApplication": "Em empresas de e-commerce, analistas usam EDA para explorar dados de vendas, identificando picos sazonais via histogramas e relações entre preço e quantidade via scatter plots, otimizando estoque e campanhas de marketing."
                            },
                            "estimatedTime": "1 hora",
                            "difficulty": "beginner",
                            "status": "not_started",
                            "prerequisites": []
                          },
                          {
                            "id": "10.1.1.1.2.3",
                            "name": "Executar limpeza de dados",
                            "description": "Identificar e tratar problemas comuns como valores ausentes, outliers e duplicatas, utilizando técnicas para preparar dados de alta qualidade para modelagem.",
                            "atomicExpansion": {
                              "steps": [
                                {
                                  "stepNumber": 1,
                                  "title": "Carregar e Explorar Dados Iniciais para Identificar Problemas",
                                  "subSteps": [
                                    "Carregue o dataset usando pandas.read_csv() ou equivalente.",
                                    "Use df.info() e df.describe() para overview de estrutura, tipos e estatísticas.",
                                    "Visualize missing values com df.isnull().sum() e heatmap via seaborn.",
                                    "Identifique duplicatas com df.duplicated().sum().",
                                    "Plote histogramas para detectar outliers potenciais."
                                  ],
                                  "verification": "Gere um relatório inicial listando contagens de missing values, duplicatas e estatísticas descritivas.",
                                  "estimatedTime": "30 minutes",
                                  "materials": [
                                    "Python com pandas e seaborn instalados",
                                    "Jupyter Notebook",
                                    "Dataset de exemplo (ex: titanic.csv)"
                                  ],
                                  "tips": "Sempre crie uma cópia do dataset original com df_copy = df.copy() para evitar perda de dados.",
                                  "learningObjective": "O aluno será capaz de realizar uma exploração inicial completa para diagnosticar problemas de qualidade nos dados.",
                                  "commonMistakes": [
                                    "Ignorar tipos de dados não numéricos",
                                    "Não visualizar graficamente para outliers",
                                    "Pular contagem de duplicatas"
                                  ]
                                },
                                {
                                  "stepNumber": 2,
                                  "title": "Tratar Valores Ausentes",
                                  "subSteps": [
                                    "Calcule porcentagem de missing values por coluna: (df.isnull().sum() / len(df)) * 100.",
                                    "Decida estratégia baseada no contexto: imputação com mean/median/mode para numéricos, 'Unknown' para categóricos, ou dropna().",
                                    "Aplique imputação: df.fillna(df.median()) ou SimpleImputer do sklearn.",
                                    "Verifique pós-tratamento com df.isnull().sum().",
                                    "Documente escolhas em um log ou comentário."
                                  ],
                                  "verification": "Confirme zero missing values no dataset tratado e compare estatísticas antes/depois.",
                                  "estimatedTime": "45 minutes",
                                  "materials": [
                                    "Pandas",
                                    "Scikit-learn (opcional para imputers avançados)",
                                    "Dataset com missings"
                                  ],
                                  "tips": "Para variáveis skew, prefira mediana sobre média; avalie impacto no target variable.",
                                  "learningObjective": "O aluno dominará técnicas de imputação e remoção de missing values, escolhendo métodos apropriados.",
                                  "commonMistakes": [
                                    "Preencher tudo com zero ou média sem análise",
                                    "Dropar linhas/colunas excessivamente sem justificativa",
                                    "Esquecer de tratar missings em variáveis categóricas"
                                  ]
                                },
                                {
                                  "stepNumber": 3,
                                  "title": "Detectar e Remover Duplicatas",
                                  "subSteps": [
                                    "Identifique duplicatas totais: df.duplicated().sum().",
                                    "Verifique duplicatas em subconjuntos: df.duplicated(subset=['ID']).sum().",
                                    "Remova duplicatas: df.drop_duplicates(inplace=True) ou subset específico.",
                                    "Reindexe o dataset: df.reset_index(drop=True).",
                                    "Confirme remoção comparando shapes antes/depois."
                                  ],
                                  "verification": "Execute df.duplicated().sum() == 0 e compare len(df) original vs final.",
                                  "estimatedTime": "20 minutes",
                                  "materials": [
                                    "Pandas",
                                    "Dataset com duplicatas intencionais"
                                  ],
                                  "tips": "Defina subset de colunas chave (ex: ID único) para evitar remoção errada de registros semelhantes válidos.",
                                  "learningObjective": "O aluno aprenderá a identificar e eliminar duplicatas de forma precisa e contextual.",
                                  "commonMistakes": [
                                    "Remover todas duplicatas sem subset, perdendo dados válidos",
                                    "Não reindexar após drop",
                                    "Confundir duplicatas com valores semelhantes"
                                  ]
                                },
                                {
                                  "stepNumber": 4,
                                  "title": "Identificar e Tratar Outliers",
                                  "subSteps": [
                                    "Use boxplots ou IQR method: Q1 - 1.5*IQR a Q3 + 1.5*IQR.",
                                    "Calcule z-scores para detecção estatística: abs(zscore) > 3.",
                                    "Decida ação: clip (winsorize), remover ou investigar causa.",
                                    "Aplique: df[(df['col'] >= lower) & (df['col'] <= upper)].",
                                    "Reavalie distribuições com plots pós-tratamento."
                                  ],
                                  "verification": "Gere boxplots antes/depois e confirme redução de outliers extremos.",
                                  "estimatedTime": "40 minutes",
                                  "materials": [
                                    "Pandas",
                                    "Matplotlib/Seaborn para plots",
                                    "Scipy.stats para zscore"
                                  ],
                                  "tips": "Investigue outliers primeiro – podem ser erros ou insights valiosos; use domínio do negócio para decidir.",
                                  "learningObjective": "O aluno aplicará métodos estatísticos para detectar e tratar outliers de maneira informada.",
                                  "commonMistakes": [
                                    "Remover todos outliers automaticamente",
                                    "Usar método único sem testar múltiplos",
                                    "Ignorar multicolinearidade entre variáveis"
                                  ]
                                },
                                {
                                  "stepNumber": 5,
                                  "title": "Validar Qualidade Final e Preparar para Modelagem",
                                  "subSteps": [
                                    "Execute checks completos: missing, duplicatas, outliers.",
                                    "Compare estatísticas descritivas originais vs finais.",
                                    "Salve dataset limpo: df.to_csv('cleaned_data.csv').",
                                    "Gere relatório summary com métricas de qualidade.",
                                    "Teste consistência em amostra para modelagem simples."
                                  ],
                                  "verification": "Relatório final mostra dataset pronto: 0 missings/duplicatas, distribuições normais.",
                                  "estimatedTime": "25 minutes",
                                  "materials": [
                                    "Pandas",
                                    "Jupyter para relatório"
                                  ],
                                  "tips": "Mantenha versão original e log de transformações para reprodutibilidade.",
                                  "learningObjective": "O aluno validará integralmente a qualidade dos dados preparados para downstream tasks.",
                                  "commonMistakes": [
                                    "Pular validação final",
                                    "Não documentar mudanças",
                                    "Alterar target variable inadvertidamente"
                                  ]
                                }
                              ],
                              "practicalExample": "Em um dataset de vendas de e-commerce (ex: 1000 registros com colunas: customer_id, product_price, quantity, date), identifique 15% missing em quantity (preencha com mediana=1), remova 50 duplicatas por customer_id+date, clip prices outliers acima de $1000 para $999, resultando em dataset limpo de 950 linhas pronto para análise de churn.",
                              "finalVerifications": [
                                "Dataset sem valores ausentes (df.isnull().sum().sum() == 0)",
                                "Sem duplicatas (df.duplicated().sum() == 0)",
                                "Distribuições sem outliers extremos (boxplots limpos)",
                                "Estatísticas descritivas estáveis vs originais (média/desvio padrão razoáveis)",
                                "Tipos de dados corretos (df.dtypes)",
                                "Tamanho final coerente (perda < 20% esperada)"
                              ],
                              "assessmentCriteria": [
                                "Escolhas de tratamento justificadas com análise contextual (80%+ acerto)",
                                "Código limpo, reprodutível e comentado",
                                "Uso correto de múltiplas técnicas (IQR, z-score, imputação variada)",
                                "Relatórios e visualizações incluídos para cada step",
                                "Impacto mínimo em variância/distribuição dos dados",
                                "Preparação efetiva para modelagem (ex: correlação com target preservada)"
                              ],
                              "crossCurricularConnections": [
                                "Estatística: Medidas de tendência central, IQR e z-scores",
                                "Programação: Manipulação de DataFrames em Python/Pandas",
                                "Ética e Negócios: Decisões de dados sem bias ou distorção",
                                "Visualização de Dados: Plots para diagnóstico",
                                "Matemática Computacional: Pré-processamento para ML"
                              ],
                              "realWorldApplication": "Em empresas como Amazon ou bancos, limpar dados de transações para treinar modelos de recomendação ou detecção de fraude, evitando erros caros em previsões e garantindo compliance com regulamentações como GDPR."
                            },
                            "estimatedTime": "1 hora",
                            "difficulty": "beginner",
                            "status": "not_started",
                            "prerequisites": []
                          },
                          {
                            "id": "10.1.1.1.2.4",
                            "name": "Ajustar e avaliar modelos",
                            "description": "Descrever o processo de treinamento de modelos, seleção de hiperparâmetros e métricas de avaliação como acurácia, precisão e recall, com exemplos simples.",
                            "atomicExpansion": {
                              "steps": [
                                {
                                  "stepNumber": 1,
                                  "title": "Preparar dados e treinar modelo baseline",
                                  "subSteps": [
                                    "Carregue um dataset simples, como Iris ou um binário de spam.",
                                    "Divida os dados em treino (80%) e teste (20%) usando train_test_split.",
                                    "Selecione um modelo simples, como LogisticRegression ou DecisionTreeClassifier.",
                                    "Treine o modelo com os dados de treino usando fit().",
                                    "Faça predições nos dados de teste."
                                  ],
                                  "verification": "Execute o código e visualize as predições iniciais; confira se o modelo roda sem erros.",
                                  "estimatedTime": "1 hora",
                                  "materials": [
                                    "Python com scikit-learn instalado",
                                    "Jupyter Notebook",
                                    "Dataset Iris do sklearn.datasets"
                                  ],
                                  "tips": "Sempre use random_state para reprodutibilidade.",
                                  "learningObjective": "Compreender o treinamento básico de um modelo de ML.",
                                  "commonMistakes": [
                                    "Não dividir dados (overfitting)",
                                    "Esquecer de importar bibliotecas",
                                    "Usar dados de teste para treino"
                                  ]
                                },
                                {
                                  "stepNumber": 2,
                                  "title": "Selecionar e entender hiperparâmetros chave",
                                  "subSteps": [
                                    "Identifique hiperparâmetros do modelo (ex: C em LogisticRegression, max_depth em DecisionTree).",
                                    "Consulte a documentação do scikit-learn para defaults e impactos.",
                                    "Crie um dicionário de parâmetros para testar (ex: {'C': [0.1, 1, 10]}).",
                                    "Explique verbalmente ou por escrito como cada hiperparâmetro afeta o modelo.",
                                    "Documente escolhas iniciais baseadas em conhecimento teórico."
                                  ],
                                  "verification": "Liste 3 hiperparâmetros com suas funções e valores iniciais em um relatório curto.",
                                  "estimatedTime": "45 minutos",
                                  "materials": [
                                    "Documentação scikit-learn",
                                    "Jupyter Notebook"
                                  ],
                                  "tips": "Comece com poucos parâmetros para evitar explosão combinatória.",
                                  "learningObjective": "Dominar o papel dos hiperparâmetros no desempenho do modelo.",
                                  "commonMistakes": [
                                    "Confundir hiperparâmetros com métricas",
                                    "Ignorar trade-offs como bias-variance",
                                    "Testar valores aleatórios sem justificativa"
                                  ]
                                },
                                {
                                  "stepNumber": 3,
                                  "title": "Otimizar hiperparâmetros usando busca sistemática",
                                  "subSteps": [
                                    "Implemente GridSearchCV ou RandomizedSearchCV com os parâmetros definidos.",
                                    "Defina uma métrica primária (ex: accuracy para baseline).",
                                    "Execute a busca com cv=5 (validação cruzada).",
                                    "Extraia o melhor modelo e hiperparâmetros com best_params_ e best_estimator_.",
                                    "Compare scores antes/depois da otimização."
                                  ],
                                  "verification": "Mostre tabela comparativa de scores pré e pós-otimização.",
                                  "estimatedTime": "1.5 horas",
                                  "materials": [
                                    "scikit-learn GridSearchCV",
                                    "Jupyter Notebook"
                                  ],
                                  "tips": "Use n_jobs=-1 para paralelizar e acelerar.",
                                  "learningObjective": "Aplicar técnicas de tuning para melhorar performance.",
                                  "commonMistakes": [
                                    "Grid muito grande causando lentidão",
                                    "Não usar CV levando a overfitting",
                                    "Ignorar warnings de convergência"
                                  ]
                                },
                                {
                                  "stepNumber": 4,
                                  "title": "Avaliar modelo com métricas detalhadas",
                                  "subSteps": [
                                    "Calcule acurácia, precisão, recall e F1-score usando classification_report.",
                                    "Gere matriz de confusão com confusion_matrix e plot com seaborn.",
                                    "Interprete resultados: ex, alto recall bom para detecção de fraudes.",
                                    "Compare com baseline e discuta trade-offs (ex: precisão vs recall).",
                                    "Teste em dados de hold-out para validação final."
                                  ],
                                  "verification": "Produza relatório com métricas, matriz de confusão e interpretação.",
                                  "estimatedTime": "1 hora",
                                  "materials": [
                                    "scikit-learn metrics",
                                    "matplotlib/seaborn",
                                    "Jupyter Notebook"
                                  ],
                                  "tips": "Lembre: acurácia falha em datasets desbalanceados.",
                                  "learningObjective": "Interpretar e selecionar métricas adequadas ao problema.",
                                  "commonMistakes": [
                                    "Focar só em acurácia",
                                    "Confundir precisão com recall",
                                    "Não plotar matriz de confusão"
                                  ]
                                }
                              ],
                              "practicalExample": "Usando dataset de emails spam/não-spam: treine LogisticRegression, otimize C e penalty, avalie com precisão/recall (alta precisão evita falsos positivos em filtros de email).",
                              "finalVerifications": [
                                "Explicar diferença entre precisão, recall e F1 com exemplo numérico.",
                                "Implementar tuning de 2+ hiperparâmetros e melhorar score em 10%.",
                                "Gerar e interpretar matriz de confusão corretamente.",
                                "Escolher métrica certa para problema desbalanceado.",
                                "Documentar processo completo em notebook reproduzível.",
                                "Discutir limitações do modelo otimizado."
                              ],
                              "assessmentCriteria": [
                                "Precisão nas definições e cálculos de métricas (90%+ correto).",
                                "Melhoria mensurável no desempenho pós-tuning.",
                                "Relatório claro com visualizações e interpretações.",
                                "Código limpo, comentado e sem erros.",
                                "Compreensão de trade-offs demonstrada em discussão.",
                                "Aplicação correta de CV e hold-out."
                              ],
                              "crossCurricularConnections": [
                                "Estatística: Entender distribuições e testes de significância em CV.",
                                "Programação: Manipulação de dados com pandas e loops em buscas.",
                                "Matemática: Álgebra linear em modelos e otimização.",
                                "Ética: Discussão de bias em métricas e datasets."
                              ],
                              "realWorldApplication": "Em empresas de tech como Google ou bancos, data scientists ajustam modelos de recomendação ou detecção de fraudes, usando essas técnicas para deploy em produção, economizando milhões em erros de predição."
                            },
                            "estimatedTime": "1 hora",
                            "difficulty": "beginner",
                            "status": "not_started",
                            "prerequisites": []
                          }
                        ]
                      },
                      {
                        "id": "10.1.1.1.3",
                        "name": "Aplicações Práticas e Aprendizado de Máquina",
                        "description": "Exemplos reais de aplicações da Ciência de Dados e introdução aos tipos básicos de aprendizado de máquina.",
                        "specificSkills": [
                          {
                            "id": "10.1.1.1.3.1",
                            "name": "Identificar aplicações práticas da Ciência de Dados",
                            "description": "Analisar estudos de caso em áreas como saúde, finanças, marketing e e-commerce, destacando impactos e benefícios reais.",
                            "atomicExpansion": {
                              "steps": [
                                {
                                  "stepNumber": 1,
                                  "title": "Pesquisar e Selecionar Estudos de Caso Relevantes",
                                  "subSteps": [
                                    "Identifique áreas chave: saúde, finanças, marketing e e-commerce.",
                                    "Busque estudos de caso reais usando fontes confiáveis como Kaggle, Towards Data Science ou relatórios de empresas (ex: Google, IBM).",
                                    "Selecione pelo menos um caso por área, priorizando aqueles com dados públicos e resultados mensuráveis.",
                                    "Registre fontes, datas e resumos iniciais em um documento.",
                                    "Crie uma tabela comparativa inicial com colunas: Área, Problema, Solução DS, Resultado."
                                  ],
                                  "verification": "Verifique se possui pelo menos 4 estudos de caso documentados com fontes citadas.",
                                  "estimatedTime": "1.5 horas",
                                  "materials": "Acesso à internet, Google Scholar, Kaggle datasets, caderno ou Google Docs.",
                                  "tips": "Use palavras-chave como 'data science case study health' para buscas eficientes.",
                                  "learningObjective": "Desenvolver habilidades de pesquisa focada em aplicações práticas de DS.",
                                  "commonMistakes": "Escolher casos irrelevantes ou sem dados reais; ignorar fontes acadêmicas."
                                },
                                {
                                  "stepNumber": 2,
                                  "title": "Analisar Estudos de Caso em Saúde e Finanças",
                                  "subSteps": [
                                    "Para saúde: Examine um caso como predição de doenças via ML (ex: detecção de câncer com imagens).",
                                    "Descreva o problema, técnicas DS usadas (regressão, clustering) e métricas de sucesso (acurácia >90%).",
                                    "Para finanças: Analise detecção de fraudes em transações (ex: modelo de PayPal).",
                                    "Identifique dados usados, algoritmos e impactos como redução de perdas financeiras.",
                                    "Compare similaridades e diferenças entre os dois setores."
                                  ],
                                  "verification": "Crie um relatório de 1 página por caso destacando problema, solução e resultados.",
                                  "estimatedTime": "2 horas",
                                  "materials": "Estudos selecionados, ferramentas como Jupyter Notebook para visualizações opcionais.",
                                  "tips": "Foque em métricas quantificáveis como ROI ou redução de erros para manter objetividade.",
                                  "learningObjective": "Capacitar análise crítica de aplicações DS em setores críticos.",
                                  "commonMistakes": "Confundir correlação com causalidade; superestimar benefícios sem evidências."
                                },
                                {
                                  "stepNumber": 3,
                                  "title": "Analisar Estudos de Caso em Marketing e E-commerce",
                                  "subSteps": [
                                    "Para marketing: Estude segmentação de clientes (ex: Netflix recomendações).",
                                    "Detalhe dados comportamentais, modelos de recomendação e benefícios como aumento de retenção.",
                                    "Para e-commerce: Analise otimização de preços ou previsão de demanda (ex: Amazon).",
                                    "Explique algoritmos como árvores de decisão e impactos em vendas (+20-30%).",
                                    "Documente lições aprendidas sobre escalabilidade em alto volume de dados."
                                  ],
                                  "verification": "Produza diagramas de fluxo ou mindmaps conectando problema a solução para cada caso.",
                                  "estimatedTime": "2 horas",
                                  "materials": "Ferramentas de diagramação como Draw.io, artigos e vídeos dos casos.",
                                  "tips": "Assista a TED Talks ou webinars para contexto visual dos casos.",
                                  "learningObjective": "Entender aplicações DS em negócios dinâmicos e orientados a dados.",
                                  "commonMistakes": "Ignorar questões éticas como privacidade de dados; generalizar resultados sem contexto."
                                },
                                {
                                  "stepNumber": 4,
                                  "title": "Sintetizar Impactos, Benefícios e Aplicações Gerais",
                                  "subSteps": [
                                    "Liste impactos comuns: eficiência, precisão, redução de custos em todas as áreas.",
                                    "Destaque benefícios quantificáveis (ex: economia de milhões em finanças, vidas salvas em saúde).",
                                    "Identifique desafios compartilhados como qualidade de dados e viés.",
                                    "Crie uma apresentação ou infográfico resumindo 3-5 aplicações chave por área.",
                                    "Reflita sobre como DS transforma decisões baseadas em intuição para data-driven."
                                  ],
                                  "verification": "Elabore um resumo final de 500 palavras com tabela de impactos mensuráveis.",
                                  "estimatedTime": "1.5 horas",
                                  "materials": "Ferramentas de apresentação como PowerPoint ou Canva, resumos anteriores.",
                                  "tips": "Use bullet points e gráficos para síntese clara e impactante.",
                                  "learningObjective": "Capacitar síntese de conhecimentos para identificação prática de DS.",
                                  "commonMistakes": "Focar só em sucessos, ignorando falhas; não quantificar benefícios."
                                }
                              ],
                              "practicalExample": "Analise o caso da IBM Watson Health para detecção de câncer de pulmão: usando DS em imagens de tomografia, reduziu falsos positivos em 94%, economizando tempo de radiologistas e melhorando diagnósticos precoces.",
                              "finalVerifications": [
                                "Pode citar pelo menos 2 aplicações específicas por área (saúde, finanças, marketing, e-commerce).",
                                "Explica impactos quantificáveis como ROI ou acurácia em exemplos reais.",
                                "Identifica benefícios como eficiência operacional e tomada de decisão melhorada.",
                                "Lista desafios comuns como viés em dados e privacidade.",
                                "Compara aplicações entre áreas com exemplos concretos.",
                                "Produz um resumo coeso destacando transformação via DS."
                              ],
                              "assessmentCriteria": [
                                "Precisão e relevância dos estudos de caso selecionados (fontes confiáveis).",
                                "Profundidade na análise de técnicas DS e métricas de sucesso.",
                                "Clareza na identificação de impactos e benefícios reais.",
                                "Capacidade de síntese e conexões interdisciplinares.",
                                "Uso de evidências quantificáveis e exemplos concretos.",
                                "Estrutura lógica e visualizações auxiliares no relatório final."
                              ],
                              "crossCurricularConnections": [
                                "Estatística e Matemática: Modelos preditivos e análise de dados.",
                                "Negócios e Economia: Otimização financeira e estratégias de marketing.",
                                "Ética e Direito: Privacidade de dados e viés algorítmico.",
                                "Biologia e Medicina: Aplicações em saúde preditiva.",
                                "Informática: Programação e ferramentas de DS como Python/R."
                              ],
                              "realWorldApplication": "No mercado de trabalho, profissionais identificam aplicações DS para propor soluções em empresas, como usar ML em bancos para antifraude (reduzindo perdas em bilhões) ou em e-commerces para personalização, aumentando conversões em 35%, impulsionando carreiras em DS e análise de negócios."
                            },
                            "estimatedTime": "1 hora",
                            "difficulty": "beginner",
                            "status": "not_started",
                            "prerequisites": []
                          },
                          {
                            "id": "10.1.1.1.3.2",
                            "name": "Introduzir Aprendizado de Máquina Supervisionado",
                            "description": "Explicar o conceito de ML supervisionado, com exemplos de regressão e classificação, e diferenciação de tarefas comuns.",
                            "atomicExpansion": {
                              "steps": [
                                {
                                  "stepNumber": 1,
                                  "title": "Entender os Conceitos Fundamentais de Aprendizado de Máquina Supervisionado",
                                  "subSteps": [
                                    "Defina Aprendizado de Máquina (ML) como um subcampo da IA onde algoritmos aprendem padrões de dados.",
                                    "Explique 'supervisionado' como o uso de dados rotulados (entradas com saídas conhecidas) para treinar o modelo.",
                                    "Descreva o processo: coleta de dados rotulados, treinamento do modelo, predição em novos dados.",
                                    "Compare com aprendizado humano: professor fornece exemplos corretos (rótulos).",
                                    "Liste componentes chave: features (entradas), labels (saídas), modelo."
                                  ],
                                  "verification": "Escreva um parágrafo explicando ML supervisionado em suas próprias palavras e identifique features e labels em um exemplo simples.",
                                  "estimatedTime": "1 hora",
                                  "materials": [
                                    "Notebook para anotações",
                                    "Vídeo introdutório no YouTube sobre ML supervisionado",
                                    "Artigo da Wikipedia: Supervised Learning"
                                  ],
                                  "tips": "Use analogias cotidianas, como ensinar uma criança a identificar frutas com exemplos rotulados.",
                                  "learningObjective": "Compreender a definição e o fluxo básico de ML supervisionado.",
                                  "commonMistakes": [
                                    "Confundir supervisionado com não-supervisionado (sem rótulos)",
                                    "Ignorar a importância dos dados rotulados",
                                    "Pensar que ML é só 'previsão' sem aprendizado"
                                  ]
                                },
                                {
                                  "stepNumber": 2,
                                  "title": "Explorar Regressão no ML Supervisionado",
                                  "subSteps": [
                                    "Defina regressão como prever valores contínuos (ex: preço, temperatura).",
                                    "Discuta regressão linear: y = mx + b, onde m e b são aprendidos dos dados.",
                                    "Examine um dataset simples: casas (área, quartos -> preço).",
                                    "Descreva treinamento: minimizar erro (MSE - Mean Squared Error).",
                                    "Teste com predição: dado nova casa, calcule preço estimado."
                                  ],
                                  "verification": "Crie um exemplo manual de regressão linear com 3 pontos de dados e calcule a predição para um novo ponto.",
                                  "estimatedTime": "1.5 horas",
                                  "materials": [
                                    "Planilha Excel ou Google Sheets para simular dados",
                                    "Tutorial Khan Academy sobre regressão linear",
                                    "Dataset exemplo: Boston Housing (via Kaggle)"
                                  ],
                                  "tips": "Visualize com gráficos de dispersão para entender a linha de melhor ajuste.",
                                  "learningObjective": "Identificar e exemplificar regressão como tarefa de ML supervisionado.",
                                  "commonMistakes": [
                                    "Confundir regressão com classificação (contínuo vs discreto)",
                                    "Esquecer de normalizar features",
                                    "Usar MSE incorretamente como métrica"
                                  ]
                                },
                                {
                                  "stepNumber": 3,
                                  "title": "Explorar Classificação no ML Supervisionado",
                                  "subSteps": [
                                    "Defina classificação como prever categorias discretas (ex: sim/não, gato/cão).",
                                    "Discuta regressão logística e árvores de decisão como algoritmos comuns.",
                                    "Examine dataset: emails (palavras-chave -> spam/não spam).",
                                    "Descreva treinamento: maximizar acurácia ou minimizar perda categórica.",
                                    "Avalie com matriz de confusão: TP, TN, FP, FN."
                                  ],
                                  "verification": "Monte uma matriz de confusão para um dataset de classificação simples com 10 amostras.",
                                  "estimatedTime": "1.5 horas",
                                  "materials": [
                                    "Python Jupyter Notebook com scikit-learn básico",
                                    "Vídeo Coursera sobre classificação",
                                    "Dataset Iris ou Titanic via Kaggle"
                                  ],
                                  "tips": "Comece com datasets pequenos para evitar sobrecarga computacional.",
                                  "learningObjective": "Diferenciar e exemplificar classificação em ML supervisionado.",
                                  "commonMistakes": [
                                    "Tratar classes desbalanceadas como balanceadas",
                                    "Confundir probabilidade com classe final",
                                    "Ignorar overfitting em árvores profundas"
                                  ]
                                },
                                {
                                  "stepNumber": 4,
                                  "title": "Diferenciar ML Supervisionado de Outras Tarefas e Tipos de ML",
                                  "subSteps": [
                                    "Compare com não-supervisionado: clustering (sem rótulos, ex: K-means).",
                                    "Discuta semi-supervisionado e reinforcement learning brevemente.",
                                    "Diferencie tarefas: regressão vs classificação vs séries temporais.",
                                    "Aborde desafios comuns: overfitting, underfitting, necessidade de dados rotulados.",
                                    "Resuma quando usar supervisionado: problemas com labels disponíveis."
                                  ],
                                  "verification": "Crie uma tabela comparando supervisionado, não-supervisionado e reinforcement learning com exemplos.",
                                  "estimatedTime": "1 hora",
                                  "materials": [
                                    "Tabela em Markdown ou Excel",
                                    "Infográfico comparativo de tipos de ML",
                                    "Leitura: 'Hands-On ML' capítulo 1"
                                  ],
                                  "tips": "Use mnemônicos: 'Supervisionado = professor corrigindo tarefas'.",
                                  "learningObjective": "Distinguir ML supervisionado de outras abordagens.",
                                  "commonMistakes": [
                                    "Achar que todo ML precisa de rótulos",
                                    "Confundir regressão com previsão de séries temporais",
                                    "Subestimar custo de rotulagem de dados"
                                  ]
                                }
                              ],
                              "practicalExample": "Em um dataset de casas (features: tamanho, localização; label: preço), treine uma regressão linear para prever preços. Para classificação, use emails (features: palavras como 'grátis'; label: spam/não spam) com regressão logística para filtrar spam.",
                              "finalVerifications": [
                                "Explique ML supervisionado em 3 frases.",
                                "Dê 2 exemplos de regressão e 2 de classificação.",
                                "Diferencie supervisionado de não-supervisionado.",
                                "Identifique features e labels em um dataset dado.",
                                "Descreva uma métrica de avaliação para cada tarefa.",
                                "Crie um fluxograma simples do processo de treinamento."
                              ],
                              "assessmentCriteria": [
                                "Precisão conceitual (definições corretas sem jargões errados).",
                                "Uso de exemplos relevantes e corretos.",
                                "Capacidade de diferenciação clara entre tarefas.",
                                "Compreensão de fluxos e componentes chave.",
                                "Aplicação prática em verificações.",
                                "Clareza na comunicação escrita/oral."
                              ],
                              "crossCurricularConnections": [
                                "Estatística: Modelos lineares e métricas de erro.",
                                "Programação: Implementação em Python (scikit-learn, pandas).",
                                "Matemática: Otimização e cálculo de gradientes.",
                                "Ciência de Dados: Preparação e limpeza de dados.",
                                "Ética: Viés em dados rotulados."
                              ],
                              "realWorldApplication": "Previsão de preços imobiliários (regressão em apps como Zillow), detecção de spam/fraude em bancos/emails (classificação), diagnóstico médico (classificar imagens de raio-X como câncer/não)."
                            },
                            "estimatedTime": "0.5 horas",
                            "difficulty": "beginner",
                            "status": "not_started",
                            "prerequisites": []
                          },
                          {
                            "id": "10.1.1.1.3.3",
                            "name": "Introduzir Aprendizado de Máquina Não-Supervisionado",
                            "description": "Descrever clustering, redução de dimensionalidade e detecção de anomalias, com exemplos como k-means e PCA.",
                            "atomicExpansion": {
                              "steps": [
                                {
                                  "stepNumber": 1,
                                  "title": "Compreender Conceitos Fundamentais de Aprendizado de Máquina Não-Supervisionado",
                                  "subSteps": [
                                    "Defina aprendizado de máquina supervisionado vs. não-supervisionado.",
                                    "Explique os principais tipos: clustering, redução de dimensionalidade e detecção de anomalias.",
                                    "Identifique cenários onde dados não rotulados são comuns (ex.: exploração de dados).",
                                    "Discuta vantagens e limitações do não-supervisionado.",
                                    "Revise pré-requisitos matemáticos como distância euclidiana e matrizes."
                                  ],
                                  "verification": "Resuma os conceitos em um diagrama mental ou mapa conceitual e explique para um par.",
                                  "estimatedTime": "2 horas",
                                  "materials": "Notebook, vídeo introdutório (ex.: Khan Academy ou Scikit-learn docs), papel para diagrama.",
                                  "tips": "Use analogias como 'agrupar frutas sem rótulos' para clustering.",
                                  "learningObjective": "Diferenciar e descrever os pilares do ML não-supervisionado.",
                                  "commonMistakes": "Confundir com supervisionado (foco em rótulos) ou ignorar pré-requisitos matemáticos."
                                },
                                {
                                  "stepNumber": 2,
                                  "title": "Explorar Clustering com K-Means",
                                  "subSteps": [
                                    "Entenda o algoritmo k-means: inicialização de centróides, atribuição e atualização.",
                                    "Implemente k-means manualmente em um dataset simples (ex.: Iris sem rótulos).",
                                    "Avalie clusters usando métricas como silhouette score e elbow method.",
                                    "Ajuste hiperparâmetros como k e visualize resultados.",
                                    "Compare com variações como k-means++."
                                  ],
                                  "verification": "Execute código e gere gráfico de clusters; interprete os resultados.",
                                  "estimatedTime": "3 horas",
                                  "materials": "Python/Jupyter, bibliotecas Scikit-learn, Pandas, Matplotlib; dataset Iris.",
                                  "tips": "Sempre normalize dados antes para evitar viés de escala.",
                                  "learningObjective": "Implementar e interpretar clustering k-means em dados reais.",
                                  "commonMistakes": "Escolher k errado sem elbow method ou não normalizar features."
                                },
                                {
                                  "stepNumber": 3,
                                  "title": "Dominar Redução de Dimensionalidade com PCA",
                                  "subSteps": [
                                    "Explique PCA: variância explicada, autovalores e autovetores.",
                                    "Aplique PCA em um dataset de alta dimensão (ex.: Wine).",
                                    "Visualize redução de 13D para 2D e analise perda de informação.",
                                    "Compare componentes principais com features originais.",
                                    "Experimente kernel PCA para dados não-lineares."
                                  ],
                                  "verification": "Gere scree plot e biplot; explique 80% variância em 2 componentes.",
                                  "estimatedTime": "2.5 horas",
                                  "materials": "Python/Jupyter, Scikit-learn, dataset Wine ou MNIST subset.",
                                  "tips": "Centralize dados (mean=0) antes de PCA para resultados precisos.",
                                  "learningObjective": "Aplicar PCA para simplificar dados mantendo informação chave.",
                                  "commonMistakes": "Ignorar escalonamento ou interpretar componentes sem contexto de dados."
                                },
                                {
                                  "stepNumber": 4,
                                  "title": "Introduzir Detecção de Anomalias",
                                  "subSteps": [
                                    "Descreva métodos: Isolation Forest, DBSCAN e One-Class SVM.",
                                    "Implemente Isolation Forest em dataset com anomalias (ex.: crédito fraude).",
                                    "Avalie com precisão/recall e visualize anomalias.",
                                    "Discuta thresholds e contaminação estimada.",
                                    "Integre com clustering/PCA para detecção híbrida."
                                  ],
                                  "verification": "Detecte anomalias em dataset teste e reporte taxa de detecção.",
                                  "estimatedTime": "2.5 horas",
                                  "materials": "Python/Jupyter, Scikit-learn, dataset Kaggle Credit Card Fraud.",
                                  "tips": "Use contaminação=0.1 para datasets desbalanceados.",
                                  "learningObjective": "Identificar e mitigar anomalias em dados não rotulados.",
                                  "commonMistakes": "Definir threshold fixo sem validação ou confundir outliers com anomalias."
                                },
                                {
                                  "stepNumber": 5,
                                  "title": "Integrar e Aplicar Conceitos em Projeto Prático",
                                  "subSteps": [
                                    "Escolha dataset real (ex.: customer segmentation).",
                                    "Aplique pipeline: PCA para redução, k-means para clusters, anomalias para outliers.",
                                    "Interprete resultados e gere relatório.",
                                    "Otimize e compare métricas.",
                                    "Documente insights e limitações."
                                  ],
                                  "verification": "Apresente relatório com visualizações e conclusões acionáveis.",
                                  "estimatedTime": "4 horas",
                                  "materials": "Jupyter, Scikit-learn, dataset Mall Customers ou similar.",
                                  "tips": "Combine técnicas para robustez; documente seed para reprodutibilidade.",
                                  "learningObjective": "Sintetizar técnicas não-supervisionadas em solução end-to-end.",
                                  "commonMistakes": "Pular interpretação ou não validar assunções de dados."
                                }
                              ],
                              "practicalExample": "Em marketing, use k-means para segmentar clientes por comportamento de compra (sem rótulos), PCA para visualizar padrões em 100 features de transações, e Isolation Forest para detectar fraudes em cartões de crédito.",
                              "finalVerifications": [
                                "Explicar diferenças entre clustering, PCA e detecção de anomalias.",
                                "Implementar k-means e PCA em dataset novo com código funcional.",
                                "Interpretar silhouette score >0.5 e variância explicada >80%.",
                                "Detectar anomalias com precisão >90% em dataset teste.",
                                "Criar pipeline integrado com relatório de insights.",
                                "Discutir limitações como sensibilidade a outliers no k-means."
                              ],
                              "assessmentCriteria": [
                                "Precisão conceitual (80%+ acerto em definições).",
                                "Qualidade de implementação (código limpo, sem erros).",
                                "Interpretação correta de métricas e visualizações.",
                                "Criatividade na aplicação prática e integração de técnicas.",
                                "Relatório claro com insights acionáveis.",
                                "Gestão de erros comuns e otimização de hiperparâmetros."
                              ],
                              "crossCurricularConnections": [
                                "Estatística: Distribuições, variância e testes de hipóteses.",
                                "Programação: Python, bibliotecas de ML e visualização de dados.",
                                "Matemática: Álgebra linear (autovalores) e cálculo de distâncias.",
                                "Negócios: Análise de dados para decisões estratégicas."
                              ],
                              "realWorldApplication": "Na detecção de fraudes bancárias, agrupa transações normais via k-means, reduz dimensões com PCA para monitoramento eficiente, e flagra anomalias com Isolation Forest, economizando milhões em perdas."
                            },
                            "estimatedTime": "0.5 horas",
                            "difficulty": "beginner",
                            "status": "not_started",
                            "prerequisites": []
                          }
                        ]
                      },
                      {
                        "id": "10.1.1.1.4",
                        "name": "Ética na Ciência de Dados",
                        "description": "Princípios éticos fundamentais no uso e manipulação de dados.",
                        "specificSkills": [
                          {
                            "id": "10.1.1.1.4.1",
                            "name": "Discutir princípios éticos no uso de dados",
                            "description": "Abordar privacidade, consentimento, viés algorítmico e transparência, com exemplos de dilemas éticos reais.",
                            "atomicExpansion": {
                              "steps": [
                                {
                                  "stepNumber": 1,
                                  "title": "Compreender privacidade e consentimento nos dados",
                                  "subSteps": [
                                    "Definir privacidade de dados conforme regulamentações como LGPD e GDPR.",
                                    "Explicar consentimento explícito e informado, diferenciando de consentimento implícito.",
                                    "Identificar cenários onde o consentimento é violado, como coleta não autorizada.",
                                    "Discutir anonimização e pseudonimização como técnicas de proteção.",
                                    "Analisar impactos da violação de privacidade em indivíduos e sociedade."
                                  ],
                                  "verification": "Criar um resumo de 1 página explicando privacidade e consentimento com exemplos.",
                                  "estimatedTime": "2 horas",
                                  "materials": "Leituras sobre LGPD/GDPR, vídeos introdutórios (ex: Khan Academy Ética em Dados).",
                                  "tips": "Use analogias cotidianas, como comparar dados a cartas pessoais.",
                                  "learningObjective": "Identificar e explicar os pilares de privacidade e consentimento ético.",
                                  "commonMistakes": "Confundir anonimização com exclusão total de dados; ignorar contextos culturais."
                                },
                                {
                                  "stepNumber": 2,
                                  "title": "Analisar viés algorítmico em sistemas de dados",
                                  "subSteps": [
                                    "Definir viés algorítmico e tipos (seleção, confirmação, histórico).",
                                    "Estudar casos como COMPAS (previsão de reincidência criminal).",
                                    "Examinar como dados enviesados perpetuam desigualdades sociais.",
                                    "Explorar técnicas de mitigação, como auditorias e conjuntos de dados diversificados.",
                                    "Simular um viés em um dataset simples usando Python ou Excel."
                                  ],
                                  "verification": "Produzir um relatório curto identificando viés em um exemplo dado.",
                                  "estimatedTime": "3 horas",
                                  "materials": "Artigos sobre COMPAS, ferramentas como Google Dataset Search, Jupyter Notebook básico.",
                                  "tips": "Sempre pergunte: 'Quem coletou os dados e por quê?'",
                                  "learningObjective": "Reconhecer origens e impactos do viés em algoritmos de dados.",
                                  "commonMistakes": "Atribuir viés apenas à IA, ignorando viés nos dados de entrada."
                                },
                                {
                                  "stepNumber": 3,
                                  "title": "Explorar transparência em processos de dados",
                                  "subSteps": [
                                    "Definir transparência como explicabilidade e rastreabilidade de decisões algorítmicas.",
                                    "Discutir o 'direito de explicação' em regulamentações.",
                                    "Analisar black-box models vs. modelos interpretáveis (ex: árvores de decisão).",
                                    "Estudar ferramentas como SHAP ou LIME para explicabilidade.",
                                    "Debater trade-offs entre transparência e propriedade intelectual."
                                  ],
                                  "verification": "Desenhar um fluxograma de um processo de dados transparente.",
                                  "estimatedTime": "2.5 horas",
                                  "materials": "Tutoriais SHAP/LIME, exemplos de modelos em Kaggle.",
                                  "tips": "Comece com modelos simples para entender transparência antes de complexos.",
                                  "learningObjective": "Avaliar a necessidade de transparência em aplicações de dados.",
                                  "commonMistakes": "Confundir transparência com precisão; superestimar interpretabilidade de deep learning."
                                },
                                {
                                  "stepNumber": 4,
                                  "title": "Discutir dilemas éticos reais com exemplos",
                                  "subSteps": [
                                    "Analisar caso Cambridge Analytica: uso indevido de dados do Facebook.",
                                    "Debater dilemas como targeting em anúncios vs. discriminação.",
                                    "Explorar trade-offs em saúde: privacidade vs. benefício público (ex: rastreamento COVID).",
                                    "Formular argumentos pró e contra em um dilema ético.",
                                    "Propor soluções éticas híbridas para dilemas identificados."
                                  ],
                                  "verification": "Participar de um debate simulado ou escrever um ensaio de 500 palavras.",
                                  "estimatedTime": "3 horas",
                                  "materials": "Casos de estudo (Cambridge Analytica, artigos da EFF), fóruns de discussão.",
                                  "tips": "Use o framework RICE (Responsibility, Impact, Consent, Equity) para estruturar debates.",
                                  "learningObjective": "Aplicar princípios éticos a cenários reais e propor resoluções.",
                                  "commonMistakes": "Focar apenas em casos famosos, ignorando dilemas cotidianos; polarizar sem nuance."
                                }
                              ],
                              "practicalExample": "Em um projeto de recomendação de Netflix, discutir como viés em dados de visualização pode excluir conteúdos minoritários, violando transparência e perpetuando desigualdades, propondo auditorias regulares.",
                              "finalVerifications": [
                                "Explicar com precisão privacidade, consentimento, viés e transparência.",
                                "Identificar viés em um dataset fornecido.",
                                "Propor soluções para um dilema ético real.",
                                "Criar um fluxograma transparente de processamento de dados.",
                                "Debater trade-offs éticos em um caso como Cambridge Analytica.",
                                "Listar 3 técnicas de mitigação de viés."
                              ],
                              "assessmentCriteria": [
                                "Clareza e precisão na definição de conceitos éticos (20%).",
                                "Profundidade na análise de exemplos reais (25%).",
                                "Capacidade de identificar e mitigar riscos éticos (20%).",
                                "Uso de evidências e referências adequadas (15%).",
                                "Criatividade em propostas de soluções (10%).",
                                "Estrutura lógica e coesão no discurso (10%)."
                              ],
                              "crossCurricularConnections": [
                                "Direito: Aplicação de LGPD/GDPR em cenários práticos.",
                                "Filosofia: Debates sobre utilitarismo vs. deontologia em ética de dados.",
                                "Sociologia: Impacto de vieses em desigualdades sociais.",
                                "Computação: Ferramentas de IA explicável como SHAP."
                              ],
                              "realWorldApplication": "Em empresas de tecnologia como Google ou bancos, profissionais usam esses princípios para auditar modelos de IA, garantir conformidade regulatória e construir confiança pública, evitando multas milionárias e danos reputacionais."
                            },
                            "estimatedTime": "0.5 horas",
                            "difficulty": "beginner",
                            "status": "not_started",
                            "prerequisites": []
                          },
                          {
                            "id": "10.1.1.1.4.2",
                            "name": "Analisar manipulação responsável de dados",
                            "description": "Explicar boas práticas para evitar manipulações intencionais ou não, garantindo integridade e equidade nos processos de dados.",
                            "atomicExpansion": {
                              "steps": [
                                {
                                  "stepNumber": 1,
                                  "title": "Compreender Conceitos Fundamentais de Manipulação de Dados",
                                  "subSteps": [
                                    "Definir manipulação de dados intencional (ex.: falsificação para ganho pessoal) e não intencional (ex.: erros de coleta).",
                                    "Explicar os princípios de integridade (dados precisos e não alterados) e equidade (ausência de viés discriminatório).",
                                    "Identificar impactos éticos, como perda de confiança pública ou decisões injustas.",
                                    "Mapear o ciclo de vida dos dados: coleta, processamento, análise e disseminação.",
                                    "Diferenciar manipulação de viés natural nos dados."
                                  ],
                                  "verification": "Escrever um resumo de 200 palavras definindo os conceitos e princípios, com exemplos simples.",
                                  "estimatedTime": "45 minutos",
                                  "materials": "Artigo 'Ética em Ciência de Dados' (ex.: do IBGE ou Kaggle), vídeo introdutório no YouTube sobre integridade de dados.",
                                  "tips": "Use analogias cotidianas, como 'dados como testemunhas em um tribunal'.",
                                  "learningObjective": "Dominar definições chave para basear análises futuras.",
                                  "commonMistakes": "Confundir manipulação com erros aleatórios sem intenção ética."
                                },
                                {
                                  "stepNumber": 2,
                                  "title": "Identificar Tipos e Exemplos de Manipulação de Dados",
                                  "subSteps": [
                                    "Listar tipos comuns: cherry-picking (seleção seletiva), p-hacking (manipulação estatística), imputação fraudulenta.",
                                    "Analisar exemplos reais: caso Enron (manipulação financeira) ou estudos clínicos falsificados.",
                                    "Classificar manipulações por estágio: pré-processamento, análise ou visualização.",
                                    "Reconhecer sinais de alerta: discrepâncias estatísticas ou fontes não confiáveis.",
                                    "Discutir manipulações não intencionais, como sampling bias em pesquisas."
                                  ],
                                  "verification": "Criar uma tabela com 5 tipos de manipulação, exemplos e impactos.",
                                  "estimatedTime": "60 minutos",
                                  "materials": "Casos de estudo (ex.: PDF do caso Cambridge Analytica), dataset público no Kaggle com anotações éticas.",
                                  "tips": "Busque notícias recentes para tornar os exemplos relevantes.",
                                  "learningObjective": "Reconhecer padrões de manipulação em datasets reais.",
                                  "commonMistakes": "Ignorar manipulações sutis como arredondamentos excessivos."
                                },
                                {
                                  "stepNumber": 3,
                                  "title": "Estudar Boas Práticas para Prevenção de Manipulações",
                                  "subSteps": [
                                    "Adotar protocolos de auditoria: versionamento de dados (Git) e logs de alterações.",
                                    "Implementar validações: checksums, testes estatísticos de consistência (ex.: Shapiro-Wilk).",
                                    "Promover transparência: documentação FAIR (Findable, Accessible, Interoperable, Reusable).",
                                    "Garantir equidade: ferramentas como AIF360 para detecção de bias.",
                                    "Treinar equipes em ética: checklists pré e pós-análise."
                                  ],
                                  "verification": "Desenvolver um checklist pessoal com 10 itens de boas práticas.",
                                  "estimatedTime": "50 minutos",
                                  "materials": "Guia FAIR (go-fair.org), biblioteca Python Pandas para validações, ferramenta AIF360.",
                                  "tips": "Integre automação com scripts simples para checks recorrentes.",
                                  "learningObjective": "Aplicar práticas preventivas em fluxos de trabalho.",
                                  "commonMistakes": "Subestimar a necessidade de auditoria externa independente."
                                },
                                {
                                  "stepNumber": 4,
                                  "title": "Aplicar Análise Responsável em um Cenário Prático",
                                  "subSteps": [
                                    "Selecionar um dataset público (ex.: saúde ou eleições).",
                                    "Auditar por manipulações: verificar outliers, distribuições e metadados.",
                                    "Aplicar boas práticas: limpar, validar e documentar mudanças.",
                                    "Avaliar equidade: testar por viés em subgrupos demográficos.",
                                    "Relatar achados em um relatório ético, propondo correções."
                                  ],
                                  "verification": "Produzir um relatório de 1 página com achados e recomendações.",
                                  "estimatedTime": "90 minutos",
                                  "materials": "Dataset Kaggle (ex.: Titanic com bias de gênero), Jupyter Notebook, bibliotecas Pandas e Fairlearn.",
                                  "tips": "Comece com datasets pequenos para prática rápida.",
                                  "learningObjective": "Executar análise ética end-to-end.",
                                  "commonMistakes": "Alterar dados sem documentar o 'porquê' exato."
                                }
                              ],
                              "practicalExample": "Em um dataset de admissões universitárias, identifique manipulação ao notar que notas de candidatos de minorias foram sistematicamente subestimadas por imputação seletiva; aplique validações para corrigir e garantir equidade na análise de mérito.",
                              "finalVerifications": [
                                "Lista e explica pelo menos 5 tipos de manipulação com exemplos.",
                                "Desenvolve um checklist completo de boas práticas para um processo de dados.",
                                "Analisa um dataset real identificando potenciais manipulações e propondo soluções.",
                                "Demonstra uso de ferramentas para detecção de bias e integridade.",
                                "Redige um relatório ético destacando impactos de equidade.",
                                "Compara práticas responsáveis com casos reais de falhas éticas."
                              ],
                              "assessmentCriteria": [
                                "Precisão e completude na identificação de manipulações (30%).",
                                "Profundidade das boas práticas propostas e sua aplicabilidade (25%).",
                                "Qualidade da análise prática com evidências concretas (20%).",
                                "Clareza e estrutura do relatório final (15%).",
                                "Integração de conceitos de integridade e equidade (10%)."
                              ],
                              "crossCurricularConnections": [
                                "Estatística: Testes de hipóteses e detecção de anomalias.",
                                "Ética Filosófica: Princípios de justiça e responsabilidade moral.",
                                "Direito Digital: Regulamentações como LGPD e GDPR.",
                                "Programação: Scripts para auditoria automatizada em Python/R."
                              ],
                              "realWorldApplication": "Em empresas de análise de dados (ex.: bancos detectando fraudes) ou jornalismo de dados (verificando eleições), garantindo decisões baseadas em dados íntegros, evitando escândalos como o da Cambridge Analytica e promovendo confiança pública."
                            },
                            "estimatedTime": "0.5 horas",
                            "difficulty": "beginner",
                            "status": "not_started",
                            "prerequisites": []
                          }
                        ]
                      }
                    ]
                  },
                  {
                    "id": "10.1.1.2",
                    "name": "Definições de Dados, Informação e Conhecimento",
                    "description": "Distinções conceituais entre dados, informação e conhecimento.",
                    "individualConcepts": [
                      {
                        "id": "10.1.1.2.1",
                        "name": "Dados",
                        "description": "Os dados representam elementos brutos e não processados, como números, símbolos, textos ou imagens, sem contexto ou interpretação, servindo como base fundamental para a ciência de dados.",
                        "specificSkills": [
                          {
                            "id": "10.1.1.2.1.1",
                            "name": "Definir o conceito de dados",
                            "description": "Explicar dados como fatos crus e observações sem significado inerente, incluindo tipos como numéricos, categóricos e textuais, com exemplos como '25' ou 'São Paulo'.",
                            "atomicExpansion": {
                              "steps": [
                                {
                                  "stepNumber": 1,
                                  "title": "Compreender a definição básica de dados",
                                  "subSteps": [
                                    "Leia a definição: dados são fatos crus e observações sem significado inerente ou interpretação.",
                                    "Analise exemplos simples como '25' (um número isolado) ou 'azul' (uma cor sem contexto).",
                                    "Discuta em voz alta ou anote por que esses fatos precisam de contexto para ganhar sentido.",
                                    "Compare com objetos cotidianos: dados são como ingredientes crus antes da receita.",
                                    "Registre sua própria definição em uma frase curta."
                                  ],
                                  "verification": "Escreva uma definição em suas palavras e compare com a original; deve coincidir em 90%.",
                                  "estimatedTime": "15 minutos",
                                  "materials": [
                                    "Folha de papel ou editor de texto",
                                    "Vídeo introdutório sobre ciência de dados (ex: Khan Academy)"
                                  ],
                                  "tips": "Imagine dados como blocos de Lego soltos: úteis, mas sem forma até montados.",
                                  "learningObjective": "Internalizar que dados são elementos brutos sem processamento ou contexto.",
                                  "commonMistakes": "Confundir dados com opiniões ou conclusões prontas, como '25 é alto'."
                                },
                                {
                                  "stepNumber": 2,
                                  "title": "Identificar os principais tipos de dados",
                                  "subSteps": [
                                    "Estude os tipos: numéricos (quantidades como 25, 3.14), categóricos (categorias como 'masculino', 'SP') e textuais (palavras como 'São Paulo').",
                                    "Classifique 10 exemplos aleatórios em uma tabela: números, categorias ou texto.",
                                    "Pesquise um exemplo real de cada tipo em uma base de dados pública (ex: IBGE).",
                                    "Crie um fluxograma simples para decidir o tipo de um dado dado.",
                                    "Explique diferenças: numéricos para cálculos, categóricos para grupos, textuais para descrições."
                                  ],
                                  "verification": "Classifique corretamente 10 exemplos mistos sem erros.",
                                  "estimatedTime": "20 minutos",
                                  "materials": [
                                    "Tabela impressa ou planilha Excel/Google Sheets",
                                    "Lista de exemplos pré-preparada"
                                  ],
                                  "tips": "Pergunte: 'Posso somar isso?' para numéricos; 'É uma palavra?' para textuais.",
                                  "learningObjective": "Classificar dados em numéricos, categóricos e textuais com precisão.",
                                  "commonMistakes": "Rotular tudo como 'texto'; ignorar que datas são numéricas ou categóricas."
                                },
                                {
                                  "stepNumber": 3,
                                  "title": "Analisar exemplos reais e contextos",
                                  "subSteps": [
                                    "Colete 5 dados de uma fonte real (ex: temperatura diária, nomes de cidades).",
                                    "Descreva cada um: tipo, por que é cru e sem significado sozinho.",
                                    "Adicione contexto fictício para transformar em informação (ex: '25°C em SP é quente').",
                                    "Registre em um diário: o que muda com o contexto?",
                                    "Debata com um parceiro: esses são dados ou já informação?"
                                  ],
                                  "verification": "Transforme 5 dados crus em informação contextualizada corretamente.",
                                  "estimatedTime": "25 minutos",
                                  "materials": [
                                    "Dados reais de sites como Weather.com ou IBGE",
                                    "Caderno para anotações"
                                  ],
                                  "tips": "Sempre isole o fato cru: remova adjetivos ou interpretações.",
                                  "learningObjective": "Aplicar a definição a exemplos reais, destacando ausência de significado inerente.",
                                  "commonMistakes": "Adicionar interpretação prematuramente, como chamar '25' de 'idade média'."
                                },
                                {
                                  "stepNumber": 4,
                                  "title": "Diferenciar dados de informação e conhecimento",
                                  "subSteps": [
                                    "Defina hierarquia: dados → informação (com contexto) → conhecimento (padrões interpretados).",
                                    "Crie uma pirâmide visual com exemplos em cada nível.",
                                    "Analise um caso: lista de vendas (dados) → total (informação) → estratégia (conhecimento).",
                                    "Teste-se: pegue uma frase e identifique o nível.",
                                    "Resuma as diferenças em bullet points."
                                  ],
                                  "verification": "Construa uma pirâmide correta e explique verbalmente.",
                                  "estimatedTime": "20 minutos",
                                  "materials": [
                                    "Papel para diagrama",
                                    "Infográfico da pirâmide DIK (Data-Information-Knowledge)"
                                  ],
                                  "tips": "Lembre: dados respondem 'o quê?', informação 'o quê e por quê?', conhecimento 'como usar'.",
                                  "learningObjective": "Posicionar dados na base da hierarquia de conhecimento.",
                                  "commonMistakes": "Equiparar dados a informação, ignorando a necessidade de processamento."
                                }
                              ],
                              "practicalExample": "Em uma planilha de vendas: 'Produto A', '10 unidades', 'R$50'. São dados crus (textual, numérico, numérico). Sem contexto, não sabemos se é bom ou ruim; com análise, vira informação.",
                              "finalVerifications": [
                                "Define dados como fatos crus sem significado inerente.",
                                "Classifica corretamente numéricos, categóricos e textuais.",
                                "Fornece exemplos precisos sem interpretação.",
                                "Diferencia dados de informação e conhecimento.",
                                "Identifica tipos em conjuntos reais de dados.",
                                "Explica a hierarquia DIK com exemplos."
                              ],
                              "assessmentCriteria": [
                                "Definição precisa e concisa (sem ambiguidades).",
                                "Classificação de tipos com 100% de acerto em testes.",
                                "Exemplos relevantes e variados.",
                                "Diferenciação clara entre dados, informação e conhecimento.",
                                "Uso correto de termos em explicações orais/escritas.",
                                "Aplicação prática em cenários reais."
                              ],
                              "crossCurricularConnections": [
                                "Estatística: coleta e classificação inicial de dados para análises.",
                                "Programação: declaração de variáveis por tipo (int, string, etc.).",
                                "Língua Portuguesa: análise de textos como dados qualitativos.",
                                "Ciências Sociais: dados demográficos em pesquisas populacionais."
                              ],
                              "realWorldApplication": "Em empresas de e-commerce, dados crus como 'ID cliente: 123, item: camisa, preço: R$30' são armazenados em bancos de dados para análises que geram relatórios de vendas e estratégias de marketing."
                            },
                            "estimatedTime": "0.5 horas",
                            "difficulty": "beginner",
                            "status": "not_started",
                            "prerequisites": []
                          },
                          {
                            "id": "10.1.1.2.1.2",
                            "name": "Identificar exemplos de dados brutos",
                            "description": "Reconhecer e listar exemplos de dados em contextos reais, como logs de sensores (ex: temperatura 30°C isolada) ou entradas de banco de dados sem processamento.",
                            "atomicExpansion": {
                              "steps": [
                                {
                                  "stepNumber": 1,
                                  "title": "Compreender a definição de dados brutos",
                                  "subSteps": [
                                    "Leia a definição: Dados brutos são observações ou medições não processadas, sem análise ou contexto agregado.",
                                    "Compare com dados processados: Dados brutos são isolados, como '30°C', enquanto processados são 'temperatura média de 28°C'.",
                                    "Anote 3 características principais: não interpretados, desestruturados ou semi-estruturados, coletados diretamente da fonte.",
                                    "Crie um fluxograma simples: Fonte → Dados Brutos → Processamento → Informação.",
                                    "Revise exemplos iniciais: log de sensor ('velocidade: 50 km/h') vs. relatório ('velocidade média: 45 km/h')."
                                  ],
                                  "verification": "Escreva uma definição em suas palavras e liste 3 exemplos corretos sem erros.",
                                  "estimatedTime": "20 minutos",
                                  "materials": "Papel e caneta ou editor de texto; acesso a um glossário de ciência de dados online.",
                                  "tips": "Use analogias cotidianas, como frutas colhidas (brutas) vs. suco (processado).",
                                  "learningObjective": "Definir precisamente dados brutos e diferenciá-los de formas processadas.",
                                  "commonMistakes": "Confundir dados brutos com informações resumidas ou agregar dados isolados prematuramente."
                                },
                                {
                                  "stepNumber": 2,
                                  "title": "Identificar características de dados brutos em contextos reais",
                                  "subSteps": [
                                    "Analise fontes comuns: sensores (temperatura, umidade), logs de apps (cliques, timestamps), entradas de DB (IDs crus).",
                                    "Liste atributos chave: falta de contexto, possível ruído, formato nativo da fonte.",
                                    "Classifique 5 amostras: marque como 'bruto' ou 'processado' (ex: 'usuário clicou' = bruto; 'taxa de cliques 2%' = processado).",
                                    "Discuta ruído: dados brutos podem ter erros, como leituras falhas de sensores.",
                                    "Pratique com variações: dados numéricos (42), textuais ('erro 404'), binários (0/1)."
                                  ],
                                  "verification": "Crie uma tabela com 5 itens reais classificados corretamente como brutos.",
                                  "estimatedTime": "25 minutos",
                                  "materials": "Amostras de logs online (ex: GitHub logs públicos); planilha simples.",
                                  "tips": "Pergunte: 'Isso veio direto da fonte sem cálculo?' Se sim, é bruto.",
                                  "learningObjective": "Reconhecer atributos diagnósticos de dados brutos em diversos formatos.",
                                  "commonMistakes": "Ignorar ruído ou contexto implícito, rotulando dados ligeiramente limpos como brutos."
                                },
                                {
                                  "stepNumber": 3,
                                  "title": "Localizar e coletar exemplos de dados brutos",
                                  "subSteps": [
                                    "Pesquise contextos reais: IoT (sensores Arduino), web (raw JSON APIs), bancos (dumps SQL).",
                                    "Colete 10 exemplos: copie logs de temperatura, acessos web, leituras de GPS.",
                                    "Documente a fonte: inclua URL ou descrição para cada exemplo.",
                                    "Valide autenticidade: confirme que não há agregação (ex: soma, média).",
                                    "Organize em categorias: numéricos, categóricos, temporais."
                                  ],
                                  "verification": "Apresente uma lista de 10 exemplos com fontes, todos validados como brutos.",
                                  "estimatedTime": "30 minutos",
                                  "materials": "Navegador web; sites como Kaggle datasets raw ou logs públicos.",
                                  "tips": "Use ferramentas como curl para puxar dados reais de APIs públicas.",
                                  "learningObjective": "Coletar exemplos autênticos de dados brutos de fontes primárias.",
                                  "commonMistakes": "Usar datasets já limpos/processados em vez de fontes raw."
                                },
                                {
                                  "stepNumber": 4,
                                  "title": "Listar e diferenciar dados brutos de processados",
                                  "subSteps": [
                                    "Compile uma lista final: 15 exemplos de dados brutos com descrições breves.",
                                    "Crie pares comparativos: bruto vs. processado para cada contexto.",
                                    "Explique transformações: como limpeza, agregação mudam dados brutos.",
                                    "Teste compreensão: rotule 10 itens mistos de um quiz auto-criado.",
                                    "Resuma aprendizados: escreva um parágrafo sobre importância dos dados brutos."
                                  ],
                                  "verification": "Gere uma lista categorizada e um quiz com 100% acerto.",
                                  "estimatedTime": "25 minutos",
                                  "materials": "Editor de texto ou Google Docs para listas e quizzes.",
                                  "tips": "Visualize com diagramas: setas de bruto → processado.",
                                  "learningObjective": "Listar exemplos com precisão e diferenciar contextualmente.",
                                  "commonMistakes": "Listar dados processados como brutos por falta de distinção clara."
                                }
                              ],
                              "practicalExample": "Analise um log de sensor IoT: ['2023-10-01 14:30:22, temp:30.5°C, hum:65%']. Identifique cada entrada como dado bruto (leituras isoladas sem média ou filtro), liste-as e explique por que não são processadas.",
                              "finalVerifications": [
                                "Lista pelo menos 10 exemplos de dados brutos de fontes reais sem erros.",
                                "Diferencia corretamente 8/10 pares bruto vs. processado.",
                                "Explica características chave (não processado, direto da fonte) em termos próprios.",
                                "Classifica amostras variadas (sensores, logs, DB) com 90% precisão.",
                                "Identifica ruído ou limitações em dados brutos exemplos.",
                                "Documenta fontes para todos os exemplos coletados."
                              ],
                              "assessmentCriteria": [
                                "Precisão na identificação (sem confusão com dados processados): 30%",
                                "Variedade de contextos e fontes (IoT, web, DB): 20%",
                                "Profundidade de explicações e diferenciações: 20%",
                                "Qualidade da lista e documentação: 15%",
                                "Compreensão de características e ruído: 15%"
                              ],
                              "crossCurricularConnections": [
                                "Programação: Manipulação de dados raw em Python (pandas.read_raw).",
                                "Estatística: Preparação inicial antes de análise descritiva.",
                                "Física/Engenharia: Leituras de sensores em experimentos.",
                                "TI: Logs de sistemas e monitoramento de redes."
                              ],
                              "realWorldApplication": "Em ciência de dados, identificar dados brutos é essencial para pipelines ETL (Extract, Transform, Load) em IoT, onde sensores geram terabytes de leituras isoladas diárias para monitoramento ambiental ou predictive maintenance em indústrias."
                            },
                            "estimatedTime": "0.5 horas",
                            "difficulty": "beginner",
                            "status": "not_started",
                            "prerequisites": []
                          },
                          {
                            "id": "10.1.1.2.1.3",
                            "name": "Explicar o papel dos dados na ciência de dados",
                            "description": "Descrever como os dados são a matéria-prima para coleta, armazenamento e análise na ciência de dados, destacando sua importância nas etapas iniciais do ciclo de vida.",
                            "atomicExpansion": {
                              "steps": [
                                {
                                  "stepNumber": 1,
                                  "title": "Definir dados como matéria-prima na ciência de dados",
                                  "subSteps": [
                                    "Pesquise definições básicas de dados, informação e conhecimento na hierarquia DIK.",
                                    "Identifique exemplos de dados brutos (ex.: números de vendas, temperaturas registradas).",
                                    "Compare dados com outros recursos em processos científicos, como ingredientes em uma receita.",
                                    "Explique por que dados são indispensáveis sem processamento posterior.",
                                    "Crie um diagrama simples mostrando dados como base da pirâmide DIK."
                                  ],
                                  "verification": "Escreva uma definição de 1 parágrafo e liste 3 exemplos de dados brutos corretos.",
                                  "estimatedTime": "30 minutos",
                                  "materials": [
                                    "Acesso à internet para pesquisa (Wikipedia, Khan Academy)",
                                    "Papel e caneta ou ferramenta de diagrama como Draw.io"
                                  ],
                                  "tips": "Use analogias cotidianas como 'leite cru antes de virar queijo' para fixar o conceito.",
                                  "learningObjective": "Compreender dados como o elemento fundamental e não processado na ciência de dados.",
                                  "commonMistakes": "Confundir dados com informação processada ou ignorar sua forma bruta e desestruturada."
                                },
                                {
                                  "stepNumber": 2,
                                  "title": "Mapear o ciclo de vida da ciência de dados e posicionar os dados",
                                  "subSteps": [
                                    "Estude o ciclo CRISP-DM ou ciclo padrão: coleta, preparação, modelagem, avaliação, deployment.",
                                    "Destaque as etapas iniciais: coleta e armazenamento como porta de entrada dos dados.",
                                    "Desenhe um fluxograma do ciclo, marcando onde os dados são introduzidos e transformados.",
                                    "Discuta impactos de dados ruins nas etapas subsequentes (efeito cascata).",
                                    "Anote 3 razões pelas quais etapas iniciais são críticas para o sucesso do projeto."
                                  ],
                                  "verification": "Produza um fluxograma rotulado corretamente e explique verbalmente o fluxo em 2 minutos.",
                                  "estimatedTime": "45 minutos",
                                  "materials": [
                                    "Vídeos tutoriais sobre CRISP-DM (YouTube ou Coursera)",
                                    "Ferramenta de fluxograma online (Lucidchart gratuito)"
                                  ],
                                  "tips": "Comece pelo final do ciclo e volte para mostrar dependência dos dados iniciais.",
                                  "learningObjective": "Visualizar o papel dos dados no contexto completo do ciclo de vida da ciência de dados.",
                                  "commonMistakes": "Omitir etapas de preparação ou superestimar o papel de modelagem sem dados sólidos."
                                },
                                {
                                  "stepNumber": 3,
                                  "title": "Explorar processos de coleta e armazenamento de dados",
                                  "subSteps": [
                                    "Liste métodos de coleta: sensores, APIs, formulários, bancos de dados existentes.",
                                    "Descreva formatos de armazenamento: CSV, SQL, NoSQL, cloud storage (ex.: AWS S3).",
                                    "Discuta desafios: volume, variedade, velocidade (3Vs do Big Data).",
                                    "Simule coleta de dados simples de uma fonte pública (ex.: dataset Kaggle).",
                                    "Avalie critérios de qualidade inicial: completude, precisão, atualidade."
                                  ],
                                  "verification": "Baixe um dataset pequeno e descreva seu método de coleta/armazenamento em um relatório curto.",
                                  "estimatedTime": "60 minutos",
                                  "materials": [
                                    "Datasets gratuitos (Kaggle.com)",
                                    "Editor de texto ou Excel para visualização inicial"
                                  ],
                                  "tips": "Priorize dados reais para entender variabilidade prática versus teoria.",
                                  "learningObjective": "Dominar como dados são coletados e armazenados como base para análise.",
                                  "commonMistakes": "Ignorar questões éticas de privacidade ou subestimar custos de armazenamento em escala."
                                },
                                {
                                  "stepNumber": 4,
                                  "title": "Analisar a importância dos dados nas etapas iniciais e síntese",
                                  "subSteps": [
                                    "Explique como dados ruins levam a 'garbage in, garbage out' (GIGO).",
                                    "Conecte dados iniciais à análise: limpeza, feature engineering, modelagem.",
                                    "Crie um resumo destacando 5 impactos de dados de qualidade nas decisões.",
                                    "Debata cenários: sucesso com bons dados vs. falha com dados pobres.",
                                    "Formule uma explicação oral ou escrita coesa sobre o papel integral dos dados."
                                  ],
                                  "verification": "Grave um vídeo de 3 minutos explicando o tópico ou escreva um ensaio de 300 palavras.",
                                  "estimatedTime": "45 minutos",
                                  "materials": [
                                    "Gravador de vídeo (celular)",
                                    "Modelos de ensaio ou estrutura de apresentação"
                                  ],
                                  "tips": "Use storytelling: 'Imagine construir uma casa sem alicerces sólidos'.",
                                  "learningObjective": "Sintetizar a criticidade dos dados nas fases iniciais para o sucesso global.",
                                  "commonMistakes": "Focar excessivamente em análise avançada sem enfatizar fundação nos dados."
                                }
                              ],
                              "practicalExample": "Em uma rede de varejo, dados de transações diárias (vendas por produto, hora, localização) são coletados via PDVs, armazenados em um banco SQL e analisados para prever demanda de estoque, evitando perdas por excesso ou falta de produtos.",
                              "finalVerifications": [
                                "Pode definir dados como matéria-prima e diferenciar de informação/conhecimento?",
                                "Desenha corretamente o ciclo de vida com ênfase nas etapas iniciais?",
                                "Lista pelo menos 3 métodos de coleta e 2 de armazenamento com exemplos?",
                                "Explica o princípio GIGO com um exemplo pessoal?",
                                "Sintetiza em 1 minuto o papel dos dados para um não-especialista?",
                                "Identifica 3 desafios dos 3Vs (volume, variedade, velocidade)?"
                              ],
                              "assessmentCriteria": [
                                "Precisão conceitual: definições e ciclo de vida corretos (30%)",
                                "Profundidade de detalhes: subpassos e exemplos acionáveis (25%)",
                                "Clareza na comunicação: linguagem acessível e estruturada (20%)",
                                "Aplicação prática: uso de exemplos reais e verificações (15%)",
                                "Completude: todos elementos do atomicExpansion abordados (10%)"
                              ],
                              "crossCurricularConnections": [
                                "Estatística: conceitos de amostragem e qualidade de dados",
                                "Programação: scripts para coleta (Python pandas) e armazenamento (SQL)",
                                "Ética e Sociedade: privacidade de dados (LGPD/GDPR)",
                                "Negócios: análise de dados para tomada de decisão estratégica"
                              ],
                              "realWorldApplication": "Na saúde, dados de pacientes coletados via wearables são armazenados em clouds e analisados para prever surtos de doenças, otimizando recursos hospitalares e salvando vidas em tempo real."
                            },
                            "estimatedTime": "0.5 horas",
                            "difficulty": "beginner",
                            "status": "not_started",
                            "prerequisites": []
                          }
                        ]
                      },
                      {
                        "id": "10.1.1.2.2",
                        "name": "Informação",
                        "description": "A informação surge do processamento e contextualização dos dados, transformando-os em padrões significativos e úteis dentro de um domínio específico.",
                        "specificSkills": [
                          {
                            "id": "10.1.1.2.2.1",
                            "name": "Definir o conceito de informação",
                            "description": "Explicar informação como dados organizados e interpretados, com contexto, como um relatório que relaciona '25°C às 14h em SP' indicando temperatura alta.",
                            "atomicExpansion": {
                              "steps": [
                                {
                                  "stepNumber": 1,
                                  "title": "Compreender o conceito de dados brutos",
                                  "subSteps": [
                                    "Defina dados como fatos crus, números, símbolos ou registros sem interpretação, como '25°C' ou 'João'.",
                                    "Identifique exemplos cotidianos de dados: temperatura isolada, nome de uma pessoa, hora em um relógio.",
                                    "Crie uma lista de pelo menos 5 dados brutos observados no ambiente escolar ou diário.",
                                    "Discuta em grupo por que esses dados sozinhos não transmitem significado completo.",
                                    "Registre a definição em um caderno de anotações."
                                  ],
                                  "verification": "Listar corretamente 5 exemplos de dados brutos e explicar que faltam interpretação.",
                                  "estimatedTime": "25 minutos",
                                  "materials": [
                                    "Caderno e caneta",
                                    "Quadro branco para exemplos coletivos"
                                  ],
                                  "tips": "Sempre pergunte: 'Isso tem significado sem mais detalhes?'",
                                  "learningObjective": "Reconhecer dados como elementos isolados sem contexto.",
                                  "commonMistakes": [
                                    "Interpretar dados prematuramente como informação.",
                                    "Confundir dados com opiniões pessoais."
                                  ]
                                },
                                {
                                  "stepNumber": 2,
                                  "title": "Diferenciar dados de informação",
                                  "subSteps": [
                                    "Explique informação como dados organizados e processados para ganhar significado.",
                                    "Compare: '25°C' (dado) vs. '25°C às 14h em SP no verão' (informação indicando calor).",
                                    "Crie uma tabela comparativa com 3 pares de dados vs. informação.",
                                    "Analise um exemplo dado pelo professor e identifique o que transforma dados em informação.",
                                    "Debata: 'Por que contexto é essencial?' em duplas."
                                  ],
                                  "verification": "Construir tabela comparativa precisa e verbalizar a diferença.",
                                  "estimatedTime": "30 minutos",
                                  "materials": [
                                    "Folha de papel para tabela",
                                    "Exemplos projetados ou impressos"
                                  ],
                                  "tips": "Foco no processamento: organização + interpretação = informação.",
                                  "learningObjective": "Distinguir dados brutos de informação contextualizada.",
                                  "commonMistakes": [
                                    "Ignorar a necessidade de organização.",
                                    "Achar que qualquer dado é informação."
                                  ]
                                },
                                {
                                  "stepNumber": 3,
                                  "title": "Explorar o papel do contexto na informação",
                                  "subSteps": [
                                    "Defina contexto como o ambiente, tempo ou relações que dão sentido aos dados.",
                                    "Examine casos: '25°C' em deserto (normal) vs. Polo (anormal).",
                                    "Monte cenários hipotéticos adicionando contexto a dados simples.",
                                    "Identifique fontes de contexto: quem, quando, onde, por quê.",
                                    "Escreva uma definição pessoal de 'informação com contexto'."
                                  ],
                                  "verification": "Criar 3 cenários onde contexto muda o significado dos mesmos dados.",
                                  "estimatedTime": "25 minutos",
                                  "materials": [
                                    "Cartões com dados para manipulação",
                                    "Marcadores"
                                  ],
                                  "tips": "Contexto responde: 'Isso importa para quê ou para quem?'",
                                  "learningObjective": "Entender como contexto transforma dados em informação útil.",
                                  "commonMistakes": [
                                    "Subestimar variações contextuais.",
                                    "Adicionar contexto irrelevante."
                                  ]
                                },
                                {
                                  "stepNumber": 4,
                                  "title": "Aplicar o conceito em um exemplo prático",
                                  "subSteps": [
                                    "Escolha um dado bruto e adicione contexto para formar informação.",
                                    "Construa um relatório simples: 'Dados: 25°C, 14h, SP → Informação: Temperatura alta para horário urbano'.",
                                    "Apresente para a turma e receba feedback.",
                                    "Reflita: 'O que aprendi sobre informação?' em um parágrafo.",
                                    "Registre o exemplo final no portfólio de aprendizado."
                                  ],
                                  "verification": "Produzir relatório coerente com dados → informação via contexto.",
                                  "estimatedTime": "20 minutos",
                                  "materials": [
                                    "Computador ou papel para relatório",
                                    "Modelos de relatório"
                                  ],
                                  "tips": "Mantenha simples: dados + contexto + interpretação breve.",
                                  "learningObjective": "Sintetizar o conceito criando informação a partir de dados.",
                                  "commonMistakes": [
                                    "Fazer relatórios longos demais.",
                                    "Esquecer de mostrar a transformação."
                                  ]
                                }
                              ],
                              "practicalExample": "Dado bruto: '25°C'. Com contexto: '25°C às 14h em São Paulo durante o verão indica temperatura alta para atividades ao ar livre, sugerindo precaução com insolação.' Isso transforma números isolados em informação acionável para decisões diárias.",
                              "finalVerifications": [
                                "Explicar verbalmente a diferença entre dados e informação com exemplo próprio.",
                                "Transformar 3 dados brutos em informação adicionando contexto.",
                                "Identificar contexto faltante em um exemplo dado.",
                                "Discutir por que informação é mais valiosa que dados brutos.",
                                "Criar um fluxograma: Dados → Contexto → Informação.",
                                "Responder quiz com 90% de acerto sobre conceitos."
                              ],
                              "assessmentCriteria": [
                                "Precisão na definição de informação (dados organizados + contexto).",
                                "Uso correto de exemplos com transformação clara.",
                                "Profundidade na explicação do papel do contexto.",
                                "Criatividade e relevância nos cenários criados.",
                                "Clareza na comunicação oral ou escrita.",
                                "Ausência de confusão entre dados, informação e conhecimento."
                              ],
                              "crossCurricularConnections": [
                                "Língua Portuguesa: Interpretação textual e contextualização de narrativas.",
                                "Ciências: Análise de medições experimentais com contexto ambiental.",
                                "Informática: Processamento de dados em bancos de dados e relatórios.",
                                "Geografia: Contextualização de dados climáticos regionais."
                              ],
                              "realWorldApplication": "Em ciência de dados, meteorologia ou negócios, profissionais transformam dados brutos de sensores (ex.: temperatura) em informações para previsões, alertas ou estratégias, como apps de tempo que avisam 'calor excessivo' baseado em contexto local."
                            },
                            "estimatedTime": "0.5 horas",
                            "difficulty": "beginner",
                            "status": "not_started",
                            "prerequisites": [
                              "10.1.1.2.1"
                            ]
                          },
                          {
                            "id": "10.1.1.2.2.2",
                            "name": "Diferenciar informação de dados",
                            "description": "Comparar dados (brutos, sem sentido) e informação (processados, com relevância), usando exemplos como uma lista de números vs. um gráfico de tendências.",
                            "atomicExpansion": {
                              "steps": [
                                {
                                  "stepNumber": 1,
                                  "title": "Compreender o conceito de Dados",
                                  "subSteps": [
                                    "Defina dados como fatos brutos e não processados, sem contexto ou interpretação.",
                                    "Identifique características dos dados: crus, desorganizados, sem significado inerente.",
                                    "Analise exemplos simples: uma lista de números de telefone ou temperaturas diárias sem análise.",
                                    "Discuta por que os dados sozinhos não respondem perguntas.",
                                    "Crie uma lista pessoal de 5 exemplos de dados do dia a dia."
                                  ],
                                  "verification": "Crie uma definição escrita de dados e liste 3 exemplos corretos sem contexto.",
                                  "estimatedTime": "20 minutos",
                                  "materials": [
                                    "Papel e caneta ou editor de texto",
                                    "Exemplos impressos de listas de dados (opcional)"
                                  ],
                                  "tips": "Sempre pergunte: 'Isso tem significado sem processamento?'",
                                  "learningObjective": "Definir dados e reconhecer suas características principais.",
                                  "commonMistakes": [
                                    "Confundir dados com informação já processada",
                                    "Achar que qualquer número é informação"
                                  ]
                                },
                                {
                                  "stepNumber": 2,
                                  "title": "Compreender o conceito de Informação",
                                  "subSteps": [
                                    "Defina informação como dados processados, organizados e com contexto que geram significado.",
                                    "Identifique o processo: coleta, processamento (análise, agregação) e interpretação.",
                                    "Analise exemplos: uma lista de temperaturas vira um gráfico de tendências climáticas.",
                                    "Explique o valor adicionado: responde a perguntas como 'Qual a tendência?'",
                                    "Crie uma lista de 5 exemplos onde dados viram informação."
                                  ],
                                  "verification": "Transforme um exemplo de dados em informação descrevendo o processamento.",
                                  "estimatedTime": "25 minutos",
                                  "materials": [
                                    "Papel e caneta",
                                    "Ferramenta simples como Excel ou Google Sheets para gráficos"
                                  ],
                                  "tips": "Pense no 'porquê' e 'como usar' para diferenciar.",
                                  "learningObjective": "Definir informação e entender o processamento necessário.",
                                  "commonMistakes": [
                                    "Ignorar o contexto ou processamento",
                                    "Chamar dados organizados sem análise de informação"
                                  ]
                                },
                                {
                                  "stepNumber": 3,
                                  "title": "Comparar Dados e Informação",
                                  "subSteps": [
                                    "Crie uma tabela comparativa: colunas para Dados vs. Informação (características, exemplos).",
                                    "Use o exemplo dado: lista de números (dados) vs. gráfico de tendências (informação).",
                                    "Discuta diferenças chave: bruto vs. processado, sem sentido vs. relevante.",
                                    "Aplique a comparação em um novo exemplo: vendas diárias vs. relatório mensal.",
                                    "Debata em duplas: 'Quando dados se tornam informação?'"
                                  ],
                                  "verification": "Apresente tabela comparativa com pelo menos 4 diferenças e 2 exemplos.",
                                  "estimatedTime": "30 minutos",
                                  "materials": [
                                    "Tabela em papel ou planilha digital",
                                    "Exemplos de gráficos prontos"
                                  ],
                                  "tips": "Use setas para mostrar transformação: Dados → Processamento → Informação.",
                                  "learningObjective": "Diferenciar claramente dados de informação por meio de comparação.",
                                  "commonMistakes": [
                                    "Misturar conceitos invertendo exemplos",
                                    "Subestimar o papel do processamento"
                                  ]
                                },
                                {
                                  "stepNumber": 4,
                                  "title": "Aplicar a Diferenciação em um Caso Prático",
                                  "subSteps": [
                                    "Escolha um dataset real (ex: notas de alunos).",
                                    "Identifique partes como dados (lista bruta) e transforme em informação (média, gráfico).",
                                    "Explique o que cada elemento revela.",
                                    "Avalie: 'Isso responde a uma pergunta útil?'",
                                    "Registre o antes/depois em um relatório curto."
                                  ],
                                  "verification": "Produza um relatório com dados crus, processamento e informação resultante.",
                                  "estimatedTime": "35 minutos",
                                  "materials": [
                                    "Dataset simples (ex: CSV de notas)",
                                    "Ferramenta de gráficos como Google Sheets"
                                  ],
                                  "tips": "Comece pequeno para evitar sobrecarga.",
                                  "learningObjective": "Aplicar conceitos para transformar dados em informação.",
                                  "commonMistakes": [
                                    "Pular o processamento",
                                    "Não justificar a relevância da informação"
                                  ]
                                },
                                {
                                  "stepNumber": 5,
                                  "title": "Reforçar e Verificar o Aprendizado",
                                  "subSteps": [
                                    "Resolva quiz: classifique 10 itens como dados ou informação.",
                                    "Crie um fluxograma: Dados → Informação.",
                                    "Discuta aplicações reais em grupo.",
                                    "Autoavalie compreensão com critérios dados.",
                                    "Compartilhe um insight pessoal."
                                  ],
                                  "verification": "Acertar 90% no quiz e completar fluxograma coerente.",
                                  "estimatedTime": "20 minutos",
                                  "materials": [
                                    "Quiz impresso ou digital",
                                    "Papel para fluxograma"
                                  ],
                                  "tips": "Revise erros comuns das steps anteriores.",
                                  "learningObjective": "Consolidar diferenciação com prática e autoavaliação.",
                                  "commonMistakes": [
                                    "Respostas superficiais no quiz",
                                    "Fluxogramas sem setas de transformação"
                                  ]
                                }
                              ],
                              "practicalExample": "Dados: Lista de temperaturas diárias [25, 28, 22, 30]. Informação: Gráfico mostrando tendência de aquecimento (média 26.25°C, pico em 30°C), respondendo 'O clima está esquentando?'",
                              "finalVerifications": [
                                "Define corretamente dados como brutos e informação como processada.",
                                "Classifica exemplos com 100% de acerto.",
                                "Explica transformação com exemplos próprios.",
                                "Cria tabela ou fluxograma comparativo preciso.",
                                "Identifica relevância em aplicações reais."
                              ],
                              "assessmentCriteria": [
                                "Precisão nas definições (clareza e completude).",
                                "Qualidade dos exemplos e transformações.",
                                "Profundidade na comparação (diferenças chave).",
                                "Criatividade na aplicação prática.",
                                "Capacidade de verificação autônoma."
                              ],
                              "crossCurricularConnections": [
                                "Ciências: Análise de dados experimentais em biologia.",
                                "Língua Portuguesa: Interpretação de textos informativos vs. listas.",
                                "História: Dados cronológicos vs. narrativas históricas.",
                                "Negócios: Vendas brutas vs. relatórios de mercado."
                              ],
                              "realWorldApplication": "Em empresas de análise de dados, diferenciar permite criar dashboards úteis (ex: Google Analytics transforma cliques em insights de comportamento do usuário)."
                            },
                            "estimatedTime": "0.5 horas",
                            "difficulty": "beginner",
                            "status": "not_started",
                            "prerequisites": [
                              "10.1.1.2.1"
                            ]
                          },
                          {
                            "id": "10.1.1.2.2.3",
                            "name": "Illustrar a transformação de dados em informação",
                            "description": "Demonstrar processos como agregação, filtragem ou visualização que convertem dados em informação, com casos como soma de vendas por região.",
                            "atomicExpansion": {
                              "steps": [
                                {
                                  "stepNumber": 1,
                                  "title": "Compreender os conceitos fundamentais de dados e informação",
                                  "subSteps": [
                                    "Defina dados como fatos brutos e não processados, como números isolados de vendas diárias.",
                                    "Defina informação como dados processados que geram significado, como totais agregados por região.",
                                    "Identifique exemplos cotidianos: lista de temperaturas (dados) vs. média semanal (informação).",
                                    "Discuta o pipeline: dados → processamento → informação.",
                                    "Crie um diagrama simples ilustrando a transformação."
                                  ],
                                  "verification": "Crie um mapa conceitual distinguindo dados de informação com pelo menos 3 exemplos cada.",
                                  "estimatedTime": "20 minutos",
                                  "materials": [
                                    "Papel e caneta ou ferramenta digital como Draw.io",
                                    "Vídeo introdutório sobre Ciência de Dados (5 min)"
                                  ],
                                  "tips": [
                                    "Use analogias como 'ingredientes crus vs. prato cozido' para fixar conceitos.",
                                    "Anote diferenças em uma tabela para revisão rápida."
                                  ],
                                  "learningObjective": "Diferenciar dados brutos de informação processada e reconhecer o valor da transformação.",
                                  "commonMistakes": [
                                    "Confundir dados com informação sem processamento.",
                                    "Ignorar contexto na definição de informação."
                                  ]
                                },
                                {
                                  "stepNumber": 2,
                                  "title": "Explorar técnicas de agregação de dados",
                                  "subSteps": [
                                    "Colete um conjunto de dados brutos, como vendas diárias: [{'regiao': 'Sul', 'vendas': 100}, {'regiao': 'Sul', 'vendas': 150}, ...].",
                                    "Aplique agregação por soma: some vendas por região usando fórmula ou ferramenta simples.",
                                    "Experimente outras agregações: média, contagem, máximo/mínimo.",
                                    "Registre os resultados em uma tabela: ex. Sul: 500 vendas totais.",
                                    "Compare dados originais com agregados para visualizar a transformação."
                                  ],
                                  "verification": "Produza uma tabela de agregação correta a partir de 10 registros de dados fornecidos.",
                                  "estimatedTime": "30 minutos",
                                  "materials": [
                                    "Planilha Excel ou Google Sheets",
                                    "Conjunto de dados de exemplo com 20 linhas de vendas"
                                  ],
                                  "tips": [
                                    "Comece com somas simples antes de médias para construir confiança.",
                                    "Use funções como SUMIF no Excel para automação."
                                  ],
                                  "learningObjective": "Executar agregação de dados para converter volumes brutos em resumos significativos.",
                                  "commonMistakes": [
                                    "Erros de agrupamento por região (ex: case-sensitive).",
                                    "Esquecer de incluir todos os registros."
                                  ]
                                },
                                {
                                  "stepNumber": 3,
                                  "title": "Aplicar filtragem e seleção de dados relevantes",
                                  "subSteps": [
                                    "Filtre dados irrelevantes: remova vendas abaixo de um threshold ou regiões específicas.",
                                    "Combine com agregação: filtre vendas >100 e some por região.",
                                    "Explique como filtragem reduz ruído, transformando dados em informação focada.",
                                    "Crie antes/depois: liste dados filtrados vs. originais.",
                                    "Teste cenários: filtre por data ou valor para diferentes insights."
                                  ],
                                  "verification": "Gere um relatório filtrado mostrando apenas regiões com vendas > média geral.",
                                  "estimatedTime": "25 minutos",
                                  "materials": [
                                    "Mesma planilha do Step 2",
                                    "Dados expandidos com filtros de exemplo"
                                  ],
                                  "tips": [
                                    "Defina critérios de filtro claros antes de aplicar.",
                                    "Salve versões filtradas para comparação visual."
                                  ],
                                  "learningObjective": "Usar filtragem para refinar dados, preparando-os para análise informativa.",
                                  "commonMistakes": [
                                    "Filtrar excessivamente, perdendo dados úteis.",
                                    "Aplicar filtro incorreto (ex: texto em vez de número)."
                                  ]
                                },
                                {
                                  "stepNumber": 4,
                                  "title": "Visualizar a transformação através de gráficos",
                                  "subSteps": [
                                    "Escolha tipo de visual: barra para somas por região.",
                                    "Crie gráfico no Excel: eixo X regiões, Y vendas agregadas.",
                                    "Adicione rótulos e títulos explicando 'Dados → Informação'.",
                                    "Compare gráfico de dados brutos (disperso) vs. agregados (claro).",
                                    "Interprete: 'Gráfico mostra Sul liderando com 40% das vendas'."
                                  ],
                                  "verification": "Produza um gráfico final com legenda explicando a transformação realizada.",
                                  "estimatedTime": "35 minutos",
                                  "materials": [
                                    "Excel/Google Sheets com gráficos",
                                    "Dados processados dos steps anteriores"
                                  ],
                                  "tips": [
                                    "Mantenha visual simples: evite sobrecarga de cores.",
                                    "Use tooltips para detalhes extras."
                                  ],
                                  "learningObjective": "Representar visualmente dados transformados em informação acionável.",
                                  "commonMistakes": [
                                    "Escalas erradas no gráfico distorcendo insights.",
                                    "Omitir rótulos, tornando visual ambíguo."
                                  ]
                                }
                              ],
                              "practicalExample": "Dados brutos: Vendas diárias - Sul:100, Norte:80, Sul:150, Norte:120. Agregação: Sul=250, Norte=200. Filtragem: Apenas >100. Visualização: Gráfico de barras mostrando Sul dominando, revelando estratégia regional.",
                              "finalVerifications": [
                                "Explica verbalmente ou por escrito a diferença entre dados e informação com exemplos.",
                                "Executa agregação e filtragem corretamente em um novo dataset de 15 itens.",
                                "Cria visualização clara que destaca insights da transformação.",
                                "Identifica pelo menos 2 aplicações reais da transformação ilustrada.",
                                "Responde quiz com 90% acerto sobre conceitos e processos."
                              ],
                              "assessmentCriteria": [
                                "Precisão na agregação e filtragem (sem erros matemáticos).",
                                "Clareza e correção conceitual na distinção dados/informação.",
                                "Qualidade da visualização (legível, informativa, sem distorções).",
                                "Profundidade dos sub-passos executados e documentados.",
                                "Capacidade de interpretação: extrai insights válidos dos resultados.",
                                "Criatividade em exemplos e conexões reais."
                              ],
                              "crossCurricularConnections": [
                                "Estatística: Uso de medidas resumo como média e soma.",
                                "Programação: Implementação em Python com Pandas (groupby, filter).",
                                "Gestão de Negócios: Análise de performance para decisões estratégicas.",
                                "Design Gráfico: Princípios de visualização de dados (Tufte).",
                                "Ciências Sociais: Transformação de dados censitários em políticas públicas."
                              ],
                              "realWorldApplication": "Em relatórios empresariais, agregando vendas por região para decisões de estoque; em saúde pública, filtrando casos de doença por idade para campanhas de vacinação direcionadas."
                            },
                            "estimatedTime": "1 hora",
                            "difficulty": "intermediate",
                            "status": "not_started",
                            "prerequisites": [
                              "10.1.1.2.1"
                            ]
                          }
                        ]
                      },
                      {
                        "id": "10.1.1.2.3",
                        "name": "Conhecimento",
                        "description": "O conhecimento é a assimilação e aplicação da informação, permitindo compreensão profunda, inferências e decisões estratégicas.",
                        "specificSkills": [
                          {
                            "id": "10.1.1.2.3.1",
                            "name": "Definir o conceito de conhecimento",
                            "description": "Descrever conhecimento como informação internalizada e acionável, como usar dados de vendas processados para prever demandas futuras.",
                            "atomicExpansion": {
                              "steps": [
                                {
                                  "stepNumber": 1,
                                  "title": "Revisar conceitos fundamentais: Dados e Informação",
                                  "subSteps": [
                                    "Ler definições de dados (fatos brutos) e informação (dados processados com contexto).",
                                    "Identificar exemplos simples: dados = números de vendas; informação = gráfico de vendas mensais.",
                                    "Comparar diferenças em uma tabela mental ou escrita.",
                                    "Discutir por que dados sozinhos não geram ações.",
                                    "Anotar 3 exemplos pessoais de dados virando informação."
                                  ],
                                  "verification": "Criar uma tabela comparativa com pelo menos 3 exemplos de dados e informação.",
                                  "estimatedTime": "20-30 minutos",
                                  "materials": [
                                    "Notebook ou papel",
                                    "Texto introdutório sobre Ciência de Dados (PDF ou site como Wikipedia)"
                                  ],
                                  "tips": "Use analogias cotidianas, como receitas de cozinha, para fixar os conceitos.",
                                  "learningObjective": "Diferenciar dados e informação como base para entender conhecimento.",
                                  "commonMistakes": "Confundir informação com conhecimento; lembrar que informação ainda é externa e não acionável."
                                },
                                {
                                  "stepNumber": 2,
                                  "title": "Definir o conceito de conhecimento",
                                  "subSteps": [
                                    "Ler definição: Conhecimento é informação internalizada e acionável pelo indivíduo.",
                                    "Destacar palavras-chave: internalizada (absorvida na mente) e acionável (pode guiar decisões).",
                                    "Escrever a definição em próprias palavras.",
                                    "Pesquisar citações clássicas (ex: de Polanyi ou Nonaka).",
                                    "Relacionar com pirâmide DIKW (Dados-Informação-Conhecimento-Sabedoria)."
                                  ],
                                  "verification": "Escrever uma definição de conhecimento em 1-2 frases que inclua 'internalizada' e 'acionável'.",
                                  "estimatedTime": "15-25 minutos",
                                  "materials": [
                                    "Dicionário online de termos de Ciência de Dados",
                                    "Vídeo curto sobre pirâmide DIKW (YouTube, 5 min)"
                                  ],
                                  "tips": "Pense em conhecimento como 'saber remar' vs. 'ler sobre remar'.",
                                  "learningObjective": "Formular uma definição precisa e pessoal de conhecimento.",
                                  "commonMistakes": "Definir conhecimento apenas como 'muita informação'; enfatize a internalização."
                                },
                                {
                                  "stepNumber": 3,
                                  "title": "Caracterizar propriedades do conhecimento",
                                  "subSteps": [
                                    "Listar propriedades: internalizado (na memória/experiência), acionável (gera decisões), contextual (depende do usuário).",
                                    "Analisar exemplos: informação = relatório de vendas; conhecimento = usar relatório para prever demanda.",
                                    "Diferenciar conhecimento tácito (intuitivo) vs. explícito (codificado).",
                                    "Mapear em um fluxograma: informação → internalização → conhecimento.",
                                    "Identificar barreiras à internalização (ex: falta de experiência)."
                                  ],
                                  "verification": "Criar um fluxograma simples mostrando transformação de informação em conhecimento.",
                                  "estimatedTime": "25-35 minutos",
                                  "materials": [
                                    "Ferramenta de desenho como Draw.io ou papel",
                                    "Artigo sobre conhecimento tácito (busca Google)"
                                  ],
                                  "tips": "Use cores no fluxograma para destacar transformações.",
                                  "learningObjective": "Identificar e explicar as principais características do conhecimento.",
                                  "commonMistakes": "Ignorar o aspecto acionável; teste se a definição leva a uma ação concreta."
                                },
                                {
                                  "stepNumber": 4,
                                  "title": "Aplicar o conceito em um exemplo prático",
                                  "subSteps": [
                                    "Escolher cenário: dados de vendas processados em relatório (informação).",
                                    "Simular internalização: ler relatório e conectar com experiência passada.",
                                    "Gerar ação: prever demanda futura e sugerir aumento de estoque.",
                                    "Avaliar: isso é acionável? Sim, guia compra de produtos.",
                                    "Refletir: como isso difere de apenas ler o relatório."
                                  ],
                                  "verification": "Descrever em parágrafo como o exemplo transforma informação em conhecimento acionável.",
                                  "estimatedTime": "20-30 minutos",
                                  "materials": [
                                    "Dados fictícios de vendas (planilha Excel simples)",
                                    "Calculadora ou Google Sheets"
                                  ],
                                  "tips": "Torne o exemplo pessoal, usando vendas de um produto que você conhece.",
                                  "learningObjective": "Demonstrar aplicação prática do conceito de conhecimento.",
                                  "commonMistakes": "Parar na informação; sempre vincule a uma decisão ou ação."
                                }
                              ],
                              "practicalExample": "Em uma loja de varejo, dados brutos de vendas diárias (ex: 50 unidades de camisetas) são processados em um relatório de tendências mensais (informação: 'vendas de camisetas aumentaram 20% no verão'). O gerente internaliza isso com sua experiência de estações passadas, gerando conhecimento acionável: 'aumentar estoque em 30% para próxima estação para atender demanda prevista', evitando rupturas.",
                              "finalVerifications": [
                                "Explicar verbalmente a diferença entre informação e conhecimento com exemplo.",
                                "Identificar em um texto real se algo é informação ou conhecimento.",
                                "Criar um exemplo próprio de conhecimento acionável a partir de dados.",
                                "Diferenciar conhecimento tácito de explícito.",
                                "Mapear um processo de negócios usando a pirâmide DIKW.",
                                "Avaliar se uma decisão pessoal foi baseada em conhecimento."
                              ],
                              "assessmentCriteria": [
                                "Precisão na definição (inclui internalização e acionabilidade).",
                                "Uso correto de exemplos concretos e relevantes.",
                                "Clareza na distinção entre dados, informação e conhecimento.",
                                "Profundidade nas características (tácito/explícito, contextual).",
                                "Capacidade de aplicar em cenários reais.",
                                "Ausência de confusões comuns (ex: equiparar informação a conhecimento)."
                              ],
                              "crossCurricularConnections": [
                                "Filosofia: Epistemologia e teoria do conhecimento (Platão, Kant).",
                                "Gestão de Negócios: Tomada de decisões baseada em conhecimento organizacional.",
                                "Inteligência Artificial: Knowledge Graphs e representação de conhecimento em ML.",
                                "Psicologia: Aprendizagem e internalização cognitiva.",
                                "Estatística: Transformação de dados em insights acionáveis."
                              ],
                              "realWorldApplication": "Na Ciência de Dados, profissionais usam conhecimento para transformar relatórios analíticos (informação) em estratégias preditivas, como em empresas de e-commerce que ajustam estoques dinamicamente com base em padrões de vendas internalizados, otimizando lucros e reduzindo desperdícios."
                            },
                            "estimatedTime": "0.5 horas",
                            "difficulty": "beginner",
                            "status": "not_started",
                            "prerequisites": [
                              "10.1.1.2.1",
                              "10.1.1.2.2"
                            ]
                          },
                          {
                            "id": "10.1.1.2.3.2",
                            "name": "Diferenciar conhecimento de informação",
                            "description": "Contrastar informação (factual, estática) com conhecimento (dinâmico, aplicado), exemplificando com análise de mercado levando a estratégias de negócio.",
                            "atomicExpansion": {
                              "steps": [
                                {
                                  "stepNumber": 1,
                                  "title": "Definir e Exemplificar Informação",
                                  "subSteps": [
                                    "Leia definições padrão: Informação é factual, estática e descritiva, como dados brutos processados.",
                                    "Identifique exemplos: Números de vendas mensais, estatísticas demográficas ou relatórios de mercado.",
                                    "Classifique exemplos: Separe fatos isolados de interpretações.",
                                    "Registre 5 exemplos de informação em um caderno.",
                                    "Compare com dados crus para reforçar a distinção."
                                  ],
                                  "verification": "Liste 5 exemplos claros de informação sem menção a ações ou contextos aplicados.",
                                  "estimatedTime": "30 minutos",
                                  "materials": [
                                    "Caderno ou documento digital",
                                    "Artigos online sobre hierarquia DIKW (Dados, Informação, Conhecimento, Sabedoria)"
                                  ],
                                  "tips": "Use fontes confiáveis como Wikipedia ou livros de ciência de dados para definições precisas.",
                                  "learningObjective": "Compreender informação como elemento estático e factual.",
                                  "commonMistakes": "Confundir informação com opinião ou interpretação pessoal."
                                },
                                {
                                  "stepNumber": 2,
                                  "title": "Definir e Exemplificar Conhecimento",
                                  "subSteps": [
                                    "Leia definições: Conhecimento é dinâmico, aplicado e contextualizado a partir da informação.",
                                    "Identifique exemplos: Usar dados de vendas para prever tendências sazonais.",
                                    "Analise como informação se transforma em conhecimento via padrões e experiência.",
                                    "Crie 5 exemplos pessoais de conhecimento derivado de informação.",
                                    "Desenhe um diagrama simples mostrando a transformação."
                                  ],
                                  "verification": "Forneça exemplos onde informação leva a insights acionáveis.",
                                  "estimatedTime": "45 minutos",
                                  "materials": [
                                    "Ferramentas de desenho como Draw.io ou papel",
                                    "Vídeos curtos sobre DIKW no YouTube"
                                  ],
                                  "tips": "Pense em 'por quê' e 'como usar' para diferenciar de mera descrição.",
                                  "learningObjective": "Reconhecer conhecimento como aplicação dinâmica de informação.",
                                  "commonMistakes": "Tratar conhecimento como mera acumulação de fatos sem contexto."
                                },
                                {
                                  "stepNumber": 3,
                                  "title": "Contrastar Informação e Conhecimento",
                                  "subSteps": [
                                    "Crie uma tabela comparativa: Colunas para características (estática vs dinâmica, factual vs aplicada).",
                                    "Discuta diferenças chave: Estática vs mutável, isolada vs contextual.",
                                    "Use setas para mostrar fluxo: Informação → Conhecimento.",
                                    "Debata com um parceiro ou grave áudio explicando 3 contrastes.",
                                    "Revise tabela adicionando exemplos negativos (erros comuns)."
                                  ],
                                  "verification": "Explique verbalmente ou por escrito 3 diferenças principais com exemplos.",
                                  "estimatedTime": "40 minutos",
                                  "materials": [
                                    "Planilha Google Sheets ou Excel para tabela",
                                    "Gravador de voz no celular"
                                  ],
                                  "tips": "Foque em verbos: Informação 'descreve', conhecimento 'age'.",
                                  "learningObjective": "Dominar contrastes fundamentais entre os conceitos.",
                                  "commonMistakes": "Ignorar o aspecto aplicado do conhecimento."
                                },
                                {
                                  "stepNumber": 4,
                                  "title": "Aplicar em Análise de Mercado para Estratégias de Negócio",
                                  "subSteps": [
                                    "Colete dados de exemplo: Vendas de sorvete por estação (informação).",
                                    "Transforme em conhecimento: Correlação com temperatura leva a estoque sazonal.",
                                    "Desenvolva estratégia: Planejar compras baseadas em padrões.",
                                    "Simule cenário: Crie relatório curto com recomendação de negócio.",
                                    "Avalie impacto: Preveja resultados da estratégia."
                                  ],
                                  "verification": "Produza um relatório de 1 página ligando info a estratégia.",
                                  "estimatedTime": "60 minutos",
                                  "materials": [
                                    "Dados fictícios ou reais de mercado (ex: Kaggle datasets)",
                                    "Editor de texto"
                                  ],
                                  "tips": "Sempre pergunte: 'Isso leva a uma ação?' para validar conhecimento.",
                                  "learningObjective": "Aplicar distinção em contexto real de ciência de dados.",
                                  "commonMistakes": "Parar na informação sem propor aplicação prática."
                                }
                              ],
                              "practicalExample": "Informação: 'Vendas de smartphones aumentaram 20% no último trimestre em região X.' Conhecimento: 'Baseado nisso e tendências econômicas, a empresa deve investir em marketing digital direcionado a jovens em X, prevendo 15% de crescimento adicional.'",
                              "finalVerifications": [
                                "Explique a diferença sem consultar notas.",
                                "Classifique 10 exemplos mistos como info ou conhecimento corretamente.",
                                "Crie um exemplo próprio de transformação info → conhecimento.",
                                "Debata com colega: convença sobre um caso controverso.",
                                "Aplique em novo dataset de mercado.",
                                "Autoavalie compreensão em escala 1-10 com justificativa."
                              ],
                              "assessmentCriteria": [
                                "Precisão nas definições (80% match com padrões DIKW).",
                                "Qualidade dos contrastes (clareza e profundidade).",
                                "Relevância e originalidade de exemplos.",
                                "Capacidade de aplicação prática em cenários reais.",
                                "Identificação correta de erros comuns.",
                                "Completude do diagrama ou tabela comparativa."
                              ],
                              "crossCurricularConnections": [
                                "Filosofia: Epistemologia e teoria do conhecimento.",
                                "Negócios: Análise estratégica e tomada de decisões.",
                                "Ciência da Computação: Processamento de dados em ML.",
                                "Estatística: Inferência a partir de dados observados."
                              ],
                              "realWorldApplication": "Em ciência de dados, analistas transformam informações de big data (ex: logs de usuário) em conhecimento acionável para estratégias de e-commerce, como personalização de recomendações que aumentam vendas em 30%."
                            },
                            "estimatedTime": "0.5 horas",
                            "difficulty": "beginner",
                            "status": "not_started",
                            "prerequisites": [
                              "10.1.1.2.1",
                              "10.1.1.2.2"
                            ]
                          },
                          {
                            "id": "10.1.1.2.3.3",
                            "name": "Explicar a hierarquia DIK (Dados, Informação, Conhecimento)",
                            "description": "Apresentar a pirâmide conceitual DIK, mostrando progressão de dados brutos para conhecimento acionável na ciência de dados.",
                            "atomicExpansion": {
                              "steps": [
                                {
                                  "stepNumber": 1,
                                  "title": "Compreender o Conceito de Dados na Hierarquia DIK",
                                  "subSteps": [
                                    "Defina dados como fatos brutos e não processados, sem contexto ou significado inerente.",
                                    "Identifique exemplos de dados: números isolados, strings ou observações sensoriais.",
                                    "Discuta características dos dados: volume alto, falta de estrutura e ausência de interpretação.",
                                    "Compare dados com ruído: explique por que dados sozinhos não geram valor.",
                                    "Crie uma lista de 5 exemplos cotidianos de dados puros."
                                  ],
                                  "verification": "Liste 5 exemplos de dados e explique por que eles não têm significado sem contexto.",
                                  "estimatedTime": "20 minutos",
                                  "materials": [
                                    "Papel e caneta ou editor de texto",
                                    "Vídeo introdutório sobre DIK (ex: Khan Academy ou YouTube: 'Data vs Information')"
                                  ],
                                  "tips": "Use analogias simples como 'números em uma planilha vazia' para fixar o conceito.",
                                  "learningObjective": "Diferenciar dados de outros níveis da hierarquia reconhecendo sua natureza bruta.",
                                  "commonMistakes": "Confundir dados com informação ao adicionar contexto prematuramente."
                                },
                                {
                                  "stepNumber": 2,
                                  "title": "Explorar o Conceito de Informação como Processamento de Dados",
                                  "subSteps": [
                                    "Defina informação como dados organizados, contextualizados e com relevância.",
                                    "Descreva processos para transformar dados em informação: limpeza, agregação e análise básica.",
                                    "Forneça exemplos: temperaturas brutas → média diária; cliques em site → taxa de conversão.",
                                    "Analise como o contexto adiciona valor: quem, o quê, quando e onde.",
                                    "Crie um fluxograma simples: Dados → Processamento → Informação."
                                  ],
                                  "verification": "Transforme 3 exemplos de dados em informação adicionando contexto e explique a diferença.",
                                  "estimatedTime": "25 minutos",
                                  "materials": [
                                    "Ferramenta de fluxograma como Draw.io ou papel",
                                    "Exemplos de datasets simples (CSV de vendas)"
                                  ],
                                  "tips": "Pergunte sempre 'o que esses dados significam no contexto?' para validar.",
                                  "learningObjective": "Reconhecer como o processamento contextualiza dados para gerar informação útil.",
                                  "commonMistakes": "Ignorar a necessidade de relevância, tratando estatísticas isoladas como informação completa."
                                },
                                {
                                  "stepNumber": 3,
                                  "title": "Dominar o Conceito de Conhecimento como Aplicação de Informação",
                                  "subSteps": [
                                    "Defina conhecimento como informação compreendida, internalizada e acionável.",
                                    "Explique níveis de compreensão: saber o quê, saber como e saber por quê.",
                                    "Dê exemplos: média de vendas (info) → estratégia de estoque baseada em padrões (conhecimento).",
                                    "Discuta taciturno vs explícito: conhecimento pessoal vs documentado.",
                                    "Reflita sobre implicações: como o conhecimento impulsiona decisões."
                                  ],
                                  "verification": "Converta um exemplo de informação em conhecimento propondo uma ação baseada nele.",
                                  "estimatedTime": "25 minutos",
                                  "materials": [
                                    "Artigo sobre Pirâmide DIK (ex: Wikipedia ou paper acadêmico)",
                                    "Diário de reflexão"
                                  ],
                                  "tips": "Pense em 'e agora, o que faço com isso?' para elevar informação a conhecimento.",
                                  "learningObjective": "Identificar conhecimento como o ápice acionável da hierarquia DIK.",
                                  "commonMistakes": "Equiparar informação acumulada a conhecimento sem compreensão profunda."
                                },
                                {
                                  "stepNumber": 4,
                                  "title": "Sintetizar a Hierarquia DIK e Visualizar a Pirâmide",
                                  "subSteps": [
                                    "Desenhe a pirâmide DIK: base larga (dados), meio (informação), topo estreito (conhecimento).",
                                    "Explique a progressão: valor aumenta com escassez e refinamento.",
                                    "Aplique a um caso real: dados de sensores IoT → insights preditivos.",
                                    "Debata limitações: adicionar Sabedoria (DIKW) opcionalmente.",
                                    "Resuma em uma frase: 'Dados são ingredientes; informação é receita; conhecimento é chef cozinhando.'"
                                  ],
                                  "verification": "Desenhe e explique a pirâmide DIK oralmente ou em diagrama, cobrindo progressão.",
                                  "estimatedTime": "30 minutos",
                                  "materials": [
                                    "Ferramenta de desenho (PowerPoint, Canva)",
                                    "Exemplo de caso de estudo em ciência de dados"
                                  ],
                                  "tips": "Use cores na pirâmide: cinza (dados), azul (info), dourado (conhecimento) para memorização.",
                                  "learningObjective": "Explicar integralmente a hierarquia DIK com visualização e exemplos integrados.",
                                  "commonMistakes": "Invertir a pirâmide, colocando conhecimento na base."
                                }
                              ],
                              "practicalExample": "Em uma fazenda inteligente: Dados (leituras de umidade do solo a cada hora), Informação (gráfico de umidade média semanal por zona), Conhecimento (regras de irrigação: irrigar se <30% por 3 dias, ajustando por previsão climática).",
                              "finalVerifications": [
                                "Defina com precisão cada nível da hierarquia DIK.",
                                "Forneça exemplos originais para dados, informação e conhecimento.",
                                "Desenhe corretamente a pirâmide DIK com legendas.",
                                "Explique verbalmente a progressão de valor de dados para conhecimento.",
                                "Aplique DIK a um cenário novo de ciência de dados.",
                                "Identifique erros comuns em classificações de DIK."
                              ],
                              "assessmentCriteria": [
                                "Precisão conceitual: definições corretas sem confusões (30%)",
                                "Exemplos relevantes: concretos e progressivos (25%)",
                                "Visualização clara: pirâmide bem estruturada (20%)",
                                "Explicação fluida: progressão lógica demonstrada (15%)",
                                "Aplicação prática: ligação com ciência de dados (10%)"
                              ],
                              "crossCurricularConnections": [
                                "Filosofia: Epistemologia e níveis de saber (Platão).",
                                "Negócios: Análise estratégica e tomada de decisões.",
                                "Informática: Processamento de dados em bancos e IA.",
                                "Estatística: De amostras brutas a inferências.",
                                "Psicologia: Aprendizado e internalização de informação."
                              ],
                              "realWorldApplication": "Na ciência de dados corporativa, profissionais usam DIK para transformar logs de usuários (dados) em relatórios de engajamento (informação) e estratégias de produto acionáveis (conhecimento), otimizando retenção em empresas como Netflix ou Google."
                            },
                            "estimatedTime": "1 hora",
                            "difficulty": "intermediate",
                            "status": "not_started",
                            "prerequisites": [
                              "10.1.1.2.1",
                              "10.1.1.2.2"
                            ]
                          }
                        ]
                      }
                    ]
                  },
                  {
                    "id": "10.1.1.3",
                    "name": "Conceitos de Modelagem de Problemas, Aprendizado e i.i.d.",
                    "description": "Modelagem de problemas, conceitos de aprendizado e ambiente independente e identicamente distribuído.",
                    "individualConcepts": [
                      {
                        "id": "10.1.1.3.1",
                        "name": "Modelagem de Problemas em Ciência de Dados",
                        "description": "Processo de transformar um problema do mundo real em um formato matemático ou computacional adequado para análise e resolução usando técnicas de ciência de dados, incluindo definição de objetivos, seleção de variáveis e formulação de hipóteses.",
                        "specificSkills": [
                          {
                            "id": "10.1.1.3.1.1",
                            "name": "Definir modelagem de problemas",
                            "description": "Explicar o que é modelagem de problemas, diferenciando dados, informação e conhecimento, e identificar as etapas iniciais como coleta, integração e armazenamento de dados.",
                            "atomicExpansion": {
                              "steps": [
                                {
                                  "stepNumber": 1,
                                  "title": "Diferenciar Dados, Informação e Conhecimento",
                                  "subSteps": [
                                    "Pesquisar definições precisas: Dados são fatos brutos e não processados (ex: números isolados); Informação é dados organizados com contexto (ex: números em uma tabela com significado); Conhecimento é informação interpretada com experiência e insight (ex: decisões baseadas na informação).",
                                    "Criar exemplos pessoais: Para cada conceito, inventar 2 exemplos cotidianos e registrá-los em uma tabela comparativa.",
                                    "Analisar hierarquia: Desenhar um diagrama de pirâmide mostrando a progressão de dados para conhecimento.",
                                    "Discutir em duplas: Explicar os conceitos para um parceiro e receber feedback.",
                                    "Resumir em bullet points: Listar 3 diferenças chave entre os três conceitos."
                                  ],
                                  "verification": "Tabela comparativa completa com exemplos e diagrama de pirâmide corretos, além de resumo escrito.",
                                  "estimatedTime": "30 minutos",
                                  "materials": [
                                    "Acesso à internet para pesquisa rápida",
                                    "Papel, caneta ou ferramenta digital como Draw.io ou Google Docs"
                                  ],
                                  "tips": "Use analogias simples, como uma lista de compras (dados), receita culinária (informação) e chef experiente (conhecimento).",
                                  "learningObjective": "Distinguir conceitualmente e hierarquicamente dados, informação e conhecimento.",
                                  "commonMistakes": "Confundir informação com conhecimento, ignorando o papel da experiência humana; tratar todos como sinônimos."
                                },
                                {
                                  "stepNumber": 2,
                                  "title": "Compreender o Conceito de Modelagem de Problemas",
                                  "subSteps": [
                                    "Definir modelagem de problemas: Processo de representar um problema real de forma simplificada e abstrata para análise computacional ou matemática.",
                                    "Estudar o propósito em Ciência de Dados: Transformar questões complexas do mundo real em estruturas tratáveis por algoritmos.",
                                    "Identificar componentes principais: Problema real, abstração, modelo matemático/estatístico e validação.",
                                    "Visualizar o fluxo: Criar um fluxograma simples mostrando entrada (problema) -> modelagem -> saída (solução).",
                                    "Relacionar com conceitos anteriores: Explicar como dados alimentam a modelagem para gerar informação e conhecimento."
                                  ],
                                  "verification": "Definição escrita clara, fluxograma completo e relação com dados/informação/conhecimento explicada.",
                                  "estimatedTime": "40 minutos",
                                  "materials": [
                                    "Vídeos curtos introdutórios no YouTube sobre 'modelagem em ciência de dados'",
                                    "Ferramenta de fluxograma como Lucidchart ou papel"
                                  ],
                                  "tips": "Pense em modelagem como um 'mapa' de um território complexo para navegação fácil.",
                                  "learningObjective": "Entender modelagem de problemas como ponte entre mundo real e análise computacional.",
                                  "commonMistakes": "Ver modelagem apenas como 'equações', ignorando o aspecto de abstração e contexto real; pular a relação com dados."
                                },
                                {
                                  "stepNumber": 3,
                                  "title": "Identificar Etapas Iniciais da Modelagem de Problemas",
                                  "subSteps": [
                                    "Explorar coleta de dados: Identificar fontes (sensores, APIs, bancos de dados, surveys) e métodos éticos de aquisição.",
                                    "Estudar integração de dados: Técnicas para combinar dados heterogêneos (ex: merge de tabelas CSV, ETL processes).",
                                    "Analisar armazenamento de dados: Formatos e ferramentas (CSV, JSON, SQL, NoSQL) e boas práticas de organização.",
                                    "Sequenciar as etapas: Criar uma lista numerada mostrando coleta -> integração -> armazenamento como base da modelagem.",
                                    "Simular com exemplo: Aplicar as etapas a um dataset simples baixado online."
                                  ],
                                  "verification": "Lista sequencial das etapas com exemplos e simulação prática em um dataset pequeno.",
                                  "estimatedTime": "45 minutos",
                                  "materials": [
                                    "Dataset de exemplo gratuito (ex: Kaggle Iris dataset)",
                                    "Excel ou Google Sheets para simulação"
                                  ],
                                  "tips": "Sempre priorize qualidade sobre quantidade na coleta; teste integração com dados pequenos primeiro.",
                                  "learningObjective": "Reconhecer e sequenciar as etapas iniciais fundamentais da modelagem.",
                                  "commonMistakes": "Ignorar questões éticas na coleta; confundir integração com mera cópia de dados; subestimar limpeza no armazenamento."
                                },
                                {
                                  "stepNumber": 4,
                                  "title": "Sintetizar e Definir Modelagem de Problemas Completamente",
                                  "subSteps": [
                                    "Integrar todos os conceitos: Escrever uma definição unificada incluindo diferenciação e etapas iniciais.",
                                    "Criar um resumo visual: Mapa conceitual ligando dados/informação/conhecimento às etapas de modelagem.",
                                    "Autoavaliar: Comparar sua definição com referências online e ajustar.",
                                    "Preparar apresentação: Ensaiar explicação em 2 minutos para um 'público leigo'.",
                                    "Documentar aprendizados: Registrar insights pessoais em um journal."
                                  ],
                                  "verification": "Definição escrita completa, mapa conceitual e gravação de apresentação de 2 minutos.",
                                  "estimatedTime": "35 minutos",
                                  "materials": [
                                    "Referências online como artigos da Wikipedia ou Towards Data Science",
                                    "Ferramenta de mapa mental como MindMeister"
                                  ],
                                  "tips": "Mantenha a definição concisa (máximo 100 palavras) mas abrangente.",
                                  "learningObjective": "Formular uma definição holística de modelagem de problemas.",
                                  "commonMistakes": "Fazer definição genérica sem mencionar etapas específicas; sobrecarregar com jargões desnecessários."
                                }
                              ],
                              "practicalExample": "Em uma loja online, modelar o problema de 'prever demanda de produtos': Coletar dados brutos de vendas passadas (dados); integrar com dados de sazonalidade e promoções (informação); armazenar em um banco SQL para análise futura (conhecimento inicial), permitindo criar um modelo preditivo simples.",
                              "finalVerifications": [
                                "Explicar corretamente as diferenças entre dados, informação e conhecimento com exemplos.",
                                "Definir modelagem de problemas em termos de abstração e propósito em Ciência de Dados.",
                                "Listar e descrever as três etapas iniciais: coleta, integração e armazenamento.",
                                "Criar um fluxograma ou mapa conectando conceitos e etapas.",
                                "Aplicar os conceitos a um exemplo prático simples.",
                                "Autoavaliar compreensão respondendo a 5 perguntas de verificação."
                              ],
                              "assessmentCriteria": [
                                "Precisão conceitual: Correção nas definições e diferenciações (30%)",
                                "Profundidade nas etapas: Detalhes e sequência corretas das etapas iniciais (25%)",
                                "Clareza e estrutura: Uso de visuais e linguagem acessível (20%)",
                                "Aplicação prática: Qualidade do exemplo e simulação (15%)",
                                "Completude: Inclusão de todos os elementos solicitados (10%)"
                              ],
                              "crossCurricularConnections": [
                                "Estatística: Uso de dados para inferência e modelagem probabilística.",
                                "Programação: Scripts para coleta e integração de dados (Python Pandas).",
                                "Gestão de Projetos: Planejamento de etapas em fluxos de trabalho ágeis.",
                                "Ética e Sociedade: Considerações éticas na coleta e uso de dados pessoais."
                              ],
                              "realWorldApplication": "Na previsão de epidemias (ex: COVID-19), cientistas coletam dados de casos, integram com mobilidade populacional e armazenam para modelar propagação, gerando conhecimento para políticas públicas."
                            },
                            "estimatedTime": "1 hora",
                            "difficulty": "beginner",
                            "status": "not_started",
                            "prerequisites": []
                          },
                          {
                            "id": "10.1.1.3.1.2",
                            "name": "Identificar tipos de problemas modeláveis",
                            "description": "Classificar problemas em ciência de dados como preditivos, descritivos ou prescritivos, com exemplos de aplicações reais e estudos de caso.",
                            "atomicExpansion": {
                              "steps": [
                                {
                                  "stepNumber": 1,
                                  "title": "Compreender as Definições Básicas dos Tipos de Problemas",
                                  "subSteps": [
                                    "Leia definições: Descritivo (o que aconteceu?), Preditivo (o que vai acontecer?), Prescritivo (o que fazer?).",
                                    "Anote diferenças chave em um quadro comparativo.",
                                    "Assista a um vídeo introdutório de 5-10 minutos sobre os três tipos.",
                                    "Resuma cada definição em uma frase própria.",
                                    "Compare com glossário oficial de ciência de dados."
                                  ],
                                  "verification": "Quadro comparativo completo com definições precisas e resumo escrito.",
                                  "estimatedTime": "30 minutos",
                                  "materials": "Vídeo introdutório (ex: Khan Academy ou YouTube), caderno ou Google Docs, glossário de ciência de dados.",
                                  "tips": "Use cores diferentes para cada tipo no quadro para visualização rápida.",
                                  "learningObjective": "Definir com precisão os três tipos de problemas em ciência de dados.",
                                  "commonMistakes": "Confundir preditivo com prescritivo; ignorar o foco em ações no prescritivo."
                                },
                                {
                                  "stepNumber": 2,
                                  "title": "Identificar Características e Exemplos Iniciais",
                                  "subSteps": [
                                    "Liste 2-3 características únicas de cada tipo (ex: descritivo usa resumos estatísticos).",
                                    "Encontre 1 exemplo real para cada: descritivo (relatório de vendas), preditivo (previsão de churn), prescritivo (otimização de estoque).",
                                    "Crie flashcards com característica no frente e exemplo no verso.",
                                    "Teste-se recitando exemplos sem olhar.",
                                    "Discuta com um par ou fórum online para validar."
                                  ],
                                  "verification": "Flashcards criados e teste com 100% de acerto em 2 rodadas.",
                                  "estimatedTime": "45 minutos",
                                  "materials": "Flashcard app (Anki), exemplos de sites como Towards Data Science.",
                                  "tips": "Associe exemplos a contextos pessoais para melhor retenção.",
                                  "learningObjective": "Reconhecer características distintivas e associar exemplos iniciais a cada tipo.",
                                  "commonMistakes": "Usar exemplos genéricos sem ligação real; superpor categorias."
                                },
                                {
                                  "stepNumber": 3,
                                  "title": "Praticar Classificação de Problemas Simples",
                                  "subSteps": [
                                    "Pegue 5 problemas hipotéticos e classifique cada um como descritivo, preditivo ou prescritivo.",
                                    "Justifique a classificação com 2 razões por problema.",
                                    "Use um dataset simples (ex: Iris ou Titanic) para identificar o tipo de problema implícito.",
                                    "Revise classificações erradas e corrija.",
                                    "Crie 3 problemas próprios e classifique-os."
                                  ],
                                  "verification": "Lista de 8 problemas classificados com justificativas corretas em 90%.",
                                  "estimatedTime": "60 minutos",
                                  "materials": "Datasets Kaggle (Iris, Titanic), planilha Excel ou Jupyter Notebook.",
                                  "tips": "Pergunte: 'Responde passado, futuro ou ação?' para guiar classificação.",
                                  "learningObjective": "Classificar independentemente problemas simples com justificativa.",
                                  "commonMistakes": "Forçar encaixe em preditivo por ser 'mais avançado'; ignorar nuances."
                                },
                                {
                                  "stepNumber": 4,
                                  "title": "Analisar Estudos de Caso Reais",
                                  "subSteps": [
                                    "Selecione 3 estudos de caso (ex: Netflix recomendações - preditivo; otimização logística - prescritivo).",
                                    "Identifique o tipo principal e subtipos se aplicável.",
                                    "Mapeie métricas usadas em cada caso.",
                                    "Escreva um relatório curto (200 palavras) por caso.",
                                    "Apresente ou grave explicação para feedback."
                                  ],
                                  "verification": "Relatórios completos com classificação precisa e mapeamento de métricas.",
                                  "estimatedTime": "90 minutos",
                                  "materials": "Artigos Harvard Business Review ou Kaggle case studies, gravador de vídeo.",
                                  "tips": "Busque cases de indústrias variadas para generalização.",
                                  "learningObjective": "Aplicar classificação em contextos complexos reais com análise profunda.",
                                  "commonMistakes": "Não considerar evolução do problema (ex: descritivo vira preditivo)."
                                }
                              ],
                              "practicalExample": "Dado o problema 'Prever quantas unidades de produto X serão vendidas no próximo mês baseado em dados históricos de vendas e sazonalidade': Classifique como preditivo, pois foca em forecasting futuro usando padrões passados. Justificativa: Usa regressão ou séries temporais para estimar vendas.",
                              "finalVerifications": [
                                "Classifica corretamente 10 problemas mistos com 95% de acerto.",
                                "Explica diferenças entre tipos com exemplos reais sem hesitação.",
                                "Identifica tipo principal em 3 estudos de caso variados.",
                                "Cria um problema original por tipo com justificativa válida.",
                                "Discute limitações de cada tipo em cenários híbridos.",
                                "Mapeia ferramentas/modelos típicos para cada tipo (ex: SQL para descritivo)."
                              ],
                              "assessmentCriteria": [
                                "Precisão na definição e distinção dos três tipos (30%).",
                                "Qualidade de justificativas em classificações (25%).",
                                "Diversidade e relevância de exemplos/estudos de caso (20%).",
                                "Profundidade na análise de características e métricas (15%).",
                                "Criatividade em problemas originais e conexões reais (10%)."
                              ],
                              "crossCurricularConnections": [
                                "Estatística: Uso de sumários descritivos e testes preditivos.",
                                "Negócios: Decisão estratégica via análise prescritiva.",
                                "Programação: Implementação em Python/R para modelagem.",
                                "Ética: Considerações em predições enviesadas.",
                                "Matemática: Modelos probabilísticos subjacentes."
                              ],
                              "realWorldApplication": "Em uma empresa de e-commerce, identificar se um problema de queda nas vendas é descritivo (analisar causas passadas), preditivo (prever tendências futuras) ou prescritivo (recomendar ajustes de preço/estoque), guiando a escolha de ferramentas como dashboards BI, ML forecasting ou otimização linear para maximizar lucros."
                            },
                            "estimatedTime": "1.5 horas",
                            "difficulty": "beginner",
                            "status": "not_started",
                            "prerequisites": []
                          },
                          {
                            "id": "10.1.1.3.1.3",
                            "name": "Aplicar etapas básicas de modelagem",
                            "description": "Descrever o fluxo de análise exploratória, visualização, limpeza de dados e preparação para modelagem, considerando ética no uso de dados.",
                            "atomicExpansion": {
                              "steps": [
                                {
                                  "stepNumber": 1,
                                  "title": "Realizar Análise Exploratória de Dados (EDA)",
                                  "subSteps": [
                                    "Carregar o dataset usando pandas.",
                                    "Examinar estrutura geral com info() e describe().",
                                    "Identificar tipos de dados, valores ausentes e estatísticas descritivas.",
                                    "Analisar distribuições univariadas e bivariadas.",
                                    "Documentar insights iniciais em um relatório."
                                  ],
                                  "verification": "Verificar se o relatório de EDA lista estatísticas resumidas e identifica anomalias principais.",
                                  "estimatedTime": "45 minutos",
                                  "materials": "Python com pandas e Jupyter Notebook; dataset exemplo (ex: Iris ou Titanic).",
                                  "tips": "Sempre comece com shape() para entender dimensões antes de mergulhar em detalhes.",
                                  "learningObjective": "Compreender a estrutura e qualidade inicial dos dados.",
                                  "commonMistakes": "Ignorar valores ausentes ou outliers na fase inicial, levando a conclusões erradas."
                                },
                                {
                                  "stepNumber": 2,
                                  "title": "Criar Visualizações de Dados",
                                  "subSteps": [
                                    "Selecionar gráficos adequados (histograma para distribuições, boxplot para outliers).",
                                    "Gerar plots com matplotlib ou seaborn.",
                                    "Explorar correlações com heatmap.",
                                    "Interpretar visualizações e anotar padrões observados.",
                                    "Salvar visualizações em um dashboard ou relatório."
                                  ],
                                  "verification": "Visualizações geradas mostram padrões claros e são interpretadas corretamente.",
                                  "estimatedTime": "40 minutos",
                                  "materials": "Python com matplotlib, seaborn; Jupyter Notebook.",
                                  "tips": "Use temas claros como seaborn.set_style('whitegrid') para melhor legibilidade.",
                                  "learningObjective": "Identificar padrões e relações nos dados por meio de visualizações.",
                                  "commonMistakes": "Sobrecarregar gráficos com muitos elementos, tornando-os ilegíveis."
                                },
                                {
                                  "stepNumber": 3,
                                  "title": "Limpar os Dados",
                                  "subSteps": [
                                    "Tratar valores ausentes (imputação, remoção).",
                                    "Remover duplicatas e corrigir inconsistências.",
                                    "Lidar com outliers usando métodos estatísticos.",
                                    "Padronizar formatos de dados (datas, categorias).",
                                    "Validar a limpeza com verificações pós-processamento."
                                  ],
                                  "verification": "Dataset limpo sem missing values, duplicatas ou inconsistências detectáveis.",
                                  "estimatedTime": "50 minutos",
                                  "materials": "Python com pandas, numpy; dataset pré-carregado.",
                                  "tips": "Registre todas as transformações em um pipeline para reprodutibilidade.",
                                  "learningObjective": "Garantir qualidade dos dados removendo ruídos e erros.",
                                  "commonMistakes": "Remover dados excessivamente, perdendo informações valiosas."
                                },
                                {
                                  "stepNumber": 4,
                                  "title": "Preparar os Dados para Modelagem",
                                  "subSteps": [
                                    "Dividir features e target.",
                                    "Realizar encoding de variáveis categóricas (one-hot ou label).",
                                    "Escalonar/normalizar features numéricas.",
                                    "Dividir em train/test sets.",
                                    "Salvar dataset preparado em formato adequado (CSV ou pickle)."
                                  ],
                                  "verification": "Dataset dividido corretamente, com features escalonadas e pronto para modelagem.",
                                  "estimatedTime": "35 minutos",
                                  "materials": "Python com scikit-learn (train_test_split, StandardScaler); pandas.",
                                  "tips": "Use ColumnTransformer para aplicar transformações seletivas.",
                                  "learningObjective": "Transformar dados brutos em formato compatível com algoritmos de ML.",
                                  "commonMistakes": "Vazar dados de teste para treino durante preparação."
                                },
                                {
                                  "stepNumber": 5,
                                  "title": "Integrar Considerações Éticas",
                                  "subSteps": [
                                    "Avaliar viés nos dados (representatividade de grupos).",
                                    "Verificar privacidade (anonimização de dados sensíveis).",
                                    "Documentar impactos potenciais de decisões de limpeza.",
                                    "Garantir conformidade com regulamentações (LGPD/GDPR).",
                                    "Incluir declaração ética no relatório final."
                                  ],
                                  "verification": "Relatório inclui análise de viés e declaração ética assinada.",
                                  "estimatedTime": "30 minutos",
                                  "materials": "Checklist ético; ferramentas como pandas-profiling para viés.",
                                  "tips": "Consulte frameworks como FairML para detecção de viés automatizada.",
                                  "learningObjective": "Incorporar ética em todas as etapas de preparação de dados.",
                                  "commonMistakes": "Ignorar viés demográfico, perpetuando desigualdades em modelos."
                                }
                              ],
                              "practicalExample": "Usando o dataset Titanic do Kaggle, realize EDA para identificar sobreviventes por classe e gênero, visualize com boxplots, limpe idades missing via imputação mediana, prepare features (encoding de sexo e embarque), divida em train/test e documente ética sobre viés de gênero/classe.",
                              "finalVerifications": [
                                "Relatório completo descreve todo o fluxo de EDA a preparação.",
                                "Dataset final está limpo, escalonado e dividido corretamente.",
                                "Visualizações interpretadas destacam insights chave.",
                                "Análise ética identifica e mitiga potenciais viés.",
                                "Pipeline é reproduzível com código comentado.",
                                "Nenhum erro de dados persiste (verificado com asserts)."
                              ],
                              "assessmentCriteria": [
                                "Fluxo lógico e completo das etapas (pontos por cobertura).",
                                "Qualidade das visualizações e interpretações (clareza e relevância).",
                                "Eficácia da limpeza (redução de ruído sem perda excessiva).",
                                "Correção na preparação para ML (sem vazamento de dados).",
                                "Profundidade da análise ética (identificação de riscos reais).",
                                "Documentação clara e reprodutível."
                              ],
                              "crossCurricularConnections": [
                                "Estatística: Medidas descritivas e testes de normalidade.",
                                "Programação: Manipulação de dados com Python/pandas.",
                                "Ética e Cidadania: Privacidade e viés em dados.",
                                "Matemática: Normalização e escalonamento vetorial.",
                                "Informática: Pipelines de dados e versionamento (Git)."
                              ],
                              "realWorldApplication": "Em empresas como bancos ou saúde, preparar dados para modelos preditivos de churn ou diagnóstico, garantindo precisão e ética para decisões impactantes em milhões de usuários."
                            },
                            "estimatedTime": "2 horas",
                            "difficulty": "intermediate",
                            "status": "not_started",
                            "prerequisites": []
                          }
                        ]
                      },
                      {
                        "id": "10.1.1.3.2",
                        "name": "Conceitos de Aprendizado",
                        "description": "Fundamentos do aprendizado de máquina, incluindo tipos de aprendizado (supervisionado e não-supervisionado), e sua relação com a modelagem de problemas em ciência de dados.",
                        "specificSkills": [
                          {
                            "id": "10.1.1.3.2.1",
                            "name": "Diferenciar aprendizado supervisionado",
                            "description": "Explicar o aprendizado supervisionado, com exemplos de regressão e classificação, e sua aplicação em problemas rotulados.",
                            "atomicExpansion": {
                              "steps": [
                                {
                                  "stepNumber": 1,
                                  "title": "Entender os Conceitos Fundamentais do Aprendizado Supervisionado",
                                  "subSteps": [
                                    "Defina aprendizado supervisionado como um tipo de machine learning onde o modelo é treinado com dados rotulados (entradas com saídas conhecidas).",
                                    "Explique a importância dos dados rotulados: eles fornecem 'respostas corretas' para o algoritmo aprender padrões.",
                                    "Diferencie de outros tipos iniciais: mencione brevemente que não-supervisionado usa dados não rotulados.",
                                    "Discuta o processo geral: treinamento, validação e teste com dados rotulados.",
                                    "Identifique componentes chave: features (entradas), labels (saídas) e modelo."
                                  ],
                                  "verification": "Escreva uma definição clara em suas próprias palavras e identifique um exemplo simples de dados rotulados.",
                                  "estimatedTime": "20 minutos",
                                  "materials": [
                                    "Notebook para anotações",
                                    "Artigo introdutório sobre ML supervisionado (ex: Scikit-learn docs)"
                                  ],
                                  "tips": "Use analogias como um professor corrigindo provas para entender o papel dos labels.",
                                  "learningObjective": "Compreender a definição e os pilares do aprendizado supervisionado.",
                                  "commonMistakes": [
                                    "Confundir com aprendizado não-supervisionado (sem labels)",
                                    "Ignorar a necessidade de dados rotulados de qualidade"
                                  ]
                                },
                                {
                                  "stepNumber": 2,
                                  "title": "Diferenciar Regressão e Classificação",
                                  "subSteps": [
                                    "Defina regressão: prever valores contínuos (ex: preço de uma casa).",
                                    "Defina classificação: prever categorias discretas (ex: email é spam ou não).",
                                    "Compare métricas: regressão usa MSE/MAE; classificação usa accuracy/precision/recall.",
                                    "Analise exemplos: regressão linear para previsão de salários; classificação logística para detecção de fraudes.",
                                    "Pratique identificando: dado um problema, classifique como regressão ou classificação."
                                  ],
                                  "verification": "Crie uma tabela comparativa com 2 exemplos de cada tipo e explique as diferenças.",
                                  "estimatedTime": "25 minutos",
                                  "materials": [
                                    "Planilha ou papel para tabela",
                                    "Vídeo curto sobre regressão vs classificação (ex: Khan Academy)"
                                  ],
                                  "tips": "Lembre-se: regressão = números reais; classificação = classes/nomes.",
                                  "learningObjective": "Distinguir e exemplificar os dois principais subtópicos do aprendizado supervisionado.",
                                  "commonMistakes": [
                                    "Confundir saída contínua com discreta",
                                    "Misturar métricas de avaliação entre os tipos"
                                  ]
                                },
                                {
                                  "stepNumber": 3,
                                  "title": "Explorar Exemplos e Aplicações em Problemas Rotulados",
                                  "subSteps": [
                                    "Estude caso de regressão: dataset de Boston Housing para prever preços.",
                                    "Estude caso de classificação: dataset Iris para classificar flores.",
                                    "Descreva o fluxo: coletar dados rotulados, treinar modelo, prever novos dados.",
                                    "Discuta desafios: overfitting em dados rotulados limitados, necessidade de balanceamento em classificação.",
                                    "Aplique conceitualmente: pense em um problema pessoal e identifique se é supervisionado."
                                  ],
                                  "verification": "Descreva verbalmente ou por escrito um exemplo completo de regressão e um de classificação.",
                                  "estimatedTime": "30 minutos",
                                  "materials": [
                                    "Datasets sample (Kaggle: Iris, Boston Housing)",
                                    "Python/Jupyter se disponível para visualização"
                                  ],
                                  "tips": "Visualize dados com gráficos para entender padrões rotulados.",
                                  "learningObjective": "Aplicar conceitos a exemplos reais e identificar aplicações.",
                                  "commonMistakes": [
                                    "Usar dados não rotulados como exemplo",
                                    "Ignorar pré-processamento de dados"
                                  ]
                                },
                                {
                                  "stepNumber": 4,
                                  "title": "Consolidar Diferenciação e Comparações",
                                  "subSteps": [
                                    "Compare supervisionado vs não-supervisionado: labels presentes vs clustering.",
                                    "Revise aplicações: saúde (classificação de doenças), finanças (regressão de riscos).",
                                    "Crie um fluxograma de decisão: dado um problema, é supervisionado? Regressão ou classificação?",
                                    "Avalie limitações: custo de rotulagem manual.",
                                    "Teste conhecimento com quizzes rápidos."
                                  ],
                                  "verification": "Construa e explique um fluxograma de decisão para tipos de ML.",
                                  "estimatedTime": "15 minutos",
                                  "materials": [
                                    "Ferramenta de fluxograma (Draw.io ou papel)",
                                    "Quiz online sobre ML types"
                                  ],
                                  "tips": "Foquem em 'labels = supervisionado' como regra de ouro.",
                                  "learningObjective": "Diferenciar aprendizado supervisionado de outros paradigmas de forma consolidada.",
                                  "commonMistakes": [
                                    "Generalizar todos os ML como supervisionados",
                                    "Subestimar custo de dados rotulados"
                                  ]
                                }
                              ],
                              "practicalExample": "Em um sistema de recomendação de preços de imóveis (regressão): dados rotulados incluem tamanho da casa, localização e preço real vendido. O modelo aprende a prever preço para novas casas. Para classificação: um filtro de spam de emails usa emails rotulados como 'spam' ou 'não-spam' para classificar novos emails.",
                              "finalVerifications": [
                                "Explique em 1 minuto a diferença entre regressão e classificação com exemplos.",
                                "Identifique corretamente 5 problemas como supervisionados (3 regressão, 2 classificação).",
                                "Descreva o papel dos dados rotulados no treinamento.",
                                "Compare supervisionado vs não-supervisionado em uma frase.",
                                "Crie um exemplo pessoal de aplicação supervisionada."
                              ],
                              "assessmentCriteria": [
                                "Precisão na definição de aprendizado supervisionado (90%+ correta).",
                                "Correta distinção entre regressão (contínua) e classificação (discreta).",
                                "Uso de exemplos relevantes e rotulados.",
                                "Compreensão de fluxos de treinamento e avaliação.",
                                "Capacidade de diferenciar de outros tipos de ML."
                              ],
                              "crossCurricularConnections": [
                                "Estatística: Modelos lineares e probabilísticos.",
                                "Programação: Implementação em Python (Scikit-learn).",
                                "Matemática: Funções de perda e otimização.",
                                "Ética: Viés em dados rotulados."
                              ],
                              "realWorldApplication": "Na medicina, modelos supervisionados classificam imagens de raio-X como 'pneumonia' ou 'normal' usando dados rotulados por experts, salvando vidas com diagnósticos rápidos; em e-commerce, regressão prevê demanda de produtos para otimizar estoque."
                            },
                            "estimatedTime": "1.5 horas",
                            "difficulty": "beginner",
                            "status": "not_started",
                            "prerequisites": []
                          },
                          {
                            "id": "10.1.1.3.2.2",
                            "name": "Diferenciar aprendizado não-supervisionado",
                            "description": "Descrever clustering e redução de dimensionalidade como técnicas não-supervisionadas, com exemplos práticos e distinção do supervisionado.",
                            "atomicExpansion": {
                              "steps": [
                                {
                                  "stepNumber": 1,
                                  "title": "Entender os Conceitos Básicos de Aprendizado Supervisionado vs Não-Supervisionado",
                                  "subSteps": [
                                    "Defina aprendizado supervisionado: usa dados rotulados (inputs e outputs conhecidos) para treinar modelos preditivos.",
                                    "Defina aprendizado não-supervisionado: usa dados não rotulados para descobrir padrões intrínsecos.",
                                    "Compare: supervisionado visa prever labels; não-supervisionado explora estrutura sem labels.",
                                    "Liste exemplos iniciais: regressão/classificação (supervisionado) vs clustering (não-supervisionado).",
                                    "Crie um diagrama mental comparando fluxos de dados em ambos."
                                  ],
                                  "verification": "Crie um resumo de 1 parágrafo diferenciando os dois tipos e valide com um colega ou autoavaliação.",
                                  "estimatedTime": "45 minutos",
                                  "materials": [
                                    "Notebook Jupyter ou papel e caneta",
                                    "Documentação Scikit-learn sobre tipos de ML"
                                  ],
                                  "tips": "Use analogias: supervisionado é como um professor corrigindo provas; não-supervisionado é explorar um museu sem guia.",
                                  "learningObjective": "Diferenciar conceitualmente supervisionado de não-supervisionado.",
                                  "commonMistakes": "Confundir com semi-supervisionado ou assumir que não-supervisionado requer labels ocultos."
                                },
                                {
                                  "stepNumber": 2,
                                  "title": "Explorar Clustering como Técnica Não-Supervisionada",
                                  "subSteps": [
                                    "Descreva clustering: agrupa dados semelhantes sem labels prévios.",
                                    "Estude K-Means: inicializa centróides, atribui pontos e itera até convergência.",
                                    "Implemente K-Means em um dataset simples como Iris (sem labels).",
                                    "Avalie com métricas como Silhouette Score.",
                                    "Experimente diferentes K valores usando o método do cotovelo."
                                  ],
                                  "verification": "Execute código e plote clusters; verifique se grupos fazem sentido visualmente.",
                                  "estimatedTime": "60 minutos",
                                  "materials": [
                                    "Python com Scikit-learn e Matplotlib",
                                    "Dataset Iris do Seaborn"
                                  ],
                                  "tips": "Visualize sempre: plots ajudam a intuitar padrões invisíveis nos dados.",
                                  "learningObjective": "Aplicar e entender clustering prático.",
                                  "commonMistakes": "Escolher K fixo sem validação; ignorar escalonamento de features."
                                },
                                {
                                  "stepNumber": 3,
                                  "title": "Explorar Redução de Dimensionalidade como Técnica Não-Supervisionada",
                                  "subSteps": [
                                    "Defina redução de dimensionalidade: projeta dados de alta dimensão para baixa mantendo informação.",
                                    "Estude PCA: decomposição em componentes principais via autovalores.",
                                    "Aplique PCA em um dataset como MNIST ou Wine, reduzindo para 2D.",
                                    "Interprete loadings: quais features contribuem mais.",
                                    "Compare variância explicada antes/depois."
                                  ],
                                  "verification": "Gere plot 2D pós-PCA e confirme que variância cumulativa >80%.",
                                  "estimatedTime": "60 minutos",
                                  "materials": [
                                    "Python com Scikit-learn e Matplotlib",
                                    "Dataset Wine do Scikit-learn"
                                  ],
                                  "tips": "PCA assume linearidade; teste com dados não-lineares para limitações.",
                                  "learningObjective": "Compreender e aplicar redução de dimensionalidade.",
                                  "commonMistakes": "Não centralizar dados antes de PCA; confundir com feature selection."
                                },
                                {
                                  "stepNumber": 4,
                                  "title": "Diferenciar e Integrar com Exemplos Práticos",
                                  "subSteps": [
                                    "Compare clustering vs redução: clustering agrupa, redução comprime espaço.",
                                    "Destaque diferenças do supervisionado: sem targets, foco em padrões não preditivos.",
                                    "Crie tabela comparativa: técnicas, dados necessários, aplicações.",
                                    "Aplique em caso real: clusterize clientes de e-commerce e reduza dims de imagens.",
                                    "Discuta quando usar cada um vs supervisionado."
                                  ],
                                  "verification": "Escreva relatório de 300 palavras com exemplos e valide critérios de distinção.",
                                  "estimatedTime": "45 minutos",
                                  "materials": [
                                    "Dataset de e-commerce (ex: Mall Customers do Kaggle)",
                                    "Google Sheets ou Markdown para tabela"
                                  ],
                                  "tips": "Pense em cenários: 'sem labels, o que descobrir?' para reforçar não-supervisionado.",
                                  "learningObjective": "Integrar conhecimentos e diferenciar claramente.",
                                  "commonMistakes": "Misturar outputs: clustering dá grupos, não classes preditivas como supervisionado."
                                }
                              ],
                              "practicalExample": "Em marketing, use K-Means para clusterizar clientes por hábitos de compra (não-supervisionado, sem labels de 'bom/mau cliente') e PCA para visualizar features de alta dimensão como histórico de compras, contrastando com um modelo supervisionado de churn prediction que usa labels históricos.",
                              "finalVerifications": [
                                "Explicar verbalmente a diferença entre supervisionado e não-supervisionado sem hesitação.",
                                "Implementar clustering em dataset novo e interpretar resultados.",
                                "Aplicar PCA e discutir % de variância preservada.",
                                "Criar tabela comparativa precisa de técnicas.",
                                "Identificar quando usar não-supervisionado vs supervisionado em problemas reais.",
                                "Avaliar limitações de cada técnica com exemplos."
                              ],
                              "assessmentCriteria": [
                                "Precisão conceitual: distinção clara sem confusões (30%)",
                                "Exemplos práticos: código executável e interpretações corretas (25%)",
                                "Profundidade: substeps detalhados e métricas usadas (20%)",
                                "Integração: conexões entre clustering, PCA e supervisionado (15%)",
                                "Clareza: comunicação via relatórios/plots (10%)"
                              ],
                              "crossCurricularConnections": [
                                "Estatística: distribuições i.i.d. e testes de hipóteses para validar clusters.",
                                "Programação: Python/Scikit-learn para implementação prática.",
                                "Matemática: Álgebra linear em PCA (autovalores/vetores).",
                                "Ciência de Dados: pipelines de pré-processamento compartilhados."
                              ],
                              "realWorldApplication": "Na genômica, clustering agrupa genes por expressão sem labels, e PCA reduz dimensões de sequências genéticas para visualização, permitindo descoberta de subtipos de câncer sem supervisão prévia, ao contrário de modelos supervisionados para diagnóstico."
                            },
                            "estimatedTime": "1.5 horas",
                            "difficulty": "beginner",
                            "status": "not_started",
                            "prerequisites": []
                          },
                          {
                            "id": "10.1.1.3.2.3",
                            "name": "Relacionar aprendizado à modelagem",
                            "description": "Integrar conceitos de aprendizado com ajuste e avaliação de modelos, incluindo métricas básicas de performance.",
                            "atomicExpansion": {
                              "steps": [
                                {
                                  "stepNumber": 1,
                                  "title": "Revisar Conceitos Fundamentais de Modelagem e Aprendizado",
                                  "subSteps": [
                                    "Defina modelagem como a representação matemática de um problema real usando dados.",
                                    "Explique aprendizado como o processo de um modelo melhorar sua performance em dados não vistos via ajuste.",
                                    "Identifique i.i.d. (independent and identically distributed) como pressuposto chave para generalização.",
                                    "Diferencie treinamento, validação e teste em conjuntos de dados.",
                                    "Liste tipos básicos de aprendizado: supervisionado, não-supervisionado e reforço."
                                  ],
                                  "verification": "Crie um diagrama simples conectando modelagem a aprendizado e i.i.d., e explique oralmente para um par.",
                                  "estimatedTime": "45 minutos",
                                  "materials": "Notebook, quadro branco ou ferramenta de diagramação como Draw.io, notas de aula sobre MC-13.",
                                  "tips": "Use analogias cotidianas, como aprender a dirigir um carro (modelo) através de prática (aprendizado).",
                                  "learningObjective": "Compreender as bases conceituais que ligam modelagem ao aprendizado supervisionado.",
                                  "commonMistakes": "Confundir modelagem com mera coleta de dados; ignorar a importância de i.i.d. para predições futuras."
                                },
                                {
                                  "stepNumber": 2,
                                  "title": "Explorar o Processo de Ajuste de Modelos",
                                  "subSteps": [
                                    "Descreva o ajuste como otimização de parâmetros para minimizar erro em dados de treino.",
                                    "Introduza funções de perda (loss functions) como MSE para regressão ou cross-entropy para classificação.",
                                    "Explique gradiente descendente como algoritmo comum de ajuste.",
                                    "Discuta divisão de dados: 70% treino, 15% validação, 15% teste.",
                                    "Implemente um ajuste simples em Python com scikit-learn."
                                  ],
                                  "verification": "Execute um código de ajuste de modelo linear e plote a curva de aprendizado.",
                                  "estimatedTime": "60 minutos",
                                  "materials": "Python/Jupyter Notebook, bibliotecas scikit-learn e matplotlib, dataset simples como Iris ou Boston Housing.",
                                  "tips": "Monitore overfitting comparando loss de treino vs. validação em cada época.",
                                  "learningObjective": "Dominar como modelos são ajustados para capturar padrões de dados.",
                                  "commonMistakes": "Ajustar hiperparâmetros no conjunto de teste; não normalizar features antes do ajuste."
                                },
                                {
                                  "stepNumber": 3,
                                  "title": "Introduzir Métricas Básicas de Performance",
                                  "subSteps": [
                                    "Aprenda métricas de regressão: MSE, RMSE, MAE e R².",
                                    "Para classificação: accuracy, precision, recall, F1-score e matriz de confusão.",
                                    "Calcule métricas manualmente em um dataset pequeno.",
                                    "Interprete resultados: alto R² indica bom ajuste, mas cheque resíduos para viés.",
                                    "Use cross-validation para estimar performance robusta."
                                  ],
                                  "verification": "Gere um relatório com métricas calculadas para um modelo ajustado, incluindo interpretação.",
                                  "estimatedTime": "50 minutos",
                                  "materials": "Jupyter Notebook, scikit-learn metrics module, dataset com labels conhecidos.",
                                  "tips": "Sempre priorize métricas alinhadas ao problema de negócio, não só accuracy.",
                                  "learningObjective": "Selecionar e interpretar métricas adequadas para avaliar modelos.",
                                  "commonMistakes": "Ignorar desbalanceamento de classes em accuracy; confundir MSE com RMSE em escala."
                                },
                                {
                                  "stepNumber": 4,
                                  "title": "Integrar Aprendizado à Modelagem em um Fluxo Completo",
                                  "subSteps": [
                                    "Monte um pipeline: modelagem do problema → divisão de dados → ajuste → avaliação.",
                                    "Analise trade-offs: bias-variance, underfitting vs. overfitting.",
                                    "Ajuste hiperparâmetros via grid search com validação cruzada.",
                                    "Documente como aprendizado melhora a modelagem iterativamente.",
                                    "Reflita sobre limitações, como viés de dados ou pressupostos violados."
                                  ],
                                  "verification": "Desenvolva e avalie um modelo end-to-end, produzindo um relatório com conclusões.",
                                  "estimatedTime": "75 minutos",
                                  "materials": "Jupyter Notebook completo, datasets reais (Kaggle), Git para versionamento.",
                                  "tips": "Itere múltiplas vezes, ajustando baseado em métricas de validação.",
                                  "learningObjective": "Construir e avaliar um ciclo completo de modelagem impulsionado por aprendizado.",
                                  "commonMistakes": "Não iterar após primeira avaliação; superestimar performance sem CV."
                                }
                              ],
                              "practicalExample": "Em um projeto de previsão de preços de casas: modele o problema como regressão linear (features: tamanho, quartos; target: preço), ajuste coeficientes minimizando MSE, avalie com R² e RMSE no conjunto de teste, interpretando como o aprendizado refina a modelagem para predições precisas.",
                              "finalVerifications": [
                                "Explicar verbalmente como i.i.d. suporta a generalização do aprendizado à modelagem.",
                                "Calcular e interpretar MSE/R² para um modelo dado.",
                                "Identificar overfitting em uma curva de loss de treino/validação.",
                                "Montar um pipeline simples de ajuste e avaliação em código.",
                                "Discutir limitações de métricas básicas em cenários reais.",
                                "Relacionar ajuste de modelo a melhoria iterativa na representação do problema."
                              ],
                              "assessmentCriteria": [
                                "Precisão conceitual na ligação entre aprendizado e modelagem (80%+ acerto em quiz).",
                                "Qualidade do código: funcional, comentado e reproduzível.",
                                "Interpretação correta de métricas com exemplos numéricos.",
                                "Profundidade dos substeps executados e verificados.",
                                "Criatividade na integração de conceitos em pipeline prático.",
                                "Capacidade de identificar e mitigar erros comuns como overfitting."
                              ],
                              "crossCurricularConnections": [
                                "Estatística: Uso de distribuições i.i.d. e testes de hipótese em avaliação.",
                                "Programação: Implementação em Python/R com bibliotecas de ML.",
                                "Matemática: Otimização via gradiente descendente e cálculo de derivadas.",
                                "Ética e Sociedade: Viés em modelagem e implicações de performance em decisões reais."
                              ],
                              "realWorldApplication": "Data scientists usam essa integração em pipelines de ML para tarefas como detecção de fraudes em bancos (classificação com F1-score), previsão de demanda em e-commerce (regressão com RMSE) ou recomendação de produtos (aprendizado iterativo), garantindo modelos robustos e acionáveis em produção."
                            },
                            "estimatedTime": "2 horas",
                            "difficulty": "intermediate",
                            "status": "not_started",
                            "prerequisites": [
                              "10.1.1.3.1"
                            ]
                          }
                        ]
                      },
                      {
                        "id": "10.1.1.3.3",
                        "name": "Ambiente Independente e Identicamente Distribuído (i.i.d.)",
                        "description": "Conceito probabilístico fundamental em aprendizado de máquina, onde amostras de dados são independentes entre si e seguem a mesma distribuição estatística.",
                        "specificSkills": [
                          {
                            "id": "10.1.1.3.3.1",
                            "name": "Definir i.i.d.",
                            "description": "Explicar a definição formal de independência e distribuição idêntica, com exemplos matemáticos simples e importância para generalização de modelos.",
                            "atomicExpansion": {
                              "steps": [
                                {
                                  "stepNumber": 1,
                                  "title": "Compreender o Conceito de Independência em Probabilidade",
                                  "subSteps": [
                                    "Revise a definição de eventos independentes: P(A ∩ B) = P(A) * P(B).",
                                    "Estude exemplos simples, como lançar duas moedas justas.",
                                    "Pratique calculando probabilidades para eventos dependentes vs. independentes.",
                                    "Identifique quando assumir independência é razoável em dados reais.",
                                    "Discuta limitações da independência em cenários complexos."
                                  ],
                                  "verification": "Explique verbalmente ou por escrito a diferença entre eventos independentes e dependentes com um exemplo numérico correto.",
                                  "estimatedTime": "45 minutos",
                                  "materials": [
                                    "Livro de probabilidade básica",
                                    "Calculadora",
                                    "Papel e caneta para exemplos"
                                  ],
                                  "tips": "Use diagramas de Venn para visualizar interseções de eventos.",
                                  "learningObjective": "Definir e ilustrar independência probabilística com precisão.",
                                  "commonMistakes": [
                                    "Confundir independência com igualdade de probabilidades",
                                    "Ignorar correlações ocultas em dados"
                                  ]
                                },
                                {
                                  "stepNumber": 2,
                                  "title": "Explorar Distribuição Idêntica",
                                  "subSteps": [
                                    "Defina distribuição idêntica: variáveis aleatórias com a mesma função de densidade de probabilidade.",
                                    "Compare distribuições idênticas vs. diferentes, usando histogramas.",
                                    "Gere exemplos de amostras de uma mesma distribuição (ex: números aleatórios uniformes).",
                                    "Analise como mudanças na distribuição afetam estatísticas amostrais.",
                                    "Pratique identificando se amostras vêm da mesma distribuição via testes visuais."
                                  ],
                                  "verification": "Crie dois conjuntos de dados idênticos e não idênticos, e prove graficamente que têm a mesma distribuição.",
                                  "estimatedTime": "40 minutos",
                                  "materials": [
                                    "Software como Python (NumPy/Matplotlib)",
                                    "Planilha Excel para simulações"
                                  ],
                                  "tips": "Sempre plote histogramas para comparar distribuições visualmente.",
                                  "learningObjective": "Reconhecer e gerar variáveis com distribuições idênticas.",
                                  "commonMistakes": [
                                    "Assumir identidade só pela média igual",
                                    "Não considerar variância ou forma da distribuição"
                                  ]
                                },
                                {
                                  "stepNumber": 3,
                                  "title": "Definir Formalmente i.i.d.",
                                  "subSteps": [
                                    "Combine independência e distribuição idêntica na definição formal: {X_i} ~ i.i.d. se ∀ i ≠ j, X_i ⊥ X_j e todos têm a mesma distribuição.",
                                    "Escreva a notação matemática e prove com um exemplo de N(0,1).",
                                    "Discuta propriedades chave: soma de i.i.d. converge para normal (TCL).",
                                    "Formule hipóteses i.i.d. em contextos de aprendizado de máquina.",
                                    "Crie uma declaração formal própria da definição."
                                  ],
                                  "verification": "Escreva a definição matemática completa de i.i.d. e aplique a um exemplo dado.",
                                  "estimatedTime": "50 minutos",
                                  "materials": [
                                    "Notas de estatística",
                                    "Editor de LaTeX ou papel para fórmulas"
                                  ],
                                  "tips": "Memorize a notação: X_1, X_2, ..., X_n ∼ \text{i.i.d. } F.",
                                  "learningObjective": "Articular a definição formal de i.i.d. com notação precisa.",
                                  "commonMistakes": [
                                    "Omitir independência mútua para mais de duas variáveis",
                                    "Confundir i.i.d. com estacionariedade"
                                  ]
                                },
                                {
                                  "stepNumber": 4,
                                  "title": "Aplicar i.i.d. com Exemplos e Importância",
                                  "subSteps": [
                                    "Gere exemplos matemáticos simples: lançamentos de dados i.i.d.",
                                    "Simule violações de i.i.d. e observe impactos em médias amostrais.",
                                    "Explique importância para generalização em ML: dados de treino/teste assumem i.i.d.",
                                    "Discuta cenários reais onde i.i.d. falha (ex: séries temporais).",
                                    "Crie um exemplo personalizado envolvendo modelagem de problemas."
                                  ],
                                  "verification": "Forneça dois exemplos matemáticos de i.i.d. e explique por que a suposição é crucial para ML.",
                                  "estimatedTime": "55 minutos",
                                  "materials": [
                                    "Python para simulações (seeding random)",
                                    "Artigos introdutórios sobre ML assumptions"
                                  ],
                                  "tips": "Use sementes fixas em simulações para reprodutibilidade.",
                                  "learningObjective": "Ilustrar i.i.d. com exemplos e destacar sua relevância em ciência de dados.",
                                  "commonMistakes": [
                                    "Subestimar violações em dados reais",
                                    "Ignorar implicações para validade de modelos"
                                  ]
                                }
                              ],
                              "practicalExample": "Simule 1000 lançamentos de uma moeda justa em Python: cada lançamento é i.i.d. (independente e Bernoulli(p=0.5)). Calcule a média amostral e observe convergência para 0.5 pela Lei dos Grandes Números.",
                              "finalVerifications": [
                                "Define corretamente independência e distribuição idêntica.",
                                "Escreve a notação formal de i.i.d. sem erros.",
                                "Fornece pelo menos dois exemplos matemáticos simples.",
                                "Explica a importância para generalização em modelos de ML.",
                                "Identifica cenários onde i.i.d. não se aplica.",
                                "Simula um exemplo prático com resultados consistentes."
                              ],
                              "assessmentCriteria": [
                                "Precisão na definição formal (100% dos componentes corretos).",
                                "Qualidade e relevância dos exemplos matemáticos (clareza e simplicidade).",
                                "Profundidade na explicação da importância para ML.",
                                "Capacidade de simular e verificar i.i.d. computacionalmente.",
                                "Identificação correta de violações comuns.",
                                "Clareza na comunicação escrita/oral."
                              ],
                              "crossCurricularConnections": [
                                "Estatística: Teorema Central do Limite e inferência.",
                                "Aprendizado de Máquina: Suposições em validação cruzada.",
                                "Física: Modelos estocásticos em partículas.",
                                "Economia: Análise de séries financeiras (violações de i.i.d.).",
                                "Computação: Geração de dados sintéticos."
                              ],
                              "realWorldApplication": "Em machine learning, a suposição i.i.d. permite que modelos treinados em dados históricos generalizem para novos dados, como em previsões de vendas ou detecção de fraudes, onde violações exigem técnicas como séries temporais."
                            },
                            "estimatedTime": "1 hora",
                            "difficulty": "intermediate",
                            "status": "not_started",
                            "prerequisites": []
                          },
                          {
                            "id": "10.1.1.3.3.2",
                            "name": "Identificar violações de i.i.d.",
                            "description": "Reconhecer cenários onde a suposição i.i.d. é violada, como dados temporais ou dependentes, e discutir impactos na modelagem.",
                            "atomicExpansion": {
                              "steps": [
                                {
                                  "stepNumber": 1,
                                  "title": "Revisar a definição e componentes de i.i.d.",
                                  "subSteps": [
                                    "Defina independência: eventos ou amostras não influenciam uns aos outros.",
                                    "Defina distribuição idêntica: todas as amostras vêm da mesma distribuição probabilística.",
                                    "Explique por que i.i.d. é crucial para teoremas como Lei dos Grandes Números e Teorema Central do Limite."
                                  ],
                                  "verification": "Escreva uma definição precisa de i.i.d. em suas próprias palavras e cite um exemplo onde ela se aplica.",
                                  "estimatedTime": "20 minutos",
                                  "materials": [
                                    "Notas de aula sobre probabilidade básica",
                                    "Artigo introdutório sobre i.i.d. em ML (ex: scikit-learn docs)"
                                  ],
                                  "tips": "Use diagramas de Venn para visualizar independência vs. dependência.",
                                  "learningObjective": "Compreender os pilares fundamentais da suposição i.i.d.",
                                  "commonMistakes": "Confundir independência estatística com ausência de correlação."
                                },
                                {
                                  "stepNumber": 2,
                                  "title": "Identificar tipos comuns de violações de i.i.d.",
                                  "subSteps": [
                                    "Estude violações de independência: dados temporais (autocorrelação), dados espaciais (clusters geográficos).",
                                    "Estude violações de distribuição idêntica: mudanças conceituais (concept drift), amostragem não uniforme.",
                                    "Classifique violações compostas: dados de redes sociais com dependências temporais e sociais."
                                  ],
                                  "verification": "Liste 3 exemplos de cada tipo de violação com justificativa breve.",
                                  "estimatedTime": "30 minutos",
                                  "materials": [
                                    "Exemplos de datasets: time series (stocks), spatial (crime maps)",
                                    "Vídeo tutorial sobre time series vs i.i.d."
                                  ],
                                  "tips": "Crie uma tabela comparativa: i.i.d. vs. violado, com colunas para tipo e exemplo.",
                                  "learningObjective": "Reconhecer padrões de violações em diferentes contextos de dados.",
                                  "commonMistakes": "Ignorar violações sutis como seasonal effects em dados diários."
                                },
                                {
                                  "stepNumber": 3,
                                  "title": "Analisar cenários reais para detectar violações",
                                  "subSteps": [
                                    "Carregue um dataset temporal (ex: preços de ações) e plote autocorrelação.",
                                    "Examine um dataset espacial (ex: dados de sensores IoT) para padrões de cluster.",
                                    "Simule dados i.i.d. vs. não-i.i.d. e compare distribuições empíricas."
                                  ],
                                  "verification": "Gere plots ou testes estatísticos (ex: Durbin-Watson para autocorrelação) mostrando violação.",
                                  "estimatedTime": "45 minutos",
                                  "materials": [
                                    "Python/Jupyter com pandas, matplotlib, statsmodels",
                                    "Datasets Kaggle: stock prices, weather data"
                                  ],
                                  "tips": "Use testes como ADF para stationarity em time series.",
                                  "learningObjective": "Aplicar ferramentas analíticas para diagnosticar violações.",
                                  "commonMistakes": "Assumir i.i.d. sem plotar correlogramas ou histograms."
                                },
                                {
                                  "stepNumber": 4,
                                  "title": "Discutir impactos na modelagem e mitigações iniciais",
                                  "subSteps": [
                                    "Explique impactos: viés em validação cruzada, overfitting em dependências não modeladas.",
                                    "Discuta consequências: previsões ruins em time series, недоestimação de variância.",
                                    "Introduza mitigações básicas: usar validação temporal, modelos específicos (ARIMA, GNNs)."
                                  ],
                                  "verification": "Escreva um parágrafo sobre impacto em um modelo específico e sugira alternativa.",
                                  "estimatedTime": "25 minutos",
                                  "materials": [
                                    "Leitura: 'Elements of Statistical Learning' capítulo sobre assumptions",
                                    "Blog posts sobre pitfalls de i.i.d. em ML"
                                  ],
                                  "tips": "Pense em termos de risco: como a violação afeta o erro de generalização.",
                                  "learningObjective": "Avaliar consequências práticas e soluções preliminares.",
                                  "commonMistakes": "Subestimar impactos em modelos ensemble que mascaram problemas."
                                }
                              ],
                              "practicalExample": "Em um dataset de preços diários de ações da AAPL, plote a autocorrelação: lag-1 > 0.8 indica violação de independência (dados temporais dependentes), invalidando split aleatório para train/test e causando otimismo irreal na performance do modelo.",
                              "finalVerifications": [
                                "Corretamente identifica 3 cenários de violação em datasets fornecidos.",
                                "Explica impacto em pelo menos 2 algoritmos de ML (ex: regressão linear, KNN).",
                                "Propõe teste diagnóstico apropriado para cada violação.",
                                "Sugere mitgação adequada sem quebrar i.i.d. desnecessariamente.",
                                "Discute trade-offs de assumir i.i.d. vs. modelar dependências."
                              ],
                              "assessmentCriteria": [
                                "Precisão na definição e exemplos de i.i.d. (30%)",
                                "Diversidade e relevância de violações identificadas (25%)",
                                "Qualidade da análise prática com evidências visuais/estatísticas (20%)",
                                "Profundidade na discussão de impactos e mitigações (15%)",
                                "Clareza e estrutura na comunicação dos achados (10%)"
                              ],
                              "crossCurricularConnections": [
                                "Estatística: Testes de independência (Chi-quadrado, Runs test)",
                                "Aprendizado de Máquina: Validação cruzada temporal e walk-forward",
                                "Processamento de Sinais: Análise de séries temporais e Fourier",
                                "Probabilidade: Cadeias de Markov para modelar dependências"
                              ],
                              "realWorldApplication": "Em finanças, detectar violações de i.i.d. em dados de trading evita modelos falhos que preveem retornos independentes, ignorando crashes sequenciais; em saúde pública, identifica dependências espaciais em surtos de doenças para alertas geolocalizados precisos."
                            },
                            "estimatedTime": "1.5 horas",
                            "difficulty": "intermediate",
                            "status": "not_started",
                            "prerequisites": []
                          },
                          {
                            "id": "10.1.1.3.3.3",
                            "name": "Aplicar i.i.d. em contextos de aprendizado",
                            "description": "Relacionar i.i.d. com avaliação de modelos e teoria do aprendizado estatístico, usando exemplos de conjuntos de dados.",
                            "atomicExpansion": {
                              "steps": [
                                {
                                  "stepNumber": 1,
                                  "title": "Revisar a Definição e Propriedades de i.i.d.",
                                  "subSteps": [
                                    "Defina independência: eventos ou amostras não influenciam uns aos outros.",
                                    "Defina distribuição idêntica: todas as amostras vêm da mesma distribuição P(X,Y).",
                                    "Discuta implicações matemáticas: E[X_i] = E[X_j] e Cov(X_i, X_j) = 0 para i ≠ j.",
                                    "Identifique quando dados violam i.i.d., como em séries temporais.",
                                    "Resuma com uma fórmula: {X_1, ..., X_n} ~ i.i.d. P."
                                  ],
                                  "verification": "Escreva uma definição precisa de i.i.d. e dê um exemplo simples de violação.",
                                  "estimatedTime": "30 minutos",
                                  "materials": "Livro de probabilidade (ex: 'Introduction to Probability' de Blitzstein), notas de aula sobre distribuições.",
                                  "tips": "Use analogias como 'lançamentos de moeda justos' para independência.",
                                  "learningObjective": "Compreender os pilares matemáticos de i.i.d. para fundamentar aplicações em ML.",
                                  "commonMistakes": "Confundir independência com não-correlacionamento; assumir i.i.d. sem verificar."
                                },
                                {
                                  "stepNumber": 2,
                                  "title": "Conectar i.i.d. à Teoria do Aprendizado Estatístico",
                                  "subSteps": [
                                    "Explique o teorema central do limite sob i.i.d.: médias amostrais convergem para a expectativa.",
                                    "Discuta generalização: erro de expectativa ≈ erro empírico sob i.i.d.",
                                    "Introduza bounds como Hoeffding's inequality para probabilidades de erro ruim.",
                                    "Relacione com PAC learning: i.i.d. garante learnability com amostras suficientes.",
                                    "Calcule um exemplo simples: probabilidade de desvio médio > ε."
                                  ],
                                  "verification": "Derive qualitativamente por que i.i.d. permite bounds probabilísticos em generalização.",
                                  "estimatedTime": "45 minutos",
                                  "materials": "Artigo 'A Tutorial on Support Vector Machines' ou notas de Vapnik, calculadora ou Python para simulações.",
                                  "tips": "Visualize com histogramas de amostras i.i.d. vs. não-i.i.d.",
                                  "learningObjective": "Relacionar i.i.d. com garantias teóricas de aprendizado, como convergência e generalização.",
                                  "commonMistakes": "Ignorar que i.i.d. é uma suposição forte, não sempre realista."
                                },
                                {
                                  "stepNumber": 3,
                                  "title": "Aplicar i.i.d. na Avaliação de Modelos",
                                  "subSteps": [
                                    "Explique train/test split: assume i.i.d. para estimar performance out-of-sample.",
                                    "Implemente k-fold cross-validation como robustez sob i.i.d.",
                                    "Avalie impacto de violações: ex. drift de distribuição causa overfitting.",
                                    "Use métricas como expected calibration error (ECE) sensíveis a i.i.d.",
                                    "Compare hold-out vs. CV em código."
                                  ],
                                  "verification": "Execute um split train/test e explique por que i.i.d. valida a estimativa de erro.",
                                  "estimatedTime": "60 minutos",
                                  "materials": "Python com scikit-learn, Jupyter Notebook, dataset pequeno como Iris.",
                                  "tips": "Sempre shuffle dados antes de split para simular i.i.d.",
                                  "learningObjective": "Aplicar i.i.d. em práticas de avaliação para validar modelos confiáveis.",
                                  "commonMistakes": "Não embaralhar dados em splits, levando a vazamento temporal."
                                },
                                {
                                  "stepNumber": 4,
                                  "title": "Analisar Exemplos de Conjuntos de Dados",
                                  "subSteps": [
                                    "Carregue um dataset (ex: Boston Housing) e verifique i.i.d. visualmente (scatterplots).",
                                    "Teste independência com correlogramas ou Durbin-Watson.",
                                    "Simule violações: adicione dependência temporal e reavalie modelo.",
                                    "Compare performance de modelo antes/depois de correções (ex: bootstrapping).",
                                    "Documente insights em relatório curto."
                                  ],
                                  "verification": "Gere relatório com plots e conclusão sobre validade i.i.d. no dataset.",
                                  "estimatedTime": "75 minutos",
                                  "materials": "Python (pandas, matplotlib, statsmodels), dataset UCI ML Repository.",
                                  "tips": "Use QQ-plots para checar distribuição idêntica.",
                                  "learningObjective": "Diagnosticar e aplicar i.i.d. em dados reais para modelagem robusta.",
                                  "commonMistakes": "Assumir i.i.d. sem testes estatísticos; ignorar features correlacionadas."
                                }
                              ],
                              "practicalExample": "Usando o dataset Iris: assuma i.i.d., faça 80/20 train/test split de um modelo KNN. Treine, avalie accuracy no test set (esperado ~95%). Simule violação trocando ordem por clusters não-i.i.d., observe queda para ~70%. Explique via teoria: sem i.i.d., generalização falha.",
                              "finalVerifications": [
                                "Explicar verbalmente por que i.i.d. é essencial para train/test split.",
                                "Identificar 3 violações comuns de i.i.d. em dados reais.",
                                "Calcular um bound simples de Hoeffding para um cenário dado.",
                                "Implementar CV em código e interpretar resultados sob i.i.d.",
                                "Diagnosticar i.i.d. em um novo dataset com pelo menos 2 testes.",
                                "Relacionar falha i.i.d. a um erro de generalização em ML."
                              ],
                              "assessmentCriteria": [
                                "Precisão conceitual: definições corretas de i.i.d. e conexões teóricas (30%).",
                                "Profundidade analítica: uso correto de bounds e testes (25%).",
                                "Implementação prática: código funcional e limpo (20%).",
                                "Análise crítica: identificação de violações e impactos (15%).",
                                "Clareza de comunicação: relatórios e explicações concisas (10%)."
                              ],
                              "crossCurricularConnections": [
                                "Probabilidade e Estatística: teoremas de convergência e inequalities.",
                                "Ciência da Computação: algoritmos de ML e validação cruzada.",
                                "Matemática Aplicada: análise de séries temporais vs. i.i.d.",
                                "Física/Engenharia: modelagem de experimentos independentes."
                              ],
                              "realWorldApplication": "Em pipelines de ML de empresas como Google ou Netflix, i.i.d. valida recomendações e detecção de fraudes; violações (ex: dados sazonais em e-commerce) exigem técnicas como windowing ou domain adaptation para deployment confiável."
                            },
                            "estimatedTime": "1.5 horas",
                            "difficulty": "intermediate",
                            "status": "not_started",
                            "prerequisites": [
                              "10.1.1.3.2"
                            ]
                          }
                        ]
                      }
                    ]
                  },
                  {
                    "id": "10.1.1.4",
                    "name": "Etapas da Ciência de Dados",
                    "description": "Coleta, integração e armazenamento; análise exploratória e visualização; limpeza; ajuste e avaliação de modelos, com exemplos e estudos de caso.",
                    "individualConcepts": [
                      {
                        "id": "10.1.1.4.1",
                        "name": "Coleta, Integração e Armazenamento de Dados",
                        "description": "Compreender as etapas iniciais do ciclo de vida dos dados na Ciência de Dados, incluindo identificação de fontes, métodos de coleta, integração de dados heterogêneos e estratégias de armazenamento eficiente.",
                        "specificSkills": [
                          {
                            "id": "10.1.1.4.1.1",
                            "name": "Identificar fontes de dados",
                            "description": "Reconhecer e classificar diferentes fontes de dados (estruturados, não estruturados, semi-estruturados) como bancos de dados relacionais, APIs, arquivos CSV/JSON e sensores IoT, considerando aspectos de qualidade e relevância para o problema de negócio.",
                            "atomicExpansion": {
                              "steps": [
                                {
                                  "stepNumber": 1,
                                  "title": "Compreender os tipos fundamentais de fontes de dados",
                                  "subSteps": [
                                    "Defina dados estruturados: dados organizados em tabelas com esquemas fixos, como linhas e colunas.",
                                    "Defina dados não estruturados: dados sem formato pré-definido, como textos, imagens ou vídeos.",
                                    "Defina dados semi-estruturados: dados com tags ou marcadores, mas sem esquema rígido, como XML ou JSON.",
                                    "Compare os três tipos com exemplos simples de cada um.",
                                    "Crie um diagrama mental ou tabela comparativa dos tipos."
                                  ],
                                  "verification": "Crie uma tabela comparativa com definições e um exemplo para cada tipo; revise se cobre todos os três.",
                                  "estimatedTime": "20 minutos",
                                  "materials": [
                                    "Papel e caneta ou editor de texto como Google Docs",
                                    "Recursos online: artigos sobre tipos de dados (ex: Wikipedia 'Big Data')"
                                  ],
                                  "tips": "Use mnemônicos: Estruturado = Tabelas (SQL), Não estruturado = Caos (textos), Semi = Meio-termo (JSON).",
                                  "learningObjective": "Diferenciar com precisão os três tipos principais de dados e fornecer definições claras.",
                                  "commonMistakes": [
                                    "Confundir semi-estruturado com estruturado; ignorar exemplos reais como imagens para não estruturado."
                                  ]
                                },
                                {
                                  "stepNumber": 2,
                                  "title": "Identificar exemplos comuns de fontes de dados",
                                  "subSteps": [
                                    "Liste fontes estruturadas: bancos de dados relacionais (SQL como MySQL), arquivos CSV.",
                                    "Liste fontes semi-estruturadas: APIs (REST/GraphQL), arquivos JSON/XML.",
                                    "Liste fontes não estruturadas: sensores IoT (logs de temperatura), documentos de texto, imagens.",
                                    "Classifique 5 fontes hipotéticas em uma das categorias.",
                                    "Pesquise um exemplo real de cada fonte online."
                                  ],
                                  "verification": "Compile uma lista de pelo menos 3 exemplos por categoria e classifique corretamente 5 fontes dadas.",
                                  "estimatedTime": "25 minutos",
                                  "materials": [
                                    "Navegador web",
                                    "Sites como Stack Overflow ou Towards Data Science para exemplos"
                                  ],
                                  "tips": "Pense em 'estrutura': se cabe em Excel facilmente, é estruturado; se é texto livre, não estruturado.",
                                  "learningObjective": "Reconhecer e categorizar fontes específicas como bancos relacionais, APIs, CSV/JSON e sensores IoT.",
                                  "commonMistakes": [
                                    "Classificar APIs como não estruturadas (elas são semi); esquecer sensores IoT como não estruturados."
                                  ]
                                },
                                {
                                  "stepNumber": 3,
                                  "title": "Avaliar qualidade e relevância das fontes",
                                  "subSteps": [
                                    "Defina critérios de qualidade: precisão, completude, atualidade, consistência e acessibilidade.",
                                    "Para relevância: verifique se a fonte atende ao problema de negócio (ex: dados de vendas para previsão).",
                                    "Aplique critérios a 3 exemplos: pontue cada em uma escala de 1-5.",
                                    "Identifique fontes ruins: dados desatualizados ou irrelevantes.",
                                    "Documente trade-offs entre qualidade e relevância."
                                  ],
                                  "verification": "Avalie 3 fontes fictícias com pontuações e justificativas para cada critério.",
                                  "estimatedTime": "30 minutos",
                                  "materials": [
                                    "Planilha Excel ou Google Sheets para pontuação",
                                    "Lista de problemas de negócio exemplo (ex: churn de clientes)"
                                  ],
                                  "tips": "Use o acrônimo PACCA (Precisão, Atualidade, Completude, Consistência, Acessibilidade) para lembrar critérios.",
                                  "learningObjective": "Aplicar métricas de qualidade e relevância para julgar fontes de dados.",
                                  "commonMistakes": [
                                    "Ignorar acessibilidade (ex: API paga); superestimar relevância sem alinhar ao negócio."
                                  ]
                                },
                                {
                                  "stepNumber": 4,
                                  "title": "Classificar fontes para um problema de negócio específico",
                                  "subSteps": [
                                    "Escolha um problema de negócio (ex: prever demanda de produtos).",
                                    "Liste 5-7 fontes potenciais e classifique por tipo.",
                                    "Avalie qualidade/relevância para o problema.",
                                    "Priorize as top 3 fontes com justificativa.",
                                    "Crie um relatório resumido de 1 página."
                                  ],
                                  "verification": "Produza um relatório priorizando fontes com classificações e avaliações corretas.",
                                  "estimatedTime": "35 minutos",
                                  "materials": [
                                    "Editor de texto ou ferramenta de apresentação",
                                    "Exemplos de problemas de negócio de sites como Kaggle"
                                  ],
                                  "tips": "Sempre volte ao problema: 'Essa fonte responde à pergunta de negócio?'",
                                  "learningObjective": "Integrar classificação e avaliação para selecionar fontes adequadas a cenários reais.",
                                  "commonMistakes": [
                                    "Não priorizar; misturar tipos sem avaliar relevância ao negócio."
                                  ]
                                }
                              ],
                              "practicalExample": "Em uma loja online analisando churn de clientes: classifique banco SQL (estruturado, alta qualidade), logs de chat (não estruturado, média relevância), API de pagamentos (semi-estruturado, alta qualidade) e priorize SQL e API por relevância ao problema.",
                              "finalVerifications": [
                                "Classifique corretamente 5 fontes em tipos (estruturado/não/semi).",
                                "Liste e explique 4 critérios de qualidade de dados.",
                                "Avalie relevância de 3 fontes para um problema de negócio dado.",
                                "Priorize fontes para um cenário real com justificativa.",
                                "Identifique erros comuns em uma classificação fornecida.",
                                "Crie uma tabela comparativa de fontes com exemplos."
                              ],
                              "assessmentCriteria": [
                                "Precisão na classificação de tipos de dados (90%+ acurácia).",
                                "Completude na avaliação de qualidade (todos os critérios cobertos).",
                                "Relevância alinhada ao contexto de negócio (justificativas lógicas).",
                                "Profundidade nos sub-steps (mínimo 4 por step).",
                                "Clareza na verificação e priorização de fontes.",
                                "Uso correto de exemplos reais (IoT, APIs, etc.)."
                              ],
                              "crossCurricularConnections": [
                                "Programação: Integração com SQL/Python para acessar fontes.",
                                "Estatística: Análise de qualidade de dados para modelagem.",
                                "Negócios: Alinhamento de fontes ao ROI e problemas empresariais.",
                                "Tecnologia: Conhecimentos em IoT e APIs web.",
                                "Ética: Considerações de privacidade em fontes de dados."
                              ],
                              "realWorldApplication": "Em empresas como Netflix ou Amazon, identificar fontes como logs de usuário (não estruturados), bancos de transações (estruturados) e APIs de recomendação (semi-estruturados) para otimizar recomendações personalizadas, avaliando qualidade para evitar decisões baseadas em dados ruins."
                            },
                            "estimatedTime": "1 hora",
                            "difficulty": "beginner",
                            "status": "not_started",
                            "prerequisites": []
                          },
                          {
                            "id": "10.1.1.4.1.2",
                            "name": "Aplicar métodos de coleta de dados",
                            "description": "Implementar técnicas de coleta como web scraping, consultas SQL, uso de bibliotecas como Pandas e Requests em Python, garantindo conformidade com ética e privacidade (ex.: LGPD).",
                            "atomicExpansion": {
                              "steps": [
                                {
                                  "stepNumber": 1,
                                  "title": "Compreender princípios éticos e legais na coleta de dados",
                                  "subSteps": [
                                    "Estude leis de privacidade como LGPD e GDPR.",
                                    "Identifique fontes de dados públicas e permissões necessárias.",
                                    "Analise riscos éticos como viés e consentimento.",
                                    "Defina diretrizes pessoais para coleta responsável.",
                                    "Crie um checklist de conformidade ética."
                                  ],
                                  "verification": "Checklist preenchido e assinado confirmando compreensão.",
                                  "estimatedTime": "1 hora",
                                  "materials": [
                                    "Documentação LGPD oficial",
                                    "Artigos sobre ética em dados",
                                    "Notebook para anotações"
                                  ],
                                  "tips": "Sempre priorize dados públicos antes de scraping privado.",
                                  "learningObjective": "Identificar e aplicar regras éticas e legais para coleta de dados.",
                                  "commonMistakes": [
                                    "Ignorar termos de uso de sites",
                                    "Coletar dados pessoais sem consentimento",
                                    "Não documentar fontes"
                                  ]
                                },
                                {
                                  "stepNumber": 2,
                                  "title": "Configurar ambiente Python para coleta de dados",
                                  "subSteps": [
                                    "Instale Python e um IDE como VS Code ou Jupyter Notebook.",
                                    "Instale bibliotecas: pip install requests pandas beautifulsoup4 sqlite3.",
                                    "Crie um projeto com estrutura de pastas (data/, scripts/).",
                                    "Teste importações e crie um script básico de teste.",
                                    "Configure virtual environment com venv."
                                  ],
                                  "verification": "Script de teste roda sem erros e importa todas bibliotecas.",
                                  "estimatedTime": "30 minutos",
                                  "materials": [
                                    "Python 3.8+",
                                    "pip",
                                    "Git para versionamento opcional"
                                  ],
                                  "tips": "Use virtualenv para isolar dependências por projeto.",
                                  "learningObjective": "Preparar um ambiente de desenvolvimento funcional para ciência de dados.",
                                  "commonMistakes": [
                                    "Não usar virtualenv levando a conflitos",
                                    "Esquecer de atualizar pip",
                                    "Ignorar erros de instalação"
                                  ]
                                },
                                {
                                  "stepNumber": 3,
                                  "title": "Implementar coleta via APIs e bibliotecas Requests/Pandas",
                                  "subSteps": [
                                    "Escolha uma API pública (ex: JSONPlaceholder ou IBGE).",
                                    "Escreva código com Requests para GET requests.",
                                    "Parse JSON response e carregue em Pandas DataFrame.",
                                    "Trate erros como 404 ou rate limits.",
                                    "Salve dados em CSV com Pandas."
                                  ],
                                  "verification": "DataFrame com dados válidos salvo em CSV legível.",
                                  "estimatedTime": "1.5 horas",
                                  "materials": [
                                    "API pública como https://jsonplaceholder.typicode.com",
                                    "Bibliotecas requests e pandas"
                                  ],
                                  "tips": "Use headers com User-Agent para simular navegador.",
                                  "learningObjective": "Coletar e estruturar dados de fontes API de forma eficiente.",
                                  "commonMistakes": [
                                    "Não tratar exceções HTTP",
                                    "Ignorar limites de taxa",
                                    "Parse incorreto de JSON"
                                  ]
                                },
                                {
                                  "stepNumber": 4,
                                  "title": "Realizar web scraping ético com Requests e BeautifulSoup",
                                  "subSteps": [
                                    "Selecione um site público (ex: Wikipedia).",
                                    "Faça request com Requests e parse HTML com BeautifulSoup.",
                                    "Extraia elementos específicos com seletores CSS/XPath.",
                                    "Implemente delays com time.sleep para evitar overload.",
                                    "Armazene dados extraídos em lista ou DataFrame."
                                  ],
                                  "verification": "Dados scrapeados corretamente extraídos e salvos sem erros.",
                                  "estimatedTime": "2 horas",
                                  "materials": [
                                    "Bibliotecas requests, beautifulsoup4",
                                    "Site de teste como quotes.toscrape.com"
                                  ],
                                  "tips": "Verifique robots.txt do site antes de scrape.",
                                  "learningObjective": "Extrair dados não-estruturados de páginas web respeitando limites.",
                                  "commonMistakes": [
                                    "Scraping sem delay causando ban",
                                    "Seletores frágeis que quebram com mudanças",
                                    "Não respeitar robots.txt"
                                  ]
                                },
                                {
                                  "stepNumber": 5,
                                  "title": "Executar consultas SQL para coleta de dados relacionais",
                                  "subSteps": [
                                    "Crie um banco SQLite de exemplo com dados simulados.",
                                    "Conecte com sqlite3 e execute SELECT queries básicas.",
                                    "Use Pandas read_sql para carregar resultados em DataFrame.",
                                    "Implemente queries JOIN e filtros WHERE avançados.",
                                    "Valide integridade dos dados coletados."
                                  ],
                                  "verification": "Query retorna DataFrame com dados corretos e sem duplicatas.",
                                  "estimatedTime": "1.5 horas",
                                  "materials": [
                                    "Biblioteca sqlite3 (built-in)",
                                    "Pandas",
                                    "Ferramenta DB Browser for SQLite opcional"
                                  ],
                                  "tips": "Sempre use parâmetros em queries para evitar SQL injection.",
                                  "learningObjective": "Coletar dados de bancos relacionais usando SQL e Python.",
                                  "commonMistakes": [
                                    "Queries sem LIMIT causando lentidão",
                                    "Erro de sintaxe SQL",
                                    "Não fechar conexões DB"
                                  ]
                                }
                              ],
                              "practicalExample": "Colete dados de preços de produtos de um e-commerce público via scraping (respeitando robots.txt), integre com API de câmbio via Requests, carregue em Pandas e consulte via SQL em SQLite para análise de tendências de preço.",
                              "finalVerifications": [
                                "Todos scripts rodam sem erros e produzem dados válidos.",
                                "Checklist ético preenchido e conformidade com LGPD verificada.",
                                "Dados salvos em formatos CSV/SQLite acessíveis.",
                                "Tempo de execução respeita limites éticos (sem overload).",
                                "DataFrame final tem shape e amostra corretas.",
                                "Documentação do processo escrita em README."
                              ],
                              "assessmentCriteria": [
                                "Precisão e completude dos dados coletados (90%+ acurácia).",
                                "Adesão a práticas éticas e legais (sem violações).",
                                "Eficiência do código (otimizado, sem loops desnecessários).",
                                "Qualidade da estrutura de dados (limpa, sem NaNs excessivos).",
                                "Documentação clara e reproduzível.",
                                "Tratamento robusto de erros e exceções."
                              ],
                              "crossCurricularConnections": [
                                "Ética e Direito Digital (LGPD/GDPR).",
                                "Programação e Estruturas de Dados.",
                                "Estatística Descritiva para validação.",
                                "Banco de Dados e Modelagem Relacional.",
                                "Análise de Dados e Visualização."
                              ],
                              "realWorldApplication": "Em marketing digital para monitorar concorrentes, em pesquisa acadêmica para datasets públicos, ou em fintech para coleta de cotações financeiras automatizadas, sempre garantindo compliance regulatório."
                            },
                            "estimatedTime": "2 horas",
                            "difficulty": "beginner",
                            "status": "not_started",
                            "prerequisites": []
                          },
                          {
                            "id": "10.1.1.4.1.3",
                            "name": "Realizar integração de dados",
                            "description": "Executar processos de ETL (Extract, Transform, Load) para unir dados de múltiplas fontes, lidando com duplicatas, normalização e resolução de esquemas conflitantes usando ferramentas como Pandas ou Apache Airflow.",
                            "atomicExpansion": {
                              "steps": [
                                {
                                  "stepNumber": 1,
                                  "title": "Extrair dados de múltiplas fontes",
                                  "subSteps": [
                                    "Identifique as fontes de dados (ex: CSV, SQL, API).",
                                    "Instale e importe bibliotecas necessárias como Pandas e SQLAlchemy.",
                                    "Escreva scripts para extrair dados: pd.read_csv() para arquivos, pd.read_sql() para bancos.",
                                    "Salve os dados extraídos em DataFrames separados.",
                                    "Verifique a integridade inicial dos dados extraídos."
                                  ],
                                  "verification": "DataFrames populados com dados corretos e sem erros de leitura.",
                                  "estimatedTime": "1 hora",
                                  "materials": [
                                    "Python com Pandas instalado",
                                    "Arquivos CSV de exemplo",
                                    "Acesso a banco de dados SQL simulado"
                                  ],
                                  "tips": "Use caminhos relativos para arquivos e teste conexões SQL antes.",
                                  "learningObjective": "Compreender e executar a fase Extract do ETL de forma eficiente.",
                                  "commonMistakes": [
                                    "Esquecer de tratar encoding em CSV",
                                    "Conexão SQL com credenciais erradas",
                                    "Não limitar linhas para testes iniciais"
                                  ]
                                },
                                {
                                  "stepNumber": 2,
                                  "title": "Realizar transformações e limpeza inicial",
                                  "subSteps": [
                                    "Inspecione os DataFrames com info(), describe() e head().",
                                    "Trate valores nulos: dropna() ou fillna().",
                                    "Normalize formatos: padronize datas com pd.to_datetime(), strings com str.lower().",
                                    "Renomeie colunas para padronização entre fontes.",
                                    "Aplique funções de limpeza personalizadas (ex: remover acentos)."
                                  ],
                                  "verification": "DataFrames limpos com tipos de dados consistentes e sem nulos excessivos.",
                                  "estimatedTime": "1.5 horas",
                                  "materials": [
                                    "Jupyter Notebook",
                                    "Dados extraídos dos steps anteriores"
                                  ],
                                  "tips": "Crie cópias dos DataFrames originais com .copy() para evitar warnings.",
                                  "learningObjective": "Dominar técnicas de limpeza e normalização de dados heterogêneos.",
                                  "commonMistakes": [
                                    "Ignorar tipos de dados mistos",
                                    "Preencher nulos sem contexto",
                                    "Alterar dados originais acidentalmente"
                                  ]
                                },
                                {
                                  "stepNumber": 3,
                                  "title": "Lidar com duplicatas e resolver conflitos de esquema",
                                  "subSteps": [
                                    "Identifique duplicatas com duplicated() e remova com drop_duplicates().",
                                    "Compare esquemas: alinhe colunas comuns e crie mapeamentos para conflitantes.",
                                    "Use merge() ou concat() para testar uniões parciais.",
                                    "Resolva conflitos: priorize valores (ex: mais recente) ou agregue.",
                                    "Valide a resolução com asserts ou comparações manuais."
                                  ],
                                  "verification": "Nenhuma duplicata remanescente e esquemas unificados sem perda de dados crítica.",
                                  "estimatedTime": "2 horas",
                                  "materials": [
                                    "Pandas DataFrames limpos",
                                    "Dicionário de mapeamento de colunas"
                                  ],
                                  "tips": "Defina uma chave primária (ex: ID cliente) para identificar duplicatas precisamente.",
                                  "learningObjective": "Resolver desafios comuns de integração como duplicatas e esquemas conflitantes.",
                                  "commonMistakes": [
                                    "Remover duplicatas sem chave única",
                                    "Perder dados em merges inner",
                                    "Ignorar conflitos de tipo em colunas"
                                  ]
                                },
                                {
                                  "stepNumber": 4,
                                  "title": "Integrar dados e realizar o Load",
                                  "subSteps": [
                                    "Una os DataFrames transformados com pd.concat() ou merge().",
                                    "Realize validações finais: shape, dtypes e amostras.",
                                    "Salve o dataset integrado: to_csv(), to_sql() ou para Airflow DAG.",
                                    "Documente o processo em um relatório ou README.",
                                    "Teste o pipeline com dados novos para reproducibilidade."
                                  ],
                                  "verification": "Arquivo ou tabela final carregada corretamente e acessível.",
                                  "estimatedTime": "1 hora",
                                  "materials": [
                                    "DataFrames integrados",
                                    "Destino de storage (CSV/SQL)"
                                  ],
                                  "tips": "Use index=False em to_csv() e if_exists='replace' em to_sql().",
                                  "learningObjective": "Executar a fase Load e validar o pipeline ETL completo.",
                                  "commonMistakes": [
                                    "Sobrescrever dados sem backup",
                                    "Índices duplicados em concat",
                                    "Não versionar o output"
                                  ]
                                }
                              ],
                              "practicalExample": "Integre dados de vendas: um CSV de e-commerce (colunas: cliente_id, data_venda, valor) e uma tabela SQL de loja física (colunas: id_cliente, data, total). Normalize datas para 'YYYY-MM-DD', remova duplicatas por cliente_id + data, resolva 'valor' vs 'total' unificando nomes, e carregue em um CSV unificado para análise.",
                              "finalVerifications": [
                                "Dataset final sem duplicatas (duplicated().sum() == 0)",
                                "Todas colunas com tipos de dados consistentes (df.dtypes)",
                                "Número total de linhas lógico (soma das fontes menos duplicatas)",
                                "Amostras aleatórias inspecionadas manualmente",
                                "Pipeline roda sem erros em execução repetida",
                                "Dados carregados acessíveis e legíveis"
                              ],
                              "assessmentCriteria": [
                                "Precisão na extração (100% dos dados originais capturados)",
                                "Eficiência das transformações (tempo < 5s para 10k linhas)",
                                "Qualidade da resolução de conflitos (zero perda injustificada)",
                                "Robustez do Load (funciona em múltiplos formatos)",
                                "Documentação clara do processo",
                                "Tratamento completo de erros comuns"
                              ],
                              "crossCurricularConnections": [
                                "Programação em Python (manipulação de dados com Pandas)",
                                "Banco de Dados (SQL queries e integração)",
                                "Estatística (validação de dados e agregações)",
                                "Gestão de Projetos (pipelines ETL automatizados com Airflow)"
                              ],
                              "realWorldApplication": "Em empresas de varejo, integra dados de CRM, e-commerce e PDV para criar visões unificadas de clientes, permitindo análises de churn e campanhas personalizadas em ferramentas de BI como Tableau."
                            },
                            "estimatedTime": "2 horas",
                            "difficulty": "intermediate",
                            "status": "not_started",
                            "prerequisites": []
                          },
                          {
                            "id": "10.1.1.4.1.4",
                            "name": "Escolher estratégias de armazenamento",
                            "description": "Selecionar e configurar opções de armazenamento como bancos NoSQL (MongoDB), data warehouses (BigQuery) ou data lakes (S3), avaliando escalabilidade, custo e acesso rápido para análise.",
                            "atomicExpansion": {
                              "steps": [
                                {
                                  "stepNumber": 1,
                                  "title": "Compreender os tipos principais de armazenamento",
                                  "subSteps": [
                                    "Estude bancos NoSQL como MongoDB: documentos flexíveis para dados semi-estruturados.",
                                    "Analise data warehouses como BigQuery: otimizados para consultas SQL em grandes volumes estruturados.",
                                    "Explore data lakes como S3: armazenamento de objetos escalável para dados brutos em qualquer formato.",
                                    "Compare características básicas: estrutura de dados, latência de acesso e casos de uso típicos.",
                                    "Identifique diferenças em termos de esquema (fixo vs. flexível) e processamento."
                                  ],
                                  "verification": "Liste as características principais de cada tipo em uma tabela comparativa.",
                                  "estimatedTime": "2 horas",
                                  "materials": "Documentação oficial do MongoDB, BigQuery e S3; vídeos tutoriais no YouTube.",
                                  "tips": "Use diagramas visuais para mapear fluxos de dados em cada sistema.",
                                  "learningObjective": "Diferenciar tipos de armazenamento e seus formatos suportados.",
                                  "commonMistakes": "Confundir data lakes com bancos relacionais; ignorar suporte a dados não estruturados."
                                },
                                {
                                  "stepNumber": 2,
                                  "title": "Identificar critérios de avaliação chave",
                                  "subSteps": [
                                    "Defina escalabilidade: capacidade de crescer horizontalmente sem downtime.",
                                    "Avalie custo: preço por armazenamento, transferência e queries (ex: GB/mês).",
                                    "Meça acesso rápido: latência para leitura/escrita e integração com ferramentas de análise.",
                                    "Considere outros fatores: segurança, conformidade (GDPR) e facilidade de integração.",
                                    "Crie uma matriz de critérios ponderados para priorizar necessidades do projeto."
                                  ],
                                  "verification": "Crie uma planilha com critérios e pesos para um cenário hipotético.",
                                  "estimatedTime": "1.5 horas",
                                  "materials": "Calculadoras de custo do AWS S3, Google Cloud Pricing; planilhas Excel/Google Sheets.",
                                  "tips": "Atribua pesos numéricos aos critérios baseados no contexto do projeto.",
                                  "learningObjective": "Estabelecer métricas quantitativas e qualitativas para decisão.",
                                  "commonMistakes": "Ignorar custos ocultos como egresso de dados; superestimar apenas escalabilidade."
                                },
                                {
                                  "stepNumber": 3,
                                  "title": "Comparar opções para um caso de uso específico",
                                  "subSteps": [
                                    "Escolha um dataset exemplo (ex: logs de usuários com 500GB).",
                                    "Simule cenários: custo anual para cada opção usando calculadoras oficiais.",
                                    "Teste performance: tempo de query em BigQuery vs. scan em S3.",
                                    "Avalie escalabilidade: projeção para 10x crescimento em 2 anos.",
                                    "Registre prós/contras em uma tabela comparativa detalhada."
                                  ],
                                  "verification": "Gere relatório com scores numéricos para cada opção (0-10).",
                                  "estimatedTime": "3 horas",
                                  "materials": "Contas gratuitas tier no GCP/AWS; datasets sample do Kaggle.",
                                  "tips": "Use ferramentas como AWS Cost Explorer para simulações realistas.",
                                  "learningObjective": "Aplicar critérios para ranquear opções de armazenamento.",
                                  "commonMistakes": "Não considerar volume de dados futuro; focar só em preço inicial."
                                },
                                {
                                  "stepNumber": 4,
                                  "title": "Selecionar e planejar configuração da estratégia escolhida",
                                  "subSteps": [
                                    "Justifique escolha com base na comparação (ex: S3 para data lake por custo/escalabilidade).",
                                    "Planeje configuração: buckets/partições no S3, índices no MongoDB ou schemas no BigQuery.",
                                    "Defina políticas de acesso: IAM roles, encryption at rest.",
                                    "Esboce pipeline de ingestão: de fonte para armazenamento escolhido.",
                                    "Documente plano de migração e monitoramento futuro."
                                  ],
                                  "verification": "Crie um diagrama de arquitetura e script de setup inicial.",
                                  "estimatedTime": "2.5 horas",
                                  "materials": "Ferramentas de diagrama como Draw.io; CLI do AWS/GCP.",
                                  "tips": "Comece com configurações mínimas viáveis (MVP) para testes.",
                                  "learningObjective": "Planejar implementação prática da estratégia selecionada.",
                                  "commonMistakes": "Subestimar configuração de segurança; pular documentação."
                                }
                              ],
                              "practicalExample": "Em um projeto de análise de e-commerce com 2TB de logs de cliques (dados não estruturados), compare MongoDB (para queries flexíveis), BigQuery (para análises SQL rápidas) e S3 (para armazenamento barato e escalável). Escolha S3 + Athena por custo 70% menor e acesso rápido via queries serverless.",
                              "finalVerifications": [
                                "Pode listar 3 prós e 3 contras de cada tipo de armazenamento.",
                                "Calcula custo aproximado para um dataset de 1TB por 12 meses em cada opção.",
                                "Explica quando usar NoSQL vs. data warehouse em um cenário dado.",
                                "Cria uma tabela comparativa com scores para escalabilidade, custo e performance.",
                                "Planeja configuração básica para a opção escolhida.",
                                "Identifica integração com ferramentas de análise como Pandas ou Spark."
                              ],
                              "assessmentCriteria": [
                                "Precisão na diferenciação de tipos de armazenamento (80%+ acerto).",
                                "Uso correto de critérios quantitativos (cálculos de custo precisos).",
                                "Profundidade na comparação com evidências (tabelas/diagramas).",
                                "Justificativa lógica e contextual da escolha final.",
                                "Plano de configuração completo e viável.",
                                "Identificação de riscos e mitigações."
                              ],
                              "crossCurricularConnections": [
                                "Matemática: Modelos estatísticos para previsão de crescimento de dados e custos.",
                                "Programação: Scripts Python para ingestão e queries em MongoDB/S3.",
                                "Negócios: Análise de ROI em escolhas de infraestrutura de dados.",
                                "Ética/Segurança: Conformidade com LGPD em armazenamento de dados sensíveis."
                              ],
                              "realWorldApplication": "Empresas como Netflix usam data lakes (S3) para petabytes de dados de streaming, permitindo análises escaláveis sem alto custo, enquanto bancos como Nubank optam por BigQuery para relatórios financeiros em tempo real."
                            },
                            "estimatedTime": "1 hora",
                            "difficulty": "intermediate",
                            "status": "not_started",
                            "prerequisites": []
                          }
                        ]
                      },
                      {
                        "id": "10.1.1.4.2",
                        "name": "Análise Exploratória e Visualização de Dados",
                        "description": "Explorar dados para identificar padrões, anomalias e insights iniciais por meio de estatísticas descritivas e representações visuais, preparando o terreno para modelagem.",
                        "specificSkills": [
                          {
                            "id": "10.1.1.4.2.1",
                            "name": "Calcular estatísticas descritivas",
                            "description": "Computar medidas como média, mediana, desvio padrão, quartis e distribuições usando bibliotecas como Pandas e NumPy, interpretando resultados para entender a estrutura dos dados.",
                            "atomicExpansion": {
                              "steps": [
                                {
                                  "stepNumber": 1,
                                  "title": "Configurar ambiente e carregar dados",
                                  "subSteps": [
                                    "Instalar bibliotecas NumPy e Pandas via pip se necessário.",
                                    "Importar as bibliotecas no Jupyter Notebook ou script Python.",
                                    "Carregar um dataset de exemplo (ex: CSV com dados numéricos como notas de alunos).",
                                    "Inspecionar os dados com .head(), .info() e .describe() para entender a estrutura.",
                                    "Tratar valores ausentes com .fillna() ou .dropna() se aplicável."
                                  ],
                                  "verification": "Dataset carregado e visualizado corretamente sem erros, com shape e tipos de dados confirmados.",
                                  "estimatedTime": "20 minutos",
                                  "materials": [
                                    "Jupyter Notebook ou IDE Python",
                                    "Dataset CSV de exemplo (ex: iris.csv ou notas_alunos.csv)",
                                    "Bibliotecas NumPy e Pandas"
                                  ],
                                  "tips": "Sempre use pd.set_option('display.max_columns', None) para visualizar todas as colunas.",
                                  "learningObjective": "Configurar ambiente Python para análise de dados e carregar datasets de forma eficiente.",
                                  "commonMistakes": [
                                    "Esquecer de importar bibliotecas",
                                    "Ignorar valores ausentes",
                                    "Carregar arquivo com caminho incorreto"
                                  ]
                                },
                                {
                                  "stepNumber": 2,
                                  "title": "Calcular medidas de tendência central",
                                  "subSteps": [
                                    "Selecionar uma coluna numérica do DataFrame (ex: df['notas']).",
                                    "Calcular média com df['coluna'].mean() e np.mean(array).",
                                    "Calcular mediana com df['coluna'].median() e np.median(array).",
                                    "Calcular moda com df['coluna'].mode() se aplicável.",
                                    "Armazenar resultados em um dicionário ou DataFrame para comparação."
                                  ],
                                  "verification": "Valores de média, mediana e moda calculados e exibidos corretamente, comparados manualmente com cálculos simples.",
                                  "estimatedTime": "25 minutos",
                                  "materials": [
                                    "DataFrame carregado do Step 1",
                                    "Documentação Pandas/NumPy"
                                  ],
                                  "tips": "Use round(2) para limitar casas decimais na exibição.",
                                  "learningObjective": "Dominar o cálculo preciso de média, mediana e moda usando Pandas e NumPy.",
                                  "commonMistakes": [
                                    "Confundir mean() com median()",
                                    "Aplicar em colunas não numéricas",
                                    "Ignorar dados categóricos para moda"
                                  ]
                                },
                                {
                                  "stepNumber": 3,
                                  "title": "Calcular medidas de dispersão e posição",
                                  "subSteps": [
                                    "Calcular desvio padrão com df['coluna'].std() e np.std(array, ddof=1).",
                                    "Calcular variância com df['coluna'].var() e np.var(array, ddof=1).",
                                    "Calcular quartis com df['coluna'].quantile([0.25, 0.5, 0.75]) ou np.percentile().",
                                    "Gerar resumo completo com df['coluna'].describe().",
                                    "Identificar assimetrias ou outliers nos quartis."
                                  ],
                                  "verification": "Desvio padrão, variância e quartis calculados e validados contra ferramentas externas como Excel.",
                                  "estimatedTime": "30 minutos",
                                  "materials": [
                                    "DataFrame do Step 1",
                                    "Calculadora ou Excel para validação"
                                  ],
                                  "tips": "Use ddof=1 para desvio padrão amostral em NumPy.",
                                  "learningObjective": "Computar e compreender medidas de dispersão e quartis para análise de variabilidade.",
                                  "commonMistakes": [
                                    "Usar ddof=0 em vez de 1 para amostras",
                                    "Confundir população com amostra",
                                    "Interpretar quartis incorretamente"
                                  ]
                                },
                                {
                                  "stepNumber": 4,
                                  "title": "Interpretar resultados e gerar relatório",
                                  "subSteps": [
                                    "Comparar média vs mediana para detectar assimetria.",
                                    "Analisar desvio padrão em relação à média para variabilidade.",
                                    "Visualizar com boxplot (sns.boxplot) ou histograma para confirmar estatísticas.",
                                    "Escrever um parágrafo resumindo insights (ex: 'Dados assimétricos à direita').",
                                    "Exportar estatísticas para CSV com to_csv()."
                                  ],
                                  "verification": "Relatório escrito com interpretações corretas e visualizações geradas sem erros.",
                                  "estimatedTime": "25 minutos",
                                  "materials": [
                                    "Biblioteca Seaborn/Matplotlib para plots",
                                    "DataFrame com estatísticas"
                                  ],
                                  "tips": "Sempre relacione números a contextos reais para melhor interpretação.",
                                  "learningObjective": "Interpretar estatísticas descritivas para extrair insights acionáveis dos dados.",
                                  "commonMistakes": [
                                    "Ignorar contexto dos dados",
                                    "Sobrepor interpretações sem evidência visual",
                                    "Esquecer de salvar resultados"
                                  ]
                                }
                              ],
                              "practicalExample": "Carregue um dataset de vendas mensais de uma loja (vendas.csv com colunas: mês, valor_venda). Calcule média, mediana, desvio padrão e quartis das vendas. Interprete: se mediana < média, há meses de vendas altas puxando a média para cima, sugerindo sazonalidade.",
                              "finalVerifications": [
                                "Cálculos de média, mediana, desvio padrão e quartis coincidem com validação manual.",
                                "Interpretação correta de assimetria baseada em média vs mediana.",
                                "Boxplot gerado mostra quartis e outliers adequadamente.",
                                "Resumo em DataFrame exportado sem erros.",
                                "Insights escritos relacionam estatísticas ao contexto dos dados.",
                                "Código executa sem warnings ou erros em ambiente limpo."
                              ],
                              "assessmentCriteria": [
                                "Precisão nos cálculos (erro < 0.01).",
                                "Correta distinção entre medidas de tendência e dispersão.",
                                "Interpretação qualitativa profunda (assimetria, variabilidade).",
                                "Uso eficiente de Pandas/NumPy sem loops desnecessários.",
                                "Visualizações claras e rotuladas.",
                                "Relatório conciso com pelo menos 3 insights acionáveis."
                              ],
                              "crossCurricularConnections": [
                                "Estatística: Fundamentos teóricos de medidas descritivas.",
                                "Programação: Manipulação de dados em Python.",
                                "Visualização de Dados: Integração com Matplotlib/Seaborn.",
                                "Negócios: Análise de KPIs como vendas ou desempenho.",
                                "Pesquisa Científica: Preparação para testes de hipóteses."
                              ],
                              "realWorldApplication": "Em empresas de e-commerce, calcular estatísticas descritivas de dados de vendas ajuda a identificar padrões sazonais, medir consistência de desempenho e detectar anomalias para otimizar estoques e estratégias de marketing."
                            },
                            "estimatedTime": "1.5 horas",
                            "difficulty": "beginner",
                            "status": "not_started",
                            "prerequisites": []
                          },
                          {
                            "id": "10.1.1.4.2.2",
                            "name": "Criar visualizações básicas",
                            "description": "Gerar gráficos como histogramas, boxplots, scatter plots e heatmaps com Matplotlib e Seaborn, destacando correlações e tendências para comunicação de insights.",
                            "atomicExpansion": {
                              "steps": [
                                {
                                  "stepNumber": 1,
                                  "title": "Configurar ambiente e importar bibliotecas",
                                  "subSteps": [
                                    "Instale as bibliotecas necessárias com pip install matplotlib seaborn pandas numpy",
                                    "Importe as bibliotecas: import matplotlib.pyplot as plt, import seaborn as sns, import pandas as pd, import numpy as np",
                                    "Carregue um dataset exemplo, como o Iris do seaborn: iris = sns.load_dataset('iris')",
                                    "Inspecione os dados com iris.head() e iris.describe() para entender a estrutura",
                                    "Configure o estilo padrão do seaborn: sns.set_style('whitegrid')"
                                  ],
                                  "verification": "Verifique se as importações funcionam sem erros e o dataset é carregado corretamente exibindo iris.head()",
                                  "estimatedTime": "15-20 minutos",
                                  "materials": [
                                    "Python 3.x",
                                    "Jupyter Notebook ou Google Colab",
                                    "Internet para instalação de pacotes"
                                  ],
                                  "tips": "Use um ambiente virtual para evitar conflitos de dependências",
                                  "learningObjective": "Configurar corretamente o ambiente de visualização de dados com Matplotlib e Seaborn",
                                  "commonMistakes": [
                                    "Esquecer de recarregar o kernel após instalações",
                                    "Não importar pandas para manipulação de dados",
                                    "Ignorar a inspeção inicial dos dados"
                                  ]
                                },
                                {
                                  "stepNumber": 2,
                                  "title": "Criar histogramas e boxplots para distribuições",
                                  "subSteps": [
                                    "Crie um histograma simples: plt.hist(iris['petal_length'], bins=20); plt.show()",
                                    "Adicione labels e título: plt.xlabel('Comprimento da Pétala'); plt.ylabel('Frequência'); plt.title('Histograma de Comprimento da Pétala')",
                                    "Crie um boxplot com Seaborn: sns.boxplot(x='species', y='petal_width', data=iris)",
                                    "Personalize o boxplot: plt.title('Boxplot de Largura da Pétala por Espécie')",
                                    "Compare distribuições lado a lado usando subplots: fig, axes = plt.subplots(1,2); sns.histplot(data=iris, x='sepal_length', ax=axes[0]); sns.boxplot(data=iris, x='species', y='sepal_length', ax=axes[1])"
                                  ],
                                  "verification": "Execute o código e confirme que histogramas e boxplots são exibidos identificando distribuições e outliers",
                                  "estimatedTime": "25-30 minutos",
                                  "materials": [
                                    "Dataset Iris ou similar",
                                    "Jupyter Notebook"
                                  ],
                                  "tips": "Ajuste o número de bins no histograma para melhor visualização da distribuição",
                                  "learningObjective": "Gerar e interpretar histogramas e boxplots para análise de distribuições e detecção de outliers",
                                  "commonMistakes": [
                                    "Não rotacionar labels em boxplots com muitas categorias",
                                    "Escolher poucos bins no histograma, perdendo detalhes",
                                    "Esquecer de salvar ou mostrar o gráfico com plt.show()"
                                  ]
                                },
                                {
                                  "stepNumber": 3,
                                  "title": "Gerar scatter plots e heatmaps para correlações",
                                  "subSteps": [
                                    "Crie um scatter plot básico: plt.scatter(iris['sepal_length'], iris['sepal_width'])",
                                    "Use Seaborn para scatter com coloração por categoria: sns.scatterplot(data=iris, x='sepal_length', y='sepal_width', hue='species')",
                                    "Calcule a matriz de correlação: corr = iris.corr()",
                                    "Crie um heatmap: sns.heatmap(corr, annot=True, cmap='coolwarm')",
                                    "Personalize o heatmap: plt.title('Matriz de Correlação das Medidas das Íris')"
                                  ],
                                  "verification": "Confirme que scatter plots mostram relações e heatmaps destacam correlações acima de 0.5 ou abaixo de -0.5",
                                  "estimatedTime": "25-35 minutos",
                                  "materials": [
                                    "Dataset numérico com variáveis contínuas",
                                    "Jupyter Notebook"
                                  ],
                                  "tips": "Use hue no scatterplot para revelar padrões escondidos por grupos",
                                  "learningObjective": "Visualizar relações bivariadas e multivariadas destacando correlações e tendências",
                                  "commonMistakes": [
                                    "Não remover variáveis categóricas da correlação",
                                    "Escala de cores inadequada no heatmap obscurecendo valores",
                                    "Ignorar a diagonal na matriz de correlação (sempre 1)"
                                  ]
                                },
                                {
                                  "stepNumber": 4,
                                  "title": "Personalizar visualizações e comunicar insights",
                                  "subSteps": [
                                    "Adicione temas e paletas: sns.set_palette('husl'); sns.pairplot(iris, hue='species')",
                                    "Salve gráficos: plt.savefig('visualizacao.png', dpi=300, bbox_inches='tight')",
                                    "Anote insights: Destaque correlações fortes (ex: petal_length e petal_width >0.8)",
                                    "Crie um dashboard simples com múltiplos plots: fig, axes = plt.subplots(2,2); ...",
                                    "Documente tendências: 'Boxplot revela outliers em setosa; scatter mostra clusters por espécie'"
                                  ],
                                  "verification": "Gere um relatório com 4 visualizações personalizadas e 3 insights descritos em texto",
                                  "estimatedTime": "20-25 minutos",
                                  "materials": [
                                    "Código dos steps anteriores",
                                    "Editor de texto para documentação"
                                  ],
                                  "tips": "Use figure size maior para dashboards: plt.figure(figsize=(12,8))",
                                  "learningObjective": "Personalizar gráficos para comunicação clara de insights e tendências",
                                  "commonMistakes": [
                                    "Sobrecarregar o gráfico com muitos elementos",
                                    "Não salvar em alta resolução",
                                    "Descrever visualizações sem conectar a insights acionáveis"
                                  ]
                                }
                              ],
                              "practicalExample": "Carregue o dataset Titanic (sns.load_dataset('titanic')). Crie: histograma da idade (distribuição etária), boxplot de fare por classe (outliers em tarifas), scatter de age vs fare colorido por survived (correlações em sobrevivência), heatmap de correlações entre variáveis numéricas. Destaque: 'Passageiros de 1ª classe pagaram mais e tiveram mais sobreviventes'.",
                              "finalVerifications": [
                                "Todas as visualizações (histograma, boxplot, scatter, heatmap) são geradas sem erros de código",
                                "Gráficos possuem títulos, labels em eixos e legendas claras",
                                "Correlações e tendências são destacadas com anotações ou texto explicativo",
                                "Gráficos são salvos em formato PNG com resolução adequada",
                                "Insights são comunicados em 3-5 frases conectando visualizações a padrões nos dados",
                                "Código é modular e reutilizável para novos datasets"
                              ],
                              "assessmentCriteria": [
                                "Precisão técnica: Código executa corretamente e produz gráficos esperados (40%)",
                                "Qualidade visual: Personalização, legibilidade e estética (20%)",
                                "Interpretação: Identificação correta de correlações, tendências e outliers (20%)",
                                "Comunicação: Insights claros e acionáveis derivados das visualizações (10%)",
                                "Eficiência: Uso otimizado de funções Seaborn/Matplotlib sem redundâncias (10%)"
                              ],
                              "crossCurricularConnections": [
                                "Estatística: Compreensão de distribuições, variância e correlação",
                                "Programação: Manipulação de dados com Pandas e automação em Python",
                                "Comunicação: Design visual para relatórios e apresentações",
                                "Matemática Computacional: Análise exploratória de dados multivariados",
                                "Negócios: Aplicação em dashboards para tomada de decisões"
                              ],
                              "realWorldApplication": "Em empresas de e-commerce, criar heatmaps de correlações entre vendas e marketing para otimizar campanhas; em saúde, scatter plots de idade vs sintomas para detectar epidemias; em finanças, boxplots de retornos de investimentos para identificar riscos e tendências de mercado."
                            },
                            "estimatedTime": "2 horas",
                            "difficulty": "beginner",
                            "status": "not_started",
                            "prerequisites": []
                          },
                          {
                            "id": "10.1.1.4.2.3",
                            "name": "Detectar padrões e anomalias",
                            "description": "Identificar outliers e distribuições via EDA, aplicando testes como Z-score ou IQR, e relacionando com o contexto do problema (ex.: vendas anômalas em dataset de varejo).",
                            "atomicExpansion": {
                              "steps": [
                                {
                                  "stepNumber": 1,
                                  "title": "Compreender Conceitos Fundamentais de Padrões e Anomalias",
                                  "subSteps": [
                                    "Defina padrões como comportamentos normais ou distribuições esperadas em dados (ex.: média e desvio padrão).",
                                    "Identifique anomalias (outliers) como pontos que desviam significativamente da distribuição normal.",
                                    "Estude tipos de anomalias: pontuais (um valor isolado), contextuais (anômalo no contexto) e coletivas (grupo anômalo).",
                                    "Revise distribuições comuns: normal, assimétrica, bimodal.",
                                    "Relacione com EDA: uso de estatísticas descritivas para baseline."
                                  ],
                                  "verification": "Resuma em 3 frases os conceitos e dê 2 exemplos de anomalias em dados reais.",
                                  "estimatedTime": "20 minutos",
                                  "materials": [
                                    "Notebook ou papel para anotações",
                                    "Artigo introdutório sobre outliers (ex.: Towards Data Science)"
                                  ],
                                  "tips": "Use analogias cotidianas, como altura humana para distribuição normal.",
                                  "learningObjective": "Dominar definições e tipos de padrões/anomalias para base conceitual.",
                                  "commonMistakes": [
                                    "Confundir variância alta com anomalias",
                                    "Ignorar contexto ao rotular outliers"
                                  ]
                                },
                                {
                                  "stepNumber": 2,
                                  "title": "Realizar Análise Exploratória de Dados (EDA) para Visualizar Distribuições",
                                  "subSteps": [
                                    "Carregue um dataset (ex.: vendas de varejo) usando pandas.",
                                    "Calcule estatísticas descritivas: mean, median, std, quartis com df.describe().",
                                    "Crie visualizações: histograma, boxplot e scatterplot com matplotlib/seaborn.",
                                    "Identifique visualmente padrões (clusters) e potenciais outliers.",
                                    "Documente observações iniciais em um relatório curto."
                                  ],
                                  "verification": "Gere 3 plots e anote 2 padrões e 1 possível anomalia observada.",
                                  "estimatedTime": "40 minutos",
                                  "materials": [
                                    "Python/Jupyter Notebook",
                                    "Bibliotecas: pandas, matplotlib, seaborn",
                                    "Dataset sample de vendas (ex.: Kaggle Retail Sales)"
                                  ],
                                  "tips": "Ajuste scales nos plots para melhor visualização; use log-scale para distribuições skew.",
                                  "learningObjective": "Visualizar dados para detectar padrões e candidatos a anomalias intuitivamente.",
                                  "commonMistakes": [
                                    "Escala inadequada nos gráficos",
                                    "Ignorar missing values antes da EDA"
                                  ]
                                },
                                {
                                  "stepNumber": 3,
                                  "title": "Aplicar Testes Quantitativos para Detecção de Outliers",
                                  "subSteps": [
                                    "Implemente método IQR: calcule Q1, Q3, IQR; defina limites (Q1-1.5*IQR, Q3+1.5*IQR).",
                                    "Calcule Z-score: (x - mean)/std; flag valores |Z| > 3 como outliers.",
                                    "Aplique em código: crie máscaras booleanas e filtre outliers.",
                                    "Compare resultados de IQR vs Z-score no dataset.",
                                    "Teste sensibilidade ajustando thresholds (ex.: 2.5 para Z-score)."
                                  ],
                                  "verification": "Execute código e liste 5 outliers detectados por cada método com seus valores.",
                                  "estimatedTime": "50 minutos",
                                  "materials": [
                                    "Jupyter Notebook",
                                    "pandas, numpy",
                                    "Dataset do Step 2"
                                  ],
                                  "tips": "Use np.percentile para quartis robustos; vectorize cálculos para eficiência.",
                                  "learningObjective": "Aplicar testes estatísticos padrão para detecção quantitativa de anomalias.",
                                  "commonMistakes": [
                                    "Usar mean em distribuições skew (prefira mediana)",
                                    "Não normalizar para Z-score em múltiplas features"
                                  ]
                                },
                                {
                                  "stepNumber": 4,
                                  "title": "Interpretar Anomalias no Contexto do Problema",
                                  "subSteps": [
                                    "Analise outliers: verifique se são erros de dados, eventos reais ou fraudes.",
                                    "Relacione com domínio: em vendas, alto valor pode ser promoção vs fraude.",
                                    "Crie narrativa: 'Este outlier em vendas de 10/2023 coincide com Black Friday'.",
                                    "Decida ação: remover, investigar ou manter.",
                                    "Documente relatório final com plots, testes e recomendações."
                                  ],
                                  "verification": "Escreva relatório de 200 palavras explicando 3 anomalias e ações propostas.",
                                  "estimatedTime": "30 minutos",
                                  "materials": [
                                    "Relatório template (Markdown)",
                                    "Plots e resultados dos steps anteriores"
                                  ],
                                  "tips": "Sempre pergunte: 'Isso faz sentido no negócio?' antes de rotular.",
                                  "learningObjective": "Contextualizar detecções para decisões acionáveis.",
                                  "commonMistakes": [
                                    "Rotular todos outliers como erros sem investigação",
                                    "Ignorar multicolinearidade entre variáveis"
                                  ]
                                }
                              ],
                              "practicalExample": "Em um dataset de vendas diárias de varejo (colunas: data, produto, vendas), detecte anomalias como picos de vendas >3x média (possível fraude) usando boxplot para visual, IQR para flag e contexto para validar (ex.: coincidir com feriados).",
                              "finalVerifications": [
                                "Identifica corretamente 80% dos outliers injetados em dataset teste.",
                                "Explica diferença entre IQR e Z-score com exemplo numérico.",
                                "Gera relatório contextualizando 3 anomalias em dataset real.",
                                "Ajusta thresholds baseado em análise de falsos positivos.",
                                "Integra visualizações com testes quantitativos coerentemente.",
                                "Discute impactos de remoção de outliers no modelo downstream."
                              ],
                              "assessmentCriteria": [
                                "Precisão na detecção (taxa de falsos positivos <20%).",
                                "Qualidade das visualizações (claras, legíveis, informativas).",
                                "Correção matemática nos cálculos (IQR/Z-score exatos).",
                                "Profundidade da interpretação contextual.",
                                "Completude do relatório (plots + testes + recomendações).",
                                "Eficiência do código (limpo, comentado, reutilizável)."
                              ],
                              "crossCurricularConnections": [
                                "Estatística: Distribuições e testes de hipótese.",
                                "Programação: Manipulação de dados com Python/pandas.",
                                "Negócios: Análise de fraudes e forecasting de vendas.",
                                "Machine Learning: Pré-processamento para modelos supervisionados.",
                                "Visualização: Design de gráficos eficazes."
                              ],
                              "realWorldApplication": "Detecção de fraudes em transações bancárias (outliers em valores), monitoramento de sensores IoT para falhas em manufatura, ou anomalias em tráfego de rede para cibersegurança."
                            },
                            "estimatedTime": "1.5 horas",
                            "difficulty": "intermediate",
                            "status": "not_started",
                            "prerequisites": []
                          }
                        ]
                      },
                      {
                        "id": "10.1.1.4.3",
                        "name": "Limpeza de Dados",
                        "description": "Tratar dados sujos para garantir qualidade, removendo ou corrigindo inconsistências, valores ausentes e ruídos, essencial para modelos precisos.",
                        "specificSkills": [
                          {
                            "id": "10.1.1.4.3.1",
                            "name": "Tratar valores ausentes",
                            "description": "Aplicar estratégias como imputação por média/mediana, interpolação ou remoção, usando Pandas (fillna, dropna), avaliando impacto na variância dos dados.",
                            "atomicExpansion": {
                              "steps": [
                                {
                                  "stepNumber": 1,
                                  "title": "Identificar e Analisar Valores Ausentes",
                                  "subSteps": [
                                    "Carregue um dataset com valores ausentes usando pd.read_csv().",
                                    "Execute df.info() e df.isnull().sum() para contar missings por coluna.",
                                    "Calcule o percentual de missings: (df.isnull().sum() / len(df)) * 100.",
                                    "Visualize padrões com seaborn.heatmap(df.isnull()) ou missingno library.",
                                    "Analise distribuição das colunas afetadas com df.describe()."
                                  ],
                                  "verification": "Relatório gerado mostrando contagem, percentual e padrões de valores ausentes por coluna.",
                                  "estimatedTime": "25 minutos",
                                  "materials": "Jupyter Notebook, Pandas, Seaborn/Missingno, dataset de exemplo (ex: Titanic.csv)",
                                  "tips": "Priorize colunas com >50% de missings para remoção em vez de imputação.",
                                  "learningObjective": "Detectar e quantificar valores ausentes para informar decisões de tratamento.",
                                  "commonMistakes": "Ignorar missings em colunas não-numéricas ou assumir ausência de padrões."
                                },
                                {
                                  "stepNumber": 2,
                                  "title": "Selecionar Estratégia de Tratamento",
                                  "subSteps": [
                                    "Classifique colunas por tipo: numéricas (média/mediana/interpolação) vs. categóricas (modo ou 'Unknown').",
                                    "Avalie impacto: se <5% missings, prefira remoção; se alto, imputação.",
                                    "Considere contexto: interpolação para séries temporais; mediana para outliers.",
                                    "Documente escolha em um dict: {'coluna': 'método', 'justificativa': '...'}",
                                    "Teste cenários hipotéticos com cópias do dataset."
                                  ],
                                  "verification": "Tabela ou dict com estratégias escolhidas e justificativas para cada coluna afetada.",
                                  "estimatedTime": "20 minutos",
                                  "materials": "Jupyter Notebook, Pandas, dataset copiado (df_backup = df.copy())",
                                  "tips": "Use mediana para dados skew; verifique normalidade com Shapiro-Wilk test.",
                                  "learningObjective": "Escolher método de tratamento baseado em análise estatística e contexto dos dados.",
                                  "commonMistakes": "Aplicar média em dados com outliers extremos, distorcendo centralidade."
                                },
                                {
                                  "stepNumber": 3,
                                  "title": "Implementar Tratamento com Pandas",
                                  "subSteps": [
                                    "Remova linhas/colunas: df.dropna(subset=['coluna_critica']) ou df.dropna(axis=1, thresh=0.8*len(df)).",
                                    "Impute numéricos: df['coluna'].fillna(df['coluna'].median(), inplace=True).",
                                    "Aplique interpolação: df['coluna'].interpolate(method='linear', inplace=True).",
                                    "Trate categóricos: df['coluna'].fillna('Missing', inplace=True).",
                                    "Crie coluna flag: df['coluna_missing_flag'] = df['coluna'].isnull().astype(int)."
                                  ],
                                  "verification": "df.isnull().sum().sum() == 0 ou missings gerenciados; flags criadas onde aplicável.",
                                  "estimatedTime": "30 minutos",
                                  "materials": "Jupyter Notebook, Pandas, dataset backup para comparação",
                                  "tips": "Sempre use inplace=True com cuidado; prefira df_tratado = df.fillna(...) para não alterar original.",
                                  "learningObjective": "Executar funções Pandas (fillna, dropna, interpolate) de forma precisa e segura.",
                                  "commonMistakes": "Usar fillna sem especificar valor correto, resultando em NaNs persistentes."
                                },
                                {
                                  "stepNumber": 4,
                                  "title": "Avaliar Impacto na Variância e Qualidade dos Dados",
                                  "subSteps": [
                                    "Calcule variância antes/depois: var_before = df_backup.var(); var_after = df_tratado.var().",
                                    "Compare: pd.DataFrame({'Antes': var_before, 'Depois': var_after}).",
                                    "Visualize mudanças: histograms lado a lado com plt.subplot().",
                                    "Mensure bias: compare mean/median antes/depois e std deviation.",
                                    "Gere relatório: '% de mudança na variância: ((var_after - var_before)/var_before)*100'."
                                  ],
                                  "verification": "Relatório comparativo gerado mostrando variação <10-20% na maioria das métricas.",
                                  "estimatedTime": "25 minutos",
                                  "materials": "Jupyter Notebook, Pandas, Matplotlib/Seaborn, datasets before/after",
                                  "tips": "Foco em colunas tratadas; ignore irrelevantes para variância.",
                                  "learningObjective": "Quantificar e interpretar impactos do tratamento na estrutura estatística dos dados.",
                                  "commonMistakes": "Comparar variâncias sem normalizar ou ignorar mudanças em distribuições."
                                }
                              ],
                              "practicalExample": "No dataset Titanic.csv, trate 'Age' (numérica, ~20% missings) com mediana e 'Cabin' (alta % missings) com dropna(axis=1). Calcule var('Age') antes (200.0) e depois (195.2), confirmando baixa distorção.",
                              "finalVerifications": [
                                "Detecta e relata >95% dos valores ausentes corretamente.",
                                "Implementa pelo menos 3 métodos (fillna média/mediana, dropna, interpolate).",
                                "Gera relatório de impacto mostrando variação na variância.",
                                "Cria flags para missings imputados.",
                                "Justifica escolhas com base em % missings e tipo de dado.",
                                "Visualiza mudanças em distribuições antes/depois."
                              ],
                              "assessmentCriteria": [
                                "Precisão na detecção e contagem de missings (100% correto).",
                                "Adequação da estratégia escolhida ao contexto dos dados (sem over-imputation).",
                                "Correta implementação de Pandas sem erros de sintaxe ou perda de dados desnecessária.",
                                "Análise de impacto quantitativa e qualitativa completa.",
                                "Documentação clara de passos e justificativas.",
                                "Eficiência: mudança na variância <15% em colunas críticas."
                              ],
                              "crossCurricularConnections": [
                                "Estatística: Medidas de tendência central (média, mediana) e dispersão (variância).",
                                "Programação: Manipulação de DataFrames em Python/Pandas.",
                                "Visualização de Dados: Heatmaps e histograms para análise exploratória.",
                                "Ética em Dados: Evitar bias introduzido por imputação inadequada.",
                                "Matemática Computacional: Interpolação linear em séries numéricas."
                              ],
                              "realWorldApplication": "Em análise de dados de saúde (ex: registros hospitalares), tratar missings em 'pressão arterial' com mediana para treinar modelos de ML sem distorcer variância, evitando previsões enviesadas em diagnósticos."
                            },
                            "estimatedTime": "1.5 horas",
                            "difficulty": "intermediate",
                            "status": "not_started",
                            "prerequisites": []
                          },
                          {
                            "id": "10.1.1.4.3.2",
                            "name": "Remover duplicatas e corrigir inconsistências",
                            "description": "Identificar e eliminar duplicatas com Pandas (duplicated, drop_duplicates), padronizar formatos (ex.: datas, categorias) e validar tipos de dados.",
                            "atomicExpansion": {
                              "steps": [
                                {
                                  "stepNumber": 1,
                                  "title": "Inspeção Inicial dos Dados para Identificar Duplicatas e Inconsistências",
                                  "subSteps": [
                                    "Carregue o dataset usando pd.read_csv() ou similar.",
                                    "Use df.info() e df.describe() para overview.",
                                    "Execute df.duplicated().sum() para contar duplicatas.",
                                    "Inspecione df.head() e df.sample() para formatos inconsistentes em datas e categorias.",
                                    "Verifique df.dtypes para tipos de dados incorretos."
                                  ],
                                  "verification": "Confirme que relatório de inspeção mostra contagem de duplicatas >0 ou inconsistências identificadas.",
                                  "estimatedTime": "20 minutos",
                                  "materials": "Jupyter Notebook, biblioteca Pandas instalada, dataset de exemplo (ex: CSV com vendas duplicadas)",
                                  "tips": "Salve o dataset original como backup com df.copy().",
                                  "learningObjective": "Aprender a diagnosticar problemas de duplicatas e inconsistências em datasets.",
                                  "commonMistakes": "Ignorar duplicatas parciais (subset); não checar NaNs como duplicatas."
                                },
                                {
                                  "stepNumber": 2,
                                  "title": "Remoção de Duplicatas Usando Pandas",
                                  "subSteps": [
                                    "Identifique duplicatas com df.duplicated(subset=['coluna_chave']) para chaves específicas.",
                                    "Visualize duplicatas com df[df.duplicated()].",
                                    "Remova com df.drop_duplicates(subset=['coluna_chave'], keep='first').",
                                    "Atualize o índice com df.reset_index(drop=True).",
                                    "Confirme remoção com df.duplicated().sum() == 0."
                                  ],
                                  "verification": "df.duplicated().sum() retorna 0 e shape do DataFrame diminuiu.",
                                  "estimatedTime": "25 minutos",
                                  "materials": "Mesmo ambiente do Step 1, documentação Pandas drop_duplicates.",
                                  "tips": "Use keep='last' se a última entrada for mais atualizada.",
                                  "learningObjective": "Dominar identificação e eliminação precisa de duplicatas.",
                                  "commonMistakes": "Remover todas sem subset, perdendo dados únicos; esquecer reset_index."
                                },
                                {
                                  "stepNumber": 3,
                                  "title": "Padronização de Formatos em Datas e Categorias",
                                  "subSteps": [
                                    "Identifique colunas de data com pd.to_datetime(df['data'], errors='coerce').",
                                    "Padronize datas: df['data'] = pd.to_datetime(df['data'], format='%d/%m/%Y').dt.strftime('%Y-%m-%d').",
                                    "Para categorias, use df['categoria'] = df['categoria'].str.lower().str.strip().",
                                    "Mapeie valores inconsistentes com df['categoria'] = df['categoria'].map({'velho': 'novo'}).",
                                    "Preencha NaNs com df['categoria'].fillna('Outros')."
                                  ],
                                  "verification": "Todas datas em formato ISO e categorias padronizadas sem variações (df['categoria'].unique()).",
                                  "estimatedTime": "30 minutos",
                                  "materials": "Pandas, dataset com datas variadas (ex: '01/01/2023', '2023-01-01')",
                                  "tips": "Use infer_datetime_format=True para eficiência em grandes datasets.",
                                  "learningObjective": "Padronizar strings e datas para consistência.",
                                  "commonMistakes": "Não tratar erros com errors='coerce'; ignorar espaços em branco."
                                },
                                {
                                  "stepNumber": 4,
                                  "title": "Validação e Correção de Tipos de Dados",
                                  "subSteps": [
                                    "Liste dtypes atuais com df.dtypes.",
                                    "Converta tipos: df['preco'] = pd.to_numeric(df['preco'], errors='coerce').",
                                    "Corrija categorias para categórico: df['categoria'] = df['categoria'].astype('category').",
                                    "Valide com df['preco'].dtype == 'float64' e sem NaNs inesperados.",
                                    "Preencha ou drop NaNs: df.dropna(subset=['preco']) ou df['preco'].fillna(0)."
                                  ],
                                  "verification": "df.dtypes mostra tipos corretos e df.isnull().sum() gerenciável.",
                                  "estimatedTime": "20 minutos",
                                  "materials": "Pandas, dataset com colunas mistas (strings numéricas)",
                                  "tips": "Sempre coerce erros para evitar crashes.",
                                  "learningObjective": "Garantir tipos de dados apropriados para análise.",
                                  "commonMistakes": "Converter sem coerce, causando erros; esquecer de preencher NaNs."
                                },
                                {
                                  "stepNumber": 5,
                                  "title": "Verificação Final e Salvamento do Dataset Limpo",
                                  "subSteps": [
                                    "Execute inspeções finais: df.duplicated().sum(), df.dtypes, df.describe().",
                                    "Compare shape original vs final.",
                                    "Gere relatório: print(f'Duplicatas removidas: {original_shape[0] - df.shape[0]}').",
                                    "Salve com df.to_csv('dataset_limpo.csv', index=False).",
                                    "Teste carregamento para validar."
                                  ],
                                  "verification": "Relatório confirma zero duplicatas, tipos corretos e formatos padronizados.",
                                  "estimatedTime": "15 minutos",
                                  "materials": "Mesmo ambiente",
                                  "tips": "Automatize verificações em uma função reutilizável.",
                                  "learningObjective": "Consolidar limpeza com validação abrangente.",
                                  "commonMistakes": "Não salvar ou testar o arquivo limpo."
                                }
                              ],
                              "practicalExample": "Em um dataset de vendas de e-commerce com 1000 linhas: remova 50 duplicatas de pedidos (ID duplicado), padronize datas de 'DD/MM/YYYY' para 'YYYY-MM-DD', converta 'preco' de string 'R$10.50' para float 10.50, e categorize 'produto' de 'celular', 'Celular' para 'celular'. Resultado: dataset limpo pronto para análise.",
                              "finalVerifications": [
                                "df.duplicated().sum() == 0",
                                "Todas datas em formato pd.to_datetime válido",
                                "Colunas categóricas têm .nunique() reduzido por padronização",
                                "Tipos numéricos sem strings (pd.to_numeric sem erros)",
                                "Shape final reflete remoções sem perda excessiva",
                                "df.isnull().sum() documentado e tratado"
                              ],
                              "assessmentCriteria": [
                                "Precisão na remoção de duplicatas sem perda de dados únicos (90%+ retenção correta)",
                                "Padronização completa de formatos sem inconsistências restantes",
                                "Tipos de dados corrigidos e validados (sem erros de conversão)",
                                "Eficiência: tempo de execução <5s para dataset médio",
                                "Relatório de verificação claro e completo",
                                "Código reproduzível e comentado"
                              ],
                              "crossCurricularConnections": [
                                "Estatística: Preparação para análise descritiva",
                                "Programação: Manipulação de estruturas de dados em Python",
                                "Banco de Dados: Conceitos de normalização e integridade",
                                "Análise de Negócios: Limpeza para insights acionáveis"
                              ],
                              "realWorldApplication": "Em análise de dados de CRM de empresas como Amazon, remover duplicatas de clientes evita envios duplicados e erros em relatórios de vendas; padronizar CPFs e datas previne fraudes e falhas em machine learning."
                            },
                            "estimatedTime": "1 hora",
                            "difficulty": "beginner",
                            "status": "not_started",
                            "prerequisites": []
                          },
                          {
                            "id": "10.1.1.4.3.3",
                            "name": "Normalizar e escalar dados",
                            "description": "Executar normalização Min-Max ou Z-score com Scikit-learn (StandardScaler, MinMaxScaler), preparando dados para algoritmos sensíveis à escala como KNN.",
                            "atomicExpansion": {
                              "steps": [
                                {
                                  "stepNumber": 1,
                                  "title": "Preparar o ambiente e carregar dados",
                                  "subSteps": [
                                    "Instale as bibliotecas necessárias: scikit-learn, pandas e numpy via pip.",
                                    "Importe as classes: from sklearn.preprocessing import MinMaxScaler, StandardScaler; import pandas as pd; import numpy as np.",
                                    "Carregue um dataset de exemplo, como o Iris do sklearn ou um CSV com features numéricas variadas em escalas diferentes (ex: idade em anos, salário em milhares).",
                                    "Separe features numéricas em um DataFrame X e verifique estatísticas descritivas com X.describe().",
                                    "Identifique colunas que precisam de escalonamento (ex: features com ranges muito diferentes)."
                                  ],
                                  "verification": "Dataset carregado corretamente, sem NaNs, e describe() mostra ranges desbalanceados confirmando necessidade de escalonamento.",
                                  "estimatedTime": "20 minutos",
                                  "materials": [
                                    "Python 3.8+, Jupyter Notebook ou Google Colab",
                                    "Dataset Iris: from sklearn.datasets import load_iris",
                                    "Bibliotecas: scikit-learn, pandas, numpy"
                                  ],
                                  "tips": "Use pd.read_csv() para datasets personalizados e dropna() para limpar dados iniciais.",
                                  "learningObjective": "Configurar ambiente e preparar dados numéricos para pré-processamento de escala.",
                                  "commonMistakes": [
                                    "Esquecer de importar classes corretas",
                                    "Não lidar com NaNs ou categóricas antes",
                                    "Usar dataset sem variação de escala"
                                  ]
                                },
                                {
                                  "stepNumber": 2,
                                  "title": "Aplicar Normalização Min-Max",
                                  "subSteps": [
                                    "Instancie o scaler: scaler = MinMaxScaler(feature_range=(0, 1)).",
                                    "Ajuste e transforme os dados: X_minmax = scaler.fit_transform(X).",
                                    "Converta de volta para DataFrame: X_minmax_df = pd.DataFrame(X_minmax, columns=X.columns).",
                                    "Verifique o resultado com X_minmax_df.describe() – valores devem estar entre 0 e 1.",
                                    "Salve o scaler treinado para uso futuro: import joblib; joblib.dump(scaler, 'minmax_scaler.pkl')."
                                  ],
                                  "verification": "describe() mostra min=0, max=1 para todas features; sem valores fora do range.",
                                  "estimatedTime": "25 minutos",
                                  "materials": [
                                    "Código do Step 1",
                                    "Jupyter Notebook"
                                  ],
                                  "tips": "Sempre fit_transform() em train e apenas transform() em test para evitar data leakage.",
                                  "learningObjective": "Executar Min-Max scaling preservando relações relativas e limitando a [0,1].",
                                  "commonMistakes": [
                                    "Usar transform() sem fit() primeiro",
                                    "Aplicar em dados com outliers extremos sem tratamento prévio",
                                    "Esquecer feature_range"
                                  ]
                                },
                                {
                                  "stepNumber": 3,
                                  "title": "Aplicar Escalonamento Z-Score (StandardScaler)",
                                  "subSteps": [
                                    "Instancie o scaler: scaler_z = StandardScaler().",
                                    "Ajuste e transforme: X_zscore = scaler_z.fit_transform(X).",
                                    "Crie DataFrame: X_zscore_df = pd.DataFrame(X_zscore, columns=X.columns).",
                                    "Verifique: X_zscore_df.describe() deve mostrar média ~0 e desvio padrão ~1.",
                                    "Compare distribuições: plote histograms antes/depois com matplotlib.pyplot.hist()."
                                  ],
                                  "verification": "Média próxima de 0 e std próxima de 1 por coluna; plots mostram distribuições centradas.",
                                  "estimatedTime": "25 minutos",
                                  "materials": [
                                    "Código anterior",
                                    "matplotlib para visualização"
                                  ],
                                  "tips": "Z-score é robusto a outliers comparado a Min-Max; ideal para algoritmos assumindo normalidade.",
                                  "learningObjective": "Implementar Z-score para centralizar e padronizar variância unitária.",
                                  "commonMistakes": [
                                    "Confundir fit_transform com transform em novos dados",
                                    "Não verificar médias após scaling",
                                    "Aplicar em features categóricas"
                                  ]
                                },
                                {
                                  "stepNumber": 4,
                                  "title": "Verificar, comparar e integrar em pipeline para KNN",
                                  "subSteps": [
                                    "Treine um KNN simples: from sklearn.neighbors import KNeighborsClassifier; knn = KNeighborsClassifier(); knn.fit(X_minmax, y).",
                                    "Compare acurácia com dados não escalados vs escalados.",
                                    "Crie pipeline: from sklearn.pipeline import Pipeline; pipe = Pipeline([('scaler', StandardScaler()), ('knn', KNeighborsClassifier())]).",
                                    "Teste cross-validation: from sklearn.model_selection import cross_val_score; scores = cross_val_score(pipe, X, y, cv=5).",
                                    "Documente escolhas: salve relatórios de describe() e scores em um notebook."
                                  ],
                                  "verification": "Acurácia KNN melhora com dados escalados; pipeline executa sem erros.",
                                  "estimatedTime": "30 minutos",
                                  "materials": [
                                    "sklearn.model_selection, sklearn.neighbors",
                                    "Dataset com labels (y)"
                                  ],
                                  "tips": "Sempre valide em conjunto de teste separado; prefira StandardScaler para KNN por sensibilidade a distâncias.",
                                  "learningObjective": "Integrar scaling em ML pipeline e validar impacto em performance.",
                                  "commonMistakes": [
                                    "Treinar KNN sem labels",
                                    "Data leakage ao fit scaler em todo dataset",
                                    "Ignorar diferenças de performance"
                                  ]
                                }
                              ],
                              "practicalExample": "Em um dataset de previsão de preços de casas (features: área em m² [0-500], quartos [1-10], idade casa [1-100 anos]), aplique MinMaxScaler na área (torna 0-1) e StandardScaler nos quartos/idade. Treine KNN: sem scale, acurácia baixa devido a área dominando distância; com scale, performance sobe 20%.",
                              "finalVerifications": [
                                "Todas features MinMax em [0,1]; Z-score com μ=0, σ=1.",
                                "Código roda sem erros; scalers salvos.",
                                "Visualizações confirmam transformações (histograms/before-after).",
                                "KNN acurácia melhora ≥10% com scaling.",
                                "Pipeline integrado funciona em novos dados.",
                                "Relatório com describe() e scores salvo."
                              ],
                              "assessmentCriteria": [
                                "Precisão na aplicação de fit_transform vs transform.",
                                "Verificação estatística correta pós-scaling.",
                                "Integração em pipeline sem leakage.",
                                "Comparação de performance em KNN quantitativa.",
                                "Código limpo, comentado e reproduzível.",
                                "Tratamento de edge cases (outliers, NaNs)."
                              ],
                              "crossCurricularConnections": [
                                "Estatística: Conceitos de média, desvio padrão e distribuições.",
                                "Programação: Manipulação de DataFrames pandas e pipelines sklearn.",
                                "Matemática: Fórmulas de MinMax (x' = (x-min)/(max-min)) e Z-score (z = (x-μ)/σ).",
                                "Ciência de Dados: Pré-processamento em CRISP-DM.",
                                "Visualização: Plots com matplotlib/seaborn para validação."
                              ],
                              "realWorldApplication": "Em recommendation systems (Netflix: normalizar ratings/idade usuário para KNN), finanças (escalar retornos/volatilidade para clustering de ações) ou saúde (padronizar BMI/idade/peso para modelos de risco cardiovascular), garantindo que algoritmos distância-baseados como KNN/SVM ignorem unidades/escalas artificiais."
                            },
                            "estimatedTime": "1.5 horas",
                            "difficulty": "intermediate",
                            "status": "not_started",
                            "prerequisites": []
                          },
                          {
                            "id": "10.1.1.4.3.4",
                            "name": "Lidar com outliers",
                            "description": "Detectar e tratar outliers via métodos estatísticos ou visuais (boxplots), decidindo por remoção, capping ou winsorização com base no domínio.",
                            "atomicExpansion": {
                              "steps": [
                                {
                                  "stepNumber": 1,
                                  "title": "Detecção Visual de Outliers",
                                  "subSteps": [
                                    "Carregue um dataset numérico usando Pandas.",
                                    "Gere boxplots para cada variável numérica com Matplotlib ou Seaborn.",
                                    "Identifique outliers como pontos além dos whiskers (1.5 * IQR).",
                                    "Crie scatterplots para variáveis bivariadas e observe desvios.",
                                    "Anote os índices dos outliers suspeitos."
                                  ],
                                  "verification": "Boxplots e scatterplots gerados corretamente, com outliers destacados e índices listados.",
                                  "estimatedTime": "1 hora",
                                  "materials": [
                                    "Python com Pandas, Matplotlib/Seaborn",
                                    "Dataset exemplo (ex: Iris ou Boston Housing)"
                                  ],
                                  "tips": "Use escalas logarítmicas para dados skewed; rotule os eixos claramente.",
                                  "learningObjective": "Visualizar e identificar outliers em dados univariados e bivariados.",
                                  "commonMistakes": [
                                    "Ignorar transformações de escala",
                                    "Confundir ruído com outliers",
                                    "Não rotular gráficos"
                                  ]
                                },
                                {
                                  "stepNumber": 2,
                                  "title": "Detecção Estatística de Outliers",
                                  "subSteps": [
                                    "Calcule estatísticas descritivas: média, mediana, IQR para cada coluna.",
                                    "Aplique método IQR: defina limites inferior (Q1 - 1.5*IQR) e superior (Q3 + 1.5*IQR).",
                                    "Use Z-score: calcule z = (x - mean)/std, marque |z| > 3 como outliers.",
                                    "Compare resultados entre IQR e Z-score para consenso.",
                                    "Crie máscaras booleanas para filtrar outliers."
                                  ],
                                  "verification": "Listas de outliers por IQR e Z-score geradas, com sobreposição analisada.",
                                  "estimatedTime": "1.5 horas",
                                  "materials": [
                                    "Python com Pandas, NumPy, SciPy",
                                    "Mesmo dataset do passo anterior"
                                  ],
                                  "tips": "Para dados não normais, prefira IQR sobre Z-score; teste múltiplos thresholds.",
                                  "learningObjective": "Aplicar métodos quantitativos para detectar outliers de forma reprodutível.",
                                  "commonMistakes": [
                                    "Usar Z-score em dados skewed",
                                    "Esquecer de tratar missing values antes",
                                    "Definir thresholds arbitrários sem justificativa"
                                  ]
                                },
                                {
                                  "stepNumber": 3,
                                  "title": "Decisão de Tratamento Baseada no Domínio",
                                  "subSteps": [
                                    "Analise o contexto do domínio: outliers são erros, raros eventos ou válidos?",
                                    "Decida estratégia: remoção (dropna), capping (clip aos limites), winsorização (substituir por percentis).",
                                    "Justifique escolha com evidências do domínio (ex: em salários, cap em 99%).",
                                    "Simule impactos: compare distribuições antes/depois.",
                                    "Documente a decisão em um relatório curto."
                                  ],
                                  "verification": "Relatório com justificativa e escolha de método documentado.",
                                  "estimatedTime": "45 minutos",
                                  "materials": [
                                    "Dataset com outliers identificados",
                                    "Notebook Jupyter para simulações"
                                  ],
                                  "tips": "Consulte literatura do domínio; evite remoção automática sem análise.",
                                  "learningObjective": "Avaliar impactos contextuais para escolher tratamento ético e eficaz.",
                                  "commonMistakes": [
                                    "Remover outliers sem verificação de domínio",
                                    "Aplicar mesmo tratamento a todos",
                                    "Ignorar perda de informação"
                                  ]
                                },
                                {
                                  "stepNumber": 4,
                                  "title": "Aplicação e Validação do Tratamento",
                                  "subSteps": [
                                    "Aplique o tratamento escolhido no dataset (ex: df.clip(lower, upper)).",
                                    "Gere gráficos comparativos: boxplots antes/depois.",
                                    "Recalcule estatísticas e compare skewness, kurtosis.",
                                    "Teste robustez com modelo simples (ex: regressão linear) antes/depois.",
                                    "Salve dataset tratado e log de mudanças."
                                  ],
                                  "verification": "Dataset tratado salvo, gráficos comparativos mostram redução de outliers sem distorção excessiva.",
                                  "estimatedTime": "1 hora",
                                  "materials": [
                                    "Python com Pandas, Scikit-learn",
                                    "Dataset tratado"
                                  ],
                                  "tips": "Sempre backup o original; valide com cross-validation em modelos.",
                                  "learningObjective": "Implementar e verificar eficácia do tratamento de outliers.",
                                  "commonMistakes": [
                                    "Sobretatar dados (perder variância)",
                                    "Não comparar métricas antes/depois",
                                    "Esquecer de atualizar índices"
                                  ]
                                }
                              ],
                              "practicalExample": "Em um dataset de salários de funcionários de uma empresa (usando Pandas), detecte outliers visuais via boxplot (salários > R$500k), confirme com IQR, decida winsorizar em 95% percentil baseado no domínio (executivos raros), aplique e valide com redução de skewness de 2.5 para 0.8.",
                              "finalVerifications": [
                                "Outliers detectados corretamente em pelo menos 90% dos casos via visual e estatístico.",
                                "Decisão de tratamento justificada com contexto de domínio.",
                                "Gráficos comparativos mostram melhoria na distribuição.",
                                "Estatísticas pós-tratamento indicam redução de extremos sem perda excessiva de dados.",
                                "Relatório documenta todo processo.",
                                "Modelo de teste (ex: regressão) melhora R² após tratamento."
                              ],
                              "assessmentCriteria": [
                                "Precisão na detecção: >85% de acurácia comparado a thresholds padrão.",
                                "Justificativa contextual: clara e alinhada ao domínio.",
                                "Escolha de método: apropriado (não genérico).",
                                "Validação quantitativa: métricas antes/depois melhoradas.",
                                "Código limpo e reprodutível.",
                                "Documentação completa sem erros."
                              ],
                              "crossCurricularConnections": [
                                "Estatística Descritiva (IQR, Z-score)",
                                "Programação em Python (Pandas, visualizações)",
                                "Análise Exploratória de Dados (EDA)",
                                "Machine Learning (impacto em modelos)",
                                "Ética em Dados (manipulação responsável)"
                              ],
                              "realWorldApplication": "Na limpeza de dados para machine learning em saúde (detectar medições erradas em exames), finanças (fraudes em transações) ou e-commerce (preços anômalos), garantindo modelos precisos e decisões confiáveis."
                            },
                            "estimatedTime": "1 hora",
                            "difficulty": "intermediate",
                            "status": "not_started",
                            "prerequisites": []
                          }
                        ]
                      },
                      {
                        "id": "10.1.1.4.4",
                        "name": "Ajuste e Avaliação de Modelos",
                        "description": "Treinar modelos de machine learning, otimizá-los e avaliá-los com métricas adequadas, incluindo exemplos práticos e estudos de caso reais.",
                        "specificSkills": [
                          {
                            "id": "10.1.1.4.4.1",
                            "name": "Treinar modelos supervisionados",
                            "description": "Implementar ajuste de modelos como regressão linear ou árvores de decisão com Scikit-learn (fit, predict), usando split train/test e validação cruzada.",
                            "atomicExpansion": {
                              "steps": [
                                {
                                  "stepNumber": 1,
                                  "title": "Preparar o ambiente e carregar os dados",
                                  "subSteps": [
                                    "Instale as bibliotecas necessárias: scikit-learn, pandas e numpy via pip.",
                                    "Carregue um dataset adequado, como o Boston Housing ou Iris, usando pandas.read_csv() ou load_dataset do sklearn.",
                                    "Explore os dados com .head(), .describe() e .info() para entender features e target.",
                                    "Separe features (X) e target (y) usando iloc ou drop.",
                                    "Trate valores ausentes com .fillna() ou SimpleImputer se necessário."
                                  ],
                                  "verification": "Dataset carregado sem erros, X e y com shapes corretos e sem NaNs (verifique com X.isnull().sum()).",
                                  "estimatedTime": "20 minutos",
                                  "materials": [
                                    "Python 3.8+, Jupyter Notebook, bibliotecas: scikit-learn, pandas, numpy"
                                  ],
                                  "tips": "Use datasets built-in do sklearn para testes rápidos, como load_boston() ou load_iris().",
                                  "learningObjective": "Configurar ambiente e preparar dados limpos para treinamento de modelo.",
                                  "commonMistakes": [
                                    "Esquecer de importar bibliotecas",
                                    "Confundir features e target",
                                    "Ignorar valores ausentes"
                                  ]
                                },
                                {
                                  "stepNumber": 2,
                                  "title": "Dividir dados em conjuntos de treino e teste",
                                  "subSteps": [
                                    "Importe train_test_split de sklearn.model_selection.",
                                    "Defina test_size=0.2 ou 0.3 e random_state=42 para reprodutibilidade.",
                                    "Execute X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42).",
                                    "Verifique shapes: len(X_train) deve ser ~80% do total.",
                                    "Opcional: Use stratify=y para classificação balanceada."
                                  ],
                                  "verification": "Shapes confirmados (ex: (400, 13) para train, (100, 13) para test) e proporções corretas.",
                                  "estimatedTime": "15 minutos",
                                  "materials": [
                                    "Código Python do Step 1"
                                  ],
                                  "tips": "Sempre use random_state para resultados consistentes em testes.",
                                  "learningObjective": "Aplicar split train/test para avaliação imparcial do modelo.",
                                  "commonMistakes": [
                                    "Usar test_size=1.0 acidentalmente",
                                    "Não definir random_state",
                                    "Vazar dados de test para train"
                                  ]
                                },
                                {
                                  "stepNumber": 3,
                                  "title": "Inicializar e treinar o modelo supervisionado",
                                  "subSteps": [
                                    "Importe o modelo: LinearRegression() ou DecisionTreeRegressor/Classifier de sklearn.",
                                    "Crie instância: model = LinearRegression() ou DecisionTreeClassifier().",
                                    "Treine com model.fit(X_train, y_train).",
                                    "Verifique se fit() executou sem erros.",
                                    "Acesse atributos como model.coef_ para regressão ou model.feature_importances_."
                                  ],
                                  "verification": "Modelo treinado (model.fit() sem exceções) e atributos acessíveis sem erro.",
                                  "estimatedTime": "25 minutos",
                                  "materials": [
                                    "Dados divididos do Step 2"
                                  ],
                                  "tips": "Comece com modelos simples como LinearRegression para depuração rápida.",
                                  "learningObjective": "Implementar ajuste (fit) de modelos supervisionados com Scikit-learn.",
                                  "commonMistakes": [
                                    "fit() com dados errados (X_test em vez de X_train)",
                                    "Esquecer parênteses em instância",
                                    "Overfitting sem CV ainda"
                                  ]
                                },
                                {
                                  "stepNumber": 4,
                                  "title": "Realizar predições e validação cruzada",
                                  "subSteps": [
                                    "Faça predições: y_pred_train = model.predict(X_train); y_pred_test = model.predict(X_test).",
                                    "Importe cross_val_score de sklearn.model_selection.",
                                    "Execute scores = cross_val_score(model, X, y, cv=5, scoring='accuracy' ou 'r2').",
                                    "Calcule métricas: mean(score), std(score).",
                                    "Compare train vs test para detectar overfitting."
                                  ],
                                  "verification": "Predições geradas, CV scores calculados (média > 0.7 para bons modelos) e sem warnings.",
                                  "estimatedTime": "30 minutos",
                                  "materials": [
                                    "Modelo treinado do Step 3"
                                  ],
                                  "tips": "Use scoring='r2' para regressão, 'accuracy' para classificação; cv=5 ou 10.",
                                  "learningObjective": "Avaliar modelo com predict e validação cruzada para robustez.",
                                  "commonMistakes": [
                                    "Predict em dados não vistos",
                                    "cv=1 (inútil)",
                                    "Ignorar std nos scores CV"
                                  ]
                                }
                              ],
                              "practicalExample": "Usando o dataset Iris: Carregue iris = load_iris(), treine DecisionTreeClassifier com split 80/20, fit em train, predict em test e cross_val_score(cv=5, scoring='accuracy') resultando em ~95% accuracy.",
                              "finalVerifications": [
                                "Modelo fit() executado sem erros em train set.",
                                "Predições geradas para test set com shapes corretos.",
                                "Validação cruzada com pelo menos 5 folds e média de score > 0.75.",
                                "Comparação train/test scores mostra pouca diferença (<0.1).",
                                "Código reproduzível com random_state=42.",
                                "Sem vazamento de dados detectado."
                              ],
                              "assessmentCriteria": [
                                "Precisão do split train/test (80/20 ratio exato).",
                                "Tempo de fit() razoável (<1s para datasets pequenos).",
                                "Score CV médio superior a baseline (ex: >0.8 para Iris).",
                                "Detecção correta de overfitting via train/test gap.",
                                "Uso apropriado de métricas (r2/accuracy conforme tarefa).",
                                "Código limpo, comentado e modular."
                              ],
                              "crossCurricularConnections": [
                                "Estatística: Entender métricas como R² e variância nos scores CV.",
                                "Programação: Manipulação de arrays NumPy e DataFrames Pandas.",
                                "Matemática: Conceitos de regressão linear e árvores de decisão.",
                                "Ética em Dados: Evitar overfitting e vazamento para modelos justos."
                              ],
                              "realWorldApplication": "Em e-commerce, treinar modelo de regressão linear para prever vendas futuras baseadas em histórico de marketing, usando CV para garantir generalização em novos clientes e otimizar campanhas."
                            },
                            "estimatedTime": "2 horas",
                            "difficulty": "intermediate",
                            "status": "not_started",
                            "prerequisites": []
                          },
                          {
                            "id": "10.1.1.4.4.2",
                            "name": "Avaliar desempenho de modelos",
                            "description": "Calcular métricas como acurácia, precisão, recall, F1-score e ROC-AUC para classificação; MSE, R² para regressão, interpretando resultados em contexto (ex.: churn prediction).",
                            "atomicExpansion": {
                              "steps": [
                                {
                                  "stepNumber": 1,
                                  "title": "Preparar dados para avaliação de modelo",
                                  "subSteps": [
                                    "Carregar o conjunto de dados de teste com labels verdadeiras (y_true)",
                                    "Gerar previsões do modelo treinado (y_pred) usando dados de teste",
                                    "Verificar dimensões e tipos de dados de y_true e y_pred (binário/multiclasse para classificação; contínuo para regressão)",
                                    "Tratar outliers, valores ausentes ou desbalanceamento de classes",
                                    "Dividir dados se necessário para validação cruzada"
                                  ],
                                  "verification": "Executar assert y_true.shape == y_pred.shape e imprimir amostras para inspeção visual.",
                                  "estimatedTime": "30 minutos",
                                  "materials": [
                                    "Notebook Jupyter ou Google Colab",
                                    "Biblioteca scikit-learn",
                                    "Dataset de teste (ex: churn.csv)"
                                  ],
                                  "tips": "Use train_test_split para garantir dados inéditos; padronize escalas para regressão.",
                                  "learningObjective": "Preparar conjuntos de dados limpos e compatíveis para cálculo preciso de métricas.",
                                  "commonMistakes": [
                                    "Avaliar com dados de treinamento",
                                    "Ignorar desbalanceamento de classes",
                                    "Confundir y_true com y_pred"
                                  ]
                                },
                                {
                                  "stepNumber": 2,
                                  "title": "Calcular métricas para tarefas de classificação",
                                  "subSteps": [
                                    "Importar métricas do sklearn.metrics: accuracy_score, precision_score, recall_score, f1_score, roc_auc_score",
                                    "Calcular acurácia: accuracy_score(y_true, y_pred)",
                                    "Calcular precisão, recall e F1: precision_score, recall_score, f1_score (especificar average='weighted' para multiclasse)",
                                    "Calcular ROC-AUC: roc_auc_score(y_true, y_proba) com probabilidades",
                                    "Gerar e visualizar matriz de confusão com confusion_matrix e heatmap"
                                  ],
                                  "verification": "Imprimir todas as métricas numéricas e matriz de confusão; comparar com valores esperados manuais.",
                                  "estimatedTime": "45 minutos",
                                  "materials": [
                                    "scikit-learn",
                                    "matplotlib ou seaborn para visualização",
                                    "Código Python pronto"
                                  ],
                                  "tips": "Para classes desbalanceadas, priorize F1-score sobre acurácia; use probabilidades para ROC-AUC.",
                                  "learningObjective": "Dominar o cálculo e interpretação inicial de métricas de classificação.",
                                  "commonMistakes": [
                                    "Usar y_pred em vez de probabilidades para ROC-AUC",
                                    "Esquecer average em multiclasse",
                                    "Interpretar acurácia como única métrica"
                                  ]
                                },
                                {
                                  "stepNumber": 3,
                                  "title": "Calcular métricas para tarefas de regressão",
                                  "subSteps": [
                                    "Importar métricas do sklearn.metrics: mean_squared_error, r2_score",
                                    "Calcular MSE e RMSE: mean_squared_error(y_true, y_pred) e raiz quadrada",
                                    "Calcular R²: r2_score(y_true, y_pred)",
                                    "Calcular MAE para robustez: mean_absolute_error(y_true, y_pred)",
                                    "Visualizar resíduos com scatter plot (y_true vs y_pred e resíduos)"
                                  ],
                                  "verification": "Imprimir MSE, RMSE, R² e MAE; verificar R² próximo de 1 para bom ajuste.",
                                  "estimatedTime": "30 minutos",
                                  "materials": [
                                    "scikit-learn",
                                    "matplotlib",
                                    "Dataset de regressão (ex: housing_prices.csv)"
                                  ],
                                  "tips": "RMSE penaliza mais erros grandes; compare com baseline (média de y_true).",
                                  "learningObjective": "Aplicar e validar métricas específicas para predições contínuas.",
                                  "commonMistakes": [
                                    "Confundir MSE com RMSE sem raiz",
                                    "Ignorar escala dos dados",
                                    "Usar R² sem contexto de baseline"
                                  ]
                                },
                                {
                                  "stepNumber": 4,
                                  "title": "Interpretar métricas e gerar relatório contextual",
                                  "subSteps": [
                                    "Comparar métricas com baselines (ex: modelo dummy ou anterior)",
                                    "Analisar trade-offs: alto recall vs precisão em churn prediction",
                                    "Visualizar curvas ROC/PR e resíduos para insights qualitativos",
                                    "Documentar limitações (ex: overfitting se validação < treino)",
                                    "Recomendar ações: refinar modelo ou coletar mais dados"
                                  ],
                                  "verification": "Produzir relatório escrito com tabelas, gráficos e conclusões acionáveis.",
                                  "estimatedTime": "45 minutos",
                                  "materials": [
                                    "Pandas para tabelas",
                                    "Matplotlib/Seaborn para plots",
                                    "Template de relatório Markdown"
                                  ],
                                  "tips": "Contextualize: em churn, priorize recall para não perder clientes; R² > 0.8 é bom em finanças.",
                                  "learningObjective": "Interpretar métricas holisticamente para decisões em ciência de dados.",
                                  "commonMistakes": [
                                    "Ignorar contexto de negócio",
                                    "Sobrevalorizar uma métrica única",
                                    "Não comparar com baselines"
                                  ]
                                }
                              ],
                              "practicalExample": "Em um dataset de churn de clientes de telecom (ex: Kaggle Telco Churn), treine um RandomForestClassifier, gere y_pred e y_proba, calcule precisão=0.82, recall=0.75, F1=0.78, ROC-AUC=0.85; interprete alto recall como sucesso em reter clientes em risco.",
                              "finalVerifications": [
                                "Calcula corretamente todas as métricas para classificação e regressão em dataset exemplo",
                                "Gera matriz de confusão e curva ROC visualmente correta",
                                "Interpreta trade-offs (ex: F1 baixo indica desbalanceamento)",
                                "Compara com baseline e justifica qualidade do modelo",
                                "Produz relatório com gráficos e recomendações",
                                "Identifica limitações como overfitting via gap treino/teste"
                              ],
                              "assessmentCriteria": [
                                "Correção matemática das métricas (acima de 95%)",
                                "Uso adequado de funções sklearn sem erros de sintaxe",
                                "Interpretação contextual precisa e alinhada ao negócio",
                                "Visualizações claras e informativas",
                                "Relatório completo com baselines e ações",
                                "Tempo de execução eficiente e código limpo"
                              ],
                              "crossCurricularConnections": [
                                "Estatística: Probabilidades condicionais em precisão/recall",
                                "Programação: Manipulação de arrays NumPy/Pandas",
                                "Negócios: Trade-offs custo-benefício em predições",
                                "Visualização de Dados: Gráficos ROC e resíduos",
                                "Ética em IA: Viés em métricas desbalanceadas"
                              ],
                              "realWorldApplication": "Em predição de churn para e-commerce, alto recall identifica 90% dos clientes em risco, permitindo campanhas de retenção que reduzem perda de receita em 15%; em regressão de preços imobiliários, R²=0.85 guia avaliações precisas para corretores."
                            },
                            "estimatedTime": "2 horas",
                            "difficulty": "intermediate",
                            "status": "not_started",
                            "prerequisites": []
                          },
                          {
                            "id": "10.1.1.4.4.3",
                            "name": "Otimizar hiperparâmetros",
                            "description": "Aplicar GridSearchCV ou RandomizedSearchCV para tuning, evitando overfitting com regularização e early stopping.",
                            "atomicExpansion": {
                              "steps": [
                                {
                                  "stepNumber": 1,
                                  "title": "Preparar o ambiente e modelo baseline",
                                  "subSteps": [
                                    "Carregue o dataset usando pandas e sklearn.datasets.",
                                    "Divida os dados em treino e teste com train_test_split (test_size=0.2, random_state=42).",
                                    "Treine um modelo baseline simples (ex: RandomForestClassifier com parâmetros default).",
                                    "Calcule métricas baseline como accuracy_score e cross_val_score.",
                                    "Visualize a curva de aprendizado inicial com learning_curve."
                                  ],
                                  "verification": "Modelo baseline treinado e métricas calculadas; score de CV reportado.",
                                  "estimatedTime": "1 hora",
                                  "materials": [
                                    "Jupyter Notebook",
                                    "Bibliotecas: scikit-learn, pandas, matplotlib",
                                    "Dataset exemplo (ex: breast_cancer)"
                                  ],
                                  "tips": "Use random_state para reprodutibilidade; comece com dataset pequeno para testes rápidos.",
                                  "learningObjective": "Estabelecer uma referência de performance antes da otimização.",
                                  "commonMistakes": [
                                    "Não dividir dados corretamente",
                                    "Ignorar random_state",
                                    "Usar todo dataset para treino"
                                  ]
                                },
                                {
                                  "stepNumber": 2,
                                  "title": "Definir o espaço de hiperparâmetros para busca",
                                  "subSteps": [
                                    "Identifique hiperparâmetros chave do modelo (ex: n_estimators, max_depth para RandomForest).",
                                    "Crie um dicionário param_grid com 3-5 valores por hiperparâmetro.",
                                    "Para RandomizedSearchCV, defina param_distributions com distribuições (ex: randint, uniform).",
                                    "Considere trade-offs: mais valores aumentam precisão mas tempo de computação.",
                                    "Documente escolhas baseadas em documentação oficial do modelo."
                                  ],
                                  "verification": "Dicionário param_grid ou param_distributions criado e validado com print.",
                                  "estimatedTime": "45 minutos",
                                  "materials": [
                                    "Documentação scikit-learn (modelos específicos)",
                                    "Jupyter Notebook"
                                  ],
                                  "tips": "Comece com grids pequenos; use log scale para parâmetros como learning_rate.",
                                  "learningObjective": "Compreender e especificar espaços de busca eficientes.",
                                  "commonMistakes": [
                                    "Incluir parâmetros não existentes",
                                    "Grid muito grande causando explosão combinatória",
                                    "Ignorar tipos de dados nos params"
                                  ]
                                },
                                {
                                  "stepNumber": 3,
                                  "title": "Executar GridSearchCV ou RandomizedSearchCV",
                                  "subSteps": [
                                    "Instancie GridSearchCV(modelo, param_grid, cv=5, scoring='accuracy').",
                                    "Ou RandomizedSearchCV com n_iter=50 para buscas mais rápidas.",
                                    "Ajuste o searcher com fit(X_train, y_train).",
                                    "Extraia melhores parâmetros com best_params_ e best_score_.",
                                    "Compare com baseline e salve resultados em DataFrame."
                                  ],
                                  "verification": "Best params e score obtidos; tempo de execução registrado.",
                                  "estimatedTime": "2 horas",
                                  "materials": [
                                    "Scikit-learn GridSearchCV/RandomizedSearchCV",
                                    "Hardware com CPU/GPU suficiente"
                                  ],
                                  "tips": "Use n_jobs=-1 para paralelização; monitore com verbose=2.",
                                  "learningObjective": "Aplicar busca exaustiva ou aleatória para tuning automatizado.",
                                  "commonMistakes": [
                                    "Esquecer cv folds",
                                    "Scoring incorreto para o problema",
                                    "Não usar n_jobs"
                                  ]
                                },
                                {
                                  "stepNumber": 4,
                                  "title": "Aplicar regularização e early stopping para evitar overfitting",
                                  "subSteps": [
                                    "Inclua hiperparâmetros de regularização no grid (ex: alpha para Ridge, C para SVM).",
                                    "Para modelos sequenciais (ex: XGBoost), configure early_stopping_rounds=10.",
                                    "Treine com validation_split e monitore perda de validação.",
                                    "Plote learning curves pós-otimização para detectar overfitting.",
                                    "Ajuste manualmente se necessário e reexecute busca."
                                  ],
                                  "verification": "Curvas mostram convergência sem gap treino/validação; early stopping ativado.",
                                  "estimatedTime": "1.5 horas",
                                  "materials": [
                                    "Scikit-learn pipelines",
                                    "XGBoost ou LightGBM para exemplos avançados"
                                  ],
                                  "tips": "Combine com Pipeline para pré-processamento; use validation_curve para análise.",
                                  "learningObjective": "Integrar técnicas anti-overfitting no processo de tuning.",
                                  "commonMistakes": [
                                    "Regularização muito forte subajustando",
                                    "Não monitorar validação separada",
                                    "Ignorar early stopping em boosts"
                                  ]
                                },
                                {
                                  "stepNumber": 5,
                                  "title": "Avaliar e finalizar o modelo otimizado",
                                  "subSteps": [
                                    "Teste o best_estimator_ no conjunto de teste com predict.",
                                    "Calcule métricas finais (confusion_matrix, classification_report).",
                                    "Compare gains vs baseline e tempo gasto.",
                                    "Salve modelo com joblib.dump para reutilização.",
                                    "Documente lições aprendidas em relatório Markdown."
                                  ],
                                  "verification": "Relatório com métricas finais, gains >5% e modelo salvo.",
                                  "estimatedTime": "45 minutos",
                                  "materials": [
                                    "Joblib",
                                    "Seaborn para visualizações"
                                  ],
                                  "tips": "Sempre valide em hold-out set; considere ensemble se gains baixos.",
                                  "learningObjective": "Validar tuning e preparar para deployment.",
                                  "commonMistakes": [
                                    "Testar em treino",
                                    "Ignorar custo computacional",
                                    "Não documentar"
                                  ]
                                }
                              ],
                              "practicalExample": "No dataset Breast Cancer do sklearn, otimize um RandomForestClassifier com GridSearchCV variando n_estimators=[50,100,200], max_depth=[None,10,20] e min_samples_split=[2,5]. Aplique regularização via max_depth e early stopping em validação, alcançando accuracy de 95% vs 92% baseline.",
                              "finalVerifications": [
                                "Score CV do modelo otimizado supera baseline em pelo menos 5%.",
                                "Curvas de aprendizado mostram ausência de overfitting (gap <10%).",
                                "Melhores hiperparâmetros identificados e reproduzíveis.",
                                "Tempo de tuning razoável (<1h para grid médio).",
                                "Modelo salvo e testado em hold-out com métricas reportadas.",
                                "Regularização aplicada com validação cruzada estável."
                              ],
                              "assessmentCriteria": [
                                "Precisão na definição do param_grid (cobertura relevante).",
                                "Correto uso de CV e scoring metrics.",
                                "Integração eficaz de regularização/early stopping.",
                                "Análise qualitativa de resultados (curvas, comparações).",
                                "Eficiência computacional (uso de n_jobs, grids otimizados).",
                                "Documentação clara de processo e lições."
                              ],
                              "crossCurricularConnections": [
                                "Estatística: Validação cruzada e distribuições probabilísticas em RandomizedSearch.",
                                "Programação: Otimização paralela e pipelines em Python.",
                                "Matemática Computacional: Problemas de otimização combinatorial.",
                                "Ética em IA: Garantir generalização para evitar bias em produção.",
                                "Engenharia de Software: Versionamento de modelos e experiment tracking."
                              ],
                              "realWorldApplication": "Em empresas como Netflix ou bancos, otimizar hiperparâmetros de modelos de recomendação ou detecção de fraudes melhora precisão em 10-20%, reduzindo falsos positivos e custos operacionais, essencial em pipelines de ML production."
                            },
                            "estimatedTime": "1.5 horas",
                            "difficulty": "advanced",
                            "status": "not_started",
                            "prerequisites": []
                          },
                          {
                            "id": "10.1.1.4.4.4",
                            "name": "Analisar estudos de caso",
                            "description": "Estudar casos reais como previsão de vendas (dataset Kaggle) ou detecção de fraudes, aplicando o pipeline completo e discutindo lições aprendidas.",
                            "atomicExpansion": {
                              "steps": [
                                {
                                  "stepNumber": 1,
                                  "title": "Selecionar e Compreender o Estudo de Caso",
                                  "subSteps": [
                                    "Pesquise e selecione um dataset relevante no Kaggle, como previsão de vendas ou detecção de fraudes.",
                                    "Leia a descrição do problema, objetivos e contexto do dataset.",
                                    "Identifique as variáveis principais, métricas de sucesso e desafios potenciais.",
                                    "Crie um resumo inicial do caso, incluindo hipóteses sobre o problema.",
                                    "Documente fontes e referências do caso real."
                                  ],
                                  "verification": "Resumo escrito do caso com hipóteses claras e dataset baixado.",
                                  "estimatedTime": "1 hora",
                                  "materials": [
                                    "Acesso ao Kaggle",
                                    "Notebook Jupyter ou Google Colab",
                                    "Internet"
                                  ],
                                  "tips": "Escolha um dataset com pelo menos 10k amostras para realismo.",
                                  "learningObjective": "Entender o contexto do problema de negócios e formular hipóteses iniciais.",
                                  "commonMistakes": [
                                    "Ignorar o contexto de negócios",
                                    "Selecionar datasets muito simples sem desafios reais"
                                  ]
                                },
                                {
                                  "stepNumber": 2,
                                  "title": "Preparar e Explorar os Dados",
                                  "subSteps": [
                                    "Carregue o dataset e realize EDA (Exploratory Data Analysis) com visualizações.",
                                    "Trate valores ausentes, outliers e codifique variáveis categóricas.",
                                    "Crie features engineeradas baseadas no domínio do problema.",
                                    "Divida o dataset em treino, validação e teste.",
                                    "Documente transformações aplicadas."
                                  ],
                                  "verification": "Dataset limpo e preparado salvo, com relatório de EDA.",
                                  "estimatedTime": "2 horas",
                                  "materials": [
                                    "Python (pandas, matplotlib, seaborn)",
                                    "Notebook Jupyter"
                                  ],
                                  "tips": "Use gráficos para identificar padrões antes de modelar.",
                                  "learningObjective": "Dominar pré-processamento de dados para um pipeline robusto.",
                                  "commonMistakes": [
                                    "Não tratar desbalanceamento de classes",
                                    "Sobrepasar o dataset de treino com features leak"
                                  ]
                                },
                                {
                                  "stepNumber": 3,
                                  "title": "Construir e Ajustar Modelos",
                                  "subSteps": [
                                    "Selecione e implemente 2-3 algoritmos adequados (ex: regressão linear, random forest, XGBoost).",
                                    "Aplique validação cruzada e ajuste hiperparâmetros com GridSearch ou Optuna.",
                                    "Treine os modelos e compare baselines.",
                                    "Salve os melhores modelos.",
                                    "Registre métricas intermediárias."
                                  ],
                                  "verification": "Modelos treinados com hiperparâmetros otimizados e métricas registradas.",
                                  "estimatedTime": "3 horas",
                                  "materials": [
                                    "Scikit-learn, XGBoost",
                                    "Notebook Jupyter"
                                  ],
                                  "tips": "Sempre compare com baseline simples para validar ganhos.",
                                  "learningObjective": "Aplicar técnicas de ajuste de modelos para performance ótima.",
                                  "commonMistakes": [
                                    "Overfitting sem validação cruzada",
                                    "Ignorar trade-offs entre bias e variância"
                                  ]
                                },
                                {
                                  "stepNumber": 4,
                                  "title": "Avaliar e Interpretar Resultados",
                                  "subSteps": [
                                    "Calcule métricas finais (ex: RMSE para previsão, AUC para fraudes) no conjunto de teste.",
                                    "Realize análise de importância de features e SHAP values para interpretabilidade.",
                                    "Compare performance entre modelos e com benchmarks do Kaggle.",
                                    "Identifique limitações e vieses.",
                                    "Visualize resultados com gráficos de performance."
                                  ],
                                  "verification": "Relatório de avaliação com métricas, gráficos e interpretações.",
                                  "estimatedTime": "2 horas",
                                  "materials": [
                                    "SHAP library",
                                    "Matplotlib/Seaborn"
                                  ],
                                  "tips": "Use métricas alinhadas ao problema de negócios, não só accuracy.",
                                  "learningObjective": "Interpretar resultados de modelos no contexto do problema.",
                                  "commonMistakes": [
                                    "Focar só em métricas técnicas sem ligação ao negócio",
                                    "Não discutir incertezas"
                                  ]
                                },
                                {
                                  "stepNumber": 5,
                                  "title": "Discutir Lições Aprendidas e Relatar",
                                  "subSteps": [
                                    "Resuma sucessos, falhas e lições do pipeline completo.",
                                    "Discuta implicações para deploy e monitoramento em produção.",
                                    "Compare com outros estudos de caso semelhantes.",
                                    "Crie um relatório final ou apresentação.",
                                    "Sugira melhorias futuras."
                                  ],
                                  "verification": "Relatório final completo com lições e recomendações.",
                                  "estimatedTime": "1.5 horas",
                                  "materials": [
                                    "Markdown ou PowerPoint para relatório"
                                  ],
                                  "tips": "Estruture o relatório como: Problema > Método > Resultados > Insights.",
                                  "learningObjective": "Sintetizar análises em insights acionáveis.",
                                  "commonMistakes": [
                                    "Não conectar lições ao mundo real",
                                    "Relatório superficial sem profundidade"
                                  ]
                                }
                              ],
                              "practicalExample": "Usando o dataset 'Store Sales - Time Series Forecasting' do Kaggle: aplique o pipeline para prever vendas semanais de lojas no Equador, ajustando modelos como Prophet e XGBoost, avaliando com RMSE e discutindo impactos de feriados e promoções.",
                              "finalVerifications": [
                                "Pipeline completo executado end-to-end sem erros.",
                                "Relatório com métricas finais superando baseline em pelo menos 10%.",
                                "Interpretação clara de features mais impactantes.",
                                "Lições aprendidas documentadas com 5+ insights.",
                                "Código reproduzível e versionado (ex: GitHub).",
                                "Discussão de limitações e próximos passos."
                              ],
                              "assessmentCriteria": [
                                "Profundidade da EDA e feature engineering (30%).",
                                "Qualidade do ajuste de modelos e métricas (25%).",
                                "Interpretabilidade e análise de resultados (20%).",
                                "Clareza das lições aprendidas e relatório (15%).",
                                "Reprodutibilidade e código limpo (10%)."
                              ],
                              "crossCurricularConnections": [
                                "Estatística: Validação cruzada e testes de hipótese.",
                                "Programação: Manipulação de dados em Python/R.",
                                "Negócios: Alinhamento de modelos com KPIs empresariais.",
                                "Ética: Detecção de vieses em detecção de fraudes.",
                                "Comunicação: Relatórios e visualizações para stakeholders."
                              ],
                              "realWorldApplication": "Em empresas como bancos (detecção de fraudes em transações) ou varejo (previsão de demanda para estoque), onde analistas de dados usam estudos de caso para otimizar modelos, reduzir custos e tomar decisões baseadas em evidências, como prever vendas para Black Friday."
                            },
                            "estimatedTime": "2 horas",
                            "difficulty": "intermediate",
                            "status": "not_started",
                            "prerequisites": []
                          }
                        ]
                      }
                    ]
                  },
                  {
                    "id": "10.1.1.5",
                    "name": "Introdução ao Aprendizado de Máquina Supervisionado e Não-supervisionado",
                    "description": "Visão geral dos paradigmas de aprendizado supervisionado e não-supervisionado.",
                    "individualConcepts": [
                      {
                        "id": "10.1.1.5.1",
                        "name": "Aprendizado de Máquina Supervisionado",
                        "description": "Paradigma de aprendizado de máquina no qual o modelo é treinado utilizando um conjunto de dados rotulados, onde cada exemplo de entrada está associado a um rótulo ou saída conhecida, permitindo que o algoritmo aprenda a mapear entradas para saídas específicas.",
                        "specificSkills": [
                          {
                            "id": "10.1.1.5.1.1",
                            "name": "Definir aprendizado supervisionado",
                            "description": "Explicar o conceito de aprendizado supervisionado, destacando a presença de dados rotulados (pares entrada-saída) e o objetivo de minimizar o erro de predição em novos dados, com exemplos como classificação e regressão.",
                            "atomicExpansion": {
                              "steps": [
                                {
                                  "stepNumber": 1,
                                  "title": "Compreender o conceito fundamental de aprendizado supervisionado",
                                  "subSteps": [
                                    "Ler definições de fontes confiáveis como livros de ML ou documentação do scikit-learn.",
                                    "Identificar que envolve treinamento com dados rotulados (pares entrada-saída).",
                                    "Explicar verbalmente ou por escrito o que diferencia do aprendizado não-supervisionado.",
                                    "Assistir a um vídeo curto (5 min) sobre o tema.",
                                    "Anotar as palavras-chave: supervisão, rótulos, predição."
                                  ],
                                  "verification": "Escrever um parágrafo de 3-5 linhas definindo o conceito sem consultar materiais.",
                                  "estimatedTime": "25 minutos",
                                  "materials": [
                                    "Acesso à internet",
                                    "Notebook para anotações",
                                    "Vídeo introdutório no YouTube (ex: 'Supervised Learning Explained')"
                                  ],
                                  "tips": "Use a analogia de um aluno aprendendo com um professor que corrige respostas.",
                                  "learningObjective": "Dominar a definição básica e suas características principais.",
                                  "commonMistakes": [
                                    "Confundir com clustering (não-supervisionado)",
                                    "Achar que não precisa de dados rotulados"
                                  ]
                                },
                                {
                                  "stepNumber": 2,
                                  "title": "Explorar dados rotulados e sua importância",
                                  "subSteps": [
                                    "Visualizar exemplos de datasets rotulados (ex: Iris dataset para classificação).",
                                    "Explicar pares entrada-saída: features (X) e labels (y).",
                                    "Discutir por que rótulos são essenciais para 'supervisão'.",
                                    "Criar um dataset simples manualmente com 5 exemplos rotulados.",
                                    "Comparar com dados não-rotulados em uma tabela."
                                  ],
                                  "verification": "Criar e apresentar um exemplo de dataset rotulado com pelo menos 3 entradas.",
                                  "estimatedTime": "30 minutos",
                                  "materials": [
                                    "Planilha Excel ou Google Sheets",
                                    "Dataset Iris do UCI ML Repository"
                                  ],
                                  "tips": "Pense nos rótulos como 'respostas corretas' em um quiz.",
                                  "learningObjective": "Entender a estrutura e o papel dos dados rotulados no processo.",
                                  "commonMistakes": [
                                    "Ignorar a qualidade dos rótulos",
                                    "Confundir features com labels"
                                  ]
                                },
                                {
                                  "stepNumber": 3,
                                  "title": "Analisar o objetivo de minimizar o erro de predição",
                                  "subSteps": [
                                    "Estudar funções de perda (ex: MSE para regressão, cross-entropy para classificação).",
                                    "Explicar o processo de treinamento: ajuste de parâmetros para reduzir erro.",
                                    "Simular com um gráfico simples de erro decrescente ao longo das épocas.",
                                    "Discutir generalização vs. overfitting em novos dados.",
                                    "Calcular erro manual em um exemplo pequeno."
                                  ],
                                  "verification": "Desenhar um gráfico mostrando redução de erro e explicar o que representa.",
                                  "estimatedTime": "35 minutos",
                                  "materials": [
                                    "Papel e caneta ou ferramenta como Desmos/GeoGebra",
                                    "Artigo sobre funções de perda"
                                  ],
                                  "tips": "Visualize o erro como a distância entre predição e realidade.",
                                  "learningObjective": "Compreender o mecanismo de otimização no aprendizado supervisionado.",
                                  "commonMistakes": [
                                    "Focar só em acurácia de treino, ignorando novos dados",
                                    "Não diferenciar perda de erro"
                                  ]
                                },
                                {
                                  "stepNumber": 4,
                                  "title": "Aplicar exemplos práticos de classificação e regressão",
                                  "subSteps": [
                                    "Estudar classificação: exemplo de spam detection (classes: spam/não-spam).",
                                    "Estudar regressão: exemplo de previsão de preços de casas (valor contínuo).",
                                    "Comparar os dois em uma tabela: tipo de output, exemplos de datasets.",
                                    "Pesquisar um caso real para cada (ex: Titanic dataset para classificação).",
                                    "Explicar como minimizar erro em cada contexto."
                                  ],
                                  "verification": "Criar flashcards com um exemplo de cada e explicá-los oralmente.",
                                  "estimatedTime": "30 minutos",
                                  "materials": [
                                    "Datasets: Titanic (Kaggle), Boston Housing",
                                    "Flashcard app como Anki"
                                  ],
                                  "tips": "Classificação é 'escolher categoria', regressão é 'prever número'.",
                                  "learningObjective": "Diferenciar e exemplificar os principais tipos de tarefas supervisionadas.",
                                  "commonMistakes": [
                                    "Confundir regressão com classificação binária",
                                    "Usar exemplos irreais"
                                  ]
                                }
                              ],
                              "practicalExample": "Em um sistema de detecção de spam: dados rotulados incluem e-mails com labels 'spam' ou 'não-spam'. O modelo aprende padrões (palavras-chave) para minimizar erros em novos e-mails, como classificar corretamente 95% dos casos de teste.",
                              "finalVerifications": [
                                "Definir aprendizado supervisionado incluindo dados rotulados.",
                                "Explicar pares entrada-saída com um exemplo.",
                                "Descrever o objetivo de minimizar erro em dados não vistos.",
                                "Dar exemplo de classificação e regressão.",
                                "Diferenciar de aprendizado não-supervisionado.",
                                "Identificar uma função de perda comum."
                              ],
                              "assessmentCriteria": [
                                "Precisão na definição (80% das características chave mencionadas).",
                                "Uso correto de terminologia (rótulos, features, predição).",
                                "Exemplos relevantes e diferenciados.",
                                "Explicação clara do processo de minimização de erro.",
                                "Demonstração de compreensão via analogias ou diagramas.",
                                "Capacidade de generalizar para novos cenários."
                              ],
                              "crossCurricularConnections": [
                                "Estatística: Cálculo de métricas de erro e distribuições probabilísticas.",
                                "Programação: Implementação com bibliotecas como scikit-learn em Python.",
                                "Matemática: Otimização e funções de perda (cálculo diferencial).",
                                "Informática: Manipulação de dados e visualização (Pandas, Matplotlib).",
                                "Ética: Viés em dados rotulados e impactos sociais."
                              ],
                              "realWorldApplication": "Aplicado em assistentes virtuais como Siri (classificação de comandos), previsão de vendas em e-commerces como Amazon (regressão), detecção de fraudes em bancos e diagnósticos médicos automatizados."
                            },
                            "estimatedTime": "1 hora",
                            "difficulty": "beginner",
                            "status": "not_started",
                            "prerequisites": []
                          },
                          {
                            "id": "10.1.1.5.1.2",
                            "name": "Identificar tipos de tarefas supervisionadas",
                            "description": "Diferenciar tarefas de classificação (predizer categorias discretas, ex: spam/não spam) e regressão (predizer valores contínuos, ex: preço de casas), relacionando com problemas reais da ciência de dados.",
                            "atomicExpansion": {
                              "steps": [
                                {
                                  "stepNumber": 1,
                                  "title": "Compreender os Fundamentos do Aprendizado de Máquina Supervisionado",
                                  "subSteps": [
                                    "Defina aprendizado supervisionado como um tipo de ML onde o modelo é treinado com dados rotulados (entradas e saídas conhecidas).",
                                    "Identifique os componentes principais: features (entradas), labels (saídas) e o objetivo de generalizar para novos dados.",
                                    "Explore a importância dos dados rotulados e como eles diferem do não-supervisionado.",
                                    "Revise exemplos iniciais de problemas supervisionados na ciência de dados.",
                                    "Anote as diferenças chave entre supervisionado e outros tipos de ML."
                                  ],
                                  "verification": "Resuma em 3 frases os conceitos fundamentais e compartilhe com um colega ou em um fórum para feedback.",
                                  "estimatedTime": "30 minutos",
                                  "materials": [
                                    "Vídeo introdutório no YouTube sobre ML Supervisionado (ex: canal StatQuest)",
                                    "Notebook Jupyter vazio para anotações"
                                  ],
                                  "tips": "Use analogias cotidianas, como ensinar uma criança a identificar frutas com exemplos rotulados.",
                                  "learningObjective": "Entender o que é aprendizado supervisionado e sua base em dados rotulados.",
                                  "commonMistakes": "Confundir com aprendizado não-supervisionado (sem labels) ou semi-supervisionado."
                                },
                                {
                                  "stepNumber": 2,
                                  "title": "Explorar Tarefas de Classificação",
                                  "subSteps": [
                                    "Defina classificação como predição de categorias discretas ou classes (ex: spam/não spam, gato/cão).",
                                    "Estude tipos: binária (2 classes) vs multiclasse (múltiplas classes).",
                                    "Analise métricas comuns: acurácia, precisão, recall e F1-score.",
                                    "Examine um dataset simples de classificação, como Iris ou emails spam.",
                                    "Implemente um modelo básico de classificação com scikit-learn (ex: KNN ou Logistic Regression)."
                                  ],
                                  "verification": "Treine um modelo simples e acerte pelo menos 85% em um conjunto de teste pequeno.",
                                  "estimatedTime": "45 minutos",
                                  "materials": [
                                    "Dataset Iris do scikit-learn",
                                    "Documentação scikit-learn para classification",
                                    "Google Colab ou Jupyter Notebook"
                                  ],
                                  "tips": "Visualize os dados com gráficos de dispersão para entender separação de classes.",
                                  "learningObjective": "Dominar conceitos e implementação básica de tarefas de classificação.",
                                  "commonMistakes": "Ignorar desbalanceamento de classes, levando a métricas enviesadas."
                                },
                                {
                                  "stepNumber": 3,
                                  "title": "Explorar Tarefas de Regressão",
                                  "subSteps": [
                                    "Defina regressão como predição de valores contínuos (ex: preço de casas, temperatura).",
                                    "Diferencie regressão linear, polinomial e outras variantes.",
                                    "Analise métricas: MSE (Mean Squared Error), RMSE e R².",
                                    "Carregue e explore um dataset de regressão, como Boston Housing ou preços de casas.",
                                    "Implemente um modelo de regressão linear simples com scikit-learn."
                                  ],
                                  "verification": "Gere predições e calcule RMSE abaixo de um threshold razoável no dataset de teste.",
                                  "estimatedTime": "45 minutos",
                                  "materials": [
                                    "Dataset California Housing do scikit-learn",
                                    "Documentação scikit-learn para regression",
                                    "Google Colab"
                                  ],
                                  "tips": "Plote predições vs valores reais para visualizar erros.",
                                  "learningObjective": "Compreender e aplicar tarefas de regressão com valores contínuos.",
                                  "commonMistakes": "Confundir com classificação ao tratar valores contínuos como categorias."
                                },
                                {
                                  "stepNumber": 4,
                                  "title": "Diferenciar Classificação e Regressão com Exemplos Reais",
                                  "subSteps": [
                                    "Compare saídas: discretas (classificação) vs contínuas (regressão).",
                                    "Mapeie problemas reais: classificação (diagnóstico médico: câncer/sim), regressão (previsão de vendas).",
                                    "Crie uma tabela comparativa com exemplos, métricas e algoritmos comuns.",
                                    "Resolva um exercício misto: identifique o tipo de tarefa em 5 problemas da ciência de dados.",
                                    "Discuta limitações e quando escolher um ou outro."
                                  ],
                                  "verification": "Crie e compartilhe uma tabela comparativa correta, identificando tipos em novos cenários.",
                                  "estimatedTime": "30 minutos",
                                  "materials": [
                                    "Lista de problemas reais de Kaggle",
                                    "Planilha Google Sheets ou Markdown para tabela"
                                  ],
                                  "tips": "Pergunte: 'A saída é uma categoria ou número exato?' para decidir rapidamente.",
                                  "learningObjective": "Diferenciar com precisão tarefas de classificação e regressão em contextos reais.",
                                  "commonMistakes": "Classificar regressões discretizadas incorretamente como classificação pura."
                                }
                              ],
                              "practicalExample": "Em um dataset de emails, use classificação para prever 'spam' ou 'não spam' (discreto); em um dataset de casas, use regressão para prever preço em dólares (contínuo). Implemente ambos no Colab e compare resultados.",
                              "finalVerifications": [
                                "Explique verbalmente a diferença entre classificação e regressão com exemplos.",
                                "Identifique corretamente o tipo de tarefa em 5 problemas dados.",
                                "Treine modelos simples para ambos e interprete métricas básicas.",
                                "Crie uma tabela comparativa precisa.",
                                "Relacione com 3 aplicações reais da ciência de dados.",
                                "Responda a um quiz com 90% de acerto."
                              ],
                              "assessmentCriteria": [
                                "Precisão na definição e diferenciação (80%+ correta).",
                                "Uso correto de exemplos reais e datasets.",
                                "Implementação funcional de modelos básicos.",
                                "Análise adequada de métricas (MSE para regressão, F1 para classificação).",
                                "Tabela comparativa clara e completa.",
                                "Conexão explícita com problemas da ciência de dados."
                              ],
                              "crossCurricularConnections": [
                                "Estatística: Distribuições discretas vs contínuas e métricas de erro.",
                                "Programação: Manipulação de dados com Python/Pandas e scikit-learn.",
                                "Matemática: Funções de perda (cross-entropy para classificação, MSE para regressão).",
                                "Ética em Dados: Viés em labels para tarefas supervisionadas.",
                                "Negócios: Aplicações em marketing (classificação de leads) e finanças (previsão de preços)."
                              ],
                              "realWorldApplication": "Na indústria, classificação detecta fraudes em transações bancárias (discreto: fraudulento/não); regressão prevê demanda de produtos em e-commerce (contínuo: unidades vendidas), otimizando estoques e reduzindo custos."
                            },
                            "estimatedTime": "1 hora",
                            "difficulty": "beginner",
                            "status": "not_started",
                            "prerequisites": []
                          },
                          {
                            "id": "10.1.1.5.1.3",
                            "name": "Descrever o processo de treinamento supervisionado",
                            "description": "Detalhar as etapas principais: divisão de dados em treino/teste, treinamento do modelo para ajustar parâmetros e avaliação usando métricas como acurácia ou erro quadrático médio, assumindo ambiente i.i.d.",
                            "atomicExpansion": {
                              "steps": [
                                {
                                  "stepNumber": 1,
                                  "title": "Compreender os pressupostos e preparar o conjunto de dados",
                                  "subSteps": [
                                    "Explicar o pressuposto de independência e identicamente distribuídos (i.i.d.) nos dados.",
                                    "Carregar e explorar o conjunto de dados (visualizar distribuições, identificar rótulos).",
                                    "Limpar dados se necessário (remover outliers ou valores ausentes).",
                                    "Definir o problema: classificação ou regressão.",
                                    "Garantir que os dados estejam rotulados para supervisão."
                                  ],
                                  "verification": "O aluno pode listar os pressupostos i.i.d. e descrever a preparação inicial dos dados com exemplos.",
                                  "estimatedTime": "15-20 minutos",
                                  "materials": [
                                    "Notebook Jupyter",
                                    "Biblioteca Pandas (Python)",
                                    "Dataset de exemplo (ex: Iris)"
                                  ],
                                  "tips": "Sempre verifique a balanceamento das classes para evitar viés.",
                                  "learningObjective": "Entender os fundamentos teóricos e preparar dados adequadamente para treinamento supervisionado.",
                                  "commonMistakes": [
                                    "Ignorar o pressuposto i.i.d.",
                                    "Não explorar dados antes da divisão",
                                    "Confundir features com labels"
                                  ]
                                },
                                {
                                  "stepNumber": 2,
                                  "title": "Dividir os dados em conjuntos de treinamento e teste",
                                  "subSteps": [
                                    "Escolher proporção de divisão (ex: 80% treino, 20% teste).",
                                    "Usar função train_test_split com random_state para reprodutibilidade.",
                                    "Garantir estratificação em problemas de classificação desbalanceados.",
                                    "Verificar tamanhos dos conjuntos resultantes.",
                                    "Documentar a divisão para transparência."
                                  ],
                                  "verification": "Executar divisão e mostrar os shapes dos conjuntos (ex: X_train.shape).",
                                  "estimatedTime": "10-15 minutos",
                                  "materials": [
                                    "Scikit-learn (train_test_split)",
                                    "Python"
                                  ],
                                  "tips": "Use random_state=42 para resultados consistentes em experimentos.",
                                  "learningObjective": "Dominar a divisão de dados para evitar overfitting e permitir avaliação imparcial.",
                                  "commonMistakes": [
                                    "Divisão desigual ou não estratificada",
                                    "Vazar dados de teste para treino",
                                    "Não fixar seed aleatória"
                                  ]
                                },
                                {
                                  "stepNumber": 3,
                                  "title": "Treinar o modelo ajustando parâmetros",
                                  "subSteps": [
                                    "Selecionar um modelo supervisionado (ex: regressão linear ou KNN).",
                                    "Ajustar o modelo no conjunto de treino usando fit().",
                                    "Monitorar o processo de otimização de parâmetros (ex: gradiente descendente).",
                                    "Salvar o modelo treinado.",
                                    "Visualizar curvas de perda durante o treinamento."
                                  ],
                                  "verification": "O modelo treinado faz previsões coerentes no conjunto de treino.",
                                  "estimatedTime": "20-25 minutos",
                                  "materials": [
                                    "Scikit-learn (modelos como LinearRegression ou KNeighborsClassifier)",
                                    "NumPy"
                                  ],
                                  "tips": "Comece com hiperparâmetros padrão e ajuste iterativamente.",
                                  "learningObjective": "Compreender como o modelo aprende padrões ajustando pesos nos dados de treino.",
                                  "commonMistakes": [
                                    "Treinar no conjunto de teste",
                                    "Não normalizar features",
                                    "Ignorar warnings de convergência"
                                  ]
                                },
                                {
                                  "stepNumber": 4,
                                  "title": "Avaliar o modelo usando métricas apropriadas",
                                  "subSteps": [
                                    "Fazer previsões no conjunto de teste com predict().",
                                    "Calcular métricas: acurácia para classificação, MSE para regressão.",
                                    "Interpretar resultados (ex: acurácia > 80% é bom?).",
                                    "Comparar com baseline (ex: média dos labels).",
                                    "Identificar overfitting se performance em treino >> teste."
                                  ],
                                  "verification": "Relatório com métricas calculadas e interpretação escrita.",
                                  "estimatedTime": "15-20 minutos",
                                  "materials": [
                                    "Scikit-learn (metrics como accuracy_score, mean_squared_error)"
                                  ],
                                  "tips": "Use cross-validation para validação mais robusta.",
                                  "learningObjective": "Avaliar a generalização do modelo e interpretar métricas no contexto i.i.d.",
                                  "commonMistakes": [
                                    "Avaliar apenas em treino",
                                    "Escolher métrica errada para o problema",
                                    "Não contextualizar resultados"
                                  ]
                                }
                              ],
                              "practicalExample": "Usando o dataset Iris (classificação de flores): 1. Divida em 80/20 treino/teste; 2. Treine um KNN com k=5; 3. Avalie acurácia no teste (esperado ~95%); 4. Compare com baseline de classe majoritária.",
                              "finalVerifications": [
                                "Descrever corretamente as 4 etapas principais do processo.",
                                "Explicar o papel da divisão treino/teste na prevenção de overfitting.",
                                "Listar pelo menos 2 métricas e seu significado.",
                                "Mencionar pressuposto i.i.d. e sua importância.",
                                "Implementar um exemplo simples em código e interpretar resultados.",
                                "Identificar cenários onde o processo falha (ex: dados não-i.i.d.)."
                              ],
                              "assessmentCriteria": [
                                "Precisão e sequência correta das etapas descritas (peso 30%).",
                                "Compreensão conceitual de treinamento e avaliação (peso 25%).",
                                "Uso correto de métricas e interpretação (peso 20%).",
                                "Incorporação de pressupostos como i.i.d. (peso 15%).",
                                "Capacidade de aplicar em exemplo prático (peso 10%)."
                              ],
                              "crossCurricularConnections": [
                                "Estatística: Distribuições i.i.d. e testes de hipótese.",
                                "Programação: Implementação em Python com Scikit-learn.",
                                "Matemática: Otimização e cálculo de gradientes.",
                                "Ciência de Dados: Exploração e pré-processamento de dados."
                              ],
                              "realWorldApplication": "Em previsão de preços de imóveis (regressão com MSE) ou detecção de fraudes em bancos (classificação com acurácia), onde modelos treinados em dados históricos avaliam transações novas assumindo padrões i.i.d."
                            },
                            "estimatedTime": "1.5 horas",
                            "difficulty": "intermediate",
                            "status": "not_started",
                            "prerequisites": []
                          }
                        ]
                      },
                      {
                        "id": "10.1.1.5.2",
                        "name": "Aprendizado de Máquina Não-supervisionado",
                        "description": "Paradigma de aprendizado de máquina que opera com dados não rotulados, buscando descobrir padrões, estruturas ou relações inerentes nos dados sem orientação prévia de saídas esperadas.",
                        "specificSkills": [
                          {
                            "id": "10.1.1.5.2.1",
                            "name": "Definir aprendizado não-supervisionado",
                            "description": "Explicar o conceito de aprendizado não-supervisionado, enfatizando a ausência de rótulos e o foco em extração de padrões intrínsecos, como agrupamentos ou reduções de dimensionalidade.",
                            "atomicExpansion": {
                              "steps": [
                                {
                                  "stepNumber": 1,
                                  "title": "Revisar Aprendizado Supervisionado para Contraste",
                                  "subSteps": [
                                    "Lembre-se que no aprendizado supervisionado, os dados têm rótulos (ex.: imagens com 'gato' ou 'cachorro').",
                                    "Identifique a dependência de rótulos para treinar o modelo prever classes ou valores.",
                                    "Discuta limitações: custo de rotulagem e viés nos rótulos.",
                                    "Compare com cenários reais onde rótulos não existem.",
                                    "Anote diferenças chave em um quadro comparativo."
                                  ],
                                  "verification": "Crie um quadro comparativo com pelo menos 3 diferenças entre supervisionado e não-supervisionado.",
                                  "estimatedTime": "20 minutos",
                                  "materials": [
                                    "Papel e caneta ou ferramenta digital como Google Docs",
                                    "Vídeo introdutório de ML (ex.: Khan Academy)"
                                  ],
                                  "tips": "Use analogias cotidianas, como um professor corrigindo provas (supervisionado) vs. crianças brincando sem instruções (não-supervisionado).",
                                  "learningObjective": "Compreender o papel dos rótulos no ML supervisionado para destacar sua ausência no não-supervisionado.",
                                  "commonMistakes": [
                                    "Confundir com reforço, que usa recompensas em vez de rótulos fixos.",
                                    "Ignorar o custo de rotulagem em grandes datasets."
                                  ]
                                },
                                {
                                  "stepNumber": 2,
                                  "title": "Definir o Conceito de Aprendizado Não-Supervisionado",
                                  "subSteps": [
                                    "Defina: processo onde o modelo aprende padrões dos dados sem rótulos fornecidos.",
                                    "Enfatize a descoberta autônoma de estruturas intrínsecas nos dados.",
                                    "Explique o foco em padrões ocultos, não em predições rotuladas.",
                                    "Diferencie de outros tipos: sem feedback externo como em supervisionado.",
                                    "Escreva uma definição pessoal em 1-2 frases."
                                  ],
                                  "verification": "Escreva e recite uma definição clara que inclua 'ausência de rótulos' e 'padrões intrínsecos'.",
                                  "estimatedTime": "15 minutos",
                                  "materials": [
                                    "Notebook para anotações",
                                    "Artigo da Wikipedia sobre Unsupervised Learning (primeira seção)"
                                  ],
                                  "tips": "Pense em 'exploração livre' dos dados, como minerar ouro sem mapa.",
                                  "learningObjective": "Formular uma definição precisa e concisa de aprendizado não-supervisionado.",
                                  "commonMistakes": [
                                    "Confundir com semi-supervisionado, que usa alguns rótulos.",
                                    "Achar que é 'aleatório' em vez de baseado em padrões estatísticos."
                                  ]
                                },
                                {
                                  "stepNumber": 3,
                                  "title": "Explorar Foco em Extração de Padrões Intrínsecos",
                                  "subSteps": [
                                    "Descreva como algoritmos detectam similaridades e agrupamentos nos dados.",
                                    "Discuta redução de dimensionalidade: comprimir dados mantendo essência (ex.: PCA).",
                                    "Exemplo: clustering agrupa clientes semelhantes sem saber categorias prévias.",
                                    "Analise densidade e anomalias como outros padrões.",
                                    "Crie um diagrama simples de dados crus vs. padrões extraídos."
                                  ],
                                  "verification": "Desenhe um diagrama mostrando dados sem rótulos transformados em clusters.",
                                  "estimatedTime": "25 minutos",
                                  "materials": [
                                    "Ferramenta de desenho como Draw.io ou papel",
                                    "Vídeo sobre K-Means (ex.: 3Blue1Brown)"
                                  ],
                                  "tips": "Visualize dados como nuvens de pontos; padrões são 'aglomerados naturais'.",
                                  "learningObjective": "Identificar e exemplificar padrões como agrupamentos e reduções de dimensionalidade.",
                                  "commonMistakes": [
                                    "Achar que precisa de rótulos para validar padrões; não, métricas internas validam.",
                                    "Ignorar que padrões são probabilísticos, não absolutos."
                                  ]
                                },
                                {
                                  "stepNumber": 4,
                                  "title": "Aplicar Conceito com Exemplos Práticos e Verificação",
                                  "subSteps": [
                                    "Escolha um dataset simples (ex.: alturas/pesos) e simule clustering manual.",
                                    "Compare com PCA: reduza 3D para 2D visualmente.",
                                    "Liste 3 aplicações: segmentação de mercado, compressão de imagens, detecção de fraudes.",
                                    "Avalie prós/cons: flexível mas interpretativo.",
                                    "Resuma lições em bullet points."
                                  ],
                                  "verification": "Explique verbalmente ou por escrito um exemplo completo de não-supervisionado.",
                                  "estimatedTime": "20 minutos",
                                  "materials": [
                                    "Dataset sample (ex.: Iris sem labels via Kaggle)",
                                    "Python/Jupyter se disponível, ou Excel para simulação"
                                  ],
                                  "tips": "Comece com datasets pequenos para intuição antes de código.",
                                  "learningObjective": "Integrar definição com exemplos para compreensão holística.",
                                  "commonMistakes": [
                                    "Overfitting a um exemplo; generalize para múltiplos casos.",
                                    "Confundir output (clusters) com predições rotuladas."
                                  ]
                                }
                              ],
                              "practicalExample": "Em marketing, use K-Means em dados de compras de clientes (sem rótulos de 'VIP' ou 'econômico') para descobrir grupos naturais de comportamento, permitindo campanhas personalizadas baseadas em padrões intrínsecos como frequência e valor de compra.",
                              "finalVerifications": [
                                "Pode definir aprendizado não-supervisionado sem mencionar rótulos errôneos?",
                                "Lista pelo menos 2 padrões intrínsecos (ex.: clustering, redução de dim)?",
                                "Diferencia corretamente de supervisionado em um quadro?",
                                "Explica um exemplo prático como segmentação de clientes?",
                                "Identifica limitações, como necessidade de interpretação humana?"
                              ],
                              "assessmentCriteria": [
                                "Definição inclui ausência de rótulos e padrões intrínsecos (peso: 30%)",
                                "Exemplos corretos de técnicas como clustering/PCA (peso: 25%)",
                                "Contraste claro com supervisionado (peso: 20%)",
                                "Aplicações reais e limitações mencionadas (peso: 15%)",
                                "Clareza e precisão na explicação verbal/escrita (peso: 10%)"
                              ],
                              "crossCurricularConnections": [
                                "Estatística: distribuições e análise de clusters",
                                "Programação: implementação em Python (scikit-learn)",
                                "Matemática: vetores, matrizes e álgebra linear para PCA",
                                "Negócios: análise de dados para segmentação de mercado"
                              ],
                              "realWorldApplication": "Na genômica, agrupa genes semelhantes em datasets massivos sem rótulos prévios para descobrir subtipos de doenças, acelerando pesquisas médicas sem necessidade de anotações caras."
                            },
                            "estimatedTime": "1 hora",
                            "difficulty": "beginner",
                            "status": "not_started",
                            "prerequisites": []
                          },
                          {
                            "id": "10.1.1.5.2.2",
                            "name": "Identificar tipos de tarefas não-supervisionadas",
                            "description": "Diferenciar tarefas como clustering (agrupamento de dados semelhantes, ex: K-means), redução de dimensionalidade (ex: PCA) e detecção de anomalias, com aplicações em exploração de dados.",
                            "atomicExpansion": {
                              "steps": [
                                {
                                  "stepNumber": 1,
                                  "title": "Compreender os fundamentos do aprendizado não-supervisionado",
                                  "subSteps": [
                                    "Definir aprendizado não-supervisionado como método que trabalha com dados sem rótulos conhecidos.",
                                    "Comparar com aprendizado supervisionado, destacando a ausência de labels para treinamento.",
                                    "Identificar objetivos principais: descoberta de padrões, estrutura oculta e exploração de dados.",
                                    "Listar características dos dados não-supervisionados: não rotulados, volume alto e variedade.",
                                    "Exemplificar com cenários reais, como análise de logs de rede sem falhas conhecidas."
                                  ],
                                  "verification": "Resumir em 3 frases as diferenças entre supervisionado e não-supervisionado, sem erros conceituais.",
                                  "estimatedTime": "30 minutos",
                                  "materials": [
                                    "Notebook com Jupyter Notebook",
                                    "Documentação scikit-learn online"
                                  ],
                                  "tips": "Use diagramas visuais para diferenciar os tipos de aprendizado de máquina.",
                                  "learningObjective": "Diferenciar aprendizado não-supervisionado de supervisionado e identificar seus objetivos principais.",
                                  "commonMistakes": [
                                    "Confundir com semi-supervisionado",
                                    "Achar que precisa de labels para qualquer ML"
                                  ]
                                },
                                {
                                  "stepNumber": 2,
                                  "title": "Explorar tarefas de Clustering (Agrupamento)",
                                  "subSteps": [
                                    "Explicar clustering como divisão de dados em grupos semelhantes baseados em similaridade.",
                                    "Estudar algoritmo K-means: inicialização de centróides, atribuição e atualização iterativa.",
                                    "Aplicar K-means em dataset simples como Iris (sem labels) para visualizar grupos.",
                                    "Discutir métricas de avaliação: Silhouette Score e Elbow Method para escolher K.",
                                    "Comparar com outros: Hierarchical Clustering e DBSCAN."
                                  ],
                                  "verification": "Executar K-means em dataset Iris e plotar clusters corretamente.",
                                  "estimatedTime": "45 minutos",
                                  "materials": [
                                    "Python com scikit-learn e matplotlib",
                                    "Dataset Iris (sklearn.datasets)"
                                  ],
                                  "tips": "Sempre normalize os dados antes de aplicar K-means para evitar viés de escala.",
                                  "learningObjective": "Implementar e interpretar clustering com K-means em dados reais.",
                                  "commonMistakes": [
                                    "Escolher K errado sem Elbow Method",
                                    "Ignorar normalização de features"
                                  ]
                                },
                                {
                                  "stepNumber": 3,
                                  "title": "Estudar Redução de Dimensionalidade",
                                  "subSteps": [
                                    "Definir redução de dimensionalidade como técnica para diminuir features mantendo informação.",
                                    "Explicar PCA: projeção em componentes principais via decomposição de autovalores.",
                                    "Aplicar PCA em dataset de alta dimensão, como MNIST ou Wine, e visualizar em 2D.",
                                    "Interpretar variância explicada e scree plot para decidir número de componentes.",
                                    "Comparar com t-SNE para visualização não-linear."
                                  ],
                                  "verification": "Aplicar PCA em dataset Wine, plotar em 2D e explicar 80% variância.",
                                  "estimatedTime": "40 minutos",
                                  "materials": [
                                    "Python com scikit-learn",
                                    "Datasets Wine ou MNIST"
                                  ],
                                  "tips": "Verifique a variância cumulativa para não perder informação crítica.",
                                  "learningObjective": "Aplicar PCA para reduzir dimensões e visualizar dados complexos.",
                                  "commonMistakes": [
                                    "Aplicar PCA sem centralizar dados",
                                    "Confundir com feature selection"
                                  ]
                                },
                                {
                                  "stepNumber": 4,
                                  "title": "Analisar Detecção de Anomalias",
                                  "subSteps": [
                                    "Definir detecção de anomalias como identificação de outliers ou padrões raros.",
                                    "Estudar métodos: Isolation Forest, One-Class SVM e Z-score para univariada.",
                                    "Implementar Isolation Forest em dataset de cartões de crédito para fraudes simuladas.",
                                    "Avaliar com precisão, recall e matriz de confusão em dados com anomalias injetadas.",
                                    "Discutir cenários: fraudes bancárias ou falhas em sensores IoT."
                                  ],
                                  "verification": "Detectar anomalias em dataset sintético e listar top 5 outliers corretamente.",
                                  "estimatedTime": "45 minutos",
                                  "materials": [
                                    "Python com scikit-learn",
                                    "Dataset KDD Cup ou sintético"
                                  ],
                                  "tips": "Balanceie o dataset se anomalias forem raras, usando undersampling.",
                                  "learningObjective": "Identificar e implementar detecção de anomalias em dados não rotulados.",
                                  "commonMistakes": [
                                    "Tratar anomalias como ruído sem contexto",
                                    "Usar threshold fixo sem validação"
                                  ]
                                },
                                {
                                  "stepNumber": 5,
                                  "title": "Diferenciar tipos e identificar em aplicações",
                                  "subSteps": [
                                    "Criar tabela comparativa: Clustering vs. Redução vs. Anomalias (objetivo, input/output, exemplos).",
                                    "Analisar cenários: 'agrupar clientes' = clustering; 'visualizar genes' = PCA; 'detectar spam' = anomalias.",
                                    "Classificar 5 tarefas hipotéticas em uma das categorias com justificativa.",
                                    "Explorar aplicações em exploração de dados: combinação de técnicas.",
                                    "Discutir limitações: subjetividade em avaliação não-supervisionada."
                                  ],
                                  "verification": "Classificar corretamente 5 cenários em tipos de tarefas não-supervisionadas.",
                                  "estimatedTime": "30 minutos",
                                  "materials": [
                                    "Planilha ou Markdown para tabela",
                                    "Exemplos de cenários impressos"
                                  ],
                                  "tips": "Pense no objetivo: grupos? Reduzir dims? Outliers?",
                                  "learningObjective": "Diferenciar e identificar tipos de tarefas não-supervisionadas em contextos reais.",
                                  "commonMistakes": [
                                    "Confundir clustering com classificação",
                                    "Ignorar que PCA não cria grupos"
                                  ]
                                }
                              ],
                              "practicalExample": "Usando o dataset Iris (150 amostras, 4 features), aplique K-means (k=3) para clustering sem labels, PCA para reduzir a 2D e visualize; isole 5% como anomalias com Isolation Forest para detectar flores atípicas.",
                              "finalVerifications": [
                                "Explicar com precisão as diferenças entre clustering, PCA e detecção de anomalias.",
                                "Implementar K-means e PCA em um dataset real sem erros de código.",
                                "Classificar 3 cenários reais em tarefas não-supervisionadas corretamente.",
                                "Interpretar resultados de Silhouette Score > 0.5 e variância >80%.",
                                "Listar 2 aplicações por tipo em exploração de dados.",
                                "Identificar limitações de cada método."
                              ],
                              "assessmentCriteria": [
                                "Precisão conceitual: 100% acerto em definições chave.",
                                "Implementação prática: Código executável sem erros em pelo menos 2 algoritmos.",
                                "Análise qualitativa: Interpretação correta de outputs (clusters, componentes, anomalias).",
                                "Classificação: 90% acerto em identificação de tarefas.",
                                "Criatividade: Exemplos originais de aplicações.",
                                "Completude: Todos campos de steps preenchidos com detalhes acionáveis."
                              ],
                              "crossCurricularConnections": [
                                "Estatística: Distribuições, variância e testes de hipóteses para avaliação.",
                                "Programação: Python, scikit-learn e visualização com matplotlib/seaborn.",
                                "Análise de Dados: Exploração EDA e pré-processamento de dados.",
                                "Matemática: Álgebra linear (PCA) e otimização (K-means).",
                                "Negócios: Aplicações em marketing e finanças."
                              ],
                              "realWorldApplication": "Em e-commerce, clustering segmenta clientes para campanhas personalizadas (K-means); PCA reduz features de imagens para análise rápida; detecção de anomalias identifica fraudes em transações, economizando milhões em perdas."
                            },
                            "estimatedTime": "1 hora",
                            "difficulty": "beginner",
                            "status": "not_started",
                            "prerequisites": []
                          },
                          {
                            "id": "10.1.1.5.2.3",
                            "name": "Descrever avaliação em não-supervisionado",
                            "description": "Explicar desafios na avaliação de modelos não-supervisionados, utilizando métricas internas como silhueta score ou métricas externas quando rótulos estão disponíveis indiretamente.",
                            "atomicExpansion": {
                              "steps": [
                                {
                                  "stepNumber": 1,
                                  "title": "Compreender os desafios fundamentais da avaliação em aprendizado não-supervisionado",
                                  "subSteps": [
                                    "Defina aprendizado não-supervisionado e contraste com supervisionado, destacando a ausência de rótulos.",
                                    "Identifique problemas principais: falta de ground truth, subjetividade e risco de overfitting a ruído.",
                                    "Discuta por que métricas supervisionadas (ex: accuracy) falham aqui.",
                                    "Explore o conceito de métricas internas vs. externas.",
                                    "Revise exemplos reais onde avaliação errada leva a conclusões falhas."
                                  ],
                                  "verification": "Escreva um parágrafo resumindo 3 desafios principais e explique por que eles ocorrem.",
                                  "estimatedTime": "45 minutos",
                                  "materials": [
                                    "Notebook para anotações",
                                    "Artigo introdutório sobre ML não-supervisionado (ex: Scikit-learn docs)"
                                  ],
                                  "tips": "Use analogias como 'agrupar frutas sem saber os nomes' para visualizar a ausência de rótulos.",
                                  "learningObjective": "Entender os motivos pelos quais a avaliação não-supervisionada é desafiadora.",
                                  "commonMistakes": [
                                    "Confundir com supervisão",
                                    "Ignorar subjetividade inerente"
                                  ]
                                },
                                {
                                  "stepNumber": 2,
                                  "title": "Explorar métricas internas para avaliação autônoma",
                                  "subSteps": [
                                    "Estude Silhouette Score: fórmula, interpretação (valores de -1 a 1) e implementação.",
                                    "Aprenda Elbow Method para K-Means: plotar inertia vs. k e identificar cotovelo.",
                                    "Investigue outras métricas: Davies-Bouldin Index, Calinski-Harabasz.",
                                    "Implemente Silhouette em código Python com Scikit-learn.",
                                    "Interprete resultados: alto score indica bons clusters coesos e separados."
                                  ],
                                  "verification": "Calcule Silhouette Score para um dataset de amostra e interprete o resultado.",
                                  "estimatedTime": "60 minutos",
                                  "materials": [
                                    "Python/Jupyter Notebook",
                                    "Biblioteca Scikit-learn",
                                    "Dataset Iris (sem rótulos)"
                                  ],
                                  "tips": "Sempre normalize dados antes de calcular scores para evitar viés de escala.",
                                  "learningObjective": "Dominar o uso e interpretação de métricas internas como Silhouette Score.",
                                  "commonMistakes": [
                                    "Interpretar scores absolutos sem contexto",
                                    "Escolher k errado no Elbow Method"
                                  ]
                                },
                                {
                                  "stepNumber": 3,
                                  "title": "Investigar métricas externas com rótulos indiretos",
                                  "subSteps": [
                                    "Explique quando rótulos indiretos surgem (ex: domínio experto ou dados históricos).",
                                    "Aprenda Adjusted Rand Index (ARI) e Normalized Mutual Information (NMI).",
                                    "Compare com métricas internas: prós e contras de cada abordagem.",
                                    "Implemente ARI em um dataset com rótulos ocultos inicialmente revelados.",
                                    "Discuta limitações: rótulos indiretos ainda podem ser ruidosos."
                                  ],
                                  "verification": "Aplique ARI a um clustering e compare com Silhouette Score.",
                                  "estimatedTime": "50 minutos",
                                  "materials": [
                                    "Python com Scikit-learn e datasets como MNIST subset",
                                    "Documentação de métricas externas"
                                  ],
                                  "tips": "Use rótulos indiretos apenas como validação secundária, priorizando internas.",
                                  "learningObjective": "Saber aplicar métricas externas quando aplicável.",
                                  "commonMistakes": [
                                    "Tratar rótulos indiretos como ground truth perfeita",
                                    "Misturar métricas sem normalização"
                                  ]
                                },
                                {
                                  "stepNumber": 4,
                                  "title": "Sintetizar e descrever uma avaliação completa",
                                  "subSteps": [
                                    "Crie um framework para relatar avaliações: liste métricas usadas e interpretações.",
                                    "Pratique descrevendo trade-offs entre métricas internas/externas.",
                                    "Desenvolva visualizações: plots de Silhouette, Elbow e comparações.",
                                    "Escreva um relatório modelo explicando desafios e métricas para um caso.",
                                    "Revise e refine com auto-perguntas sobre robustez."
                                  ],
                                  "verification": "Redija uma descrição de avaliação para um clustering fictício, citando métricas.",
                                  "estimatedTime": "40 minutos",
                                  "materials": [
                                    "Templates de relatório",
                                    "Ferramentas de plotagem (Matplotlib/Seaborn)"
                                  ],
                                  "tips": "Estruture descrições com: introdução aos desafios, métricas escolhidas, resultados e caveats.",
                                  "learningObjective": "Capacidade de descrever avaliações não-supervisionadas de forma clara e completa.",
                                  "commonMistakes": [
                                    "Omitir limitações das métricas",
                                    "Focar só em números sem narrativa"
                                  ]
                                }
                              ],
                              "practicalExample": "Em segmentação de clientes de e-commerce: aplique K-Means nos dados de compra, calcule Silhouette Score (0.65 indica clusters razoáveis), use Elbow Method para k=4, e valide com ARI usando rótulos de churn histórico indireto, descrevendo que clusters separam bem compradores fiéis de churners potenciais apesar da falta de labels diretos.",
                              "finalVerifications": [
                                "Explica corretamente 3 desafios da avaliação não-supervisionada.",
                                "Calcula e interpreta Silhouette Score >0.5 em exemplo prático.",
                                "Distingue métricas internas de externas com exemplos.",
                                "Descreve framework completo de avaliação em parágrafo coerente.",
                                "Identifica erros comuns em interpretações de métricas."
                              ],
                              "assessmentCriteria": [
                                "Precisão na explicação de desafios (80% cobertura).",
                                "Correta implementação e interpretação de pelo menos 2 métricas internas.",
                                "Uso apropriado de métricas externas com caveats.",
                                "Clareza e estrutura na descrição final.",
                                "Inclusão de visualizações e trade-offs.",
                                "Ausência de confusões com conceitos supervisionados."
                              ],
                              "crossCurricularConnections": [
                                "Estatística: Distribuições e análise de variância em scores.",
                                "Programação: Implementação em Python/Scikit-learn.",
                                "Visualização de Dados: Plots para Elbow e Silhouette.",
                                "Negócios: Aplicações em segmentação de mercado."
                              ],
                              "realWorldApplication": "Na detecção de anomalias em transações bancárias, onde clusters de comportamento normal são avaliados via Silhouette Score sem labels diretos, ajudando a identificar fraudes emergentes e otimizando modelos de risco em tempo real."
                            },
                            "estimatedTime": "1.5 horas",
                            "difficulty": "intermediate",
                            "status": "not_started",
                            "prerequisites": []
                          }
                        ]
                      },
                      {
                        "id": "10.1.1.5.3",
                        "name": "Comparação entre Paradigmas Supervisionado e Não-supervisionado",
                        "description": "Análise das diferenças fundamentais entre aprendizado supervisionado e não-supervisionado, incluindo requisitos de dados, objetivos, aplicações e considerações éticas na manipulação de dados.",
                        "specificSkills": [
                          {
                            "id": "10.1.1.5.3.1",
                            "name": "Comparar requisitos de dados",
                            "description": "Contrastar a necessidade de dados rotulados no supervisionado versus dados não rotulados no não-supervisionado, discutindo custos de rotulagem e cenários de aplicação prática.",
                            "atomicExpansion": {
                              "steps": [
                                {
                                  "stepNumber": 1,
                                  "title": "Definir Conceitos Básicos de Dados Rotulados e Não Rotulados",
                                  "subSteps": [
                                    "Pesquise definições de dados rotulados: pares de entrada-saída com labels explícitos.",
                                    "Pesquise definições de dados não rotulados: apenas entradas sem labels.",
                                    "Identifique exemplos simples: e-mails rotulados como spam/não-spam vs. e-mails sem rótulos.",
                                    "Crie um diagrama comparativo visual dos dois tipos.",
                                    "Registre diferenças iniciais em estrutura e preparação."
                                  ],
                                  "verification": "Crie um glossário com definições e exemplos; revise se cobre ambos os tipos sem ambiguidades.",
                                  "estimatedTime": "20 minutos",
                                  "materials": [
                                    "Notebook ou papel para diagramas",
                                    "Acesso a internet para definições (Wikipedia, Towards Data Science)"
                                  ],
                                  "tips": "Use analogias cotidianas, como etiquetar fotos de família (rotulado) vs. fotos sem nomes (não rotulado).",
                                  "learningObjective": "Compreender as diferenças fundamentais entre dados rotulados e não rotulados.",
                                  "commonMistakes": [
                                    "Confundir rotulagem com feature engineering",
                                    "Ignorar que dados semi-rotulados existem em cenários híbridos"
                                  ]
                                },
                                {
                                  "stepNumber": 2,
                                  "title": "Analisar Requisitos de Dados no Aprendizado Supervisionado",
                                  "subSteps": [
                                    "Liste requisitos: grande volume de dados rotulados de alta qualidade.",
                                    "Discuta fontes de rotulagem: humanos, crowdsourcing (ex: Amazon MTurk).",
                                    "Calcule estimativas de tempo/custo para rotular 1000 amostras.",
                                    "Examine impactos de dados rotulados ruins: overfitting ou bias.",
                                    "Compare com poucos dados: por que supervisionado falha sem rótulos."
                                  ],
                                  "verification": "Escreva um relatório curto listando 3 requisitos chave e 1 exemplo de falha.",
                                  "estimatedTime": "25 minutos",
                                  "materials": [
                                    "Planilha para cálculos de custo",
                                    "Vídeos tutoriais sobre ML supervisionado (Coursera ou YouTube)"
                                  ],
                                  "tips": "Considere custo por hora de um rotulador: multiplique por volume para realismo.",
                                  "learningObjective": "Identificar e quantificar necessidades específicas de dados rotulados no supervisionado.",
                                  "commonMistakes": [
                                    "Subestimar esforço de rotulagem",
                                    "Achar que qualquer dado rotulado serve, ignorando balanceamento"
                                  ]
                                },
                                {
                                  "stepNumber": 3,
                                  "title": "Analisar Requisitos de Dados no Aprendizado Não-Supervisionado",
                                  "subSteps": [
                                    "Liste requisitos: volume alto de dados não rotulados para padrões emergentes.",
                                    "Descreva algoritmos chave: K-means, PCA, autoencoders.",
                                    "Discuta vantagens: sem custo de rotulagem, bom para exploração.",
                                    "Identifique limitações: interpretação subjetiva de clusters.",
                                    "Compare volume necessário vs. supervisionado."
                                  ],
                                  "verification": "Desenhe fluxograma de um algoritmo não-supervisionado sem rótulos.",
                                  "estimatedTime": "25 minutos",
                                  "materials": [
                                    "Software como Python com scikit-learn para simulação simples",
                                    "Documentação de bibliotecas ML"
                                  ],
                                  "tips": "Teste um clustering rápido em dados aleatórios para visualizar.",
                                  "learningObjective": "Entender como o não-supervisionado opera sem rótulos e suas demandas.",
                                  "commonMistakes": [
                                    "Confundir com reinforcement learning",
                                    "Acreditar que não precisa de dados limpos"
                                  ]
                                },
                                {
                                  "stepNumber": 4,
                                  "title": "Comparar Custos, Vantagens e Cenários Práticos",
                                  "subSteps": [
                                    "Calcule custos: rotulagem supervisionado (~$0.01-1 por amostra) vs. zero no não-supervisionado.",
                                    "Liste cenários: saúde (supervisionado para diagnóstico) vs. marketing (não-supervisionado para segmentação).",
                                    "Discuta trade-offs: precisão vs. custo/escalabilidade.",
                                    "Crie tabela comparativa: requisitos, custos, aplicações.",
                                    "Debata quando escolher cada um."
                                  ],
                                  "verification": "Preencha tabela comparativa com pelo menos 5 cenários e valide com fontes.",
                                  "estimatedTime": "30 minutos",
                                  "materials": [
                                    "Planilha Excel/Google Sheets",
                                    "Artigos sobre custos de ML (ex: O'Reilly reports)"
                                  ],
                                  "tips": "Use dados reais: custo médio de rotulagem de imagens é alto devido a experts.",
                                  "learningObjective": "Contrastar custos e aplicações práticas para decisão informada.",
                                  "commonMistakes": [
                                    "Ignorar custos ocultos como validação",
                                    "Generalizar sem contexto de domínio"
                                  ]
                                }
                              ],
                              "practicalExample": "Em um e-commerce: Supervisionado usa dados rotulados de compras passadas para prever churn (custo: rotular 10k registros = $5k); Não-supervisionado clusteriza padrões de navegação sem rótulos para segmentar usuários (custo: zero rotulagem, foco em volume).",
                              "finalVerifications": [
                                "Explicar verbalmente diferenças em requisitos de dados sem hesitação.",
                                "Preencher tabela comparativa corretamente em <5 min.",
                                "Identificar cenário ideal para cada paradigma em 3 exemplos.",
                                "Calcular custo aproximado de rotulagem para um dataset de 1k amostras.",
                                "Discutir trade-offs de precisão vs. custo com exemplos.",
                                "Criar diagrama de dados rotulados vs. não rotulados."
                              ],
                              "assessmentCriteria": [
                                "Precisão conceitual: definições corretas sem erros (30%)",
                                "Profundidade na comparação de custos e cenários (25%)",
                                "Uso de exemplos práticos e tabelas/diagramas (20%)",
                                "Clareza na explicação de trade-offs (15%)",
                                "Quantificação realista de tempos/custos (10%)"
                              ],
                              "crossCurricularConnections": [
                                "Estatística: Distribuições e amostragem de dados.",
                                "Programação: Implementação em Python (scikit-learn).",
                                "Economia/Gestão: Análise custo-benefício em projetos de dados.",
                                "Ética: Bias em rotulagem humana vs. discovery em não-supervisionado."
                              ],
                              "realWorldApplication": "Na indústria farmacêutica, supervisionado usa dados rotulados caros para prever eficácia de drogas; em redes sociais, não-supervisionado agrupa tendências virais sem rótulos para recomendação de conteúdo, economizando milhões em labeling."
                            },
                            "estimatedTime": "0.5 hora",
                            "difficulty": "beginner",
                            "status": "not_started",
                            "prerequisites": []
                          },
                          {
                            "id": "10.1.1.5.3.2",
                            "name": "Discutir aplicações e exemplos",
                            "description": "Relacionar paradigmas a casos reais, como previsão de churn (supervisionado) versus segmentação de clientes (não-supervisionado), citando estudos de caso da bibliografia.",
                            "atomicExpansion": {
                              "steps": [
                                {
                                  "stepNumber": 1,
                                  "title": "Revisar Conceitos Fundamentais dos Paradigmas",
                                  "subSteps": [
                                    "Defina aprendizado supervisionado: algoritmos treinados com dados rotulados para prever outcomes.",
                                    "Defina aprendizado não-supervisionado: algoritmos que identificam padrões em dados não rotulados.",
                                    "Liste diferenças chave: presença de labels, objetivos (predição vs clustering), exemplos iniciais.",
                                    "Crie um quadro comparativo simples com colunas para cada paradigma.",
                                    "Relacione com a descrição da habilidade: churn (labels de saída) vs segmentação (sem labels)."
                                  ],
                                  "verification": "Quadro comparativo completo criado e conceitos explicados corretamente em notas.",
                                  "estimatedTime": "30 minutos",
                                  "materials": "Notas de aula, slides de MC-13, quadro branco ou ferramenta digital como Draw.io.",
                                  "tips": "Use analogias cotidianas, como classificar frutas com etiquetas (supervisionado) vs agrupar por cor sem etiquetas.",
                                  "learningObjective": "Compreender as bases conceituais para relacionar com aplicações reais.",
                                  "commonMistakes": "Confundir os paradigmas invertendo exemplos de labels."
                                },
                                {
                                  "stepNumber": 2,
                                  "title": "Identificar Aplicações Típicas e Exemplos Reais",
                                  "subSteps": [
                                    "Liste 3 aplicações supervisionadas: previsão de churn, detecção de fraudes, classificação de imagens.",
                                    "Liste 3 aplicações não-supervisionadas: segmentação de clientes, detecção de anomalias, redução de dimensionalidade.",
                                    "Pesquise exemplos específicos: churn em telecom (ex: dataset IBM), segmentação em e-commerce (ex: K-means em RFM).",
                                    "Anote métricas usadas: accuracy/precision para supervisionado, silhouette score para não-supervisionado.",
                                    "Compare churn (supervisionado: prever quem sai) vs segmentação (não-supervisionado: grupos de comportamento)."
                                  ],
                                  "verification": "Lista de aplicações com pelo menos 3 exemplos por paradigma e uma comparação escrita.",
                                  "estimatedTime": "45 minutos",
                                  "materials": "Bibliografia do curso, Google Scholar, Kaggle datasets (ex: Telco Churn).",
                                  "tips": "Busque papers recentes (2020+) para relevância; foque em indústrias como finanças e varejo.",
                                  "learningObjective": "Mapear paradigmas a cenários práticos do mundo real.",
                                  "commonMistakes": "Escolher exemplos híbridos sem distinguir claramente os labels."
                                },
                                {
                                  "stepNumber": 3,
                                  "title": "Analisar Estudos de Caso da Bibliografia",
                                  "subSteps": [
                                    "Selecione 2 estudos de caso da bibliografia: um supervisionado (ex: churn prediction paper), um não-supervisionado (ex: customer segmentation case).",
                                    "Extraia detalhes: dataset usado, algoritmo (ex: Logistic Regression vs K-means), resultados e lições aprendidas.",
                                    "Identifique forças e limitações: supervisionado precisa de labels caros; não-supervisionado revela insights ocultos.",
                                    "Crie resumo em bullet points para cada caso.",
                                    "Relacione de volta aos paradigmas: como labels impactam o churn vs padrões emergentes na segmentação."
                                  ],
                                  "verification": "Resumos de 2 estudos de caso com análise comparativa em um documento.",
                                  "estimatedTime": "60 minutos",
                                  "materials": "Bibliografia recomendada, PDFs de papers, ferramenta de leitura como Zotero.",
                                  "tips": "Use Ctrl+F para buscar 'churn' ou 'segmentation' nos papers; anote citações APA.",
                                  "learningObjective": "Extrair evidências empíricas de literatura para apoiar discussões.",
                                  "commonMistakes": "Não citar fontes ou usar estudos irrelevantes/outdated."
                                },
                                {
                                  "stepNumber": 4,
                                  "title": "Discutir e Sintetizar Aplicações e Exemplos",
                                  "subSteps": [
                                    "Escreva uma discussão: 'Em churn prediction (supervisionado), usamos labels históricos para prever saídas, como no estudo X; em segmentação (não-supervisionado), agrupamos clientes sem labels, como no estudo Y.'",
                                    "Compare vantagens/desvantagens em cenários reais.",
                                    "Prepare exemplos verbais para discussão em grupo.",
                                    "Crie um slide ou mindmap visualizando as relações.",
                                    "Autoavalie: cubro casos reais e citações?"
                                  ],
                                  "verification": "Texto de discussão de 300-500 palavras com exemplos e citações integradas.",
                                  "estimatedTime": "45 minutos",
                                  "materials": "Editor de texto (Google Docs), Canva para visual, bibliografia aberta.",
                                  "tips": "Estruture como: introdução > exemplos > comparação > conclusão para fluidez.",
                                  "learningObjective": "Articular relações entre teoria e prática de forma clara e citada.",
                                  "commonMistakes": "Fazer lista seca sem conectar paradigmas aos casos."
                                }
                              ],
                              "practicalExample": "Discuta como prever churn de clientes em uma operadora de telefonia (supervisionado: usar dados históricos de saída com Random Forest para prever quem cancelará, alcançando 85% accuracy, como no paper de Amin et al., 2019) versus segmentar clientes em um e-commerce (não-supervisionado: aplicar K-means em dados RFM para grupos de alto valor, revelando padrões ocultos sem labels prévios, como no case da Amazon).",
                              "finalVerifications": [
                                "Pode explicar 2 aplicações supervisionadas e 2 não-supervisionadas com exemplos reais?",
                                "Discute diferenças chave usando churn vs segmentação?",
                                "Cita pelo menos 2 estudos de caso da bibliografia corretamente?",
                                "Compara vantagens/desvantagens em contextos práticos?",
                                "Produz visual ou quadro comparativo claro?",
                                "Responde perguntas sobre limitações de cada paradigma?"
                              ],
                              "assessmentCriteria": [
                                "Precisão conceitual: distinção clara entre supervisionado e não-supervisionado (30%)",
                                "Relevância de exemplos: casos reais e citados da bibliografia (25%)",
                                "Profundidade de análise: comparação de aplicações com métricas (20%)",
                                "Clareza na discussão: estrutura lógica e linguagem acessível (15%)",
                                "Criatividade: conexões originais ou visuais (10%)"
                              ],
                              "crossCurricularConnections": [
                                "Estatística: métricas de avaliação como AUC-ROC (supervisionado) vs silhouette score",
                                "Negócios: aplicações em marketing (churn reduction) e CRM (segmentação)",
                                "Programação: implementação em Python (Scikit-learn para ambos paradigmas)",
                                "Ética em IA: viés em labels supervisionados vs privacidade em clustering"
                              ],
                              "realWorldApplication": "Em empresas como Netflix (supervisionado para prever churn de assinantes, reduzindo perdas em milhões) ou bancos (não-supervisionado para segmentar clientes em perfis de risco, otimizando ofertas personalizadas e aumentando retenção em 15-20%)."
                            },
                            "estimatedTime": "1 hora",
                            "difficulty": "intermediate",
                            "status": "not_started",
                            "prerequisites": []
                          }
                        ]
                      }
                    ]
                  },
                  {
                    "id": "10.1.1.6",
                    "name": "Ética no Uso e Manipulação de Dados",
                    "description": "Princípios éticos na coleta, análise e uso de dados.",
                    "individualConcepts": [
                      {
                        "id": "10.1.1.6.1",
                        "name": "Privacidade e Proteção de Dados",
                        "description": "Princípios éticos relacionados à salvaguarda da privacidade individual e à proteção de dados sensíveis durante a coleta, armazenamento e análise.",
                        "specificSkills": [
                          {
                            "id": "10.1.1.6.1.1",
                            "name": "Identificar riscos de privacidade em conjuntos de dados",
                            "description": "Reconhecer potenciais violações de privacidade, como identificação de indivíduos a partir de dados anonimizados, e avaliar conformidade com regulamentações como LGPD ou GDPR.",
                            "atomicExpansion": {
                              "steps": [
                                {
                                  "stepNumber": 1,
                                  "title": "Compreender Conceitos Fundamentais de Privacidade e Anonimização",
                                  "subSteps": [
                                    "Defina privacidade de dados e anonimização em termos simples.",
                                    "Estude tipos de anonimização: supressão, generalização, perturbação e k-anonimato.",
                                    "Revise definições de dados pessoais e sensíveis conforme LGPD e GDPR.",
                                    "Identifique diferenças entre anonimização e pseudonimização.",
                                    "Explore exemplos de falhas históricas, como o caso Netflix Prize."
                                  ],
                                  "verification": "Resuma em 3 frases os conceitos chave e cite um exemplo de falha.",
                                  "estimatedTime": "45 minutos",
                                  "materials": [
                                    "Documentação LGPD/GDPR (PDFs oficiais)",
                                    "Artigos sobre anonimização (ex: Wikipedia, papers acadêmicos)"
                                  ],
                                  "tips": "Use diagramas para visualizar tipos de anonimização.",
                                  "learningObjective": "Dominar terminologia e princípios básicos de privacidade em dados.",
                                  "commonMistakes": [
                                    "Confundir anonimização com criptografia",
                                    "Ignorar dados quasi-identificadores"
                                  ]
                                },
                                {
                                  "stepNumber": 2,
                                  "title": "Estudar Técnicas de Re-identificação e Riscos Associados",
                                  "subSteps": [
                                    "Aprenda ataques de linkage: combinar datasets auxiliares para re-identificar.",
                                    "Analise linkage attacks com ZIP codes, gênero e data de nascimento (ex: estudo de Sweeney).",
                                    "Estude inferência probabilística e machine learning para re-identificação.",
                                    "Identifique quasi-identificadores comuns (idade, localização, profissão).",
                                    "Simule um ataque simples com dados fictícios."
                                  ],
                                  "verification": "Descreva um ataque de linkage passo a passo usando um exemplo hipotético.",
                                  "estimatedTime": "60 minutos",
                                  "materials": [
                                    "Vídeos tutoriais sobre re-identificação (YouTube/Khan Academy)",
                                    "Datasets de exemplo anonimizados (Kaggle)"
                                  ],
                                  "tips": "Pratique com ferramentas como Python Pandas para simular linkage.",
                                  "learningObjective": "Reconhecer mecanismos que quebram anonimização.",
                                  "commonMistakes": [
                                    "Subestimar poder de dados auxiliares públicos",
                                    "Focar só em nomes e CPFs"
                                  ]
                                },
                                {
                                  "stepNumber": 3,
                                  "title": "Analisar Conjuntos de Dados para Identificar Riscos",
                                  "subSteps": [
                                    "Selecione um dataset real ou sintético (ex: dados de saúde ou e-commerce).",
                                    "Examine colunas para quasi-identificadores e unique combinations.",
                                    "Calcule métricas como k-anonimato (verifique se k >= 5).",
                                    "Teste re-identificação cruzando com dados públicos (ex: IBGE census).",
                                    "Documente riscos potenciais em um relatório simples."
                                  ],
                                  "verification": "Produza um relatório de 1 página listando 3 riscos identificados no dataset.",
                                  "estimatedTime": "90 minutos",
                                  "materials": [
                                    "Python/Jupyter Notebook",
                                    "Datasets Kaggle (Adult UCI, etc.)",
                                    "Ferramenta ARX para anonimização"
                                  ],
                                  "tips": "Use groupby no Pandas para contar combinações únicas.",
                                  "learningObjective": "Aplicar análise prática para detectar vulnerabilidades.",
                                  "commonMistakes": [
                                    "Não considerar agregações temporais",
                                    "Ignorar dados ausentes como risco"
                                  ]
                                },
                                {
                                  "stepNumber": 4,
                                  "title": "Avaliar Conformidade com Regulamentações e Propor Mitigações",
                                  "subSteps": [
                                    "Mapeie riscos identificados aos artigos da LGPD (ex: Art. 5º - anonimato) e GDPR (Art. 4).",
                                    "Verifique princípios como minimização de dados e consentimento.",
                                    "Proponha mitigações: supressão, agregação, diferencial privacy.",
                                    "Crie um checklist de conformidade para o dataset.",
                                    "Discuta trade-offs entre utilidade e privacidade."
                                  ],
                                  "verification": "Crie um checklist de 10 itens e aplique ao dataset analisado.",
                                  "estimatedTime": "45 minutos",
                                  "materials": [
                                    "Textos completos LGPD/GDPR",
                                    "Templates de DPIA (Data Protection Impact Assessment)"
                                  ],
                                  "tips": "Priorize riscos altos primeiro (alta probabilidade x impacto).",
                                  "learningObjective": "Avaliar e mitigar riscos em conformidade legal.",
                                  "commonMistakes": [
                                    "Interpretar regulamentações literalmente sem contexto",
                                    "Esquecer de atualizar mitigações"
                                  ]
                                }
                              ],
                              "practicalExample": "Analise o dataset Adult UCI do Kaggle: identifique que combinações de idade, educação e salário podem ser cruzadas com dados públicos do IBGE para re-identificar indivíduos, violando LGPD Art. 15 (direito à anonimidade). Mitigue generalizando faixas etárias.",
                              "finalVerifications": [
                                "Identifica corretamente quasi-identificadores em um dataset dado.",
                                "Explica um ataque de re-identificação com exemplo real.",
                                "Calcula k-anonimato > 5 para um subconjunto de dados.",
                                "Mapeia riscos a artigos específicos de LGPD/GDPR.",
                                "Propõe pelo menos 3 mitigações viáveis.",
                                "Discute trade-offs privacidade vs. utilidade."
                              ],
                              "assessmentCriteria": [
                                "Precisão na identificação de riscos (90%+ acurácia).",
                                "Profundidade da análise de conformidade legal.",
                                "Criatividade e viabilidade das mitigações propostas.",
                                "Clareza no relatório e verificações.",
                                "Integração de conceitos teóricos com prática.",
                                "Uso correto de métricas como k-anonimato."
                              ],
                              "crossCurricularConnections": [
                                "Ética: Discussão de princípios morais em dados (Filosofia).",
                                "Direito: Aplicação prática de leis de proteção de dados.",
                                "Informática: Programação para análise de datasets (Python/SQL).",
                                "Estatística: Cálculo de probabilidades de re-identificação.",
                                "Ciências Sociais: Impacto na sociedade e desigualdades de privacidade."
                              ],
                              "realWorldApplication": "Em empresas como bancos ou hospitais, analistas usam essa habilidade para auditar datasets antes de compartilhar com parceiros, evitando multas milionárias da ANPD (LGPD) ou sanções da UE (GDPR), como no caso Cambridge Analytica."
                            },
                            "estimatedTime": "1 hora",
                            "difficulty": "beginner",
                            "status": "not_started",
                            "prerequisites": []
                          },
                          {
                            "id": "10.1.1.6.1.2",
                            "name": "Aplicar técnicas de anonimização e pseudonimização",
                            "description": "Implementar métodos básicos como supressão, generalização e k-anonimato para proteger identidades em datasets, garantindo utilidade para análise.",
                            "atomicExpansion": {
                              "steps": [
                                {
                                  "stepNumber": 1,
                                  "title": "Compreender Conceitos Fundamentais de Anonimização e Pseudonimização",
                                  "subSteps": [
                                    "Estude definições: anonimização remove identificadores permanentemente; pseudonimização substitui por pseudônimos reversíveis.",
                                    "Identifique identificadores diretos (nome, CPF) e quasi-identificadores (idade, CEP).",
                                    "Revise métodos: supressão (remover dados), generalização (agregar valores), k-anonimato (garantir k registros indistinguíveis).",
                                    "Analise trade-off entre privacidade e utilidade dos dados.",
                                    "Leia exemplos de falhas como re-identificação em datasets de saúde."
                                  ],
                                  "verification": "Resuma em um documento os conceitos chave e trade-offs, com pelo menos 3 exemplos de quasi-identificadores.",
                                  "estimatedTime": "1-2 horas",
                                  "materials": [
                                    "Documentação GDPR/ LGPD sobre privacidade",
                                    "Artigos introdutórios sobre k-anonimato (ex: papers de Samarati e Sweeney)",
                                    "Notebook Jupyter vazio"
                                  ],
                                  "tips": "Use diagramas para visualizar quasi-identificadores vs. identificadores diretos.",
                                  "learningObjective": "Diferenciar anonimização de pseudonimização e identificar riscos em datasets.",
                                  "commonMistakes": [
                                    "Confundir pseudonimização com anonimização",
                                    "Ignorar quasi-identificadores",
                                    "Subestimar trade-offs de utilidade"
                                  ]
                                },
                                {
                                  "stepNumber": 2,
                                  "title": "Preparar e Aplicar Supressão e Generalização em um Dataset",
                                  "subSteps": [
                                    "Carregue um dataset exemplo com dados sensíveis (ex: pacientes com idade, gênero, CEP, diagnóstico).",
                                    "Aplique supressão: remova colunas de identificadores diretos como nome e CPF.",
                                    "Aplique generalização: transforme idade em faixas (ex: 20-30), CEP em região (ex: primeiro 3 dígitos).",
                                    "Salve versões intermediárias e compare tamanhos/originalidade.",
                                    "Teste queries simples para verificar perda de utilidade."
                                  ],
                                  "verification": "Gere relatório mostrando dataset original vs. modificado, com estatísticas descritivas iguais em utilidade básica.",
                                  "estimatedTime": "2-3 horas",
                                  "materials": [
                                    "Python com Pandas e NumPy",
                                    "Dataset exemplo: Adult UCI ou sintético de pacientes (Kaggle)",
                                    "Jupyter Notebook"
                                  ],
                                  "tips": "Comece com supressão em poucas colunas para evitar perda excessiva de dados.",
                                  "learningObjective": "Implementar supressão e generalização para reduzir riscos de re-identificação.",
                                  "commonMistakes": [
                                    "Remover dados demais, perdendo utilidade",
                                    "Generalizar insuficientemente, permitindo linkage attacks",
                                    "Não documentar mudanças"
                                  ]
                                },
                                {
                                  "stepNumber": 3,
                                  "title": "Implementar k-Anonimato em Datasets",
                                  "subSteps": [
                                    "Instale bibliotecas como ARX ou use Pandas para simular k-anonimato.",
                                    "Selecione atributos quasi-identificadores (ex: idade faixa, gênero, região).",
                                    "Aplique algoritmo: generalize/suprima até cada combinação ter pelo menos k=5 registros iguais.",
                                    "Verifique k-anonimato com queries de contagem em grupos.",
                                    "Ajuste k e compare versões para otimizar privacidade/utilidade."
                                  ],
                                  "verification": "Execute query provando que todo grupo quasi-identificador tem >=k registros; gere log de anonimização.",
                                  "estimatedTime": "3-4 horas",
                                  "materials": [
                                    "Biblioteca Python: pandas-anonymizer ou ARX tool",
                                    "Dataset preparado do Step 2",
                                    "Jupyter Notebook"
                                  ],
                                  "tips": "Use k=5 inicialmente; aumente se necessário, monitorando utilidade com métricas como entropy.",
                                  "learningObjective": "Aplicar k-anonimato para garantir indistinguibilidade em datasets.",
                                  "commonMistakes": [
                                    "Escolher k muito baixo",
                                    "Ignorar atributos correlacionados",
                                    "Não validar com ataques simulados"
                                  ]
                                },
                                {
                                  "stepNumber": 4,
                                  "title": "Avaliar e Refinar a Anonimização",
                                  "subSteps": [
                                    "Meça privacidade: calcule risco de re-identificação (ex: uniqueness score).",
                                    "Meça utilidade: compare acurácia de modelo ML simples (ex: classificação) antes/depois.",
                                    "Simule ataques: tente re-identificar com dataset auxiliar público.",
                                    "Refina: itere supressão/generalização se utilidade baixa ou risco alto.",
                                    "Documente processo completo em relatório final."
                                  ],
                                  "verification": "Relatório com métricas: k>=5, utilidade >70% original, risco <5%.",
                                  "estimatedTime": "2-3 horas",
                                  "materials": [
                                    "Scikit-learn para ML simples",
                                    "Ferramentas de risco: IBM ARX ou sdcMicro R",
                                    "Datasets auxiliares públicos"
                                  ],
                                  "tips": "Use métricas quantitativas como KL-divergence para utilidade.",
                                  "learningObjective": "Avaliar efetividade de técnicas e balancear privacidade/utilidade.",
                                  "commonMistakes": [
                                    "Focar só privacidade, ignorando utilidade",
                                    "Não testar ataques",
                                    "Métricas subjetivas"
                                  ]
                                }
                              ],
                              "practicalExample": "Em um hospital, anonimizar dataset de 1000 pacientes: suprimir nomes/CPF, generalizar idades para faixas de 10 anos e CEP para estado, aplicar k=10-anonimato em {idade_faixa, gênero, diagnóstico}. Resultado permite análise agregada de doenças sem risco individual.",
                              "finalVerifications": [
                                "Dataset final sem identificadores diretos e quasi-identificadores anonimizados.",
                                "Verificação de k-anonimato com k>=5 em todos grupos.",
                                "Utilidade preservada: queries analíticas retornam resultados similares ao original.",
                                "Relatório documenta métodos, métricas e trade-offs.",
                                "Teste de ataque simulado falha em re-identificar >1% registros.",
                                "Conformidade com LGPD/GDPR em princípios de minimização."
                              ],
                              "assessmentCriteria": [
                                "Precisão na implementação: métodos corretos sem vazamentos.",
                                "Balanceamento privacidade/utilidade: métricas quantitativas dentro de limites.",
                                "Documentação completa: passos reproduzíveis.",
                                "Criatividade em refinamentos: otimizações baseadas em avaliação.",
                                "Compreensão conceitual: explicações claras de trade-offs.",
                                "Qualidade do código: limpo, comentado e eficiente."
                              ],
                              "crossCurricularConnections": [
                                "Ética e Direito: Conformidade com LGPD/GDPR e princípios éticos de dados.",
                                "Programação: Uso de Python/Pandas para manipulação de dados.",
                                "Estatística: Métricas de risco e utilidade (entropy, divergence).",
                                "Inteligência Artificial: Impacto em treinamento de ML com dados anonimizados.",
                                "Ciências Sociais: Implicações de privacidade em big data societal."
                              ],
                              "realWorldApplication": "Em bancos, anonimizar transações para detecção de fraudes sem expor contas individuais; em saúde pública, compartilhar dados COVID para epidemiologia mantendo anonimato de pacientes."
                            },
                            "estimatedTime": "2 horas",
                            "difficulty": "intermediate",
                            "status": "not_started",
                            "prerequisites": []
                          },
                          {
                            "id": "10.1.1.6.1.3",
                            "name": "Avaliar impactos éticos no armazenamento de dados",
                            "description": "Analisar escolhas de armazenamento (nuvem vs. local) em termos de segurança, acesso controlado e conformidade ética.",
                            "atomicExpansion": {
                              "steps": [
                                {
                                  "stepNumber": 1,
                                  "title": "Entender os Fundamentos de Armazenamento em Nuvem vs. Local",
                                  "subSteps": [
                                    "Defina armazenamento em nuvem (ex.: AWS S3, Google Cloud) e local (ex.: servidores on-premise).",
                                    "Identifique características principais: escalabilidade, custo inicial, manutenção.",
                                    "Pesquise exemplos reais de cada tipo em contextos de dados sensíveis.",
                                    "Crie um quadro comparativo básico de acessibilidade e dependência de internet.",
                                    "Discuta implicações iniciais para dados éticos como PII (Personally Identifiable Information)."
                                  ],
                                  "verification": "Quadro comparativo completo com pelo menos 5 características listadas e definidas.",
                                  "estimatedTime": "45 minutos",
                                  "materials": "Acesso à internet, papel ou ferramenta digital como Google Sheets, artigos introdutórios sobre nuvem (ex.: AWS docs).",
                                  "tips": "Use diagramas visuais para diferenciar nuvem (distribuída) de local (centralizada).",
                                  "learningObjective": "Diferenciar tecnicamente armazenamento nuvem e local para basear análises éticas.",
                                  "commonMistakes": "Confundir nuvem com 'gratuita' ou ignorar custos recorrentes vs. investimento único local."
                                },
                                {
                                  "stepNumber": 2,
                                  "title": "Analisar Impactos na Segurança de Dados",
                                  "subSteps": [
                                    "Liste riscos de segurança na nuvem: breaches compartilhados, DDoS.",
                                    "Liste riscos locais: falhas físicas, desatualizações de hardware.",
                                    "Compare criptografia, backups e recuperação de desastres em ambos.",
                                    "Avalie métricas como tempo de inatividade média (ex.: 99.99% uptime na nuvem).",
                                    "Simule um cenário de vazamento e trace responsabilidades."
                                  ],
                                  "verification": "Relatório de 1 página com riscos, mitigações e comparação quantitativa (ex.: estatísticas de breaches).",
                                  "estimatedTime": "60 minutos",
                                  "materials": "Relatórios de segurança (ex.: Verizon DBIR), calculadora para estimar custos de breaches.",
                                  "tips": "Priorize dados sensíveis como biométricos para contextualizar riscos éticos.",
                                  "learningObjective": "Identificar e quantificar riscos de segurança específicos a cada método de armazenamento.",
                                  "commonMistakes": "Superestimar segurança local sem considerar manutenção humana falível."
                                },
                                {
                                  "stepNumber": 3,
                                  "title": "Avaliar Controle de Acesso e Privacidade",
                                  "subSteps": [
                                    "Descreva mecanismos de acesso: IAM na nuvem vs. firewalls locais.",
                                    "Analise granularidade de permissões e auditoria de logs.",
                                    "Discuta privacidade: soberania de dados (local em jurisdição nacional vs. nuvem global).",
                                    "Mapeie violações potenciais de princípios como minimização de dados.",
                                    "Crie fluxograma de fluxo de acesso para um dataset hipotético."
                                  ],
                                  "verification": "Fluxograma funcional mostrando caminhos de acesso e pontos de falha em ambos os cenários.",
                                  "estimatedTime": "50 minutos",
                                  "materials": "Ferramentas de diagramação (ex.: Lucidchart gratuito), docs de IAM (AWS/Google).",
                                  "tips": "Considere o princípio 'least privilege' para ambos os sistemas.",
                                  "learningObjective": "Mapear controles de acesso e identificar gaps éticos em privacidade.",
                                  "commonMistakes": "Ignorar acessos de terceiros na nuvem ou acessos internos não auditados localmente."
                                },
                                {
                                  "stepNumber": 4,
                                  "title": "Sintetizar Conformidade Ética e Trade-offs",
                                  "subSteps": [
                                    "Revise regulamentações: GDPR, LGPD para nuvem vs. local.",
                                    "Pese trade-offs éticos: conveniência vs. controle soberano.",
                                    "Crie matriz de decisão ética com scores (1-10) para segurança, acesso, conformidade.",
                                    "Recomende escolha baseada em contexto (ex.: dados de saúde preferem local).",
                                    "Reflita sobre dilemas éticos como transparência com usuários."
                                  ],
                                  "verification": "Matriz de decisão final com recomendações justificadas e scores totais.",
                                  "estimatedTime": "55 minutos",
                                  "materials": "Textos de leis de dados (GDPR summary), planilha para matriz.",
                                  "tips": "Use pesos éticos (ex.: privacidade 40%) na matriz para objetividade.",
                                  "learningObjective": "Integrar análises em avaliação ética holística com recomendações acionáveis.",
                                  "commonMistakes": "Focar só em custo, ignorando impactos éticos de longo prazo como confiança pública."
                                }
                              ],
                              "practicalExample": "Uma clínica médica avaliando armazenamento de registros de pacientes: nuvem (fácil acesso remoto, mas risco de breaches internacionais) vs. local (controle total, mas vulnerável a incêndios). Analisar se nuvem viola LGPD por dados fora do país.",
                              "finalVerifications": [
                                "Pode listar 5 prós/contras éticos de cada método com exemplos.",
                                "Matriz de decisão ética completa e balanceada.",
                                "Explicar verbalmente trade-offs para um stakeholder não-técnico.",
                                "Identificar cenários onde um método é eticamente superior.",
                                "Simular auditoria ética de uma escolha de armazenamento."
                              ],
                              "assessmentCriteria": [
                                "Profundidade da análise de riscos (cobertura completa de segurança/acesso).",
                                "Uso de evidências reais (estatísticas, leis citadas).",
                                "Clareza na matriz de trade-offs e recomendações.",
                                "Integração ética (além de técnico, considerando impacto humano).",
                                "Criatividade em exemplos práticos e verificações.",
                                "Precisão factual (sem mitos sobre nuvem/local)."
                              ],
                              "crossCurricularConnections": [
                                "Ética Filosófica: Debates sobre privacidade como direito humano.",
                                "Direito: Estudo de conformidade com LGPD/GDPR.",
                                "Ciências Sociais: Impacto na desigualdade digital (acesso à nuvem).",
                                "Gestão de TI: Planejamento de infraestrutura segura."
                              ],
                              "realWorldApplication": "Empresas como bancos usam armazenamento híbrido para equilibrar segurança ética (local para dados críticos) e escalabilidade (nuvem para analytics), evitando multas milionárias por violações como o caso Cambridge Analytica."
                            },
                            "estimatedTime": "1 hora",
                            "difficulty": "beginner",
                            "status": "not_started",
                            "prerequisites": []
                          }
                        ]
                      },
                      {
                        "id": "10.1.1.6.2",
                        "name": "Consentimento Informado e Transparência",
                        "description": "Garantir que os indivíduos saibam e concordem com o uso de seus dados, promovendo transparência em processos de coleta e análise.",
                        "specificSkills": [
                          {
                            "id": "10.1.1.6.2.1",
                            "name": "Explicar o conceito de consentimento informado",
                            "description": "Descrever os elementos necessários para um consentimento válido: voluntariedade, informação clara sobre uso de dados e direito de revogação.",
                            "atomicExpansion": {
                              "steps": [
                                {
                                  "stepNumber": 1,
                                  "title": "Compreender a Definição Básica de Consentimento Informado",
                                  "subSteps": [
                                    "Pesquise definições de consentimento informado em fontes éticas como GDPR ou LGPD.",
                                    "Identifique o contexto específico em ciência de dados: proteção de dados pessoais em coleta e processamento.",
                                    "Anote os princípios fundamentais: autonomia do indivíduo e transparência.",
                                    "Compare com consentimento não informado (ex.: termos obscuros em apps).",
                                    "Registre em um mapa mental os componentes iniciais."
                                  ],
                                  "verification": "Crie um resumo de 1 parágrafo definindo o conceito e compartilhe com um colega para feedback.",
                                  "estimatedTime": "20 minutos",
                                  "materials": [
                                    "Acesso à internet para GDPR/LGPD",
                                    "Ferramenta de mapa mental (ex.: MindMeister ou papel)"
                                  ],
                                  "tips": "Use exemplos cotidianos como formulários de inscrição em newsletters para tornar abstrato concreto.",
                                  "learningObjective": "Definir consentimento informado como um processo ético e legal para uso de dados pessoais.",
                                  "commonMistakes": "Confundir com mera aceitação de termos sem compreensão voluntária."
                                },
                                {
                                  "stepNumber": 2,
                                  "title": "Explorar o Elemento de Voluntariedade",
                                  "subSteps": [
                                    "Defina voluntariedade: ausência de coação, pressão ou obrigações implícitas.",
                                    "Analise cenários: consentimento válido vs. inválido (ex.: 'Aceite ou não use o serviço').",
                                    "Discuta alternativas: opções de opt-out claras e granulares.",
                                    "Crie um fluxograma de como garantir voluntariedade em um formulário de dados.",
                                    "Avalie exemplos reais de apps que falham nisso."
                                  ],
                                  "verification": "Desenhe um fluxograma simples e verifique se ele evita armadilhas de coação.",
                                  "estimatedTime": "25 minutos",
                                  "materials": [
                                    "Ferramenta de fluxograma (ex.: Lucidchart ou papel)",
                                    "Exemplos de pop-ups de cookies"
                                  ],
                                  "tips": "Pergunte: 'O usuário se sente livre para dizer não sem perda?'",
                                  "learningObjective": "Explicar voluntariedade como pilar essencial para validade do consentimento.",
                                  "commonMistakes": "Assumir que checkboxes pré-marcados são voluntários."
                                },
                                {
                                  "stepNumber": 3,
                                  "title": "Analisar Informação Clara sobre Uso de Dados",
                                  "subSteps": [
                                    "Liste elementos obrigatórios: propósito do uso, tipo de dados, destinatários e duração de retenção.",
                                    "Estude linguagem: evite jargões; use plain language (ex.: 'Seus dados de saúde serão compartilhados com provedores?').",
                                    "Crie um modelo de formulário com seções claras para cada info.",
                                    "Teste legibilidade: leia em voz alta e ajuste para 8º ano de escolaridade.",
                                    "Compare com exemplos ruins: walls of text em políticas de privacidade."
                                  ],
                                  "verification": "Redija um parágrafo de informação clara para um cenário hipotético e avalie com critérios de clareza.",
                                  "estimatedTime": "30 minutos",
                                  "materials": [
                                    "Modelos de formulários online",
                                    "Ferramenta de edição de texto"
                                  ],
                                  "tips": "Use bullet points e FAQs para quebrar informações complexas.",
                                  "learningObjective": "Descrever como fornecer informações transparentes e compreensíveis sobre dados.",
                                  "commonMistakes": "Incluir termos técnicos sem explicação, levando a consentimento mal informado."
                                },
                                {
                                  "stepNumber": 4,
                                  "title": "Entender o Direito de Revogação",
                                  "subSteps": [
                                    "Defina revogação: direito de retirar consentimento a qualquer momento, sem penalidades.",
                                    "Explique mecanismos: botões 'Revogar', e-mails simples e confirmação imediata.",
                                    "Discuta impactos: exclusão ou anonimização de dados pós-revogação.",
                                    "Simule um processo de revogação em um caso de estudo.",
                                    "Avalie conformidade legal em contextos como pesquisa de dados."
                                  ],
                                  "verification": "Descreva um processo de revogação passo a passo para um app de dados.",
                                  "estimatedTime": "25 minutos",
                                  "materials": [
                                    "Casos de estudo éticos (ex.: artigos sobre Cambridge Analytica)",
                                    "Simulador de formulário"
                                  ],
                                  "tips": "Torne revogação tão fácil quanto conceder consentimento.",
                                  "learningObjective": "Articular o direito de revogação como garantia de autonomia contínua.",
                                  "commonMistakes": "Ignorar que revogação deve ser prospectiva e retroativa nos dados."
                                }
                              ],
                              "practicalExample": "Em um app de saúde como o Google Fit, o consentimento informado é obtido via pop-up explicando: 'Coletaremos seus passos diários para análise de fitness (voluntário: continue sem compartilhar), dados vão para nosso servidor por 1 ano (info clara), clique 'Gerenciar Consentimento' para revogar a qualquer momento'.",
                              "finalVerifications": [
                                "Liste e defina os três elementos chave: voluntariedade, informação clara e revogação.",
                                "Identifique falhas em um formulário de consentimento real fornecido.",
                                "Explique por que ausência de um elemento invalida o consentimento.",
                                "Crie um checklist para validar consentimento em um projeto de dados.",
                                "Discuta um exemplo pessoal de consentimento ruim e como corrigi-lo.",
                                "Resuma em 3 frases o conceito para um leigo."
                              ],
                              "assessmentCriteria": [
                                "Precisão: Definição e elementos alinhados com padrões éticos/legais (30%).",
                                "Clareza: Explicações concisas e acessíveis, sem jargões (25%).",
                                "Profundidade: Inclui exemplos e mecanismos práticos (20%).",
                                "Completude: Cobre todos os três elementos sem omissões (15%).",
                                "Aplicação: Demonstra entendimento em cenários reais (10%)."
                              ],
                              "crossCurricularConnections": [
                                "Direito: Conformidade com LGPD/GDPR e direitos fundamentais.",
                                "Psicologia: Teoria da autonomia e vieses na tomada de decisão.",
                                "Tecnologia da Informação: Implementação de UI/UX para consentimento granular.",
                                "Filosofia Ética: Princípios de Kant sobre autonomia individual."
                              ],
                              "realWorldApplication": "Em ciência de dados, empresas como Meta ou pesquisas médicas usam consentimento informado para coletar dados de usuários, evitando multas milionárias (ex.: GDPR violações) e construindo confiança, como em estudos de IA onde dados biométricos exigem revogação fácil."
                            },
                            "estimatedTime": "1 hora",
                            "difficulty": "beginner",
                            "status": "not_started",
                            "prerequisites": []
                          },
                          {
                            "id": "10.1.1.6.2.2",
                            "name": "Documentar processos transparentes em projetos de dados",
                            "description": "Criar relatórios e dashboards que expliquem fontes de dados, métodos de análise e limitações, facilitando auditoria ética.",
                            "atomicExpansion": {
                              "steps": [
                                {
                                  "stepNumber": 1,
                                  "title": "Planejar a estrutura da documentação",
                                  "subSteps": [
                                    "Definir o público-alvo (ex: auditores, stakeholders, reguladores).",
                                    "Listar seções obrigatórias: fontes, métodos, limitações, conclusões.",
                                    "Criar um outline ou template em Markdown ou Jupyter Notebook.",
                                    "Estabelecer padrões de formatação (tabelas, diagramas de fluxo).",
                                    "Revisar requisitos éticos e regulatórios (ex: LGPD, GDPR)."
                                  ],
                                  "verification": "Verificar se o outline cobre todas as seções principais e é aprovado por um peer review.",
                                  "estimatedTime": "1 hora",
                                  "materials": "Editor de texto (Google Docs, Markdown), Jupyter Notebook.",
                                  "tips": "Use templates prontos de relatórios de dados para agilizar.",
                                  "learningObjective": "Entender como estruturar documentação para transparência e auditabilidade.",
                                  "commonMistakes": "Ignorar o público-alvo, resultando em documentação genérica e confusa."
                                },
                                {
                                  "stepNumber": 2,
                                  "title": "Documentar fontes de dados",
                                  "subSteps": [
                                    "Listar todas as fontes (bancos de dados, APIs, arquivos CSV).",
                                    "Incluir metadados: origem, data de coleta, licenças e termos de uso.",
                                    "Descrever pré-processamento inicial (limpeza, filtros aplicados).",
                                    "Criar tabela com colunas: Fonte, Volume, Qualidade, Consentimento.",
                                    "Adicionar links ou hashes para verificação de integridade."
                                  ],
                                  "verification": "Conferir se cada fonte tem metadados completos e links funcionais.",
                                  "estimatedTime": "2 horas",
                                  "materials": "Planilhas (Excel/Google Sheets), diagramas (Draw.io).",
                                  "tips": "Sempre cite consentimento explícito para dados pessoais.",
                                  "learningObjective": "Garantir rastreabilidade completa das origens dos dados.",
                                  "commonMistakes": "Omitir licenças, expondo a riscos legais."
                                },
                                {
                                  "stepNumber": 3,
                                  "title": "Documentar métodos de análise e processamento",
                                  "subSteps": [
                                    "Descrever pipeline de dados (ETL: Extract, Transform, Load).",
                                    "Explicar algoritmos usados (ex: regressão linear, clustering).",
                                    "Incluir código-fonte ou pseudocódigo com parâmetros chave.",
                                    "Criar fluxogramas visuais do processo.",
                                    "Registrar seeds para reproducibilidade em análises estocásticas."
                                  ],
                                  "verification": "Executar o pipeline documentado para reproduzir resultados originais.",
                                  "estimatedTime": "3 horas",
                                  "materials": "Jupyter Notebook, Python/R, ferramentas de diagrama (Lucidchart).",
                                  "tips": "Use versionamento (Git) para rastrear mudanças nos métodos.",
                                  "learningObjective": "Permitir reprodução exata da análise por terceiros.",
                                  "commonMistakes": "Não especificar hiperparâmetros, tornando resultados irreprodutíveis."
                                },
                                {
                                  "stepNumber": 4,
                                  "title": "Documentar limitações, riscos e considerações éticas",
                                  "subSteps": [
                                    "Listar vieses potenciais nos dados ou métodos.",
                                    "Descrever suposições feitas e cenários de falha.",
                                    "Explicar impactos éticos (privacidade, discriminação).",
                                    "Incluir análise de sensibilidade e métricas de incerteza.",
                                    "Recomendar ações para mitigar riscos identificados."
                                  ],
                                  "verification": "Avaliar se limitações cobrem pelo menos 80% dos riscos potenciais via checklist.",
                                  "estimatedTime": "2 horas",
                                  "materials": "Templates de risco ético, checklists (ex: de AI Ethics Guidelines).",
                                  "tips": "Quantifique limitações sempre que possível (ex: 'viés de 15% em gênero').",
                                  "learningObjective": "Promover auditoria ética ao destacar fraquezas transparentemente.",
                                  "commonMistakes": "Subestimar vieses, levando a conclusões enganosas."
                                },
                                {
                                  "stepNumber": 5,
                                  "title": "Criar relatórios e dashboards interativos",
                                  "subSteps": [
                                    "Integrar toda documentação em um relatório unificado (PDF ou web).",
                                    "Construir dashboard com tooltips explicativos (ex: Power BI, Tableau).",
                                    "Adicionar seções de FAQ e glossário de termos.",
                                    "Testar usabilidade com usuários simulados.",
                                    "Publicar com versão controlada e acesso auditável."
                                  ],
                                  "verification": "Realizar teste de usabilidade: um auditor deve entender em <30min.",
                                  "estimatedTime": "4 horas",
                                  "materials": "Tableau/Power BI, Streamlit/Dash para dashboards web.",
                                  "tips": "Use hyperlinks internos para navegação rápida entre seções.",
                                  "learningObjective": "Tornar a documentação acessível e visualmente auditável.",
                                  "commonMistakes": "Sobrecarregar com jargão técnico sem glossário."
                                }
                              ],
                              "practicalExample": "Em um projeto de análise de dados de saúde pública, criar um dashboard no Tableau que mostra: fontes (API do SUS com links), métodos (modelo de regressão com código Jupyter embutido), limitações (viés urbano de 20%, sem dados rurais recentes), permitindo que auditores éticos repliquem e critiquem o modelo.",
                              "finalVerifications": [
                                "Documentação permite reprodução completa do projeto por terceiros.",
                                "Todas fontes têm metadados e consentimentos explícitos.",
                                "Limitações e vieses são quantificados e mitigados.",
                                "Dashboard é interativo e intuitivo para não-especialistas.",
                                "Relatório atende checklists éticos (LGPD/GDPR).",
                                "Versão é versionada e acessível publicamente."
                              ],
                              "assessmentCriteria": [
                                "Completude: Todas seções (fontes, métodos, limitações) presentes e detalhadas (peso 25%).",
                                "Clareza: Linguagem acessível, visuais auxiliares (peso 20%).",
                                "Reprodutibilidade: Pipeline executável sem ambiguidades (peso 25%).",
                                "Transparência ética: Riscos explicitados e quantificados (peso 20%).",
                                "Usabilidade: Tempo para auditoria <1 hora (peso 10%)."
                              ],
                              "crossCurricularConnections": [
                                "Ética e Filosofia: Discussão de vieses e responsabilidade moral.",
                                "Comunicação: Redação técnica e visualização de dados.",
                                "Programação: Integração de código em relatórios (Jupyter).",
                                "Gestão de Projetos: Versionamento e planejamento de auditoria."
                              ],
                              "realWorldApplication": "Em empresas como Google ou bancos, documentação transparente é mandatória para compliance regulatório (ex: relatórios anuais de IA para auditorias da ANPD), evitando multas e construindo confiança pública em análises de dados sensíveis."
                            },
                            "estimatedTime": "2 horas",
                            "difficulty": "intermediate",
                            "status": "not_started",
                            "prerequisites": []
                          },
                          {
                            "id": "10.1.1.6.2.3",
                            "name": "Implementar mecanismos de revogação de consentimento",
                            "description": "Desenvolver fluxos para remoção ou exclusão de dados pessoais quando solicitado, integrando com pipelines de dados.",
                            "atomicExpansion": {
                              "steps": [
                                {
                                  "stepNumber": 1,
                                  "title": "Analisar Requisitos Legais e Definir Fluxo de Revogação",
                                  "subSteps": [
                                    "Estude regulamentações como LGPD ou GDPR para entender direitos de revogação de consentimento.",
                                    "Mapeie os dados pessoais coletados e onde são armazenados (bancos, logs, terceiros).",
                                    "Desenhe um diagrama de fluxo: solicitação → autenticação → processamento → confirmação.",
                                    "Defina escopos de revogação (exclusão total, parcial ou opt-out).",
                                    "Documente políticas de retenção mínima para compliance."
                                  ],
                                  "verification": "Diagrama de fluxo aprovado e requisitos documentados em um relatório.",
                                  "estimatedTime": "4 horas",
                                  "materials": [
                                    "Documentação LGPD/GDPR",
                                    "Ferramentas de diagramação como Draw.io ou Lucidchart",
                                    "Templates de fluxos de privacidade"
                                  ],
                                  "tips": "Priorize anonimização em vez de exclusão total quando possível para otimizar performance.",
                                  "learningObjective": "Compreender obrigações legais e mapear fluxos de dados para revogação ética.",
                                  "commonMistakes": [
                                    "Ignorar dados em terceiros",
                                    "Subestimar tempo de propagação em pipelines distribuídos"
                                  ]
                                },
                                {
                                  "stepNumber": 2,
                                  "title": "Projetar Interface e API para Solicitações de Revogação",
                                  "subSteps": [
                                    "Crie endpoint API seguro (ex: POST /revoke-consent) com autenticação JWT ou OAuth.",
                                    "Desenvolva formulário frontend intuitivo com checkboxes para tipos de consentimento.",
                                    "Implemente validação de identidade (2FA ou e-mail verificado).",
                                    "Adicione logs auditáveis para rastrear solicitações.",
                                    "Integre notificações por e-mail para confirmação."
                                  ],
                                  "verification": "API testada com Postman retorna 200 OK com token de rastreio.",
                                  "estimatedTime": "6 horas",
                                  "materials": [
                                    "Postman ou Insomnia para testes API",
                                    "Swagger para documentação",
                                    "Bibliotecas como Express.js ou Flask"
                                  ],
                                  "tips": "Use rate limiting para prevenir abusos na API.",
                                  "learningObjective": "Projetar interfaces seguras e user-friendly para gerenciamento de consentimento.",
                                  "commonMistakes": [
                                    "Falta de autenticação forte",
                                    "APIs sem versionamento"
                                  ]
                                },
                                {
                                  "stepNumber": 3,
                                  "title": "Implementar Lógica de Revogação nos Pipelines de Dados",
                                  "subSteps": [
                                    "Identifique pipelines (ETL, streaming) e adicione flags de revogação (ex: user_id: revoked).",
                                    "Crie jobs para exclusão em massa: SQL DELETE com soft-delete ou anonymização.",
                                    "Integre com storage distribuído (S3, BigQuery) usando propagação assíncrona.",
                                    "Atualize modelos ML para excluir dados revogados em retraining.",
                                    "Configure filas (Kafka/RabbitMQ) para processar revogações em background."
                                  ],
                                  "verification": "Simulação de revogação remove dados de todos os storages visíveis em queries.",
                                  "estimatedTime": "8 horas",
                                  "materials": [
                                    "Airflow ou Luigi para orquestração",
                                    "SQL/NoSQL clients",
                                    "Bibliotecas de anonymização como Faker"
                                  ],
                                  "tips": "Use soft-delete para permitir reversão em casos de erro.",
                                  "learningObjective": "Integrar revogação em pipelines complexos mantendo integridade de dados.",
                                  "commonMistakes": [
                                    "Exclusão síncrona causando timeouts",
                                    "Esquecimento de caches/replicas"
                                  ]
                                },
                                {
                                  "stepNumber": 4,
                                  "title": "Testar, Integrar e Monitorar o Mecanismo Completo",
                                  "subSteps": [
                                    "Execute testes end-to-end: solicitação → processamento → verificação de remoção.",
                                    "Simule cenários edge-case (revogação em massa, dados em trânsito).",
                                    "Configure monitoramento (Prometheus/Grafana) para métricas de revogação.",
                                    "Realize auditoria com ferramentas como OWASP ZAP para segurança.",
                                    "Documente o sistema e treine equipe para suporte."
                                  ],
                                  "verification": "Relatório de testes com 100% cobertura de cenários críticos.",
                                  "estimatedTime": "5 horas",
                                  "materials": [
                                    "JUnit/Pytest para testes",
                                    "Selenium para E2E",
                                    "ELK Stack para logs"
                                  ],
                                  "tips": "Automatize testes de revogação em CI/CD pipeline.",
                                  "learningObjective": "Garantir robustez e conformidade através de testes e monitoramento contínuo.",
                                  "commonMistakes": [
                                    "Testes insuficientes em escala",
                                    "Falta de monitoramento pós-deploy"
                                  ]
                                }
                              ],
                              "practicalExample": "Em um e-commerce, um usuário revoga consentimento para marketing via app. O sistema autentica, flagga o user_id em Kafka, processa exclusão de e-mails em 24h via Airflow (remove de Mailchimp, BigQuery e S3), confirma por e-mail e loga para auditoria GDPR.",
                              "finalVerifications": [
                                "Dados pessoais removidos de todos storages e pipelines confirmados por query.",
                                "Logs auditáveis mostram processamento completo sem erros.",
                                "Usuário recebe confirmação e não recebe mais comunicações.",
                                "Sistema resiste a 100 revogações simultâneas sem falhas.",
                                "Anonimização aplicada onde exclusão total não é viável.",
                                "Relatório de compliance gerado automaticamente."
                              ],
                              "assessmentCriteria": [
                                "Fluxo completo implementado com <5% taxa de erro em testes.",
                                "Tempo de processamento de revogação <24h para 95% dos casos.",
                                "Cobertura de segurança: autenticação, logs e rate limiting presentes.",
                                "Integração seamless com pipelines existentes sem downtime.",
                                "Documentação clara com diagramas e APIs documentadas.",
                                "Escalabilidade demonstrada em simulações de carga."
                              ],
                              "crossCurricularConnections": [
                                "Direito Digital: Aplicação de LGPD/GDPR em cenários reais.",
                                "Desenvolvimento de Software: Design de APIs e microsserviços.",
                                "Segurança da Informação: Autenticação e auditoria.",
                                "Gestão de Dados: ETL e governança de dados.",
                                "Ética Computacional: Privacidade by design."
                              ],
                              "realWorldApplication": "Implementação essencial para compliance em apps como redes sociais (ex: Twitter/X permite 'right to be forgotten'), bancos digitais e healthtechs, evitando multas milionárias por violações de privacidade e construindo confiança do usuário."
                            },
                            "estimatedTime": "1.5 horas",
                            "difficulty": "intermediate",
                            "status": "not_started",
                            "prerequisites": []
                          }
                        ]
                      },
                      {
                        "id": "10.1.1.6.3",
                        "name": "Mitigação de Viés e Equidade",
                        "description": "Identificar e corrigir vieses em dados e modelos para promover decisões justas e inclusivas.",
                        "specificSkills": [
                          {
                            "id": "10.1.1.6.3.1",
                            "name": "Detectar viés em conjuntos de dados",
                            "description": "Utilizar métricas como distribuição demográfica e testes estatísticos para identificar vieses de amostragem ou representatividade.",
                            "atomicExpansion": {
                              "steps": [
                                {
                                  "stepNumber": 1,
                                  "title": "Compreender Conceitos Fundamentais de Viés em Dados",
                                  "subSteps": [
                                    "Defina viés de amostragem e viés de representatividade com exemplos reais.",
                                    "Liste tipos comuns de viés: seleção, confirmação, demográfico e de medição.",
                                    "Revise métricas chave: distribuição demográfica, proporções por grupo e testes como qui-quadrado.",
                                    "Estude impactos éticos do viés não detectado em aplicações de IA.",
                                    "Crie um glossário pessoal com 5 termos relacionados."
                                  ],
                                  "verification": "Resuma em um parágrafo os tipos de viés e métricas principais.",
                                  "estimatedTime": "45 minutos",
                                  "materials": "Documentação online sobre viés em ML (ex: Towards Data Science), notebook Jupyter vazio.",
                                  "tips": "Use analogias cotidianas, como uma pesquisa de opinião enviesada por horário.",
                                  "learningObjective": "Identificar e diferenciar tipos de viés para contextualizar análises futuras.",
                                  "commonMistakes": "Confundir viés de amostragem com outliers; ignorar viés demográfico sutil."
                                },
                                {
                                  "stepNumber": 2,
                                  "title": "Explorar e Visualizar o Conjunto de Dados",
                                  "subSteps": [
                                    "Carregue o dataset usando pandas (ex: pd.read_csv()).",
                                    "Inspecione estrutura: df.info(), df.describe() e df.head().",
                                    "Identifique colunas demográficas (idade, gênero, etnia) e variáveis alvo.",
                                    "Gere visualizações: histogramas, boxplots e gráficos de barras para distribuições por grupo.",
                                    "Calcule proporções básicas: value_counts() normalizado por grupos."
                                  ],
                                  "verification": "Produza 3 gráficos mostrando distribuições desbalanceadas.",
                                  "estimatedTime": "60 minutos",
                                  "materials": "Python com pandas, matplotlib/seaborn; dataset exemplo (ex: Adult UCI ou Kaggle loan dataset).",
                                  "tips": "Sempre stratifique por múltiplas dimensões demográficas simultaneamente.",
                                  "learningObjective": "Visualizar padrões de representatividade no dataset.",
                                  "commonMistakes": "Ignorar valores ausentes que mascaram viés; usar escalas inadequadas em plots."
                                },
                                {
                                  "stepNumber": 3,
                                  "title": "Aplicar Métricas e Testes Estatísticos para Detecção",
                                  "subSteps": [
                                    "Calcule métricas demográficas: % de cada grupo vs. população real.",
                                    "Execute teste qui-quadrado para independência entre variáveis demográficas e alvo.",
                                    "Use teste t ou Mann-Whitney para diferenças médias entre grupos.",
                                    "Calcule índices de viés como Disparate Impact (razão de taxas positivas).",
                                    "Interprete p-valores: <0.05 indica viés estatisticamente significativo."
                                  ],
                                  "verification": "Gere relatório com 4 métricas/testes e seus resultados numéricos.",
                                  "estimatedTime": "75 minutos",
                                  "materials": "Python com scipy.stats, pandas; dataset do passo anterior.",
                                  "tips": "Compare sempre com benchmarks populacionais reais (ex: censos).",
                                  "learningObjective": "Quantificar viés usando estatística rigorosa.",
                                  "commonMistakes": "Interpretar p-valores como prova absoluta; não ajustar por múltiplos testes."
                                },
                                {
                                  "stepNumber": 4,
                                  "title": "Interpretar Resultados e Documentar Viés Detectado",
                                  "subSteps": [
                                    "Classifique o viés: baixo/médio/alto baseado em thresholds (ex: disparate impact <0.8).",
                                    "Discuta causas potenciais: coleta de dados, exclusão de grupos.",
                                    "Proponha mitigação inicial: reamostragem ou pesos.",
                                    "Crie relatório visual: dashboard com métricas e conclusões.",
                                    "Valide com dataset de teste independente."
                                  ],
                                  "verification": "Escreva um relatório de 1 página com evidências e recomendações.",
                                  "estimatedTime": "50 minutos",
                                  "materials": "Jupyter notebook; ferramentas de visualização como plotly.",
                                  "tips": "Use tabelas para comparar 'dataset vs. população real'.",
                                  "learningObjective": "Transformar análises em insights acionáveis e éticos.",
                                  "commonMistakes": "Sobrestimar viés em datasets pequenos; omitir contexto ético."
                                }
                              ],
                              "practicalExample": "Em um dataset de aprovações de empréstimos (ex: German Credit Dataset), calcule a taxa de aprovação por gênero: 75% homens vs. 55% mulheres. Aplique qui-quadrado (p<0.01) e disparate impact (0.73), confirmando viés de gênero na amostragem.",
                              "finalVerifications": [
                                "Lista corretamente 3+ métricas usadas para detecção de viés.",
                                "Interpreta resultados de testes estatísticos com p-valores e thresholds.",
                                "Identifica pelo menos um tipo de viés no dataset exemplo.",
                                "Propõe uma mitigação básica para o viés detectado.",
                                "Gera visualizações claras mostrando desbalanceamento demográfico.",
                                "Compara distribuições do dataset com dados populacionais reais."
                              ],
                              "assessmentCriteria": [
                                "Precisão na identificação de viés (90%+ acurácia em métricas).",
                                "Uso correto de testes estatísticos com interpretação adequada.",
                                "Qualidade das visualizações (claras, rotuladas, informativas).",
                                "Profundidade da análise demográfica (múltiplas dimensões).",
                                "Relatório completo com evidências quantitativas e qualitativas.",
                                "Consideração ética e sugestões de mitigação viáveis."
                              ],
                              "crossCurricularConnections": [
                                "Estatística: Testes de hipótese e distribuições.",
                                "Ética e Filosofia: Implicações de justiça e equidade em dados.",
                                "Programação: Manipulação de dados com Python/pandas.",
                                "Ciência da Computação: Pré-processamento em ML pipelines.",
                                "Sociologia: Análise de desigualdades demográficas."
                              ],
                              "realWorldApplication": "Em algoritmos de recrutamento da Amazon, detecção de viés de gênero em currículos levou à reformulação do modelo; em saúde, identificar viés racial em datasets de COVID-19 previne diagnósticos errôneos em minorias."
                            },
                            "estimatedTime": "2 horas",
                            "difficulty": "intermediate",
                            "status": "not_started",
                            "prerequisites": []
                          },
                          {
                            "id": "10.1.1.6.3.2",
                            "name": "Aplicar técnicas de balanceamento de dados",
                            "description": "Implementar oversampling, undersampling ou SMOTE para mitigar desequilíbrios que causam viés em modelos de machine learning.",
                            "atomicExpansion": {
                              "steps": [
                                {
                                  "stepNumber": 1,
                                  "title": "Detectar e Quantificar Desequilíbrio nos Dados",
                                  "subSteps": [
                                    "Carregue o dataset usando pandas.",
                                    "Calcule a distribuição das classes com value_counts().",
                                    "Visualize o desequilíbrio com gráficos de barras ou pie charts usando matplotlib/seaborn.",
                                    "Calcule métricas como ratio de minoria/maioria e IMB (Imbalance Ratio).",
                                    "Documente o nível de desequilíbrio (leve, moderado, severo)."
                                  ],
                                  "verification": "Confirme que o relatório mostra proporções de classes abaixo de 1:10 para minoria, com visualizações salvas.",
                                  "estimatedTime": "20 minutos",
                                  "materials": "Python, pandas, matplotlib/seaborn, Jupyter Notebook",
                                  "tips": "Sempre use log scale para ratios extremos em visualizações.",
                                  "learningObjective": "Identificar e quantificar desequilíbrios de classe em datasets.",
                                  "commonMistakes": "Ignorar classes com menos de 5% dos dados ou confundir features com labels."
                                },
                                {
                                  "stepNumber": 2,
                                  "title": "Implementar Undersampling",
                                  "subSteps": [
                                    "Instale e importe imbalanced-learn (imblearn).",
                                    "Aplique RandomUnderSampler do imblearn.under_sampling.",
                                    "Ajuste parâmetros como sampling_strategy='auto' ou 'majority'.",
                                    "Aplique o sampler: X_res, y_res = sampler.fit_resample(X, y).",
                                    "Verifique a nova distribuição de classes."
                                  ],
                                  "verification": "Dataset balanceado com classes próximas a 50/50, sem perda excessiva de dados.",
                                  "estimatedTime": "15 minutos",
                                  "materials": "imbalanced-learn, scikit-learn, pandas",
                                  "tips": "Use cluster_centroids para undersampling mais inteligente em datasets grandes.",
                                  "learningObjective": "Reduzir amostras da classe majoritária de forma controlada.",
                                  "commonMistakes": "Aplicar undersampling no dataset completo sem split train/test, causando data leakage."
                                },
                                {
                                  "stepNumber": 3,
                                  "title": "Implementar Oversampling",
                                  "subSteps": [
                                    "Importe RandomOverSampler do imblearn.over_sampling.",
                                    "Configure sampling_strategy='minority' ou 'auto'.",
                                    "Aplique: X_res, y_res = over_sampler.fit_resample(X_train, y_train).",
                                    "Visualize o dataset resultante.",
                                    "Salve amostras duplicadas para análise."
                                  ],
                                  "verification": "Classes balanceadas com duplicatas na minoria confirmadas via value_counts().",
                                  "estimatedTime": "15 minutos",
                                  "materials": "imbalanced-learn, scikit-learn",
                                  "tips": "Combine com SMOTE para evitar overfitting puro por duplicatas.",
                                  "learningObjective": "Aumentar amostras da classe minoritária via replicação.",
                                  "commonMistakes": "Oversampling no conjunto de teste, inflando métricas artificialmente."
                                },
                                {
                                  "stepNumber": 4,
                                  "title": "Implementar SMOTE e Avaliar Impacto",
                                  "subSteps": [
                                    "Importe SMOTE do imblearn.over_sampling.",
                                    "Aplique SMOTE(k_neighbors=5) no train set.",
                                    "Treine um modelo baseline (ex: LogisticRegression) sem balanceamento.",
                                    "Treine modelo com dados balanceados e compare métricas (F1, AUC-ROC).",
                                    "Use cross-validation para robustez."
                                  ],
                                  "verification": "Melhoria em F1-score para classe minoritária > 10% comparado ao baseline.",
                                  "estimatedTime": "30 minutos",
                                  "materials": "imbalanced-learn, scikit-learn, pandas",
                                  "tips": "Ajuste k_neighbors baseado no tamanho da minoria; evite em datasets com poucas amostras.",
                                  "learningObjective": "Gerar amostras sintéticas e medir mitigação de viés.",
                                  "commonMistakes": "Usar SMOTE em dados com alta dimensionalidade sem redução prévia."
                                },
                                {
                                  "stepNumber": 5,
                                  "title": "Comparar Técnicas e Selecionar a Melhor",
                                  "subSteps": [
                                    "Crie um pipeline comparativo com ColumnTransformer e make_pipeline.",
                                    "Execute GridSearchCV para hiperparâmetros de cada técnica.",
                                    "Compare métricas em validação cruzada (precision, recall, F1 por classe).",
                                    "Escolha técnica baseada em trade-offs (ex: undersampling para speed).",
                                    "Documente resultados em tabela."
                                  ],
                                  "verification": "Tabela comparativa salva mostrando a técnica ótima com evidências.",
                                  "estimatedTime": "25 minutos",
                                  "materials": "scikit-learn pipelines, pandas",
                                  "tips": "Priorize recall para classe minoritária em cenários de custo assimétrico.",
                                  "learningObjective": "Avaliar e selecionar balanceamento ótimo para cenários específicos.",
                                  "commonMistakes": "Avaliar apenas accuracy global, ignorando métricas por classe."
                                }
                              ],
                              "practicalExample": "Em um dataset de detecção de churn de clientes (90% não churn, 10% churn), aplique undersampling para reduzir não-churn para 10k amostras, oversampling para replicar churn, e SMOTE para gerar 5k amostras sintéticas de churn. Treine LogisticRegression e observe F1-churn subir de 0.25 para 0.65 com SMOTE.",
                              "finalVerifications": [
                                "Distribuição de classes balanceada (ratio < 1:1.5) em todos os métodos testados.",
                                "Melhoria mensurável em F1-score ou AUC-PR para classe minoritária.",
                                "Ausência de data leakage (balanceamento só em train).",
                                "Relatório comparativo com gráficos e tabelas gerado.",
                                "Modelo final selecionado com justificativa ética (mitigação de viés).",
                                "Código reproduzível e comentado."
                              ],
                              "assessmentCriteria": [
                                "Precisão na detecção de desequilíbrio (>95% accuracy em métricas).",
                                "Implementação correta sem leakage (verificado por seed reproducibility).",
                                "Escolha de técnica justificada por métricas (F1 > baseline).",
                                "Uso de validação cruzada para robustez.",
                                "Documentação clara de trade-offs e limitações.",
                                "Aplicação ética: discussão de viés residual."
                              ],
                              "crossCurricularConnections": [
                                "Estatística: Cálculo de distribuições e testes de hipóteses para viés.",
                                "Programação: Manipulação de dados com pandas e pipelines sklearn.",
                                "Ética em IA: Mitigação de viés para equidade em decisões automatizadas.",
                                "Matemática Computacional: Vetores sintéticos em SMOTE via interpolação.",
                                "Ciência de Dados: Avaliação de modelos com métricas não-binárias."
                              ],
                              "realWorldApplication": "Em sistemas de detecção de fraudes bancárias (fraudes raras), balanceamento com SMOTE previne viés para transações normais, melhorando recall de fraudes de 40% para 85%, reduzindo perdas financeiras em milhões."
                            },
                            "estimatedTime": "2 horas",
                            "difficulty": "intermediate",
                            "status": "not_started",
                            "prerequisites": []
                          },
                          {
                            "id": "10.1.1.6.3.3",
                            "name": "Avaliar equidade em modelos preditivos",
                            "description": "Calcular métricas de fairness como equalized odds ou demographic parity para validar a equidade de predições.",
                            "atomicExpansion": {
                              "steps": [
                                {
                                  "stepNumber": 1,
                                  "title": "Compreender Conceitos Fundamentais de Métricas de Fairness",
                                  "subSteps": [
                                    "Estude definições de fairness em ML: demographic parity (taxas de predição iguais entre grupos) e equalized odds (taxas de TP/FP iguais entre grupos).",
                                    "Revise confusão matrix: TP, FP, TN, FN para grupos protegidos e não protegidos.",
                                    "Leia documentação oficial ou papers introdutórios sobre essas métricas (ex: AIF360 library).",
                                    "Anote diferenças entre métricas e quando usar cada uma.",
                                    "Discuta exemplos reais de viés em predições."
                                  ],
                                  "verification": "Resuma em um parágrafo as diferenças entre demographic parity e equalized odds, com fórmulas corretas.",
                                  "estimatedTime": "45 minutos",
                                  "materials": "Documentação AIF360, paper 'Fairness and Machine Learning' (capítulo intro), notebook Jupyter vazio.",
                                  "tips": "Use diagramas visuais para matriz de confusão para fixar conceitos.",
                                  "learningObjective": "Dominar definições e fórmulas matemáticas das métricas de equidade.",
                                  "commonMistakes": "Confundir demographic parity com equal opportunity; ignorar dependência em labels verdadeiros para equalized odds."
                                },
                                {
                                  "stepNumber": 2,
                                  "title": "Preparar Dataset com Atributos Sensíveis",
                                  "subSteps": [
                                    "Selecione ou carregue um dataset com predições, labels verdadeiros e atributo sensível (ex: gênero, raça).",
                                    "Limpe dados: remova NaNs, codifique categóricos.",
                                    "Separe grupos protegidos (ex: A=0, B=1) e calcule tamanhos de amostra.",
                                    "Gere predições se necessário usando um modelo simples (ex: Logistic Regression).",
                                    "Crie matriz de confusão manualmente para um subconjunto pequeno."
                                  ],
                                  "verification": "Dataset carregado com colunas: y_true, y_pred, sensitive_attr; amostras balanceadas por grupo exibidas.",
                                  "estimatedTime": "30 minutos",
                                  "materials": "Dataset Adult UCI ou German Credit (Kaggle), Python com pandas, scikit-learn.",
                                  "tips": "Use train_test_split para simular cenários reais; garanta pelo menos 1000 amostras por grupo.",
                                  "learningObjective": "Preparar dados estruturados para análise de fairness.",
                                  "commonMistakes": "Não estratificar por atributo sensível no split; usar dados desbalanceados sem notar."
                                },
                                {
                                  "stepNumber": 3,
                                  "title": "Implementar Cálculo de Métricas de Fairness",
                                  "subSteps": [
                                    "Instale e importe bibliotecas: AIF360 ou fairlearn.",
                                    "Calcule demographic parity: |P(y_hat=1|A=0) - P(y_hat=1|A=1)| < threshold (ex: 0.1).",
                                    "Calcule equalized odds: verifique TPR e FPR iguais entre grupos.",
                                    "Implemente funções customizadas sem biblioteca para validar entendimento.",
                                    "Teste com dados sintéticos para edge cases (ex: predições perfeitas)."
                                  ],
                                  "verification": "Códigos executados retornam valores numéricos para DP e EO; compare com biblioteca oficial.",
                                  "estimatedTime": "60 minutos",
                                  "materials": "Python/Jupyter, bibliotecas AIF360, fairlearn, numpy, matplotlib para plots.",
                                  "tips": "Visualize disparidades com bar charts para intuição.",
                                  "learningObjective": "Codificar e computar métricas de equidade programaticamente.",
                                  "commonMistakes": "Erro na fórmula de TPR/FPR; dividir por zero em grupos vazios."
                                },
                                {
                                  "stepNumber": 4,
                                  "title": "Interpretar Resultados e Validar Equidade",
                                  "subSteps": [
                                    "Analise valores: DP < 0.1 e EO próximo de 1 indica equidade.",
                                    "Compare com baselines (ex: modelo sem correção).",
                                    "Gere relatório: tabela com métricas por grupo.",
                                    "Teste sensibilidade variando thresholds de predição.",
                                    "Documente conclusões: modelo é equitativo? Por quê?"
                                  ],
                                  "verification": "Relatório gerado com tabelas/plots mostrando métricas e thresholds de aceitação.",
                                  "estimatedTime": "45 minutos",
                                  "materials": "Mesmo notebook anterior, seaborn para visualizações avançadas.",
                                  "tips": "Defina thresholds baseados em regulamentações (ex: 80% rule).",
                                  "learningObjective": "Interpretar métricas para decisões sobre viés em modelos.",
                                  "commonMistakes": "Ignorar trade-offs entre fairness e accuracy; interpretação errada de valores absolutos vs relativos."
                                },
                                {
                                  "stepNumber": 5,
                                  "title": "Aplicar e Documentar em Cenário Completo",
                                  "subSteps": [
                                    "Integre avaliação em pipeline ML end-to-end.",
                                    "Automatize cálculos em função reutilizável.",
                                    "Salve resultados em relatório PDF ou Markdown.",
                                    "Revise com peer ou self-checklist.",
                                    "Planeje próximos passos para mitigação se viés detectado."
                                  ],
                                  "verification": "Função testada em novo dataset; relatório completo submetido.",
                                  "estimatedTime": "30 minutos",
                                  "materials": "Pipeline completo, GitHub para versionamento.",
                                  "tips": "Use templates de relatório para consistência.",
                                  "learningObjective": "Consolidar avaliação de equidade em workflows profissionais.",
                                  "commonMistakes": "Não versionar código; pular documentação."
                                }
                              ],
                              "practicalExample": "Em um modelo de aprovação de empréstimos usando dataset German Credit: calcule demographic parity por gênero. Se P(aprovação|mulher) = 0.45 e P(aprovação|homem) = 0.60, DP=0.15 > 0.1, indicando viés contra mulheres. Para equalized odds, verifique se FPR é similar (ex: 0.20 vs 0.25).",
                              "finalVerifications": [
                                "Métricas DP e EO calculadas corretamente para ambos os grupos.",
                                "Disparidades visualizadas em gráficos.",
                                "Relatório interpreta se modelo é equitativo com thresholds padrão.",
                                "Função customizada bate com biblioteca (erro < 0.01).",
                                "Testes em dados sintéticos passam.",
                                "Documentação cobre fórmulas e resultados."
                              ],
                              "assessmentCriteria": [
                                "Precisão matemática nas fórmulas (100% correto).",
                                "Código executável e comentado.",
                                "Interpretação contextualizada com exemplos.",
                                "Uso adequado de thresholds regulatórios.",
                                "Visualizações claras e informativas.",
                                "Tratamento de edge cases demonstrado."
                              ],
                              "crossCurricularConnections": [
                                "Estatística: Probabilidades condicionais e testes de hipótese.",
                                "Ética e Filosofia: Princípios de justiça e discriminação algorítmica.",
                                "Programação: Bibliotecas Python para ML ético.",
                                "Direito: Regulamentações como GDPR e AI Act.",
                                "Negócios: Impacto de viés em decisões corporativas."
                              ],
                              "realWorldApplication": "Em bancos para aprovações de crédito equitativas, evitando ações judiciais por discriminação; em RH para triagem de currículos sem viés racial/gênero; em justiça criminal para predizer reincidência sem disparidades étnicas."
                            },
                            "estimatedTime": "1.5 horas",
                            "difficulty": "advanced",
                            "status": "not_started",
                            "prerequisites": []
                          },
                          {
                            "id": "10.1.1.6.3.4",
                            "name": "Promover inclusão em práticas de coleta de dados",
                            "description": "Planejar coletas de dados que considerem diversidade populacional para evitar exclusão de grupos minoritários.",
                            "atomicExpansion": {
                              "steps": [
                                {
                                  "stepNumber": 1,
                                  "title": "Mapear a diversidade populacional relevante",
                                  "subSteps": [
                                    "Identifique o contexto do estudo e a população alvo geral.",
                                    "Pesquise dados demográficos disponíveis (ex.: censos, relatórios governamentais) sobre grupos minoritários como etnia, gênero, idade, localização geográfica e status socioeconômico.",
                                    "Liste pelo menos 5-7 grupos sub-representados potenciais no domínio do estudo.",
                                    "Crie um mapa visual (tabela ou diagrama) da composição demográfica esperada.",
                                    "Documente fontes de dados usadas para garantir confiabilidade."
                                  ],
                                  "verification": "Verifique se o mapa inclui pelo menos 5 grupos minoritários com fontes citadas e porcentagens aproximadas.",
                                  "estimatedTime": "2-3 horas",
                                  "materials": "Acesso à internet para censos (IBGE, ONU), ferramentas como Google Sheets ou Draw.io para mapeamento.",
                                  "tips": "Comece com dados locais para maior relevância; priorize grupos historicamente excluídos.",
                                  "learningObjective": "Compreender a composição demográfica para identificar riscos de exclusão.",
                                  "commonMistakes": "Ignorar interseccionalidades (ex.: gênero + etnia) ou usar dados desatualizados."
                                },
                                {
                                  "stepNumber": 2,
                                  "title": "Avaliar riscos de viés e exclusão",
                                  "subSteps": [
                                    "Analise o mapa demográfico contra práticas comuns de coleta de dados no campo.",
                                    "Identifique barreiras potenciais para cada grupo minoritário (ex.: idioma, acesso à internet, desconfiança cultural).",
                                    "Pontue riscos em uma escala de 1-5 para cada grupo e priorize os de alto risco.",
                                    "Consulte literatura acadêmica sobre vieses em coletas semelhantes.",
                                    "Registre suposições e incertezas na análise."
                                  ],
                                  "verification": "Confira se há uma tabela de riscos com pontuações e justificativas para todos os grupos mapeados.",
                                  "estimatedTime": "1-2 horas",
                                  "materials": "Artigos acadêmicos (Google Scholar), planilhas para tabela de riscos.",
                                  "tips": "Use frameworks como o 'Equity Impact Assessment' para estruturar a análise.",
                                  "learningObjective": "Desenvolver habilidade em diagnosticar exclusões potenciais baseadas em evidências.",
                                  "commonMistakes": "Subestimar barreiras culturais ou focar apenas em demografia superficial."
                                },
                                {
                                  "stepNumber": 3,
                                  "title": "Projetar estratégias de coleta inclusivas",
                                  "subSteps": [
                                    "Defina métodos de amostragem inclusivos (ex.: estratificada, snowball para hard-to-reach groups).",
                                    "Adapte instrumentos de coleta (ex.: múltiplos idiomas, opções acessíveis como telefone para baixa conectividade).",
                                    "Planeje incentivos e comunicações culturalmente sensíveis para engajar grupos minoritários.",
                                    "Inclua quotas mínimas para representação de cada grupo no plano de amostra.",
                                    "Escreva um protocolo detalhado com cronograma e responsáveis."
                                  ],
                                  "verification": "Revise o protocolo para confirmar inclusão de quotas, adaptações e métodos específicos para minorias.",
                                  "estimatedTime": "3-4 horas",
                                  "materials": "Modelos de protocolos de pesquisa (de universidades), software de design de surveys como Google Forms ou Qualtrics.",
                                  "tips": "Teste o instrumento com um pequeno grupo diverso antes de finalizar.",
                                  "learningObjective": "Criar planos práticos que garantam representação equitativa.",
                                  "commonMistakes": "Definir quotas irreais sem considerar orçamento ou viabilidade logística."
                                },
                                {
                                  "stepNumber": 4,
                                  "title": "Validar e iterar o plano de coleta",
                                  "subSteps": [
                                    "Realize um piloto com amostra pequena e diversa (mínimo 10-20 participantes de grupos variados).",
                                    "Colete feedback qualitativo sobre acessibilidade e inclusão durante o piloto.",
                                    "Analise taxas de resposta por grupo e ajuste quotas ou métodos conforme necessário.",
                                    "Documente lições aprendidas e atualize o protocolo final.",
                                    "Obtenha aprovação ética (se aplicável) destacando medidas inclusivas."
                                  ],
                                  "verification": "Examine relatório do piloto com métricas de inclusão e versão atualizada do protocolo.",
                                  "estimatedTime": "4-6 horas (incluindo piloto)",
                                  "materials": "Ferramentas de survey online/offline, gravador para feedback, comitê ético simulado.",
                                  "tips": "Registre tudo digitalmente para rastreabilidade; itere pelo menos uma vez.",
                                  "learningObjective": "Aplicar iteração baseada em evidências para refinar práticas inclusivas.",
                                  "commonMistakes": "Pular o piloto por 'falta de tempo' ou ignorar feedback de minorias."
                                }
                              ],
                              "practicalExample": "Ao planejar uma coleta de dados sobre impacto de apps de saúde mental em usuários brasileiros, mapear grupos como indígenas rurais, LGBTQ+ idosos e baixa renda; usar amostragem snowball via ONGs e surveys em português/linguagens indígenas, garantindo 20% de representação de cada grupo minoritário.",
                              "finalVerifications": [
                                "O plano inclui mapeamento demográfico com fontes citadas?",
                                "Riscos de exclusão foram avaliados e priorizados por grupo?",
                                "Estratégias específicas (quotas, adaptações) cobrem todos os grupos mapeados?",
                                "Piloto foi realizado com feedback de diversidade?",
                                "Protocolo final documenta iterações e métricas de inclusão?",
                                "Medidas éticas de equidade estão explícitas?"
                              ],
                              "assessmentCriteria": [
                                "Profundidade do mapeamento demográfico (cobertura de interseccionalidades).",
                                "Precisão na identificação de riscos e barreiras culturais.",
                                "Criatividade e viabilidade das estratégias inclusivas.",
                                "Qualidade do piloto e uso de dados para iteração.",
                                "Clareza e completude do protocolo final.",
                                "Demonstração de compreensão ética de equidade em dados."
                              ],
                              "crossCurricularConnections": [
                                "Ética e Filosofia: Discussão de justiça distributiva em dados.",
                                "Sociologia: Análise de desigualdades sociais e interseccionalidade.",
                                "Estatística: Técnicas de amostragem estratificada e ponderação.",
                                "Direitos Humanos: Convenções da ONU sobre inclusão de minorias."
                              ],
                              "realWorldApplication": "Em projetos de IA para saúde pública, como o desenvolvimento de algoritmos de diagnóstico que evitam vieses raciais ao incluir dados de populações indígenas e quilombolas, melhorando precisão e equidade em políticas governamentais."
                            },
                            "estimatedTime": "1 hora",
                            "difficulty": "beginner",
                            "status": "not_started",
                            "prerequisites": []
                          }
                        ]
                      },
                      {
                        "id": "10.1.1.6.4",
                        "name": "Responsabilidade e Accountability",
                        "description": "Estabelecer accountability por decisões baseadas em dados e promover uso responsável.",
                        "specificSkills": [
                          {
                            "id": "10.1.1.6.4.1",
                            "name": "Definir papéis de responsabilidade em equipes de dados",
                            "description": "Mapear responsabilidades éticas (data steward, cientista de dados) em projetos, incluindo auditorias regulares.",
                            "atomicExpansion": {
                              "steps": [
                                {
                                  "stepNumber": 1,
                                  "title": "Identificar papéis chave em equipes de dados",
                                  "subSteps": [
                                    "Listar papéis comuns como data steward, data scientist, data engineer e data analyst.",
                                    "Pesquisar definições padrão de cada papel em fontes confiáveis como DAMA-DMBOK.",
                                    "Analisar o contexto do projeto específico para papéis adicionais necessários.",
                                    "Criar uma tabela inicial com papéis e descrições gerais.",
                                    "Consultar stakeholders para validar a lista de papéis."
                                  ],
                                  "verification": "Tabela completa com pelo menos 4 papéis identificados e validados por stakeholders.",
                                  "estimatedTime": "2 horas",
                                  "materials": "Documentação DAMA-DMBOK, planilha Google Sheets ou Excel, acesso a stakeholders.",
                                  "tips": "Priorize papéis com impacto ético alto, como aqueles que lidam com coleta e processamento de dados.",
                                  "learningObjective": "Compreender e listar papéis fundamentais em equipes de dados com foco ético.",
                                  "commonMistakes": "Ignorar papéis de suporte como compliance officer; assumir papéis genéricos sem contexto do projeto."
                                },
                                {
                                  "stepNumber": 2,
                                  "title": "Mapear responsabilidades éticas para cada papel",
                                  "subSteps": [
                                    "Para cada papel, listar responsabilidades éticas como privacidade, viés e transparência.",
                                    "Atribuir tarefas específicas, ex: data steward garante governança de dados.",
                                    "Referenciar frameworks éticos como GDPR ou Principles for Ethical AI.",
                                    "Garantir que responsabilidades sejam claras, mensuráveis e não sobrepostas.",
                                    "Revisar mapeamento para cobrir ciclo completo de vida dos dados (coleta a descarte)."
                                  ],
                                  "verification": "Matriz RACI (Responsible, Accountable, Consulted, Informed) preenchida para cada responsabilidade ética.",
                                  "estimatedTime": "3 horas",
                                  "materials": "Matriz RACI template, frameworks éticos (GDPR PDF), software de diagramação como Lucidchart.",
                                  "tips": "Use verbos de ação (ex: 'auditar', 'revisar') para tornar responsabilidades acionáveis.",
                                  "learningObjective": "Atribuir responsabilidades éticas específicas e claras a cada papel da equipe.",
                                  "commonMistakes": "Sobrecarregar um papel com múltiplas responsabilidades; omitir ética em fases downstream como deployment."
                                },
                                {
                                  "stepNumber": 3,
                                  "title": "Incorporar auditorias regulares no mapeamento",
                                  "subSteps": [
                                    "Definir frequência de auditorias (mensal, trimestral) para cada papel.",
                                    "Especificar escopo: verificação de conformidade ética, relatórios de viés, logs de acesso.",
                                    "Atribuir responsáveis por conduzir e reportar auditorias.",
                                    "Criar templates de checklist para auditorias padronizadas.",
                                    "Integrar auditorias ao fluxo de trabalho da equipe via ferramentas como Jira."
                                  ],
                                  "verification": "Calendário de auditorias definido com templates e responsáveis atribuídos.",
                                  "estimatedTime": "2 horas",
                                  "materials": "Templates de checklist (Google Docs), ferramenta de gerenciamento de projetos (Jira/Trello), calendário compartilhado.",
                                  "tips": "Comece com auditorias leves para ganhar adesão da equipe antes de escalar.",
                                  "learningObjective": "Estabelecer mecanismos de auditoria contínua para accountability ética.",
                                  "commonMistakes": "Definir auditorias muito frequentes sem recursos; não incluir métricas quantificáveis."
                                },
                                {
                                  "stepNumber": 4,
                                  "title": "Documentar, validar e comunicar o mapeamento",
                                  "subSteps": [
                                    "Compilar tudo em um documento único (ex: PDF ou wiki).",
                                    "Realizar revisão com equipe e stakeholders para feedback.",
                                    "Incorporar ajustes baseados em feedback e testar em um mini-projeto.",
                                    "Comunicar via reunião e disponibilizar em repositório acessível.",
                                    "Planejar revisões anuais do mapeamento."
                                  ],
                                  "verification": "Documento final aprovado, com atas de reunião e repositório atualizado.",
                                  "estimatedTime": "1.5 horas",
                                  "materials": "Ferramenta de documentação (Confluence/Notion), plataforma de vídeo para reunião (Zoom).",
                                  "tips": "Use visualizações como organogramas para facilitar compreensão.",
                                  "learningObjective": "Finalizar e disseminar um mapeamento robusto e acionável.",
                                  "commonMistakes": "Não validar com stakeholders reais; falhar em comunicação clara, levando a confusão."
                                }
                              ],
                              "practicalExample": "Em um projeto de análise preditiva para um hospital, o data steward é responsável por anonimizar dados de pacientes (GDPR), o data scientist audita modelos por viés racial, com auditorias trimestrais reportadas ao comitê ético.",
                              "finalVerifications": [
                                "Todos os papéis chave estão mapeados com responsabilidades éticas claras.",
                                "Matriz RACI cobre ciclo de vida dos dados sem lacunas.",
                                "Calendário de auditorias está integrado e com responsáveis definidos.",
                                "Documento foi validado por pelo menos 3 stakeholders.",
                                "Comunicação foi feita e feedback incorporado.",
                                "Métricas de sucesso para auditorias estão definidas."
                              ],
                              "assessmentCriteria": [
                                "Completude: Cobertura de 100% dos papéis e responsabilidades éticas.",
                                "Clareza: Linguagem acionável e sem ambiguidades (nota 1-5).",
                                "Alinhamento ético: Referência a frameworks como GDPR (sim/não).",
                                "Praticidade: Auditorias viáveis com tempos e recursos realistas.",
                                "Integração: Conexão clara com fluxo de projeto (nota 1-5).",
                                "Inovação: Inclusão de práticas avançadas como automação de auditorias."
                              ],
                              "crossCurricularConnections": [
                                "Gestão de Projetos: Uso de RACI e Jira para planejamento.",
                                "Direito e Compliance: Aplicação de GDPR e leis de proteção de dados.",
                                "Liderança e Comunicação: Validação e disseminação com stakeholders.",
                                "Matemática Computacional: Auditoria de viés em modelos de ML."
                              ],
                              "realWorldApplication": "Em empresas como Google ou bancos como Itaú, mapeamentos assim garantem conformidade regulatória, evitam multas (ex: €20M GDPR) e constroem confiança em produtos de IA, com data stewards liderando governança em times ágeis."
                            },
                            "estimatedTime": "1 hora",
                            "difficulty": "beginner",
                            "status": "not_started",
                            "prerequisites": []
                          },
                          {
                            "id": "10.1.1.6.4.2",
                            "name": "Documentar impactos éticos de modelos",
                            "description": "Criar matrizes de risco ético para avaliar consequências sociais de aplicações de dados, como discriminação ou privacidade.",
                            "atomicExpansion": {
                              "steps": [
                                {
                                  "stepNumber": 1,
                                  "title": "Identificar riscos éticos potenciais do modelo",
                                  "subSteps": [
                                    "Revise a descrição do modelo de dados e suas aplicações pretendidas.",
                                    "Liste riscos comuns como discriminação algorítmica, violações de privacidade, viés em dados de treinamento e impactos sociais indesejados.",
                                    "Consulte frameworks éticos padrão, como os princípios da AI Ethics Guidelines da UE ou NIST AI Risk Management Framework.",
                                    "Brainstorm impactos em grupos vulneráveis (ex.: minorias étnicas, gênero).",
                                    "Documente pelo menos 5-10 riscos iniciais em uma lista preliminar."
                                  ],
                                  "verification": "Lista preliminar de riscos com pelo menos 5 itens descritos, incluindo exemplos específicos do modelo.",
                                  "estimatedTime": "45 minutos",
                                  "materials": "Documentação do modelo, frameworks éticos online (ex.: PDF do NIST), caderno ou Google Docs.",
                                  "tips": "Use perguntas guiadas: 'Quem pode ser prejudicado? Como os dados podem perpetuar desigualdades?'",
                                  "learningObjective": "Compreender e catalogar riscos éticos relevantes para aplicações de dados.",
                                  "commonMistakes": "Ignorar riscos indiretos ou focar apenas em aspectos técnicos, negligenciando sociais."
                                },
                                {
                                  "stepNumber": 2,
                                  "title": "Mapear stakeholders afetados",
                                  "subSteps": [
                                    "Identifique stakeholders primários (usuários, desenvolvedores, sociedade) e secundários (reguladores, comunidades impactadas).",
                                    "Para cada risco da lista anterior, associe stakeholders específicos afetados.",
                                    "Avalie perspectivas de cada stakeholder usando empatia (ex.: 'Como uma mulher negra se sentiria com viés no algoritmo?').",
                                    "Crie uma tabela simples: Colunas para Stakeholder, Risco Associado e Impacto Descrito.",
                                    "Priorize stakeholders de alto impacto."
                                  ],
                                  "verification": "Tabela de stakeholders com pelo menos 4-6 entradas, cada uma com descrição de impacto.",
                                  "estimatedTime": "30 minutos",
                                  "materials": "Planilha Google Sheets ou Excel, lista de riscos do Step 1.",
                                  "tips": "Inclua stakeholders inesperados, como concorrentes ou meio ambiente, para visão holística.",
                                  "learningObjective": "Mapear impactos éticos em termos de partes interessadas reais.",
                                  "commonMistakes": "Limitar a stakeholders internos, esquecendo impactos amplos na sociedade."
                                },
                                {
                                  "stepNumber": 3,
                                  "title": "Construir a matriz de risco ético",
                                  "subSteps": [
                                    "Crie uma matriz 2D: Linhas para riscos identificados, colunas para Probabilidade (Baixa/Média/Alta), Severidade (Baixa/Média/Alta) e Score (Prob x Sev).",
                                    "Preencha cada célula com evidências qualitativas ou quantitativas do modelo.",
                                    "Inclua coluna para 'Evidências de Viés' (ex.: métricas de fairness como disparate impact).",
                                    "Calcule scores totais e classifique riscos em vermelho (alto), amarelo (médio), verde (baixo).",
                                    "Adicione coluna para mitigações iniciais sugeridas."
                                  ],
                                  "verification": "Matriz completa com pelo menos 5 riscos, scores calculados e coloração visual.",
                                  "estimatedTime": "60 minutos",
                                  "materials": "Excel/Google Sheets com template de matriz de risco, calculadora ou fórmulas de planilha.",
                                  "tips": "Use escala numérica (1-5) para probabilidade e severidade para facilitar cálculos (ex.: Score = P * S).",
                                  "learningObjective": "Estruturar riscos éticos em uma matriz quantificável e visual.",
                                  "commonMistakes": "Subjetividade excessiva sem evidências; evite pontuações arbitrárias."
                                },
                                {
                                  "stepNumber": 4,
                                  "title": "Avaliar, priorizar e documentar recomendações",
                                  "subSteps": [
                                    "Analise a matriz para priorizar top 3 riscos altos.",
                                    "Para cada risco prioritário, proponha 2-3 mitigações acionáveis (ex.: auditoria de dados, treinamento fairness-aware).",
                                    "Escreva um relatório resumido: Introdução, Matriz, Prioridades, Recomendações e Assinatura.",
                                    "Inclua referências a leis relevantes (ex.: GDPR para privacidade).",
                                    "Revise por completude e clareza."
                                  ],
                                  "verification": "Relatório final de 1-2 páginas com matriz incorporada, recomendações e referências.",
                                  "estimatedTime": "45 minutos",
                                  "materials": "Documento Word/Google Docs, matriz do Step 3, fontes legais/éticas online.",
                                  "tips": "Torne recomendações SMART (Specific, Measurable, Achievable, Relevant, Time-bound).",
                                  "learningObjective": "Transformar análise em documentação acionável e responsável.",
                                  "commonMistakes": "Fazer recomendações vagas; sempre vincule a métricas mensuráveis."
                                }
                              ],
                              "practicalExample": "Para um modelo de ML de recrutamento usando dados de LinkedIn: Risco de discriminação de gênero (alta probabilidade/alta severidade devido a viés histórico em perfis); matriz mostra score 25/25, recomendação: aplicar reamostragem balanceada de dados e teste de fairness.",
                              "finalVerifications": [
                                "Matriz de risco preenchida com todos os riscos identificados e scores calculados.",
                                "Todos os stakeholders mapeados com impactos descritos.",
                                "Recomendações específicas e mensuráveis para top riscos.",
                                "Relatório completo com introdução, análise e referências.",
                                "Verificação de ausência de vieses na própria documentação.",
                                "Autoavaliação: matriz cobre pelo menos 80% dos riscos potenciais."
                              ],
                              "assessmentCriteria": [
                                "Completude da identificação de riscos (cobertura ampla e relevante: 25%)",
                                "Precisão e evidência na matriz de risco (scores justificados: 25%)",
                                "Qualidade do mapeamento de stakeholders e impactos (empatia demonstrada: 20%)",
                                "Clareza e profissionalismo da documentação final (estrutura e linguagem: 15%)",
                                "Relevância e ação das recomendações/mitigações (praticidade: 15%)"
                              ],
                              "crossCurricularConnections": [
                                "Filosofia: Análise ética utilitarista vs. deontológica em riscos.",
                                "Direito: Aplicação de regulamentações como LGPD/GDPR em privacidade de dados.",
                                "Sociologia: Estudo de desigualdades sociais amplificadas por algoritmos.",
                                "Estatística: Métricas de viés e fairness em análise de dados."
                              ],
                              "realWorldApplication": "Em empresas como Google ou bancos, equipes de ética de IA usam matrizes semelhantes para auditar modelos antes do deploy, evitando multas milionárias por discriminação (ex.: caso Amazon hiring tool) e construindo confiança pública."
                            },
                            "estimatedTime": "2 horas",
                            "difficulty": "intermediate",
                            "status": "not_started",
                            "prerequisites": []
                          }
                        ]
                      }
                    ]
                  }
                ]
              },
              {
                "id": "10.1.2",
                "name": "Conceitos Fundamentais de Modelagem e Aprendizado",
                "description": "Conceitos de modelagem de problemas, aprendizado e ambiente independente e identicamente distribuído (IID).",
                "totalSkills": 56,
                "atomicTopics": [
                  {
                    "id": "10.1.2.1",
                    "name": "Modelagem de Problemas em Ciência de Dados",
                    "description": "Conceitos para formular problemas reais como tarefas de modelagem em ciência de dados.",
                    "individualConcepts": [
                      {
                        "id": "13.1.2.1.1",
                        "name": "Definições Fundamentais em Ciência de Dados",
                        "description": "Compreensão dos conceitos básicos de dados, informação, conhecimento e assunções como i.i.d. para formular problemas reais.",
                        "specificSkills": [
                          {
                            "id": "13.1.2.1.1.1",
                            "name": "Diferenciar dados, informação e conhecimento",
                            "description": "Explicar as diferenças entre dados brutos, informação processada e conhecimento acionável, com exemplos de como transformá-los em contextos de modelagem de problemas reais.",
                            "atomicExpansion": {
                              "steps": [
                                {
                                  "stepNumber": 1,
                                  "title": "Entender o conceito de Dados Brutos",
                                  "subSteps": [
                                    "Defina dados brutos como fatos isolados sem contexto ou processamento, como números, símbolos ou observações cruas.",
                                    "Colete exemplos simples de dados brutos do dia a dia, como '25', 'azul' ou '10/05/2023'.",
                                    "Discuta características: objetivos, sem significado inerente, volume alto e desestruturados.",
                                    "Crie uma lista de 5 fontes comuns de dados brutos em ciência de dados (sensores, logs, formulários).",
                                    "Registre em um diagrama simples: Dados → (sem setas ainda)."
                                  ],
                                  "verification": "Conseguiu listar 5 exemplos e 3 características sem consultar referências?",
                                  "estimatedTime": "20 minutos",
                                  "materials": [
                                    "Papel e caneta ou editor de texto",
                                    "Acesso a exemplos online como datasets Kaggle (opcional)"
                                  ],
                                  "tips": "Pense em dados como 'ingredientes crus' antes de cozinhar.",
                                  "learningObjective": "Identificar e exemplificar dados brutos corretamente.",
                                  "commonMistakes": [
                                    "Confundir dados com informação já processada",
                                    "Ignorar que dados podem ser qualitativos ou quantitativos"
                                  ]
                                },
                                {
                                  "stepNumber": 2,
                                  "title": "Entender o conceito de Informação Processada",
                                  "subSteps": [
                                    "Defina informação como dados processados com contexto, análise ou organização, ganhando relevância.",
                                    "Transforme exemplos de dados em informação: '25 vendas em 10/05/2023' → 'Média de 5 vendas/dia na semana'.",
                                    "Liste métodos de processamento: agregação, filtragem, categorização e visualização básica.",
                                    "Crie um fluxograma: Dados Brutos → Processamento → Informação.",
                                    "Identifique 4 perguntas que a informação responde (o quê, quando, onde, quanto)."
                                  ],
                                  "verification": "Pode converter 3 exemplos de dados em informação com justificativa?",
                                  "estimatedTime": "25 minutos",
                                  "materials": [
                                    "Planilha Excel ou Google Sheets",
                                    "Exemplos de datasets simples"
                                  ],
                                  "tips": "Informação responde 'o que isso significa?' aos dados.",
                                  "learningObjective": "Explicar como dados se tornam informação através de processamento.",
                                  "commonMistakes": [
                                    "Achar que qualquer dado é informação",
                                    "Não adicionar contexto suficiente"
                                  ]
                                },
                                {
                                  "stepNumber": 3,
                                  "title": "Entender o conceito de Conhecimento Acionável",
                                  "subSteps": [
                                    "Defina conhecimento como informação interpretada, contextualizada com experiência, levando a decisões.",
                                    "Exemplifique: Informação 'vendas baixas em produto X' → Conhecimento 'reduzir estoque devido a demanda sazonal'.",
                                    "Discuta elementos: experiência, regras, inferência e aplicação prática.",
                                    "Atualize o fluxograma: Dados → Informação → Interpretação → Conhecimento.",
                                    "Brainstorm 3 ações baseadas em conhecimento em cenários reais."
                                  ],
                                  "verification": "Conseguiu ligar informação a uma decisão acionável em pelo menos 2 casos?",
                                  "estimatedTime": "25 minutos",
                                  "materials": [
                                    "Diagrama anterior",
                                    "Casos de estudo curtos de negócios ou ciência"
                                  ],
                                  "tips": "Conhecimento é 'o que fazer agora?' baseado na informação.",
                                  "learningObjective": "Diferenciar conhecimento como aplicação prática da informação.",
                                  "commonMistakes": [
                                    "Parar na informação sem ação",
                                    "Confundir opinião pessoal com conhecimento baseado em evidências"
                                  ]
                                },
                                {
                                  "stepNumber": 4,
                                  "title": "Diferenciar e Aplicar em Modelagem de Problemas",
                                  "subSteps": [
                                    "Compare os três em uma tabela: Definição, Exemplos, Transformação, Uso em modelagem.",
                                    "Aplique em um problema real: dados de sensores de temperatura → info de médias → conhecimento de previsão climática.",
                                    "Crie um mapa conceitual integrando todos os conceitos.",
                                    "Debata cenários onde falhas na transformação impactam modelagem (ex: viés em dados).",
                                    "Teste com quiz interno: classifique 5 itens como dado/info/conhecimento."
                                  ],
                                  "verification": "Tabela e mapa conceitual completos e corretos?",
                                  "estimatedTime": "30 minutos",
                                  "materials": [
                                    "Ferramenta de diagrama como Draw.io ou papel",
                                    "Exemplo de dataset real"
                                  ],
                                  "tips": "Use pirâmide DIKW: Dados base, Informação meio, Conhecimento topo.",
                                  "learningObjective": "Integrar conceitos em contextos de modelagem de problemas em ciência de dados.",
                                  "commonMistakes": [
                                    "Misturar níveis na hierarquia",
                                    "Ignorar contexto de modelagem"
                                  ]
                                }
                              ],
                              "practicalExample": "Em uma loja online: Dados = lista de cliques por usuário (ex: usuário123 clicou 5x em tênis). Informação = taxa de conversão de 20% para tênis vermelhos. Conhecimento = priorizar estoque de tênis vermelhos na Black Friday para maximizar vendas.",
                              "finalVerifications": [
                                "Pode definir cada termo com precisão em suas próprias palavras?",
                                "Consegue transformar dados em informação e depois em conhecimento com exemplos?",
                                "Identifica corretamente itens como dado/info/conhecimento em um quiz de 10 itens?",
                                "Explica o impacto na modelagem de problemas se confundir os níveis?",
                                "Cria um fluxograma completo da pirâmide DIKW?",
                                "Aplica os conceitos a um problema novo de ciência de dados?"
                              ],
                              "assessmentCriteria": [
                                "Precisão nas definições (80% acerto mínimo)",
                                "Qualidade e relevância dos exemplos fornecidos",
                                "Capacidade de demonstrar transformações sequenciais",
                                "Profundidade na aplicação a modelagem de problemas",
                                "Clareza no fluxograma ou mapa conceitual",
                                "Identificação correta de erros comuns em cenários"
                              ],
                              "crossCurricularConnections": [
                                "Filosofia: Epistemologia e níveis de saber (conhecimento como crença verdadeira justificada)",
                                "Informática: Processamento de dados em bancos de dados e ETL",
                                "Negócios: Análise SWOT e tomada de decisões baseadas em dados",
                                "Estatística: Agregação e inferência a partir de amostras",
                                "Psicologia: Processamento cognitivo de informação em memória"
                              ],
                              "realWorldApplication": "Em empresas como Google ou Amazon, analistas usam essa distinção para transformar logs de usuário (dados) em insights de comportamento (informação) e estratégias de recomendação personalizada (conhecimento), otimizando receitas em bilhões."
                            },
                            "estimatedTime": "1 hora",
                            "difficulty": "beginner",
                            "status": "not_started",
                            "prerequisites": []
                          },
                          {
                            "id": "13.1.2.1.1.2",
                            "name": "Compreender assunção i.i.d. (Independente e Identicamente Distribuído)",
                            "description": "Descrever o que significa dados i.i.d. e sua importância na modelagem estatística e de aprendizado de máquina para problemas reais.",
                            "atomicExpansion": {
                              "steps": [
                                {
                                  "stepNumber": 1,
                                  "title": "Entender os Conceitos Básicos de Distribuição de Probabilidade",
                                  "subSteps": [
                                    "Revise a definição de uma variável aleatória e sua distribuição de probabilidade.",
                                    "Estude exemplos de distribuições comuns como normal, binomial e uniforme.",
                                    "Identifique como amostras de uma distribuição compartilham as mesmas propriedades estatísticas.",
                                    "Pratique gerando amostras simuladas para visualizar distribuições.",
                                    "Compare distribuições diferentes para contrastar com 'identicamente distribuídas'."
                                  ],
                                  "verification": "Gere um gráfico de histograma de 1000 amostras de uma distribuição normal e confirme que segue a forma esperada.",
                                  "estimatedTime": "45 minutos",
                                  "materials": [
                                    "Notebook Jupyter ou Python com NumPy/Matplotlib",
                                    "Vídeo introdutório sobre distribuições probabilísticas (Khan Academy)"
                                  ],
                                  "tips": "Use funções como np.random.normal() para simulações rápidas e visualize imediatamente.",
                                  "learningObjective": "Compreender o que significa amostras virem da mesma distribuição de probabilidade.",
                                  "commonMistakes": [
                                    "Confundir distribuição com valor médio único; lembre-se que é sobre a lei de probabilidade inteira."
                                  ]
                                },
                                {
                                  "stepNumber": 2,
                                  "title": "Explorar a Independência entre Observações",
                                  "subSteps": [
                                    "Defina independência estatística: P(X,Y) = P(X)P(Y).",
                                    "Diferencie independência de não-correlacionados com exemplos contrários.",
                                    "Simule dados independentes vs. dependentes (ex: série temporal).",
                                    "Calcule covariância zero para pares independentes.",
                                    "Teste independência com scatter plots e testes estatísticos simples."
                                  ],
                                  "verification": "Crie dois conjuntos de dados: um independente (lançamentos de moedas) e um dependente (incrementos de temperatura); plote e compare.",
                                  "estimatedTime": "40 minutos",
                                  "materials": [
                                    "Python com Pandas e Seaborn para plots",
                                    "Artigo sobre independência condicional"
                                  ],
                                  "tips": "Sempre verifique visualmente antes de testes formais para intuição.",
                                  "learningObjective": "Reconhecer quando observações não influenciam umas às outras.",
                                  "commonMistakes": [
                                    "Assumir que ausência de correlação linear implica independência total."
                                  ]
                                },
                                {
                                  "stepNumber": 3,
                                  "title": "Combinar em Assunção i.i.d.: Independente e Identicamente Distribuído",
                                  "subSteps": [
                                    "Integre os conceitos: cada observação é independente E da mesma distribuição.",
                                    "Escreva a notação formal: X1, X2, ..., Xn ~ i.i.d. P.",
                                    "Discuta propriedades derivadas como Lei dos Grandes Números e Teorema Central do Limite.",
                                    "Simule conjuntos i.i.d. e verifique médias amostrais convergindo.",
                                    "Identifique cenários onde i.i.d. não se aplica (ex: dados hierárquicos)."
                                  ],
                                  "verification": "Gere 1000 amostras i.i.d. e compute a média; repita 100 vezes e veja a distribuição das médias.",
                                  "estimatedTime": "50 minutos",
                                  "materials": [
                                    "Código Python para simulações i.i.d.",
                                    "Livro 'Elements of Statistical Learning' capítulo 2 (seção online)"
                                  ],
                                  "tips": "Use loops para múltiplas simulações e plote histogramas de médias para ver normalidade.",
                                  "learningObjective": "Definir precisamente i.i.d. e suas implicações matemáticas.",
                                  "commonMistakes": [
                                    "Ignorar que i.i.d. requer AMBOS independência e identidade; teste separadamente."
                                  ]
                                },
                                {
                                  "stepNumber": 4,
                                  "title": "Analisar Importância e Violações em Modelagem Estatística e ML",
                                  "subSteps": [
                                    "Explique por que i.i.d. é crucial para estimação máxima de verossimilhança e validação cruzada.",
                                    "Discuta impactos de violações: bias em modelos, falha em generalização.",
                                    "Examine exemplos reais: dados de sensores vs. surveys aleatórias.",
                                    "Aprenda técnicas para lidar com não-i.i.d.: bootstrapping, modelos hierárquicos.",
                                    "Avalie um dataset real se é aproximadamente i.i.d."
                                  ],
                                  "verification": "Pegue um dataset (ex: Iris) e argumente se assume i.i.d.; sugira testes como Durbin-Watson para independência.",
                                  "estimatedTime": "55 minutos",
                                  "materials": [
                                    "Dataset Iris do sklearn",
                                    "Documentação scikit-learn sobre assunções"
                                  ],
                                  "tips": "Comece com datasets simples antes de complexos para construir confiança.",
                                  "learningObjective": "Aplicar i.i.d. a contextos de ML e identificar quando relaxar a assunção.",
                                  "commonMistakes": [
                                    "Superestimar robustez de modelos a violações i.i.d.; sempre diagnostique dados."
                                  ]
                                }
                              ],
                              "practicalExample": "Em um A/B test de um site, cliques de usuários diferentes são assumidos i.i.d.: independentes (um usuário não afeta outro) e identicamente distribuídos (todos sob mesmo design inicial). Simule 1000 usuários com np.random.binomial para taxas de clique e compare médias.",
                              "finalVerifications": [
                                "Explique i.i.d. em suas palavras sem jargão.",
                                "Identifique se uma sequência de temperaturas diárias é i.i.d. e por quê.",
                                "Simule e plote evidência de convergência de médias sob i.i.d.",
                                "Descreva uma violação comum em dados de redes sociais.",
                                "Liste 3 teoremas que dependem de i.i.d.",
                                "Avalie um dataset pequeno como i.i.d."
                              ],
                              "assessmentCriteria": [
                                "Precisão na definição de independência e distribuição idêntica (80%+ correto).",
                                "Capacidade de simular e interpretar dados i.i.d. vs. não-i.i.d.",
                                "Explicação clara da importância em ML (ex: por que cross-validation funciona).",
                                "Identificação correta de violações em exemplos reais.",
                                "Uso apropriado de ferramentas computacionais para verificação.",
                                "Conexão com teoremas fundamentais como LLN e CLT."
                              ],
                              "crossCurricularConnections": [
                                "Probabilidade e Estatística: Base para inferência bayesiana.",
                                "Aprendizado de Máquina: Assunção em regressão linear e árvores.",
                                "Física: Experimentos repetidos com condições idênticas.",
                                "Economia: Modelos de surveys e testes de hipóteses."
                              ],
                              "realWorldApplication": "Em machine learning, a assunção i.i.d. permite treinar modelos em dados de treinamento e avaliá-los em testes independentes, essencial para apps como recomendadores da Netflix (usuários como amostras i.i.d.) ou detecção de fraudes em bancos."
                            },
                            "estimatedTime": "1 hora",
                            "difficulty": "beginner",
                            "status": "not_started",
                            "prerequisites": []
                          },
                          {
                            "id": "13.1.2.1.1.3",
                            "name": "Identificar aplicações da Ciência de Dados",
                            "description": "Listar e exemplificar aplicações reais da Ciência de Dados que envolvem modelagem de problemas, relacionando com definições fundamentais.",
                            "atomicExpansion": {
                              "steps": [
                                {
                                  "stepNumber": 1,
                                  "title": "Revisar Definições Fundamentais da Ciência de Dados",
                                  "subSteps": [
                                    "Defina Ciência de Dados como a interseção de estatística, programação e conhecimento de domínio para extrair insights de dados.",
                                    "Explique modelagem de problemas como o processo de representar problemas reais com dados e algoritmos.",
                                    "Liste conceitos chave: dados estruturados/não-estruturados, aprendizado supervisionado/não-supervisionado, overfitting/underfitting.",
                                    "Identifique componentes principais: coleta de dados, limpeza, feature engineering e avaliação de modelos.",
                                    "Crie um mapa mental conectando definições a aplicações potenciais."
                                  ],
                                  "verification": "Crie um resumo de 1 página listando 5 definições fundamentais com exemplos breves; revise se cobre modelagem.",
                                  "estimatedTime": "30 minutos",
                                  "materials": [
                                    "Artigo 'What is Data Science?' do Towards Data Science",
                                    "Vídeo introdutório no YouTube sobre modelagem em DS",
                                    "Notebook Jupyter vazio para anotações"
                                  ],
                                  "tips": "Use analogias cotidianas, como prever o tempo, para fixar conceitos.",
                                  "learningObjective": "Compreender e articular definições fundamentais que sustentam aplicações reais de DS.",
                                  "commonMistakes": [
                                    "Confundir DS com análise de dados simples; ignorar o papel da modelagem preditiva."
                                  ]
                                },
                                {
                                  "stepNumber": 2,
                                  "title": "Identificar Domínios e Aplicações Comuns da Ciência de Dados",
                                  "subSteps": [
                                    "Liste 5 domínios principais: saúde, finanças, e-commerce, transporte e marketing.",
                                    "Para cada domínio, pesquise 1-2 aplicações reais envolvendo modelagem (ex: previsão de churn em e-commerce).",
                                    "Classifique aplicações por tipo de modelagem: regressão, classificação, clustering.",
                                    "Registre fontes confiáveis para cada aplicação (ex: case studies da Kaggle).",
                                    "Priorize aplicações que envolvam problemas reais modelados com dados."
                                  ],
                                  "verification": "Compile uma tabela com domínios, aplicações e tipos de modelagem; verifique se há pelo menos 8 entradas únicas.",
                                  "estimatedTime": "45 minutos",
                                  "materials": [
                                    "Site Kaggle Datasets e Competitions",
                                    "Relatórios da McKinsey sobre DS em indústrias",
                                    "Google para 'data science applications in [domain]'"
                                  ],
                                  "tips": "Foque em aplicações recentes (pós-2020) para relevância.",
                                  "learningObjective": "Mapear domínios onde DS é aplicada, destacando modelagem de problemas.",
                                  "commonMistakes": [
                                    "Listar aplicações genéricas sem menção a modelagem; depender só de memória sem pesquisa."
                                  ]
                                },
                                {
                                  "stepNumber": 3,
                                  "title": "Exemplificar Aplicações Reais com Detalhes de Modelagem",
                                  "subSteps": [
                                    "Selecione 3 aplicações de domínios diferentes e descreva o problema real.",
                                    "Detalhe o processo de modelagem: coleta de dados, pré-processamento, escolha de algoritmo.",
                                    "Inclua métricas de sucesso (ex: accuracy > 85% em detecção de fraudes).",
                                    "Crie diagramas simples de fluxo de dados para cada exemplo.",
                                    "Adicione desafios reais enfrentados (ex: dados desbalanceados)."
                                  ],
                                  "verification": "Apresente 3 exemplos completos com fluxogramas; um par revise e confirme viabilidade.",
                                  "estimatedTime": "60 minutos",
                                  "materials": [
                                    "Ferramenta Draw.io para diagramas",
                                    "Case studies da Netflix ou Uber",
                                    "Datasets públicos no UCI ML Repository"
                                  ],
                                  "tips": "Use storytelling: 'problema -> dados -> modelo -> impacto'.",
                                  "learningObjective": "Desenvolver exemplos concretos que ilustrem modelagem em contextos reais.",
                                  "commonMistakes": [
                                    "Descrições vagas sem passos de modelagem; omitir métricas ou desafios."
                                  ]
                                },
                                {
                                  "stepNumber": 4,
                                  "title": "Relacionar Aplicações com Definições Fundamentais",
                                  "subSteps": [
                                    "Para cada exemplo, ligue explicitamente a 2-3 definições do Step 1 (ex: regressão como aprendizado supervisionado).",
                                    "Analise como a modelagem resolve o problema relacionando com conceitos como feature engineering.",
                                    "Discuta limitações e ética (ex: bias em modelos de saúde).",
                                    "Sintetize em uma matriz: aplicação x definição x benefício.",
                                    "Reflita sobre uma aplicação pessoal ou hipotética."
                                  ],
                                  "verification": "Gere uma matriz de relações; autoavalie cobertura de pelo menos 80% das definições iniciais.",
                                  "estimatedTime": "45 minutos",
                                  "materials": [
                                    "Planilha Google Sheets para matriz",
                                    "Artigo sobre ética em DS da Harvard Business Review"
                                  ],
                                  "tips": "Use setas ou links visuais na matriz para clareza.",
                                  "learningObjective": "Conectar aplicações práticas às bases teóricas de DS.",
                                  "commonMistakes": [
                                    "Relações superficiais; ignorar aspectos éticos ou limitações."
                                  ]
                                }
                              ],
                              "practicalExample": "Em e-commerce como a Amazon, modelar churn de clientes: coletar dados de compras/sessões, feature engineering (recência/frequência), treinar modelo de classificação (Random Forest), prever saídas e reduzir com campanhas personalizadas, alcançando 20% menos churn.",
                              "finalVerifications": [
                                "Lista pelo menos 5 aplicações reais de DS em domínios variados.",
                                "Descreve modelagem em cada aplicação com pelo menos 3 passos.",
                                "Relaciona cada aplicação a 2 definições fundamentais.",
                                "Inclui métricas ou impactos reais em exemplos.",
                                "Identifica 1 desafio ou limitação por aplicação.",
                                "Cria visualizações (tabela/matriz/diagrama) para síntese."
                              ],
                              "assessmentCriteria": [
                                "Precisão e relevância das aplicações listadas (baseadas em fontes reais).",
                                "Profundidade na descrição de modelagem de problemas.",
                                "Qualidade das relações com definições fundamentais.",
                                "Criatividade e concretude nos exemplos práticos.",
                                "Cobertura de domínios diversos e inclusão de ética/desafios.",
                                "Clareza e organização das visualizações/sínteses."
                              ],
                              "crossCurricularConnections": [
                                "Estatística: Uso de testes de hipótese em validação de modelos.",
                                "Programação: Implementação em Python/R para protótipos de modelagem.",
                                "Negócios: Análise de ROI em aplicações de DS.",
                                "Ética e Sociedade: Discussão de viés e privacidade em dados.",
                                "Matemática: Álgebra linear em algoritmos de ML."
                              ],
                              "realWorldApplication": "Na saúde pública, como durante a COVID-19, modelar propagação de vírus com dados epidemiológicos (SIR models), prevendo picos para otimizar alocação de vacinas e reduzindo mortalidade em 15% em regiões aplicadoras."
                            },
                            "estimatedTime": "0.5 hora",
                            "difficulty": "beginner",
                            "status": "not_started",
                            "prerequisites": []
                          }
                        ]
                      },
                      {
                        "id": "13.1.2.1.2",
                        "name": "Etapas do Processo de Modelagem em Ciência de Dados",
                        "description": "Descrição das principais etapas para transformar problemas reais em tarefas de modelagem, incluindo coleta, análise e avaliação.",
                        "specificSkills": [
                          {
                            "id": "13.1.2.1.2.1",
                            "name": "Descrever coleta, integração e armazenamento de dados",
                            "description": "Explicar como coletar dados de fontes reais, integrá-los e armazená-los de forma eficiente para preparar a modelagem de problemas.",
                            "atomicExpansion": {
                              "steps": [
                                {
                                  "stepNumber": 1,
                                  "title": "Planejar a identificação e seleção de fontes de dados",
                                  "subSteps": [
                                    "Analisar o problema de modelagem para definir os tipos de dados necessários (estruturados, não estruturados, etc.).",
                                    "Listar fontes reais potenciais, como APIs, bancos de dados públicos, arquivos CSV ou sensores.",
                                    "Avaliar qualidade, disponibilidade e legalidade das fontes (licenças, privacidade).",
                                    "Definir critérios de amostragem e volume de dados a coletar.",
                                    "Criar um diagrama de fluxo de dados para visualizar o processo."
                                  ],
                                  "verification": "Verificar se o plano escrito inclui lista de fontes, critérios e diagrama completo.",
                                  "estimatedTime": "45 minutos",
                                  "materials": "Papel ou ferramenta de diagramação (ex: Draw.io), acesso à internet para pesquisa de fontes.",
                                  "tips": "Priorize fontes abertas como Kaggle ou APIs públicas para evitar problemas legais iniciais.",
                                  "learningObjective": "Compreender como planejar fontes de dados alinhadas ao problema de modelagem.",
                                  "commonMistakes": "Ignorar questões éticas ou de privacidade; superestimar volume de dados sem necessidade."
                                },
                                {
                                  "stepNumber": 2,
                                  "title": "Coletar dados de fontes reais",
                                  "subSteps": [
                                    "Configurar acesso às fontes selecionadas (chaves de API, downloads de arquivos).",
                                    "Implementar scripts ou ferramentas para extração (ex: requests em Python para APIs).",
                                    "Registrar metadados durante a coleta (data/hora, origem, tamanho).",
                                    "Realizar coletas iniciais em lotes pequenos para testes.",
                                    "Documentar qualquer falha ou limitação encontrada."
                                  ],
                                  "verification": "Conferir se os dados brutos foram salvos em formato legível com metadados anexados.",
                                  "estimatedTime": "1 hora",
                                  "materials": "Python com bibliotecas (pandas, requests), arquivos CSV de exemplo, chaves de API gratuitas.",
                                  "tips": "Use try-except em scripts para lidar com erros de rede automaticamente.",
                                  "learningObjective": "Dominar técnicas práticas de coleta de dados reais de múltiplas fontes.",
                                  "commonMistakes": "Coletar dados excessivos sem limpeza prévia; esquecer de validar formatos iniciais."
                                },
                                {
                                  "stepNumber": 3,
                                  "title": "Integrar os dados coletados",
                                  "subSteps": [
                                    "Carregar dados de diferentes fontes em um ambiente comum (ex: DataFrame no pandas).",
                                    "Identificar chaves comuns para junção (merge ou join).",
                                    "Tratar inconsistências (formatos de data, valores ausentes, duplicatas).",
                                    "Realizar transformações necessárias (normalização, agregação).",
                                    "Validar a integridade do dataset integrado com estatísticas descritivas."
                                  ],
                                  "verification": "Executar queries ou resumos para confirmar ausência de duplicatas e completude.",
                                  "estimatedTime": "1 hora e 30 minutos",
                                  "materials": "Python (pandas, numpy), Jupyter Notebook para visualização.",
                                  "tips": "Sempre use inner join inicialmente para evitar perda de dados desnecessária.",
                                  "learningObjective": "Aprender a unir datasets heterogêneos de forma eficiente e sem perda de informação.",
                                  "commonMistakes": "Não tratar valores nulos antes da integração; usar junções erradas que criam cartesianos."
                                },
                                {
                                  "stepNumber": 4,
                                  "title": "Armazenar os dados de forma eficiente",
                                  "subSteps": [
                                    "Escolher formato de armazenamento (CSV para simples, Parquet para grandes, banco SQL/NoSQL).",
                                    "Implementar salvamento com compressão e indexação.",
                                    "Configurar estrutura otimizada (particionamento por datas ou categorias).",
                                    "Testar consultas de leitura para medir performance.",
                                    "Documentar schema e localização do armazenamento final."
                                  ],
                                  "verification": "Ler dados armazenados e comparar com originais; medir tempo de query.",
                                  "estimatedTime": "45 minutos",
                                  "materials": "SQLite ou PostgreSQL local, bibliotecas pyarrow para Parquet.",
                                  "tips": "Prefira formatos colunares como Parquet para datasets grandes em ciência de dados.",
                                  "learningObjective": "Entender escolhas de armazenamento que preparam para modelagem escalável.",
                                  "commonMistakes": "Armazenar em formatos não otimizados levando a lentidão; esquecer backups."
                                }
                              ],
                              "practicalExample": "Coletar dados de temperaturas via API OpenWeatherMap, integrar com um CSV local de umidade de sensores IoT e armazenar em um banco SQLite para análise de previsão climática em uma cidade específica.",
                              "finalVerifications": [
                                "Todos os dados originais foram integrados sem perda significativa.",
                                "O dataset final possui schema documentado e metadados completos.",
                                "Consultas de teste rodam em menos de 5 segundos.",
                                "Qualidade dos dados validada (sem duplicatas >1%, nulos <5%).",
                                "Processo documentado permite replicação por outra pessoa.",
                                "Conformidade com privacidade (anonimização se aplicável)."
                              ],
                              "assessmentCriteria": [
                                "Precisão na identificação e coleta de fontes relevantes (nota 1-10).",
                                "Eficiência da integração (tempo e qualidade do merge).",
                                "Escolha adequada de armazenamento com justificativa técnica.",
                                "Documentação clara e completa do processo.",
                                "Tratamento robusto de erros comuns (nulos, inconsistências).",
                                "Alinhamento com objetivos de modelagem de problemas."
                              ],
                              "crossCurricularConnections": [
                                "Programação: Uso de Python/pandas para manipulação.",
                                "Estatística: Análise descritiva para validação de qualidade.",
                                "Ética e Cidadania: Considerações de privacidade e dados abertos.",
                                "Tecnologias da Informação: Bancos de dados relacionais e NoSQL."
                              ],
                              "realWorldApplication": "Em empresas de e-commerce, coletar dados de vendas (API), integrar com logs de usuários (bancos internos) e armazenar em data lakes para modelos de recomendação personalizada, otimizando receitas em milhões."
                            },
                            "estimatedTime": "1.5 horas",
                            "difficulty": "intermediate",
                            "status": "not_started",
                            "prerequisites": [
                              "13.1.2.1.1.1"
                            ]
                          },
                          {
                            "id": "13.1.2.1.2.2",
                            "name": "Realizar análise exploratória e visualização de dados",
                            "description": "Aplicar técnicas de análise exploratória e visualização para entender problemas reais e formular modelos adequados.",
                            "atomicExpansion": {
                              "steps": [
                                {
                                  "stepNumber": 1,
                                  "title": "Preparar o ambiente e carregar os dados",
                                  "subSteps": [
                                    "Instalar e importar bibliotecas essenciais: pandas, numpy, matplotlib e seaborn.",
                                    "Carregar o dataset de um arquivo CSV ou fonte similar usando pd.read_csv().",
                                    "Executar df.head(), df.info() e df.describe() para inspeção inicial.",
                                    "Verificar presença de valores ausentes com df.isnull().sum().",
                                    "Salvar uma cópia do dataset original para backup."
                                  ],
                                  "verification": "Dataset carregado corretamente, exibindo shape, tipos de dados e estatísticas descritivas sem erros.",
                                  "estimatedTime": "45 minutos",
                                  "materials": [
                                    "Python 3.x",
                                    "Jupyter Notebook ou Google Colab",
                                    "Dataset exemplo (ex: Titanic.csv)"
                                  ],
                                  "tips": "Use %matplotlib inline no Jupyter para visualizações inline e defina pd.set_option('display.max_columns', None) para ver todas as colunas.",
                                  "learningObjective": "Configurar um ambiente funcional e carregar dados de forma eficiente para análise.",
                                  "commonMistakes": [
                                    "Esquecer de especificar encoding='utf-8' em arquivos com acentos.",
                                    "Não verificar duplicatas iniciais com df.duplicated().sum()."
                                  ]
                                },
                                {
                                  "stepNumber": 2,
                                  "title": "Realizar limpeza e preparação dos dados",
                                  "subSteps": [
                                    "Tratar valores ausentes: preencher com mediana/média ou remover linhas/colunas conforme contexto.",
                                    "Remover duplicatas usando df.drop_duplicates().",
                                    "Converter tipos de dados: de object para datetime ou categorical.",
                                    "Criar novas features derivadas, como bins de idade ou categorias de renda.",
                                    "Normalizar ou escalar variáveis numéricas se necessário."
                                  ],
                                  "verification": "Dataset limpo sem valores ausentes, duplicatas ou tipos incorretos; df.info() mostra dados consistentes.",
                                  "estimatedTime": "1 hora",
                                  "materials": [
                                    "Bibliotecas pandas e numpy",
                                    "Dataset carregado do passo anterior"
                                  ],
                                  "tips": "Priorize imputação baseada no contexto do domínio; use df.fillna() com método='ffill' para séries temporais.",
                                  "learningObjective": "Transformar dados brutos em um formato limpo e pronto para análise exploratória.",
                                  "commonMistakes": [
                                    "Remover dados ausentes indiscriminadamente sem análise de impacto.",
                                    "Ignorar outliers que podem ser válidos no contexto."
                                  ]
                                },
                                {
                                  "stepNumber": 3,
                                  "title": "Executar análise univariada",
                                  "subSteps": [
                                    "Calcular estatísticas descritivas para variáveis numéricas (média, mediana, desvios).",
                                    "Gerar histogramas e boxplots para distribuições com matplotlib/seaborn.",
                                    "Analisar variáveis categóricas com value_counts() e gráficos de barras.",
                                    "Identificar skewness e outliers em distribuições.",
                                    "Documentar padrões observados em um relatório inicial."
                                  ],
                                  "verification": "Gráficos univariados gerados para pelo menos 80% das variáveis principais; insights anotados.",
                                  "estimatedTime": "1 hora e 30 minutos",
                                  "materials": [
                                    "Seaborn e matplotlib",
                                    "Dataset limpo"
                                  ],
                                  "tips": "Use sns.histplot() com kde=True para ver densidade; aplique log-transform em variáveis skewed.",
                                  "learningObjective": "Compreender distribuições individuais de variáveis e detectar anomalias.",
                                  "commonMistakes": [
                                    "Usar escalas inadequadas nos gráficos, ocultando padrões.",
                                    "Não rotular eixos corretamente nos plots."
                                  ]
                                },
                                {
                                  "stepNumber": 4,
                                  "title": "Realizar análise bivariada e multivariada",
                                  "subSteps": [
                                    "Calcular correlações com df.corr() e heatmap de correlação.",
                                    "Criar scatterplots e pairplots para relações entre variáveis numéricas.",
                                    "Usar boxplots e crosstabs para relações numérica-categórica.",
                                    "Explorar interações multivariadas com sns.pairplot() ou facet grids.",
                                    "Identificar correlações fortes e potenciais multicolinearidades."
                                  ],
                                  "verification": "Mapa de calor de correlações e pelo menos 5 visualizações bivariadas interpretadas.",
                                  "estimatedTime": "1 hora e 30 minutos",
                                  "materials": [
                                    "Seaborn para heatmaps e pairplots",
                                    "Dataset preparado"
                                  ],
                                  "tips": "Filtre variáveis com alta correlação (>0.8) para evitar multicolinearidade em modelos futuros; use hue para categóricas.",
                                  "learningObjective": "Detectar relações entre variáveis e padrões de dependência.",
                                  "commonMistakes": [
                                    "Interpretar correlação como causalidade.",
                                    "Sobrecarregar gráficos com muitas variáveis sem filtros."
                                  ]
                                },
                                {
                                  "stepNumber": 5,
                                  "title": "Visualizar resultados e sintetizar insights",
                                  "subSteps": [
                                    "Criar dashboard consolidado com subplots ou seaborn FacetGrid.",
                                    "Documentar 5-10 insights chave, hipóteses e próximos passos.",
                                    "Exportar visualizações como PNG/PDF e relatório em Markdown.",
                                    "Validar insights com testes estatísticos simples (ex: Shapiro para normalidade).",
                                    "Preparar dados para modelagem subsequente."
                                  ],
                                  "verification": "Relatório com insights, gráficos salvos e recomendações claras para modelagem.",
                                  "estimatedTime": "45 minutos",
                                  "materials": [
                                    "Matplotlib para subplots",
                                    "Notebook Jupyter para relatório"
                                  ],
                                  "tips": "Use plt.suptitle() para títulos de dashboard; priorize insights acionáveis.",
                                  "learningObjective": "Interpretar visualizações para formular hipóteses de modelagem.",
                                  "commonMistakes": [
                                    "Focar só em gráficos bonitos sem interpretação.",
                                    "Não conectar insights ao problema de negócio."
                                  ]
                                }
                              ],
                              "practicalExample": "No dataset Titanic (disponível no Kaggle), carregue os dados de passageiros, limpe valores ausentes em 'Age' com mediana por classe, gere histogramas de 'Age' e 'Fare', boxplots de 'Age' por 'Pclass', heatmap de correlações e identifique que classe alta tem maior taxa de sobrevivência, formulando hipótese para modelo preditivo.",
                              "finalVerifications": [
                                "Dataset limpo e explorado com shape, estatísticas e ausência de erros.",
                                "Pelo menos 10 visualizações geradas e salvas.",
                                "Relatório com 5+ insights documentados.",
                                "Correlações chave identificadas (>0.5).",
                                "Hipóteses para modelagem propostas.",
                                "Tempo total respeitado e materiais utilizados corretamente."
                              ],
                              "assessmentCriteria": [
                                "Precisão e completude da limpeza de dados (sem perda desnecessária).",
                                "Qualidade e relevância das visualizações (legíveis, rotuladas).",
                                "Profundidade da interpretação de padrões e outliers.",
                                "Identificação correta de correlações e relações.",
                                "Clareza na síntese de insights acionáveis.",
                                "Conformidade com boas práticas de EDA (reprodutibilidade)."
                              ],
                              "crossCurricularConnections": [
                                "Estatística: Distribuições, testes de normalidade e correlação.",
                                "Programação: Manipulação de dados em Python com pandas.",
                                "Negócios: Extração de insights para decisões estratégicas.",
                                "Matemática: Álgebra linear em heatmaps de correlação.",
                                "Design: Princípios de visualização de dados (Tufte/Gramma)."
                              ],
                              "realWorldApplication": "Em empresas de e-commerce, analistas usam EDA para entender padrões de compra por região e idade, visualizando churn e revenue, permitindo otimizar campanhas de marketing e precificação dinâmica antes de treinar modelos de ML."
                            },
                            "estimatedTime": "1.5 horas",
                            "difficulty": "intermediate",
                            "status": "not_started",
                            "prerequisites": [
                              "13.1.2.1.2.1"
                            ]
                          },
                          {
                            "id": "13.1.2.1.2.3",
                            "name": "Executar limpeza e preparação de dados",
                            "description": "Identificar e tratar problemas comuns em dados reais, como valores ausentes e outliers, para habilitar modelagem precisa.",
                            "atomicExpansion": {
                              "steps": [
                                {
                                  "stepNumber": 1,
                                  "title": "Inspeção Inicial dos Dados",
                                  "subSteps": [
                                    "Carregue o dataset usando pandas (pd.read_csv()).",
                                    "Examine a estrutura com info(), describe() e head().",
                                    "Identifique valores ausentes com isnull().sum().",
                                    "Verifique duplicatas com duplicated().sum().",
                                    "Gere visualizações como histogramas e boxplots para padrões."
                                  ],
                                  "verification": "Relatório de inspeção gerado mostrando estatísticas resumidas, missing values e duplicatas detectadas.",
                                  "estimatedTime": "1 hora",
                                  "materials": [
                                    "Python com pandas, numpy, matplotlib/seaborn",
                                    "Jupyter Notebook",
                                    "Dataset de exemplo (ex: Titanic ou Housing)"
                                  ],
                                  "tips": "Sempre combine estatísticas numéricas com visualizações para insights mais profundos.",
                                  "learningObjective": "Compreender a estrutura dos dados e identificar problemas comuns como missing values, duplicatas e inconsistências.",
                                  "commonMistakes": [
                                    "Ignorar visualizações e confiar apenas em números",
                                    "Não checar tipos de dados (ex: strings como números)",
                                    "Pular verificação de duplicatas"
                                  ]
                                },
                                {
                                  "stepNumber": 2,
                                  "title": "Tratamento de Valores Ausentes",
                                  "subSteps": [
                                    "Analise padrões de missing values com heatmap de correlação.",
                                    "Decida estratégia: imputação média/mediana para numéricos, moda para categóricos ou remoção.",
                                    "Aplique imputação com fillna() ou SimpleImputer do sklearn.",
                                    "Crie coluna indicadora para missing originais se relevante.",
                                    "Valide com sum() pós-tratamento."
                                  ],
                                  "verification": "Zero valores ausentes confirmados por dataset.isnull().sum().sum() == 0.",
                                  "estimatedTime": "1.5 horas",
                                  "materials": [
                                    "pandas",
                                    "sklearn.impute",
                                    "seaborn para heatmaps"
                                  ],
                                  "tips": "Prefira imputação baseada no contexto do domínio em vez de métodos genéricos.",
                                  "learningObjective": "Selecionar e aplicar métodos apropriados para lidar com missing data sem introduzir bias significativo.",
                                  "commonMistakes": [
                                    "Remover linhas aleatoriamente sem análise",
                                    "Imputar com média em dados skew",
                                    "Esquecer de documentar escolhas"
                                  ]
                                },
                                {
                                  "stepNumber": 3,
                                  "title": "Detecção e Tratamento de Outliers",
                                  "subSteps": [
                                    "Use método IQR: Q1 - 1.5*IQR e Q3 + 1.5*IQR para detectar.",
                                    "Visualize com boxplots por feature.",
                                    "Decida ação: remoção (drop), capping (winsorizing) ou transformação log.",
                                    "Aplique com quantile() e clip() ou replace().",
                                    "Reavalie distribuições pós-tratamento."
                                  ],
                                  "verification": "Boxplots mostram distribuição sem outliers extremos; Z-score < 3 para todas as observações.",
                                  "estimatedTime": "1 hora",
                                  "materials": [
                                    "pandas",
                                    "numpy",
                                    "matplotlib"
                                  ],
                                  "tips": "Considere o contexto: outliers podem ser válidos em dados reais.",
                                  "learningObjective": "Identificar e tratar outliers de forma justificada para evitar distorções na modelagem.",
                                  "commonMistakes": [
                                    "Remover todos os outliers cegamente",
                                    "Usar Z-score em dados não-normais",
                                    "Não visualizar antes/depois"
                                  ]
                                },
                                {
                                  "stepNumber": 4,
                                  "title": "Limpeza Final e Preparação para Modelagem",
                                  "subSteps": [
                                    "Corrija tipos de dados com astype() e pd.to_datetime().",
                                    "Remova ou trate duplicatas com drop_duplicates().",
                                    "Aplique encoding categórico (OneHotEncoder ou LabelEncoder).",
                                    "Normalize/escala features com StandardScaler ou MinMaxScaler.",
                                    "Salve dataset limpo e gere summary final."
                                  ],
                                  "verification": "Dataset final com tipos corretos, sem duplicatas/missing/outliers, features escaladas uniformemente.",
                                  "estimatedTime": "1 hora",
                                  "materials": [
                                    "pandas",
                                    "sklearn.preprocessing",
                                    "Jupyter Notebook"
                                  ],
                                  "tips": "Mantenha um pipeline com ColumnTransformer para reprodutibilidade.",
                                  "learningObjective": "Finalizar o dataset pronto para modelagem com transformações padronizadas.",
                                  "commonMistakes": [
                                    "Encoding categórico sem checar cardinalidade",
                                    "Escalar targets por engano",
                                    "Não salvar versões intermediárias"
                                  ]
                                }
                              ],
                              "practicalExample": "Em um dataset de preços de casas do Kaggle (Ames Housing), identifique missing em 'LotFrontage' (imputar mediana por bairro), remova outliers em 'SalePrice' via IQR, encode 'Neighborhood' com OneHot, e normalize numéricas com StandardScaler. Resultado: dataset pronto para regressão.",
                              "finalVerifications": [
                                "Nenhum valor ausente ou duplicata no dataset final.",
                                "Distribuições de features numéricas normalizadas (média ~0, std ~1).",
                                "Outliers tratados confirmados por boxplots limpos.",
                                "Tipos de dados consistentes e encoding aplicado corretamente.",
                                "Summary estatístico final gerado e comparado com original.",
                                "Pipeline de limpeza salvo e reproduzível."
                              ],
                              "assessmentCriteria": [
                                "Precisão e justificativa nas escolhas de tratamento (ex: imputação vs remoção).",
                                "Qualidade do código: limpo, comentado e eficiente.",
                                "Validações adequadas em cada etapa (antes/depois).",
                                "Manuseio correto de diferentes tipos de dados (numérico/categórico).",
                                "Ausência de vazamento de dados ou bias introduzido.",
                                "Documentação de decisões e impactos no dataset."
                              ],
                              "crossCurricularConnections": [
                                "Estatística: Uso de medidas como IQR, Z-score e distribuições.",
                                "Programação: Manipulação avançada com pandas e sklearn pipelines.",
                                "Matemática: Transformações lineares e normalização.",
                                "Domínio Específico: Análise contextual em áreas como saúde ou finanças.",
                                "Ética em Dados: Considerações de bias em tratamentos."
                              ],
                              "realWorldApplication": "Em empresas de e-commerce, limpar dados de transações para modelos de previsão de demanda, removendo missing em 'quantidade_vendida' e outliers em 'preço', garantindo precisão em recomendações e estoque."
                            },
                            "estimatedTime": "1 hora",
                            "difficulty": "intermediate",
                            "status": "not_started",
                            "prerequisites": [
                              "13.1.2.1.2.2"
                            ]
                          },
                          {
                            "id": "13.1.2.1.2.4",
                            "name": "Ajustar e avaliar modelos com exemplos",
                            "description": "Demonstrar o processo de ajuste de modelos e avaliação usando métricas, com estudos de caso reais de problemas modelados.",
                            "atomicExpansion": {
                              "steps": [
                                {
                                  "stepNumber": 1,
                                  "title": "Preparar dados e ambiente para ajuste do modelo",
                                  "subSteps": [
                                    "Instalar bibliotecas necessárias como scikit-learn, pandas e numpy via pip.",
                                    "Carregar dataset de exemplo (ex: Boston Housing para regressão).",
                                    "Explorar dados: verificar shape, missing values e estatísticas descritivas.",
                                    "Dividir dados em treino (80%) e teste (20%) usando train_test_split.",
                                    "Pré-processar: normalizar features com StandardScaler."
                                  ],
                                  "verification": "Dados divididos corretamente, sem erros de importação e pré-processamento confirmado por shape e mean=0 em features escaladas.",
                                  "estimatedTime": "45 minutos",
                                  "materials": [
                                    "Python 3.x",
                                    "Jupyter Notebook",
                                    "scikit-learn",
                                    "pandas",
                                    "numpy",
                                    "Dataset Boston Housing (sklearn.datasets)"
                                  ],
                                  "tips": "Use random_state=42 para reproducibilidade em splits.",
                                  "learningObjective": "Configurar ambiente e preparar dados limpos para treinamento confiável.",
                                  "commonMistakes": [
                                    "Ignorar missing values levando a NaNs no modelo.",
                                    "Não escalar features causando viés em algoritmos sensíveis como SVM.",
                                    "Split incorreto expondo dados de teste ao treino."
                                  ]
                                },
                                {
                                  "stepNumber": 2,
                                  "title": "Ajustar (treinar) o modelo candidato",
                                  "subSteps": [
                                    "Selecionar modelo inicial, ex: LinearRegression() para regressão.",
                                    "Instanciar o modelo e usar fit(X_train, y_train).",
                                    "Ajustar hiperparâmetros iniciais se necessário (ex: via GridSearchCV básico).",
                                    "Treinar múltiplas iterações se aplicável e salvar o modelo treinado.",
                                    "Registrar tempo de treinamento e logs de convergência."
                                  ],
                                  "verification": "Modelo treinado com .score(X_train, y_train) > 0.7 e sem warnings de convergência.",
                                  "estimatedTime": "30 minutos",
                                  "materials": [
                                    "scikit-learn (models e GridSearchCV)",
                                    "Jupyter Notebook"
                                  ],
                                  "tips": "Comece com modelo baseline simples antes de complexos para comparação.",
                                  "learningObjective": "Executar treinamento eficiente e entender impacto de hiperparâmetros.",
                                  "commonMistakes": [
                                    "Overfitting por não usar validação cruzada.",
                                    "Treinar em dados não escalados.",
                                    "Ignorar warnings de multicolinearidade em regressão."
                                  ]
                                },
                                {
                                  "stepNumber": 3,
                                  "title": "Avaliar o modelo usando métricas apropriadas",
                                  "subSteps": [
                                    "Prever em conjunto de teste: y_pred = model.predict(X_test).",
                                    "Calcular métricas: MSE, RMSE, R² para regressão ou accuracy, F1 para classificação.",
                                    "Usar cross_val_score para validação cruzada (k=5 folds).",
                                    "Visualizar: plot de resíduos, confusion matrix ou learning curves.",
                                    "Comparar métricas com baseline (ex: média dos targets)."
                                  ],
                                  "verification": "Métricas calculadas (ex: R² > 0.6 no teste) e gráficos gerados sem erros.",
                                  "estimatedTime": "40 minutos",
                                  "materials": [
                                    "scikit-learn (metrics, cross_val_score)",
                                    "matplotlib",
                                    "seaborn"
                                  ],
                                  "tips": "Sempre avalie em hold-out set para estimar generalização.",
                                  "learningObjective": "Aplicar e interpretar métricas para validar performance do modelo.",
                                  "commonMistakes": [
                                    "Avaliar apenas em treino ignorando overfitting.",
                                    "Escolher métrica errada (ex: accuracy em datasets desbalanceados).",
                                    "Não plotar resíduos revelando padrões não lineares."
                                  ]
                                },
                                {
                                  "stepNumber": 4,
                                  "title": "Interpretar resultados com estudo de caso real",
                                  "subSteps": [
                                    "Aplicar modelo a um caso real: prever preços de casas com features como quartos e localização.",
                                    "Analisar feature importance ou coeficientes do modelo.",
                                    "Simular cenários: 'o que se aumentarmos quartos em 1?'.",
                                    "Documentar insights: forças, fraquezas e sugestões de melhoria.",
                                    "Iterar: retreinar com novo hiperparâmetro baseado em avaliação."
                                  ],
                                  "verification": "Relatório com predições, importância de features e pelo menos uma iteração documentada.",
                                  "estimatedTime": "35 minutos",
                                  "materials": [
                                    "Dataset real (ex: Kaggle Housing)",
                                    "pandas para análise"
                                  ],
                                  "tips": "Use SHAP ou coef_ para interpretabilidade em casos reais.",
                                  "learningObjective": "Conectar avaliação teórica a aplicações práticas via casos reais.",
                                  "commonMistakes": [
                                    "Ignorar causalidade confundindo correlação.",
                                    "Não iterar após avaliação ruim.",
                                    "Generalizar excessivamente de um dataset."
                                  ]
                                }
                              ],
                              "practicalExample": "Usando o dataset Boston Housing, treine uma regressão linear para prever preços de casas baseados em features como número de quartos e taxa de criminalidade. Avalie com R²=0.75 no teste, identifique 'LSTAT' como feature mais impactante e simule: 'reduzir criminalidade em 1% aumenta preço em $2000'.",
                              "finalVerifications": [
                                "Modelo treinado e salvo sem erros.",
                                "Métricas de treino e teste calculadas e comparadas.",
                                "Gráficos de avaliação (resíduos, importance) gerados.",
                                "Estudo de caso com predições e insights documentados.",
                                "Iteração realizada com melhoria em pelo menos uma métrica.",
                                "Relatório final explica escolhas de modelo e métricas."
                              ],
                              "assessmentCriteria": [
                                "Precisão das métricas (R² > 0.6 no teste).",
                                "Correta interpretação de overfitting/underfitting via curvas.",
                                "Uso adequado de validação cruzada (CV score consistente).",
                                "Análise de feature importance alinhada aos dados.",
                                "Relatório claro com substeps seguidos e erros evitados.",
                                "Criatividade em cenários do estudo de caso real."
                              ],
                              "crossCurricularConnections": [
                                "Estatística: Interpretação de métricas como R² e resíduos.",
                                "Programação: Implementação em Python com bibliotecas ML.",
                                "Análise de Dados: Exploração e visualização pré e pós-modelo.",
                                "Ética em Dados: Discussão de bias em features como criminalidade."
                              ],
                              "realWorldApplication": "Em empresas de imóveis, ajustar modelos para prever preços de imóveis auxilia na precificação dinâmica; em saúde, avaliar modelos de risco de doenças otimiza alocação de recursos hospitalares, reduzindo custos em 20% via predições precisas."
                            },
                            "estimatedTime": "2 horas",
                            "difficulty": "intermediate",
                            "status": "not_started",
                            "prerequisites": [
                              "13.1.2.1.2.3"
                            ]
                          }
                        ]
                      },
                      {
                        "id": "13.1.2.1.3",
                        "name": "Formulação de Problemas como Tarefas de Aprendizado",
                        "description": "Mapeamento de problemas reais para tarefas de aprendizado supervisionado e não-supervisionado, considerando ética.",
                        "specificSkills": [
                          {
                            "id": "13.1.2.1.3.1",
                            "name": "Classificar problemas em aprendizado supervisionado",
                            "description": "Identificar quando um problema real se enquadra em aprendizado supervisionado (regressão/classificação) e formular a tarefa de modelagem.",
                            "atomicExpansion": {
                              "steps": [
                                {
                                  "stepNumber": 1,
                                  "title": "Compreender os Fundamentos do Aprendizado Supervisionado",
                                  "subSteps": [
                                    "Defina aprendizado supervisionado como um tipo de ML onde o modelo é treinado com dados rotulados (inputs e outputs conhecidos).",
                                    "Identifique os componentes essenciais: conjunto de treinamento com pares (feature, label).",
                                    "Diferencie de aprendizado não supervisionado, destacando a ausência de rótulos.",
                                    "Revise exemplos básicos: previsão de notas de alunos com base em horas de estudo (label: nota).",
                                    "Anote as vantagens, como precisão em tarefas preditivas."
                                  ],
                                  "verification": "Escreva uma definição em suas palavras e liste 3 exemplos de dados rotulados.",
                                  "estimatedTime": "1 hora",
                                  "materials": [
                                    "Artigo introdutório sobre ML supervisionado",
                                    "Vídeo Khan Academy ou Coursera sobre conceitos básicos de ML"
                                  ],
                                  "tips": "Sempre pergunte: 'Há rótulos disponíveis?' para confirmar supervisão.",
                                  "learningObjective": "Dominar a definição e componentes chave do aprendizado supervisionado.",
                                  "commonMistakes": [
                                    "Confundir com clustering (não supervisionado)",
                                    "Ignorar a necessidade de dados rotulados",
                                    "Achar que supervisão é só para imagens"
                                  ]
                                },
                                {
                                  "stepNumber": 2,
                                  "title": "Identificar Características de Problemas Reais para Supervisão",
                                  "subSteps": [
                                    "Analise um problema real: pergunte se há histórico de dados com resultados conhecidos.",
                                    "Verifique presença de features (variáveis de entrada) e target (variável de saída rotulada).",
                                    "Classifique se o problema é preditivo (prever futuro) ou descritivo (agrupar).",
                                    "Liste critérios: dados históricos rotulados, objetivo de previsão.",
                                    "Pratique com 2 problemas cotidianos, justificando por que se enquadram ou não."
                                  ],
                                  "verification": "Para 3 problemas dados, justifique se são supervisionados com evidências.",
                                  "estimatedTime": "1.5 horas",
                                  "materials": [
                                    "Lista de problemas reais de datasets Kaggle",
                                    "Planilha para anotar features e labels"
                                  ],
                                  "tips": "Pense em 'professor supervisionando aluno': labels são as respostas corretas.",
                                  "learningObjective": "Reconhecer quando um problema real requer supervisão.",
                                  "commonMistakes": [
                                    "Assumir supervisão sem labels históricos",
                                    "Confundir com regras fixas (não ML)",
                                    "Ignorar viabilidade de rotulagem"
                                  ]
                                },
                                {
                                  "stepNumber": 3,
                                  "title": "Diferenciar Regressão de Classificação",
                                  "subSteps": [
                                    "Defina regressão: prever valor contínuo (ex: preço, temperatura).",
                                    "Defina classificação: prever categoria discreta (ex: sim/não, classes múltiplas).",
                                    "Compare métricas: regressão (MSE), classificação (accuracy, precision).",
                                    "Examine datasets: housing prices (regressão), iris flowers (classificação).",
                                    "Crie uma tabela comparativa com 4 exemplos de cada."
                                  ],
                                  "verification": "Classifique 5 problemas como regressão ou classificação com justificativa.",
                                  "estimatedTime": "1 hora",
                                  "materials": [
                                    "Datasets exemplo: Boston Housing, Titanic (Kaggle)",
                                    "Tabela comparativa em Excel/Google Sheets"
                                  ],
                                  "tips": "Regressão = números reais; Classificação = categorias/nomes.",
                                  "learningObjective": "Distinguir subtipos de supervisão com precisão.",
                                  "commonMistakes": [
                                    "Tratar categorias como números (ordinal regression)",
                                    "Usar regressão para poucas classes",
                                    "Confundir output contínuo com discreto"
                                  ]
                                },
                                {
                                  "stepNumber": 4,
                                  "title": "Formular a Tarefa de Modelagem",
                                  "subSteps": [
                                    "Defina a tarefa: 'Dado X features, prever Y label'.",
                                    "Especifique features, target, tipo (regressão/classificação).",
                                    "Descreva split: train/test/validation.",
                                    "Formule hipóteses: 'Modelo deve prever com 85% accuracy'.",
                                    "Escreva uma declaração formal para um problema escolhido."
                                  ],
                                  "verification": "Redija formulações completas para 2 problemas reais.",
                                  "estimatedTime": "1 hora",
                                  "materials": [
                                    "Template de formulação de tarefa ML",
                                    "Exemplos de papers ou notebooks Jupyter"
                                  ],
                                  "tips": "Seja específico: inclua tipo de label e métrica esperada.",
                                  "learningObjective": "Formular tarefas de modelagem claras e acionáveis.",
                                  "commonMistakes": [
                                    "Omitir split de dados",
                                    "Não especificar tipo de supervisão",
                                    "Definir target ambíguo"
                                  ]
                                }
                              ],
                              "practicalExample": "Problema: Prever se um paciente tem diabetes baseado em idade, IMC e glicose (dataset Pima Indians). Features: idade, IMC, glicose (contínuas); Target: sim/não (classificação binária). Formulação: 'Treinar modelo supervisionado de classificação para prever diabetes com dados rotulados históricos.'",
                              "finalVerifications": [
                                "Classificar corretamente 90% de 10 problemas como supervisionados/não.",
                                "Diferenciar regressão/classificação em 8/10 casos.",
                                "Formular tarefas de modelagem completas para 3 problemas reais.",
                                "Explicar diferenças entre supervisão e outros tipos de ML.",
                                "Identificar features e target em datasets exemplo.",
                                "Justificar escolhas com critérios claros."
                              ],
                              "assessmentCriteria": [
                                "Precisão na identificação de supervisão (rótulos presentes).",
                                "Correta distinção regressão vs classificação (contínuo/discreto).",
                                "Clareza e completude na formulação da tarefa.",
                                "Uso de exemplos reais e justificativas lógicas.",
                                "Ausência de confusões comuns (ex: unsupervised).",
                                "Capacidade de aplicar em contextos variados."
                              ],
                              "crossCurricularConnections": [
                                "Estatística: Análise de distribuições de labels e features.",
                                "Programação: Manipulação de dados com Python/Pandas para preparar datasets.",
                                "Matemática: Funções de perda e otimização em modelagem.",
                                "Ética: Viés em dados rotulados e implicações sociais."
                              ],
                              "realWorldApplication": "Em bancos para classificar transações fraudulentas (classificação) ou prever churn de clientes (regressão); em saúde para diagnosticar doenças via sintomas rotulados; em e-commerce para prever preços dinâmicos de produtos."
                            },
                            "estimatedTime": "1 hora",
                            "difficulty": "intermediate",
                            "status": "not_started",
                            "prerequisites": [
                              "13.1.2.1.2.4"
                            ]
                          },
                          {
                            "id": "13.1.2.1.3.2",
                            "name": "Classificar problemas em aprendizado não-supervisionado",
                            "description": "Reconhecer cenários para clustering, redução de dimensionalidade ou detecção de anomalias em problemas reais e defini-los como tarefas.",
                            "atomicExpansion": {
                              "steps": [
                                {
                                  "stepNumber": 1,
                                  "title": "Revisar Conceitos Fundamentais de Aprendizado Não-Supervisionado",
                                  "subSteps": [
                                    "Defina aprendizado não-supervisionado como aprendizado sem rótulos.",
                                    "Liste as três tarefas principais: clustering, redução de dimensionalidade e detecção de anomalias.",
                                    "Explique a diferença entre supervisionado e não-supervisionado com exemplos simples.",
                                    "Identifique quando usar não-supervisionado: dados sem labels ou exploração de padrões.",
                                    "Resuma algoritmos clássicos para cada tarefa (ex: K-means para clustering)."
                                  ],
                                  "verification": "Crie um mapa mental ou tabela resumindo as três tarefas e suas diferenças; revise com um colega ou autoavaliação.",
                                  "estimatedTime": "45 minutos",
                                  "materials": "Notebook, acesso a slides ou vídeo introdutório sobre ML não-supervisionado (ex: Khan Academy ou Coursera).",
                                  "tips": "Use analogias cotidianas, como agrupar frutas por semelhança (clustering) sem saber nomes.",
                                  "learningObjective": "Compreender os pilares do aprendizado não-supervisionado e suas tarefas principais.",
                                  "commonMistakes": "Confundir com supervisionado; achar que sempre precisa de labels."
                                },
                                {
                                  "stepNumber": 2,
                                  "title": "Estudar Características Detalhadas de Cada Tarefa Não-Supervisionada",
                                  "subSteps": [
                                    "Para clustering: entenda agrupamento baseado em similaridade (ex: distância euclidiana).",
                                    "Para redução de dimensionalidade: aprenda compressão de features preservando informação (ex: PCA).",
                                    "Para detecção de anomalias: identifique outliers como desvios de padrões normais.",
                                    "Compare métricas: silhouette score para clustering, variância explicada para PCA, precisão para anomalias.",
                                    "Pratique visualizando datasets sintéticos para cada tarefa."
                                  ],
                                  "verification": "Responda a um quiz com 10 perguntas sobre diferenças entre tarefas; acerte 90%.",
                                  "estimatedTime": "60 minutos",
                                  "materials": "Documentação scikit-learn, Jupyter Notebook com datasets de exemplo (Iris para clustering).",
                                  "tips": "Desenhe gráficos: clusters como bolhas, dim reduction como projeção 3D para 2D.",
                                  "learningObjective": "Diferenciar precisamente clustering, redução de dimensionalidade e detecção de anomalias.",
                                  "commonMistakes": "Ignorar pré-processamento de dados; superestimar uma tarefa para todos os cenários."
                                },
                                {
                                  "stepNumber": 3,
                                  "title": "Identificar Cenários Reais para Cada Tarefa",
                                  "subSteps": [
                                    "Liste 5 cenários para clustering (ex: segmentação de clientes).",
                                    "Liste 5 para redução de dim (ex: visualização de dados genéticos).",
                                    "Liste 5 para anomalias (ex: detecção de fraudes em cartões).",
                                    "Analise um dataset real e classifique seu problema principal.",
                                    "Discuta ambiguidades: quando um problema pode se encaixar em mais de uma categoria."
                                  ],
                                  "verification": "Classifique 5 problemas reais descritos em texto; justifique escolhas por escrito.",
                                  "estimatedTime": "75 minutos",
                                  "materials": "Datasets Kaggle (ex: Mall Customers para clustering), artigos de caso reais.",
                                  "tips": "Pergunte: 'Há grupos naturais? Dados demais para visualizar? Pontos isolados?'",
                                  "learningObjective": "Reconhecer padrões em problemas reais que indicam uma tarefa específica.",
                                  "commonMistakes": "Forçar clustering em dados lineares; confundir anomalias com ruído comum."
                                },
                                {
                                  "stepNumber": 4,
                                  "title": "Formular Problemas como Tarefas Não-Supervisionadas",
                                  "subSteps": [
                                    "Para um cenário dado, escreva: 'Dado dataset X sem labels, aplicar Y para Z objetivo.'",
                                    "Defina métricas de sucesso para cada formulação.",
                                    "Crie pseudocódigo para implementação básica.",
                                    "Teste formulação em um mini-projeto com código.",
                                    "Revise e refine com base em feedback simulado."
                                  ],
                                  "verification": "Formule 3 problemas reais como tarefas; execute um com código e valide resultados.",
                                  "estimatedTime": "90 minutos",
                                  "materials": "Python/Jupyter, bibliotecas scikit-learn, um dataset pessoal ou público.",
                                  "tips": "Comece com 'O objetivo é...' para clareza na formulação.",
                                  "learningObjective": "Definir formalmente problemas reais como tarefas de aprendizado não-supervisionado.",
                                  "commonMistakes": "Especificar algoritmo cedo demais; ignorar validação de resultados."
                                }
                              ],
                              "practicalExample": "Em um e-commerce, dados de navegação de usuários sem compras rotuladas: classifique como clustering para segmentar usuários em perfis de comportamento (ex: compradores impulsivos vs. pesquisadores), use K-means para agrupar por tempo de sessão e páginas vistas.",
                              "finalVerifications": [
                                "Classifique corretamente 8/10 cenários reais como clustering, dim reduction ou anomalias.",
                                "Formule 3 problemas com definições claras incluindo objetivos e métricas.",
                                "Explique diferenças entre tarefas em uma apresentação de 2 minutos.",
                                "Implemente e valide uma tarefa simples em código com resultados interpretáveis.",
                                "Identifique ambiguidades em cenários híbridos e proponha abordagens.",
                                "Crie um fluxograma de decisão para classificação de problemas."
                              ],
                              "assessmentCriteria": [
                                "Precisão na identificação de tarefas (90%+ em testes).",
                                "Profundidade nas justificativas (cita características do dataset).",
                                "Clareza na formulação de tarefas (estrutura: dado-input, tarefa, output-esperado).",
                                "Uso apropriado de exemplos reais e métricas de avaliação.",
                                "Capacidade de lidar com casos ambíguos ou edge cases.",
                                "Integração de conceitos em projetos práticos com código funcional."
                              ],
                              "crossCurricularConnections": [
                                "Estatística: Distribuições e testes de normalidade para anomalias.",
                                "Programação: Implementação em Python/R com scikit-learn.",
                                "Negócios: Aplicações em marketing e análise de clientes.",
                                "Visualização de Dados: Gráficos para validar clusters e reduções.",
                                "Ética em IA: Privacidade em dados não-supervisionados sensíveis."
                              ],
                              "realWorldApplication": "Em bancos, detectar fraudes em transações (anomalias); em saúde, agrupar pacientes por sintomas semelhantes (clustering); em imagens satélites, reduzir dimensões para monitorar desmatamento (PCA)."
                            },
                            "estimatedTime": "1 hora",
                            "difficulty": "intermediate",
                            "status": "not_started",
                            "prerequisites": [
                              "13.1.2.1.3.1"
                            ]
                          },
                          {
                            "id": "13.1.2.1.3.3",
                            "name": "Aplicar princípios éticos na modelagem",
                            "description": "Discutir questões éticas como privacidade, viés e transparência ao formular problemas de modelagem com dados reais.",
                            "atomicExpansion": {
                              "steps": [
                                {
                                  "stepNumber": 1,
                                  "title": "Compreender os Princípios Éticos Fundamentais em Modelagem de Dados",
                                  "subSteps": [
                                    "Estude definições de privacidade (ex.: anonimato de dados pessoais), viés (ex.: discriminação algorítmica) e transparência (ex.: explicabilidade do modelo).",
                                    "Leia casos reais como o escândalo Cambridge Analytica para ilustrar impactos éticos.",
                                    "Liste os três princípios principais e dê exemplos de violações em contextos de dados reais.",
                                    "Compare princípios éticos com regulamentações como GDPR e LGPD.",
                                    "Crie um glossário pessoal com 5 termos éticos relacionados."
                                  ],
                                  "verification": "Aluno fornece definições corretas e exemplos relevantes dos três princípios em um quiz curto.",
                                  "estimatedTime": "45 minutos",
                                  "materials": [
                                    "Artigo 'Ética em IA' do MIT",
                                    "Vídeo TED sobre viés algorítmico",
                                    "Resumo do GDPR"
                                  ],
                                  "tips": "Use analogias cotidianas, como comparar viés a preconceitos humanos, para fixar conceitos.",
                                  "learningObjective": "Identificar e definir privacidade, viés e transparência no contexto de modelagem.",
                                  "commonMistakes": "Confundir viés com erro técnico; ignorar transparência como mero 'log'."
                                },
                                {
                                  "stepNumber": 2,
                                  "title": "Identificar Questões Éticas em Problemas de Modelagem Reais",
                                  "subSteps": [
                                    "Escolha um dataset real (ex.: dados de recrutamento) e analise fontes potenciais de viés (ex.: gênero, etnia).",
                                    "Mapeie riscos de privacidade, como reidentificação de indivíduos anônimos.",
                                    "Avalie transparência: o modelo será uma 'caixa preta'?",
                                    "Crie uma matriz de riscos éticos com probabilidade e impacto para cada princípio.",
                                    "Discuta com pares um risco identificado em um fórum."
                                  ],
                                  "verification": "Matriz de riscos preenchida com pelo menos 3 questões identificadas por princípio ético.",
                                  "estimatedTime": "60 minutos",
                                  "materials": [
                                    "Dataset Kaggle de recrutamento",
                                    "Ferramenta Google Sheets para matriz",
                                    "Checklist ético da IEEE"
                                  ],
                                  "tips": "Pergunte sempre: 'Quem pode ser prejudicado?' para guiar a identificação.",
                                  "learningObjective": "Detectar questões éticas específicas em datasets e problemas de modelagem.",
                                  "commonMistakes": "Focar só em viés óbvio, ignorando privacidade em dados agregados."
                                },
                                {
                                  "stepNumber": 3,
                                  "title": "Incorporar Princípios Éticos na Formulação do Problema",
                                  "subSteps": [
                                    "Reformule o problema original incluindo restrições éticas (ex.: 'prever contratações sem viés de gênero').",
                                    "Defina métricas éticas ao lado de métricas de performance (ex.: fairness score).",
                                    "Projete safeguards como pré-processamento para anonimato ou auditoria de viés.",
                                    "Escreva uma declaração ética para o problema.",
                                    "Teste a reformulação com um cenário hipotético de falha ética."
                                  ],
                                  "verification": "Declaração ética e reformulação do problema submetidas, com métricas éticas explícitas.",
                                  "estimatedTime": "50 minutos",
                                  "materials": [
                                    "Template de formulação ética (Google Doc)",
                                    "Biblioteca Fairlearn para métricas de viés"
                                  ],
                                  "tips": "Integre ética desde o início, não como 'pós-processo', para evitar retrabalho.",
                                  "learningObjective": "Reformular problemas de modelagem incorporando salvaguardas éticas.",
                                  "commonMistakes": "Adicionar ética superficialmente sem alterar o objetivo principal."
                                },
                                {
                                  "stepNumber": 4,
                                  "title": "Avaliar e Documentar Decisões Éticas",
                                  "subSteps": [
                                    "Realize uma revisão ética simulada: pontue riscos residuais pós-mitigação.",
                                    "Documente trade-offs (ex.: transparência vs. precisão).",
                                    "Crie um relatório com seções para cada princípio e ações tomadas.",
                                    "Simule uma apresentação para stakeholders éticos.",
                                    "Atualize o relatório com feedback simulado."
                                  ],
                                  "verification": "Relatório completo com avaliação de riscos e documentação submetido.",
                                  "estimatedTime": "40 minutos",
                                  "materials": [
                                    "Modelo de relatório ético (template)",
                                    "Ferramenta de apresentação como Canva"
                                  ],
                                  "tips": "Use linguagem acessível no relatório para não especialistas.",
                                  "learningObjective": "Documentar e avaliar decisões éticas de forma transparente.",
                                  "commonMistakes": "Omitir trade-offs, criando ilusão de solução perfeita."
                                },
                                {
                                  "stepNumber": 5,
                                  "title": "Aplicar e Refletir sobre a Modelagem Ética",
                                  "subSteps": [
                                    "Implemente um modelo simples com considerações éticas (ex.: regressão logística com remoção de viés).",
                                    "Teste verificações finais e ajuste com base nelas.",
                                    "Reflita em um diário: o que aprendeu sobre ética em prática?",
                                    "Compartilhe o modelo em um repositório com nota ética.",
                                    "Planeje aplicação em um novo problema."
                                  ],
                                  "verification": "Modelo implementado com README ético e reflexão submetida.",
                                  "estimatedTime": "70 minutos",
                                  "materials": [
                                    "Python/Jupyter Notebook",
                                    "Biblioteca AIF360 para auditoria ética"
                                  ],
                                  "tips": "Automatize checagens éticas com scripts para eficiência.",
                                  "learningObjective": "Aplicar modelagem ética end-to-end e refletir criticamente.",
                                  "commonMistakes": "Ignorar reflexão, tratando ética como checklist."
                                }
                              ],
                              "practicalExample": "Ao modelar um sistema de recomendação de empréstimos bancários com dados reais de clientes, identifique viés em scores de crédito por raça (viés), garanta anonimato de CPF (privacidade) e forneça explicações interpretáveis das decisões (transparência), reformulando o problema para incluir métricas de equidade.",
                              "finalVerifications": [
                                "Lista completa de riscos éticos identificados no dataset.",
                                "Reformulação do problema com salvaguardas explícitas.",
                                "Relatório documentado com trade-offs e mitigação.",
                                "Modelo testado com métricas éticas (ex.: fairness > 0.8).",
                                "Reflexão escrita sobre lições aprendidas.",
                                "Apresentação ética convincente para stakeholders."
                              ],
                              "assessmentCriteria": [
                                "Precisão na identificação de pelo menos 3 riscos por princípio ético.",
                                "Integração robusta de ética na formulação (não superficial).",
                                "Documentação clara e completa de decisões.",
                                "Uso correto de ferramentas/métricas éticas.",
                                "Profundidade na reflexão crítica e trade-offs.",
                                "Criatividade em mitigação e exemplos práticos."
                              ],
                              "crossCurricularConnections": [
                                "Filosofia: Debates sobre utilitarismo vs. deontologia em IA.",
                                "Direito: Aplicação de leis de proteção de dados (GDPR/LGPD).",
                                "Sociologia: Impacto de viés algorítmico em desigualdades sociais.",
                                "Psicologia: Comportamento humano e percepção de transparência.",
                                "Comunicação: Apresentação ética para públicos leigos."
                              ],
                              "realWorldApplication": "Em empresas como Google ou bancos, profissionais usam esses princípios para evitar multas milionárias por viés (ex.: caso Amazon hiring tool) e construir confiança pública em modelos de IA, como em saúde (previsão de doenças sem discriminação) ou finanças (crédito justo)."
                            },
                            "estimatedTime": "1 hora",
                            "difficulty": "advanced",
                            "status": "not_started",
                            "prerequisites": [
                              "13.1.2.1.3.2"
                            ]
                          }
                        ]
                      }
                    ]
                  },
                  {
                    "id": "10.1.2.2",
                    "name": "Conceitos Básicos de Aprendizado",
                    "description": "Definições e princípios fundamentais do processo de aprendizado em machine learning.",
                    "individualConcepts": [
                      {
                        "id": "10.1.2.2.1",
                        "name": "Hierarquia Dados, Informação e Conhecimento",
                        "description": "Compreensão da distinção entre dados brutos, informação processada e conhecimento acionável no contexto do aprendizado de máquina.",
                        "specificSkills": [
                          {
                            "id": "10.1.2.2.1.1",
                            "name": "Diferenciar dados de informação",
                            "description": "Identificar e explicar a diferença entre dados brutos (observações numéricas ou categóricas sem contexto) e informação (dados processados com contexto que revelam padrões ou relações).",
                            "atomicExpansion": {
                              "steps": [
                                {
                                  "stepNumber": 1,
                                  "title": "Entender e Definir Dados Brutos",
                                  "subSteps": [
                                    "Leia a definição de dados brutos: observações numéricas ou categóricas sem contexto ou processamento.",
                                    "Identifique exemplos simples, como '23', '45', 'azul', 'vermelho'.",
                                    "Anote 5 exemplos de dados brutos do dia a dia.",
                                    "Explique por que esses são dados e não informação (falta contexto).",
                                    "Crie uma tabela com colunas: Exemplo, Tipo (numérico/categórico), Por quê é dado bruto."
                                  ],
                                  "verification": "Concluído quando tabela com 5 exemplos estiver preenchida corretamente e explicada verbalmente.",
                                  "estimatedTime": "20 minutos",
                                  "materials": [
                                    "Papel e caneta ou editor de texto",
                                    "Exemplos online de dados brutos (ex: listas de números aleatórios)"
                                  ],
                                  "tips": "Comece com observações sensoriais simples para facilitar a compreensão.",
                                  "learningObjective": "Definir dados brutos e identificar exemplos precisos.",
                                  "commonMistakes": [
                                    "Confundir dados com números processados (ex: médias)",
                                    "Adicionar contexto prematuramente"
                                  ]
                                },
                                {
                                  "stepNumber": 2,
                                  "title": "Entender e Definir Informação",
                                  "subSteps": [
                                    "Leia a definição de informação: dados processados com contexto que revelam padrões ou relações.",
                                    "Identifique exemplos, como 'A temperatura média em SP é 23°C em janeiro'.",
                                    "Anote 5 exemplos onde dados ganham contexto (ex: soma, média, comparação).",
                                    "Explique o processamento aplicado (ex: cálculo de média, categorização).",
                                    "Compare um dado bruto com sua versão como informação na mesma tabela do step 1."
                                  ],
                                  "verification": "Tabela atualizada com exemplos de informação e explicações de processamento.",
                                  "estimatedTime": "25 minutos",
                                  "materials": [
                                    "Mesma tabela do step 1",
                                    "Calculadora ou planilha simples (Google Sheets)"
                                  ],
                                  "tips": "Sempre pergunte: 'O que esse dado significa sem mais contexto?'",
                                  "learningObjective": "Definir informação e demonstrar como dados se transformam nela.",
                                  "commonMistakes": [
                                    "Considerar dados brutos como informação sem processamento",
                                    "Ignorar o papel do contexto"
                                  ]
                                },
                                {
                                  "stepNumber": 3,
                                  "title": "Comparar e Diferenciar Dados de Informação",
                                  "subSteps": [
                                    "Crie um diagrama de Venn ou tabela comparativa: colunas para Dados (características), Informação (características), Diferenças.",
                                    "Preencha com pelo menos 4 características cada (ex: bruto vs. processado, sem contexto vs. com contexto).",
                                    "Discuta com um parceiro ou grave áudio explicando 3 diferenças chave.",
                                    "Identifique cenários onde a distinção é crítica (ex: relatório vs. lista raw).",
                                    "Resuma em uma frase única a diferença principal."
                                  ],
                                  "verification": "Diagrama/tabela completa e resumo verbal ou escrito.",
                                  "estimatedTime": "30 minutos",
                                  "materials": [
                                    "Ferramenta de desenho (Draw.io, papel)",
                                    "Gravação de áudio ou parceiro de estudo"
                                  ],
                                  "tips": "Use setas para mostrar transformação: dados → processamento → informação.",
                                  "learningObjective": "Articular diferenças claras entre dados e informação.",
                                  "commonMistakes": [
                                    "Misturar conceitos, dizendo que informação é só 'mais dados'",
                                    "Subestimar o contexto"
                                  ]
                                },
                                {
                                  "stepNumber": 4,
                                  "title": "Aplicar a Diferenciação em Exemplos Práticos",
                                  "subSteps": [
                                    "Pegue um conjunto de dados reais (ex: lista de notas de alunos).",
                                    "Classifique cada item como dado ou informação, justificando.",
                                    "Transforme dados em informação adicionando contexto (ex: calcular média e interpretar).",
                                    "Crie 3 exercícios semelhantes e resolva-os.",
                                    "Avalie se sua transformação revela padrões (ex: maioria aprovada?)."
                                  ],
                                  "verification": "3 exercícios resolvidos com classificações e transformações corretas.",
                                  "estimatedTime": "35 minutos",
                                  "materials": [
                                    "Conjunto de dados sample (ex: CSV de notas baixado online)",
                                    "Planilha para processamento"
                                  ],
                                  "tips": "Escolha dados familiares para maior engajamento.",
                                  "learningObjective": "Aplicar diferenciação em contextos reais.",
                                  "commonMistakes": [
                                    "Processamento inadequado que não adiciona valor",
                                    "Falhar em revelar padrões"
                                  ]
                                }
                              ],
                              "practicalExample": "Dados brutos: [25, 30, 22, 28] (temperaturas diárias em °C). Informação: 'A temperatura média da semana foi 26.25°C, com variação de 6°C, indicando estabilidade climática.'",
                              "finalVerifications": [
                                "Pode listar 5 exemplos corretos de dados brutos sem contexto.",
                                "Explica como adicionar contexto transforma dados em informação.",
                                "Identifica corretamente dados vs. informação em conjuntos mistos.",
                                "Cria uma transformação válida de dados para informação.",
                                "Articula pelo menos 3 diferenças chave verbalmente.",
                                "Aplica o conceito em um exemplo real sem erros."
                              ],
                              "assessmentCriteria": [
                                "Precisão nas definições (100% correto).",
                                "Qualidade dos exemplos e transformações (revelam padrões claros).",
                                "Profundidade da comparação (múltiplas características).",
                                "Criatividade na aplicação prática.",
                                "Ausência de confusões comuns (ex: contexto ignorado).",
                                "Clareza na comunicação (diagramas, resumos)."
                              ],
                              "crossCurricularConnections": [
                                "Ciências: Observações experimentais (dados brutos) vs. conclusões (informação).",
                                "Informática: Bancos de dados raw vs. relatórios SQL processados.",
                                "Estatística: Amostras brutas vs. análises descritivas.",
                                "Língua Portuguesa: Palavras isoladas (dados) vs. frases com significado (informação)."
                              ],
                              "realWorldApplication": "Em análise de vendas de uma empresa: lista de valores de transações (dados) vs. relatório de 'vendas médias por região cresceram 15%' (informação), guiando decisões estratégicas."
                            },
                            "estimatedTime": "1 hora",
                            "difficulty": "beginner",
                            "status": "not_started",
                            "prerequisites": []
                          },
                          {
                            "id": "10.1.2.2.1.2",
                            "name": "Explicar transformação em conhecimento",
                            "description": "Descrever como a informação, por meio de modelagem e aprendizado, gera conhecimento preditivo ou explicativo aplicável a novos dados.",
                            "atomicExpansion": {
                              "steps": [
                                {
                                  "stepNumber": 1,
                                  "title": "Entender a Hierarquia: Dados, Informação e Conhecimento",
                                  "subSteps": [
                                    "Defina 'dados' como fatos brutos sem contexto (ex: números de temperatura).",
                                    "Explique 'informação' como dados processados com contexto (ex: temperatura média diária).",
                                    "Descreva 'conhecimento' como padrões compreendidos e generalizáveis (ex: previsão de clima baseada em padrões).",
                                    "Compare os três níveis com um diagrama simples.",
                                    "Discuta limitações: dados sozinhos não geram insights sem processamento."
                                  ],
                                  "verification": "Crie um diagrama ou tabela comparando os três conceitos com exemplos reais.",
                                  "estimatedTime": "45 minutos",
                                  "materials": "Papel/caneta ou ferramenta digital como Draw.io; vídeo introdutório sobre DIKW pyramid (YouTube).",
                                  "tips": "Use analogias cotidianas como receita de bolo: ingredientes (dados), instruções (informação), resultado esperado (conhecimento).",
                                  "learningObjective": "Diferenciar claramente dados, informação e conhecimento na hierarquia DIKW.",
                                  "commonMistakes": "Confundir informação com conhecimento; achar que mais dados automaticamente geram conhecimento."
                                },
                                {
                                  "stepNumber": 2,
                                  "title": "Explorar o Papel da Modelagem na Transformação de Informação em Conhecimento",
                                  "subSteps": [
                                    "Descreva modelagem como criação de representações matemáticas ou computacionais dos dados.",
                                    "Explique como modelos extraem padrões da informação (ex: regressão linear para tendências).",
                                    "Discuta iterações: ajuste de parâmetros para melhorar precisão.",
                                    "Exemplo: modelar vendas passadas para identificar fatores influentes.",
                                    "Avalie qualidade do modelo com métricas como R² ou erro médio."
                                  ],
                                  "verification": "Construa um modelo simples em Python (usando scikit-learn) e interprete seus resultados.",
                                  "estimatedTime": "60 minutos",
                                  "materials": "Jupyter Notebook; dataset simples (ex: Iris ou Boston Housing do sklearn); tutorial de regressão linear.",
                                  "tips": "Comece com modelos lineares simples antes de complexos para construir intuição.",
                                  "learningObjective": "Compreender como modelagem abstrai informação em regras generalizáveis.",
                                  "commonMistakes": "Ignorar overfitting: modelo bom em dados antigos mas ruim em novos."
                                },
                                {
                                  "stepNumber": 3,
                                  "title": "Descrever Processos de Aprendizado para Gerar Conhecimento Preditivo e Explicativo",
                                  "subSteps": [
                                    "Defina aprendizado supervisionado para preditivo (ex: classificação de emails como spam).",
                                    "Explique aprendizado não-supervisionado para explicativo (ex: clustering de clientes).",
                                    "Compare preditivo (prever futuro) vs. explicativo (entender causas).",
                                    "Discuta validação cruzada para garantir aplicabilidade a novos dados.",
                                    "Exemplo: árvore de decisão para prever churn de clientes e explicar razões."
                                  ],
                                  "verification": "Classifique um dataset novo usando um modelo treinado e explique predições.",
                                  "estimatedTime": "75 minutos",
                                  "materials": "Python com pandas, sklearn; dataset de churn (Kaggle); documentação sklearn.",
                                  "tips": "Visualize árvores de decisão com graphviz para melhor compreensão explicativa.",
                                  "learningObjective": "Distinguir e aplicar aprendizado preditivo vs. explicativo.",
                                  "commonMistakes": "Confundir correlação com causalidade no conhecimento explicativo."
                                },
                                {
                                  "stepNumber": 4,
                                  "title": "Aplicar Conhecimento a Novos Dados e Avaliar Generalização",
                                  "subSteps": [
                                    "Teste modelo em dados não vistos (hold-out set).",
                                    "Meça performance com métricas como precisão, recall para preditivo.",
                                    "Interprete feature importance para explicativo.",
                                    "Discuta atualização contínua de modelos (aprendizado online).",
                                    "Reflita: como falhas indicam gaps no conhecimento?"
                                  ],
                                  "verification": "Gere relatório com métricas em dados de teste e insights sobre generalização.",
                                  "estimatedTime": "50 minutos",
                                  "materials": "Mesmo ambiente Python; dados de teste separados.",
                                  "tips": "Sempre divida dados em train/test (80/20) para simular mundo real.",
                                  "learningObjective": "Avaliar se o conhecimento gerado é robusto para novos contextos.",
                                  "commonMistakes": "Treinar e testar no mesmo dataset, levando a ilusão de performance."
                                }
                              ],
                              "practicalExample": "Usando dados de vendas históricas de uma loja (informação: vendas por dia, clima, feriados), treine um modelo de regressão para prever vendas futuras (conhecimento preditivo) e use SHAP para explicar quais fatores mais influenciam (conhecimento explicativo), aplicando a uma semana nova de dados.",
                              "finalVerifications": [
                                "Explica corretamente a hierarquia dados-informação-conhecimento com exemplos.",
                                "Descreve modelagem e aprendizado como pontes para conhecimento.",
                                "Diferencia conhecimento preditivo de explicativo.",
                                "Demonstra aplicação a novos dados com métricas.",
                                "Identifica limitações como overfitting ou bias.",
                                "Cria diagrama ou código funcional ilustrando o processo."
                              ],
                              "assessmentCriteria": [
                                "Precisão conceitual: 30% (definições corretas e hierarquia clara).",
                                "Profundidade técnica: 25% (explicação de modelagem/aprendizado com exemplos).",
                                "Aplicação prática: 20% (uso em exemplo real com código ou diagrama).",
                                "Generalização: 15% (ênfase em novos dados e validação).",
                                "Clareza e estrutura: 10% (comunicação acionável e sem erros comuns)."
                              ],
                              "crossCurricularConnections": [
                                "Filosofia: Epistemologia e teoria do conhecimento (DIKW pyramid).",
                                "Estatística: Inferência e testes de hipóteses para validação de modelos.",
                                "Ciência da Computação: Algoritmos de ML e programação prática.",
                                "Negócios: Análise de dados para tomada de decisão estratégica."
                              ],
                              "realWorldApplication": "Em medicina, dados de pacientes (ex: sintomas) viram informação processada, modelada em algoritmos de ML para prever diagnósticos (preditivo) ou identificar biomarcadores causais (explicativo), aplicados a novos pacientes para tratamentos personalizados."
                            },
                            "estimatedTime": "1 hora",
                            "difficulty": "beginner",
                            "status": "not_started",
                            "prerequisites": []
                          },
                          {
                            "id": "10.1.2.2.1.3",
                            "name": "Aplicar hierarquia em exemplos de ML",
                            "description": "Analisar um conjunto de dados real (ex.: Iris dataset) para ilustrar a progressão de dados para conhecimento via treinamento de modelo.",
                            "atomicExpansion": {
                              "steps": [
                                {
                                  "stepNumber": 1,
                                  "title": "Carregar e Explorar Dados Brutos (Nível: Dados)",
                                  "subSteps": [
                                    "Baixe o dataset Iris de uma fonte confiável como UCI ML Repository ou scikit-learn.",
                                    "Carregue o dataset em Python usando pandas: pd.read_csv() ou load_iris().",
                                    "Examine a estrutura: shape, columns, head(), describe() para visualizar valores brutos.",
                                    "Identifique tipos de dados: features numéricas (sepal length, etc.) e target (espécies).",
                                    "Crie um resumo inicial: contagens de classes e estatísticas básicas."
                                  ],
                                  "verification": "Dataset carregado corretamente e resumo impresso sem erros, com 150 amostras e 4 features.",
                                  "estimatedTime": "20 minutos",
                                  "materials": "Python com pandas e scikit-learn instalados; Jupyter Notebook.",
                                  "tips": "Use display(df) no Jupyter para visualização interativa.",
                                  "learningObjective": "Compreender dados como valores brutos sem contexto.",
                                  "commonMistakes": "Ignorar valores ausentes ou assumir dados limpos sem verificação."
                                },
                                {
                                  "stepNumber": 2,
                                  "title": "Processar Dados para Gerar Informação",
                                  "subSteps": [
                                    "Limpe os dados: verifique e trate missing values ou outliers usando df.isnull().sum().",
                                    "Crie visualizações: scatter plots (sepal vs petal) por espécie com seaborn pairplot.",
                                    "Extraia insights: calcule médias por feature e classe; observe padrões de separação.",
                                    "Codifique o target: use LabelEncoder para converter espécies em números.",
                                    "Divida em train/test: train_test_split com 80/20."
                                  ],
                                  "verification": "Gráficos mostram clusters claros por espécie; conjuntos train/test criados com proporções corretas.",
                                  "estimatedTime": "30 minutos",
                                  "materials": "Bibliotecas: matplotlib, seaborn, sklearn.preprocessing.",
                                  "tips": "Padronize escalas com StandardScaler para features com unidades diferentes.",
                                  "learningObjective": "Transformar dados em informação acionável via análise e visualização.",
                                  "commonMistakes": "Não normalizar features, levando a plots distorcidos."
                                },
                                {
                                  "stepNumber": 3,
                                  "title": "Treinar Modelo de Machine Learning",
                                  "subSteps": [
                                    "Selecione um modelo simples: KNN ou Decision Tree do sklearn.",
                                    "Ajuste hiperparâmetros iniciais: n_neighbors=3 para KNN.",
                                    "Treine o modelo: fit(X_train, y_train).",
                                    "Faça previsões no test set: predict(X_test).",
                                    "Calcule métricas básicas: accuracy_score e confusion_matrix."
                                  ],
                                  "verification": "Modelo treinado com accuracy > 90% no Iris dataset.",
                                  "estimatedTime": "25 minutos",
                                  "materials": "sklearn.neighbors.KNeighborsClassifier ou sklearn.tree.DecisionTreeClassifier.",
                                  "tips": "Use cross-validation (cross_val_score) para robustez.",
                                  "learningObjective": "Aplicar algoritmos para extrair padrões dos dados processados.",
                                  "commonMistakes": "Treinar no full dataset sem split, causando overfitting."
                                },
                                {
                                  "stepNumber": 4,
                                  "title": "Interpretar Resultados como Conhecimento",
                                  "subSteps": [
                                    "Analise a matriz de confusão: identifique erros entre espécies semelhantes.",
                                    "Explique a hierarquia: dados brutos → info via plots → conhecimento via predições precisas.",
                                    "Gere relatório: resuma insights como 'Petal length separa Setosa perfeitamente'.",
                                    "Teste com novos dados: crie amostras sintéticas e preveja.",
                                    "Discuta limitações: generalização para outros datasets florais."
                                  ],
                                  "verification": "Relatório escrito conectando explicitamente dados-informação-conhecimento.",
                                  "estimatedTime": "25 minutos",
                                  "materials": "sklearn.metrics; Markdown para relatório no Jupyter.",
                                  "tips": "Use feature_importances_ em árvores para explicar contribuições.",
                                  "learningObjective": "Sintetizar aprendizado de máquina como elevação a conhecimento acionável.",
                                  "commonMistakes": "Parar na accuracy sem interpretação qualitativa."
                                }
                              ],
                              "practicalExample": "Usando o Iris dataset, carregue dados brutos de medidas de flores (dados), visualize clusters por espécie (informação), treine um KNN com 95% accuracy (conhecimento), permitindo identificar novas flores selvagens corretamente.",
                              "finalVerifications": [
                                "Explicação oral ou escrita da hierarquia dados-informação-conhecimento aplicada ao Iris.",
                                "Código executável reproduzindo a análise com accuracy documentada.",
                                "Visualizações claras mostrando progressão de insights.",
                                "Previsões corretas em 90%+ de amostras de teste.",
                                "Relatório de 1 página resumindo limitações e extensões."
                              ],
                              "assessmentCriteria": [
                                "Precisão e completude da análise de dados (30%)",
                                "Clareza das visualizações e insights gerados (25%)",
                                "Desempenho do modelo treinado (20%)",
                                "Conexão explícita com a hierarquia DIK (15%)",
                                "Qualidade do relatório e verificações finais (10%)"
                              ],
                              "crossCurricularConnections": [
                                "Estatística: Análise descritiva e testes de separabilidade.",
                                "Biologia: Classificação taxonômica de plantas.",
                                "Programação: Manipulação de dados com Python/pandas.",
                                "Visualização de Dados: Uso de gráficos para comunicação científica."
                              ],
                              "realWorldApplication": "Em agricultura, analisar dados de sensores em plantações para prever pragas (dados) → identificar padrões regionais (info) → modelos preditivos otimizando colheitas (conhecimento), reduzindo perdas em 20-30%."
                            },
                            "estimatedTime": "2 horas",
                            "difficulty": "beginner",
                            "status": "not_started",
                            "prerequisites": []
                          }
                        ]
                      },
                      {
                        "id": "10.1.2.2.2",
                        "name": "Assunção de Ambiente i.i.d.",
                        "description": "Princípio fundamental de que os dados de treinamento e teste são independentes e identicamente distribuídos (i.i.d.), essencial para a validade do aprendizado.",
                        "specificSkills": [
                          {
                            "id": "10.1.2.2.2.1",
                            "name": "Definir independência e distribuição idêntica",
                            "description": "Explicar os conceitos de independência estatística (ausência de correlação entre amostras) e distribuição idêntica (mesma probabilidade subjacente para todas as amostras).",
                            "atomicExpansion": {
                              "steps": [
                                {
                                  "stepNumber": 1,
                                  "title": "Revisar Fundamentos de Probabilidade para Amostras",
                                  "subSteps": [
                                    "Revise a definição de amostra aleatória e espaço de probabilidade.",
                                    "Entenda a notação P(X) para a distribuição de probabilidade de uma variável aleatória X.",
                                    "Discuta o que significa uma sequência de amostras X1, X2, ..., Xn.",
                                    "Identifique exemplos cotidianos de sequências de eventos aleatórios.",
                                    "Anote as diferenças entre amostras dependentes e independentes intuitivamente."
                                  ],
                                  "verification": "Escreva um resumo de 3-5 frases explicando amostras aleatórias e forneça um exemplo pessoal.",
                                  "estimatedTime": "20 minutos",
                                  "materials": "Folha de papel, acesso a notas de probabilidade básica ou vídeo introdutório (ex: Khan Academy).",
                                  "tips": "Use analogias como 'lançamentos de moeda' para visualizar sequências de amostras.",
                                  "learningObjective": "Compreender o contexto probabilístico de amostras antes de independência e idêntica.",
                                  "commonMistakes": "Confundir amostras com população inteira; lembre-se que amostras são realizações finitas."
                                },
                                {
                                  "stepNumber": 2,
                                  "title": "Definir e Exemplificar Independência Estatística",
                                  "subSteps": [
                                    "Defina independência: Para amostras Xi e Xj, P(Xi, Xj) = P(Xi) * P(Xj).",
                                    "Explique ausência de correlação: independência implica Cov(Xi, Xj) = 0, mas não o inverso.",
                                    "Simule dois eventos independentes (ex: dois dados justos).",
                                    "Simule dependência (ex: sorteio sem reposição).",
                                    "Calcule probabilidades conjuntas manualmente para verificar."
                                  ],
                                  "verification": "Crie um exemplo numérico onde P(A e B) ≠ P(A)P(B) e outro onde é igual, mostrando cálculos.",
                                  "estimatedTime": "30 minutos",
                                  "materials": "Calculadora, papel para cálculos, Jupyter Notebook para simulação simples em Python (opcional).",
                                  "tips": "Pense em 'um resultado não influencia o outro' para intuição rápida.",
                                  "learningObjective": "Dominar a definição formal e intuitiva de independência entre amostras.",
                                  "commonMistakes": "Confundir independência com descorrelação; teste com variáveis não-lineares."
                                },
                                {
                                  "stepNumber": 3,
                                  "title": "Definir e Exemplificar Distribuição Idêntica",
                                  "subSteps": [
                                    "Defina distribuição idêntica: Todas as amostras Xi ~ P(X), mesma distribuição para i=1 a n.",
                                    "Discuta implicações: mesma média, variância, etc., para todas amostras.",
                                    "Exemplo: Múltiplos lançamentos de uma mesma moeda viciada.",
                                    "Contraste com não-idênticas: ex: amostras de populações diferentes ao longo do tempo.",
                                    "Verifique graficamente: histograma de amostras deve ser similar."
                                  ],
                                  "verification": "Gere 100 amostras de uma distribuição (ex: Normal) e confirme que estatísticas descritivas são consistentes.",
                                  "estimatedTime": "25 minutos",
                                  "materials": "Python com NumPy/Matplotlib ou calculadora para estatísticas manuais.",
                                  "tips": "Sempre pergunte: 'Todas as amostras vêm da mesma 'máquina' probabilística?'",
                                  "learningObjective": "Entender que idêntica significa 'mesmo processo gerador' para todas amostras.",
                                  "commonMistakes": "Assumir idêntica só para distribuições normais; aplica a qualquer P(X)."
                                },
                                {
                                  "stepNumber": 4,
                                  "title": "Integrar em Assunção i.i.d. e Discutir Implicações",
                                  "subSteps": [
                                    "Combine: i.i.d. = Independente E Idêntica Distribuição.",
                                    "Explique notação formal: {X1, ..., Xn} ~ i.i.d. P(X).",
                                    "Discuta por que i.i.d. é crucial em ML: permite generalização e teoremas de concentração.",
                                    "Identifique violações comuns em dados reais (ex: séries temporais).",
                                    "Crie um fluxograma resumindo os conceitos."
                                  ],
                                  "verification": "Escreva uma definição completa de i.i.d. e dê um exemplo de dataset que satisfaz/não satisfaz.",
                                  "estimatedTime": "25 minutos",
                                  "materials": "Papel para fluxograma, acesso a artigo curto sobre i.i.d. em ML.",
                                  "tips": "Lembre: i.i.d. é uma idealização; dados reais muitas vezes precisam de ajustes.",
                                  "learningObjective": "Aplicar os conceitos juntos no contexto de modelagem de dados.",
                                  "commonMistakes": "Ignorar que i.i.d. é uma assunção forte, não sempre realista."
                                }
                              ],
                              "practicalExample": "Em um experimento de lançamento de uma moeda justa 100 vezes: cada lançamento é independente (resultado anterior não afeta o próximo) e idêntico (todos têm P(cara)=0.5). Simule em Python: np.random.binomial(1, 0.5, 100) e verifique autocorrelação ≈0 e histograma uniforme.",
                              "finalVerifications": [
                                "Define corretamente independência com fórmula probabilística.",
                                "Explica distribuição idêntica com exemplo de consistência estatística.",
                                "Descreve i.i.d. como combinação dos dois conceitos.",
                                "Identifica um exemplo real onde i.i.d. falha (ex: dados de ações).",
                                "Calcula P(X1,X2) para amostras i.i.d. corretamente.",
                                "Explica impacto de violar i.i.d. em aprendizado de máquina."
                              ],
                              "assessmentCriteria": [
                                "Precisão na definição formal de independência (ausência de correlação via produto de probabilidades).",
                                "Clareza na distinção entre idêntica e independente.",
                                "Uso de exemplos concretos com cálculos ou simulações.",
                                "Compreensão de implicações em Ciência de Dados (generalização de modelos).",
                                "Identificação de erros comuns como confundir com estacionariedade.",
                                "Capacidade de criar verificações práticas para dados."
                              ],
                              "crossCurricularConnections": [
                                "Estatística: Teoremas de convergência como Lei dos Grandes Números.",
                                "Programação: Simulações em Python/R para testar i.i.d.",
                                "Ciência de Dados: Pré-processamento de dados para aproximar i.i.d.",
                                "Machine Learning: Assunções em validação cruzada e teoremas PAC."
                              ],
                              "realWorldApplication": "Em treinamento de modelos de ML, assumir dados i.i.d. permite prever performance em novos dados; usado em recomendadores da Netflix (amostras de usuários independentes e similares) ou detecção de fraudes (transações idênticas em distribuição)."
                            },
                            "estimatedTime": "1 hora",
                            "difficulty": "beginner",
                            "status": "not_started",
                            "prerequisites": []
                          },
                          {
                            "id": "10.1.2.2.2.2",
                            "name": "Identificar violações de i.i.d.",
                            "description": "Reconhecer cenários onde a assunção i.i.d. é violada, como dados temporais com dependência sequencial ou mudanças na distribuição (concept drift).",
                            "atomicExpansion": {
                              "steps": [
                                {
                                  "stepNumber": 1,
                                  "title": "Compreender a definição e componentes da assunção i.i.d.",
                                  "subSteps": [
                                    "Defina i.i.d. como Independent and Identically Distributed (dados independentes e com a mesma distribuição probabilística).",
                                    "Explique independência: ausência de correlação entre amostras (ex: P(X_{t+1}|X_t) = P(X_{t+1})).",
                                    "Explique distribuição idêntica: todas as amostras vêm da mesma distribuição P(X).",
                                    "Estude implicações para aprendizado de máquina: modelos assumem i.i.d. para validação cruzada e generalização.",
                                    "Revise exemplos ideais de i.i.d., como amostras aleatórias de uma urna."
                                  ],
                                  "verification": "Resuma em 3 frases os componentes de i.i.d. e dê um exemplo correto sem consulta.",
                                  "estimatedTime": "30 minutos",
                                  "materials": "Notas de aula sobre i.i.d., vídeo introdutório (ex: StatQuest no YouTube), papel e caneta.",
                                  "tips": "Use diagramas para visualizar independência (setas sem conexões entre pontos de dados).",
                                  "learningObjective": "Dominar os pilares teóricos de i.i.d. para basear detecções de violações.",
                                  "commonMistakes": "Confundir independência com ausência de variância; ignorar que i.i.d. é assunção, não realidade absoluta."
                                },
                                {
                                  "stepNumber": 2,
                                  "title": "Identificar tipos comuns de violações de i.i.d.",
                                  "subSteps": [
                                    "Estude violações de independência: dependência temporal (autocorrelação em séries temporais), dependência espacial (imagens vizinhas).",
                                    "Aprenda sobre concept drift: mudanças na distribuição ao longo do tempo (ex: P(X) muda de treino para teste).",
                                    "Explore covariate shift: mudança na distribuição de features, mas label condicional constante.",
                                    "Analise prior shift: mudança na distribuição de labels.",
                                    "Liste 5 violações com exemplos: dados de ações (temporal), spam emails (drift por novas táticas)."
                                  ],
                                  "verification": "Crie uma tabela com 4 tipos de violações, descrições e exemplos; revise sem erros.",
                                  "estimatedTime": "45 minutos",
                                  "materials": "Artigo 'i.i.d. Assumptions in ML' (ex: Towards Data Science), planilha Excel para tabela.",
                                  "tips": "Agrupe violações em 'independência quebrada' vs 'distribuição não idêntica' para memorização.",
                                  "learningObjective": "Catalogar violações padrão para reconhecimento rápido em cenários reais.",
                                  "commonMistakes": "Confundir concept drift com overfitting; subestimar violações espaciais em dados não-temporais."
                                },
                                {
                                  "stepNumber": 3,
                                  "title": "Analisar cenários e datasets para detectar violações",
                                  "subSteps": [
                                    "Colete um dataset não-i.i.d. (ex: série temporal de temperaturas via Kaggle).",
                                    "Calcule autocorrelação (ACF plot) para checar independência.",
                                    "Compare distribuições de treino/teste com KS-test ou histograms para drift.",
                                    "Visualize mudanças: plot temporal de features/labels.",
                                    "Documente evidências: 'Autocorr=0.8 indica dependência sequencial'."
                                  ],
                                  "verification": "Gere relatório de 1 página identificando violações em um dataset fornecido.",
                                  "estimatedTime": "60 minutos",
                                  "materials": "Python/Jupyter com pandas, matplotlib, statsmodels; dataset de série temporal (ex: Airline Passengers).",
                                  "tips": "Sempre plote dados crus primeiro: padrões visuais revelam violações antes de testes estatísticos.",
                                  "learningObjective": "Aplicar ferramentas analíticas para evidenciar violações empiricamente.",
                                  "commonMistakes": "Ignorar tamanho da amostra em testes (poder estatístico baixo); confundir ruído com dependência."
                                },
                                {
                                  "stepNumber": 4,
                                  "title": "Praticar diagnóstico e propor mitigações",
                                  "subSteps": [
                                    "Analise 3 cenários hipotéticos: dados de sensores IoT, reviews de produtos, tráfego web.",
                                    "Para cada um, identifique violação e sugira fixes (ex: windowing para temporal, domain adaptation para drift).",
                                    "Simule em código: divida dataset em treino/teste com drift artificial.",
                                    "Avalie impacto: treine modelo simples e compare performance i.i.d. vs violado.",
                                    "Reflita: quando i.i.d. é 'boa o suficiente' vs quando quebrar o modelo."
                                  ],
                                  "verification": "Apresente análise de 3 cenários com código e conclusões em um notebook compartilhável.",
                                  "estimatedTime": "75 minutos",
                                  "materials": "Jupyter Notebook, scikit-learn para modelo baseline (ex: regressão logística), datasets sintéticos.",
                                  "tips": "Use testes não-paramétricos como KS para robustez contra distribuições desconhecidas.",
                                  "learningObjective": "Integrar detecção com implicações práticas para modelagem robusta.",
                                  "commonMistakes": "Propor soluções sem validar violação; ignorar custo computacional de mitigações."
                                }
                              ],
                              "practicalExample": "Em um dataset de preços de ações diários (ex: AAPL), plote ACF mostrando autocorrelação alta (violação de independência). Compare distribuição de retornos 2010 vs 2020 via KS-test (p<0.05, concept drift devido a eventos como COVID). Isso invalida validação hold-out padrão.",
                              "finalVerifications": [
                                "Explica i.i.d. e lista 3 violações sem hesitação.",
                                "Detecta dependência temporal em plot ACF de série temporal.",
                                "Identifica concept drift comparando histograms de treino/teste.",
                                "Propõe mitigações apropriadas para 2 cenários dados.",
                                "Analisa dataset real e gera relatório com evidências estatísticas.",
                                "Discute quando violações são críticas para ML."
                              ],
                              "assessmentCriteria": [
                                "Precisão na definição de i.i.d. (100% componentes corretos).",
                                "Número e diversidade de violações identificadas (mín. 4 tipos).",
                                "Uso correto de ferramentas diagnósticas (plots, testes estatísticos).",
                                "Profundidade de análise em exemplos (evidências quantitativas).",
                                "Criatividade em mitigações e conexões reais.",
                                "Clareza no relatório (estrutura, visualizações)."
                              ],
                              "crossCurricularConnections": [
                                "Estatística: Testes de hipótese (KS, ACF) e distribuições probabilísticas.",
                                "Aprendizado de Máquina: Validação cruzada, robustez de modelos.",
                                "Ciência de Dados: Pré-processamento de séries temporais e domain adaptation.",
                                "Matemática: Probabilidade condicional e processos estocásticos."
                              ],
                              "realWorldApplication": "Em finanças, detectar violações em dados de mercado para evitar modelos falhos em forecasting; em saúde, identificar concept drift em dados de pacientes para atualizar modelos de previsão de epidemias; em recomendação, lidar com dependência sequencial em histórico de usuários para personalização precisa."
                            },
                            "estimatedTime": "1.5 horas",
                            "difficulty": "intermediate",
                            "status": "not_started",
                            "prerequisites": []
                          },
                          {
                            "id": "10.1.2.2.2.3",
                            "name": "Justificar importância para ML",
                            "description": "Argumentar por que a i.i.d. garante generalização de modelos e cite exemplos de falhas em contextos reais, como previsão de vendas sazonais.",
                            "atomicExpansion": {
                              "steps": [
                                {
                                  "stepNumber": 1,
                                  "title": "Compreender a definição de i.i.d.",
                                  "subSteps": [
                                    "Defina 'independentemente' (independência): eventos ou amostras não influenciam uns aos outros.",
                                    "Defina 'identically distributed' (mesma distribuição): todas as amostras vêm da mesma distribuição probabilística.",
                                    "Explique i.i.d. como a assunção fundamental em estatística e ML: dados de treino e teste são amostras i.i.d. da população.",
                                    "Compare com cenários não-i.i.d., como dados temporais sequenciais.",
                                    "Resuma em uma frase: 'i.i.d. assume que draws são independentes e da mesma P(X,Y).'"
                                  ],
                                  "verification": "Escreva uma definição clara de i.i.d. e dê um exemplo simples, como sorteios de dados justos.",
                                  "estimatedTime": "30 minutos",
                                  "materials": [
                                    "Livro de estatística introdutória (ex: 'Introduction to Probability' de Blitzstein)",
                                    "Notas ou slides sobre assunções em ML"
                                  ],
                                  "tips": "Use analogias cotidianas, como moedas lançadas isoladamente, para fixar o conceito.",
                                  "learningObjective": "Definir precisamente i.i.d. e distinguir independência de distribuição idêntica.",
                                  "commonMistakes": "Confundir independência com uniformidade; achar que i.i.d. implica dados idênticos (são distribuídos igualmente, mas variam)."
                                },
                                {
                                  "stepNumber": 2,
                                  "title": "Explicar o papel da i.i.d. na generalização de modelos",
                                  "subSteps": [
                                    "Descreva generalização: performance no treino ≈ performance em novos dados (teste).",
                                    "Ligue i.i.d.: sob i.i.d., risco empírico converge para risco verdadeiro (lei dos grandes números).",
                                    "Discuta teoremas fundamentais: Hoeffding's inequality ou VC dimension dependem de i.i.d. para bounds de generalização.",
                                    "Ilustre com diagrama: distribuição P → treino (i.i.d.) → modelo → teste (i.i.d. da mesma P).",
                                    "Argumente: sem i.i.d., overfit no treino não prediz teste."
                                  ],
                                  "verification": "Explique verbalmente ou por escrito por que i.i.d. permite que o erro de treino estime o erro populacional.",
                                  "estimatedTime": "45 minutos",
                                  "materials": [
                                    "Artigo 'Understanding Machine Learning' de Shalev-Shwartz (cap. 2)",
                                    "Ferramenta de desenho como Draw.io para diagramas"
                                  ],
                                  "tips": "Pense em termos de 'amostra representativa': i.i.d. garante que treino representa a população.",
                                  "learningObjective": "Articular como i.i.d. sustenta a teoria de generalização em ML.",
                                  "commonMistakes": "Ignorar que i.i.d. é assunção teórica, não sempre real; superestimar generalização sem ela."
                                },
                                {
                                  "stepNumber": 3,
                                  "title": "Analisar exemplos reais de falhas devido a violação de i.i.d.",
                                  "subSteps": [
                                    "Exemplo 1: Previsão de vendas sazonais – dados temporais têm dependência (não independentes), modelo overfita padrões passados mas falha em mudanças climáticas.",
                                    "Exemplo 2: Detecção de fraudes em transações – distribuição muda ao longo do tempo (drift), violando 'identically distributed'.",
                                    "Exemplo 3: Imagens médicas – amostras de pacientes correlacionadas por hospital (não independentes).",
                                    "Quantifique falhas: calcule métricas como MSE em treino vs. teste em dataset não-i.i.d.",
                                    "Discuta soluções iniciais: time-series models (ARIMA) ou domain adaptation."
                                  ],
                                  "verification": "Identifique violações de i.i.d. em um dataset fornecido e prediga falhas de generalização.",
                                  "estimatedTime": "60 minutos",
                                  "materials": [
                                    "Dataset Kaggle de vendas sazonais (ex: Rossmann Store Sales)",
                                    "Python/Jupyter com pandas e matplotlib para visualização"
                                  ],
                                  "tips": "Plote autocorrelações para detectar dependências ocultas nos dados.",
                                  "learningObjective": "Citar e analisar pelo menos 3 exemplos reais onde violação de i.i.d. causa falhas em ML.",
                                  "commonMistakes": "Usar exemplos irrelevantes; não distinguir dependência de mudança de distribuição."
                                },
                                {
                                  "stepNumber": 4,
                                  "title": "Formular argumentos completos justificando a importância para ML",
                                  "subSteps": [
                                    "Estruture argumento: Definição → Generalização → Evidências empíricas → Contraexemplos → Implicações práticas.",
                                    "Redija parágrafo: 'i.i.d. é crucial porque garante que minimizar perda empírica leva a bom desempenho esperado'.",
                                    "Inclua citações: Hastie et al. 'Elements of Statistical Learning' ou papers sobre covariate shift.",
                                    "Pratique debate: defenda contra objeções como 'ML funciona sem i.i.d.'.",
                                    "Sintetize: importância prática em validação cruzada e baselines i.i.d."
                                  ],
                                  "verification": "Escreva um ensaio curto (300 palavras) justificando i.i.d. com exemplos de falhas.",
                                  "estimatedTime": "45 minutos",
                                  "materials": [
                                    "Modelo de redação ou template argumentativo",
                                    "Referências: ESL book PDF"
                                  ],
                                  "tips": "Use estrutura PEEL (Point, Evidence, Explanation, Link) para argumentos coesos.",
                                  "learningObjective": "Construir argumentos persuasivos sobre a centralidade de i.i.d. em ML.",
                                  "commonMistakes": "Argumentos vagos sem evidências; ignorar nuances como i.i.d. fraca em deep learning."
                                }
                              ],
                              "practicalExample": "Em previsão de vendas sazonais de uma rede de varejo, um modelo treinado em dados de 2019 (i.i.d. assumido) falha em 2020 devido a COVID-19 alterando a distribuição (não identically distributed). O erro de treino é baixo, mas teste explode, ilustrando perda de generalização sem i.i.d.",
                              "finalVerifications": [
                                "Explicar i.i.d. corretamente em <1 minuto.",
                                "Desenhar diagrama ligando i.i.d. a generalização.",
                                "Citar 2 exemplos reais de falhas e soluções.",
                                "Escrever argumento de 1 parágrafo justificando importância.",
                                "Identificar violação de i.i.d. em dataset simples.",
                                "Discutir limitações de i.i.d. em ML moderno."
                              ],
                              "assessmentCriteria": [
                                "Precisão conceitual: definição e ligação a generalização corretas (30%).",
                                "Uso de exemplos: relevantes, concretos e analisados (25%).",
                                "Clareza argumentativa: lógico, persuasivo e estruturado (20%).",
                                "Profundidade: inclui teoremas ou evidências quantitativas (15%).",
                                "Originalidade: conexões pessoais ou extensões (10%)."
                              ],
                              "crossCurricularConnections": [
                                "Estatística: Leis dos grandes números e desigualdades de concentração.",
                                "Probabilidade: Distribuições conjuntas e independência.",
                                "Programação: Validação cruzada e detecção de drift em Python (scikit-learn).",
                                "Ética em IA: Viés de distribuição em aplicações sociais.",
                                "Física: Modelos estocásticos em simulações (ex: partículas i.i.d.)."
                              ],
                              "realWorldApplication": "Em finanças, bancos assumem i.i.d. para backtesting de modelos de risco; violações (crises como 2008) levam a subestimação de perdas, custando bilhões. Técnicas como stress testing mitigam, destacando necessidade de justificar e testar i.i.d."
                            },
                            "estimatedTime": "1 hora",
                            "difficulty": "beginner",
                            "status": "not_started",
                            "prerequisites": []
                          }
                        ]
                      },
                      {
                        "id": "10.1.2.2.3",
                        "name": "Tipos Básicos de Aprendizado",
                        "description": "Introdução aos paradigmas de aprendizado supervisionado e não-supervisionado, com foco em definições e diferenças.",
                        "specificSkills": [
                          {
                            "id": "10.1.2.2.3.1",
                            "name": "Caracterizar aprendizado supervisionado",
                            "description": "Descrever o aprendizado supervisionado como o processo de treinar modelos com dados rotulados (entradas e saídas conhecidas) para previsão ou classificação.",
                            "atomicExpansion": {
                              "steps": [
                                {
                                  "stepNumber": 1,
                                  "title": "Entender o Conceito de Dados Rotulados",
                                  "subSteps": [
                                    "Defina dados rotulados como pares de entradas (features) e saídas (labels) conhecidas.",
                                    "Identifique exemplos de entradas, como imagens de gatos com label 'gato'.",
                                    "Diferencie de dados não rotulados, onde saídas são desconhecidas.",
                                    "Explore fontes comuns de dados rotulados, como datasets públicos (ex: Iris dataset).",
                                    "Pratique identificando labels em um dataset simples."
                                  ],
                                  "verification": "Explique verbalmente ou por escrito o que são dados rotulados com um exemplo pessoal.",
                                  "estimatedTime": "30 minutos",
                                  "materials": "Notebook, acesso a datasets online (Kaggle Iris dataset), vídeo introdutório sobre ML.",
                                  "tips": "Use analogias cotidianas, como rotular fotos em um álbum familiar.",
                                  "learningObjective": "Compreender a base fundamental dos dados rotulados no aprendizado supervisionado.",
                                  "commonMistakes": "Confundir dados rotulados com não rotulados ou assumir que todos os dados precisam de labels manuais."
                                },
                                {
                                  "stepNumber": 2,
                                  "title": "Descrever o Processo de Treinamento do Modelo",
                                  "subSteps": [
                                    "Divida o dataset em treino, validação e teste.",
                                    "Explique como o modelo aprende padrões mapeando entradas para saídas durante epochs.",
                                    "Descreva o papel da função de perda (loss function) para medir erros.",
                                    "Discuta ajustes de parâmetros via backpropagation e otimizadores como Gradient Descent.",
                                    "Simule um ciclo de treinamento com um diagrama simples."
                                  ],
                                  "verification": "Desenhe um fluxograma do processo de treinamento e rotule cada etapa.",
                                  "estimatedTime": "45 minutos",
                                  "materials": "Papel e caneta para fluxograma, Python com scikit-learn instalado, tutorial Jupyter Notebook.",
                                  "tips": "Pense no modelo como um aluno estudando exemplos resolvidos para imitar respostas.",
                                  "learningObjective": "Mapear o fluxo completo de treinamento com dados rotulados.",
                                  "commonMistakes": "Ignorar a divisão de dados, levando a overfitting, ou pular a explicação da função de perda."
                                },
                                {
                                  "stepNumber": 3,
                                  "title": "Identificar Tarefas de Previsão e Classificação",
                                  "subSteps": [
                                    "Defina classificação como prever categorias discretas (ex: spam/não spam).",
                                    "Defina regressão como prever valores contínuos (ex: preço de casa).",
                                    "Compare métricas: acurácia para classificação, MSE para regressão.",
                                    "Analise exemplos: classificar flores Iris ou prever temperaturas.",
                                    "Crie uma tabela comparativa entre os dois tipos."
                                  ],
                                  "verification": "Classifique e preveja manualmente em um mini-dataset fornecido.",
                                  "estimatedTime": "40 minutos",
                                  "materials": "Dataset Iris (CSV), planilha Excel ou Google Sheets, vídeo sobre regressão vs classificação.",
                                  "tips": "Use cores diferentes para visualizar classes em gráficos de dispersão.",
                                  "learningObjective": "Distinguir e exemplificar as principais tarefas do aprendizado supervisionado.",
                                  "commonMistakes": "Confundir regressão com classificação ou ignorar a natureza discreta/contínua das saídas."
                                },
                                {
                                  "stepNumber": 4,
                                  "title": "Caracterizar Vantagens e Limitações",
                                  "subSteps": [
                                    "Liste vantagens: precisão alta com labels, interpretabilidade em tarefas específicas.",
                                    "Descreva limitações: custo de rotulagem, necessidade de dados balanceados.",
                                    "Compare brevemente com aprendizado não supervisionado.",
                                    "Discuta cenários ideais de uso (ex: quando labels estão disponíveis).",
                                    "Resuma a definição completa em um parágrafo coeso."
                                  ],
                                  "verification": "Escreva um resumo de 200 palavras caracterizando o aprendizado supervisionado.",
                                  "estimatedTime": "35 minutos",
                                  "materials": "Editor de texto, artigos comparativos sobre tipos de ML (ex: Towards Data Science).",
                                  "tips": "Estruture o resumo como: definição + processo + tarefas + prós/contras.",
                                  "learningObjective": "Sintetizar uma caracterização completa e precisa do aprendizado supervisionado.",
                                  "commonMistakes": "Superestimar generalização sem dados de teste ou omitir limitações reais."
                                }
                              ],
                              "practicalExample": "Treinar um modelo para classificar e-mails como 'spam' ou 'não spam' usando um dataset rotulado como o Enron Spam Dataset: entradas são palavras-chave e remetente, saídas são labels 'spam' (1) ou 'ham' (0). Divida em 80% treino, treine com Naive Bayes, avalie acurácia em 20% teste.",
                              "finalVerifications": [
                                "Definição precisa de aprendizado supervisionado com dados rotulados.",
                                "Descrição correta do processo de treinamento e divisão de dados.",
                                "Exemplos válidos de classificação e regressão.",
                                "Identificação de pelo menos duas vantagens e limitações.",
                                "Resumo coeso integrando todos os elementos.",
                                "Capacidade de diferenciar de aprendizado não supervisionado."
                              ],
                              "assessmentCriteria": [
                                "Precisão conceitual (30%): Definições exatas sem erros.",
                                "Profundidade de explicação (25%): Cobertura completa de processo e tarefas.",
                                "Uso de exemplos (20%): Relevância e concretude dos exemplos.",
                                "Clareza e estrutura (15%): Organização lógica em steps e resumo.",
                                "Compreensão de limitações (10%): Inclusão equilibrada de prós/contras."
                              ],
                              "crossCurricularConnections": [
                                "Estatística: Funções de perda e métricas de avaliação.",
                                "Programação: Implementação em Python com bibliotecas como scikit-learn.",
                                "Matemática: Otimização via gradiente descendente.",
                                "Ética: Viés em dados rotulados e privacidade."
                              ],
                              "realWorldApplication": "Em diagnósticos médicos, treinar modelos com imagens de raio-X rotuladas como 'pneumonia' ou 'normal' para auxiliar médicos em classificações rápidas e precisas, reduzindo erros humanos em hospitais."
                            },
                            "estimatedTime": "1 hora",
                            "difficulty": "beginner",
                            "status": "not_started",
                            "prerequisites": []
                          },
                          {
                            "id": "10.1.2.2.3.2",
                            "name": "Caracterizar aprendizado não-supervisionado",
                            "description": "Explicar o aprendizado não-supervisionado como descoberta de padrões em dados não rotulados, como clustering ou redução de dimensionalidade.",
                            "atomicExpansion": {
                              "steps": [
                                {
                                  "stepNumber": 1,
                                  "title": "Definir Aprendizado Não-Supervisionado",
                                  "subSteps": [
                                    "Leia a definição oficial: descoberta de padrões em dados não rotulados sem orientação externa.",
                                    "Identifique características chave: ausência de labels, foco em estrutura inerente dos dados.",
                                    "Escreva sua própria definição em 2-3 frases.",
                                    "Liste 3 exemplos de dados não rotulados do dia a dia (ex: histórico de compras sem categorias).",
                                    "Desenhe um diagrama simples mostrando dados de entrada vs saída implícita."
                                  ],
                                  "verification": "Explique a definição em suas palavras para um colega e confirme se ele entendeu sem erros.",
                                  "estimatedTime": "25 minutos",
                                  "materials": [
                                    "Notebook para anotações",
                                    "Artigo introdutório sobre ML (ex: Wikipedia - Unsupervised Learning)"
                                  ],
                                  "tips": "Comece com intuição: imagine organizar uma pilha de fotos desconhecidas por similaridade.",
                                  "learningObjective": "Compreender e articular a definição essencial de aprendizado não-supervisionado.",
                                  "commonMistakes": [
                                    "Confundir com aprendizado supervisionado (que usa labels)",
                                    "Achar que precisa de rótulos ocultos"
                                  ]
                                },
                                {
                                  "stepNumber": 2,
                                  "title": "Comparar com Aprendizado Supervisionado e de Reforço",
                                  "subSteps": [
                                    "Revise definições rápidas: supervisionado (dados rotulados, predição), reforço (agente e recompensas).",
                                    "Crie uma tabela comparativa com colunas: tipo, dados de entrada, objetivo, exemplos.",
                                    "Preencha a tabela com pelo menos 3 linhas de comparação.",
                                    "Discuta 2 cenários onde não-supervisionado é preferível (ex: dados caros para rotular).",
                                    "Resuma as diferenças em um mapa mental."
                                  ],
                                  "verification": "Apresente a tabela comparativa e responda perguntas sobre diferenças chave.",
                                  "estimatedTime": "30 minutos",
                                  "materials": [
                                    "Planilha ou papel para tabela",
                                    "Vídeo curto de 5 min sobre tipos de ML (ex: YouTube - 3Blue1Brown)"
                                  ],
                                  "tips": "Use analogias: supervisionado é como um professor corrigindo provas; não-supervisionado é explorar sozinho.",
                                  "learningObjective": "Diferenciar aprendizado não-supervisionado de outros tipos para caracterizá-lo claramente.",
                                  "commonMistakes": [
                                    "Ignorar o reforço, focando só em supervisionado",
                                    "Misturar redução de dimensionalidade com classificação"
                                  ]
                                },
                                {
                                  "stepNumber": 3,
                                  "title": "Explorar Técnicas Principais: Clustering e Redução de Dimensionalidade",
                                  "subSteps": [
                                    "Estude clustering: agrupe dados similares (ex: K-means divide em K grupos).",
                                    "Aprenda redução de dimensionalidade: preserve info essencial (ex: PCA projeta em menos dimensões).",
                                    "Liste 2 algoritmos por técnica com funcionamento básico.",
                                    "Simule manualmente clustering em 5 pontos 2D no papel.",
                                    "Compare: clustering encontra grupos; redução simplifica visualização."
                                  ],
                                  "verification": "Descreva como K-means e PCA funcionam em um parágrafo cada e demonstre com desenho.",
                                  "estimatedTime": "40 minutos",
                                  "materials": [
                                    "Papel e caneta para simulação",
                                    "Notebook Jupyter com scikit-learn (opcional para visualização)"
                                  ],
                                  "tips": "Visualize: clustering é como separar frutas por cor/tamanho sem saber nomes.",
                                  "learningObjective": "Identificar e caracterizar técnicas centrais do aprendizado não-supervisionado.",
                                  "commonMistakes": [
                                    "Confundir clustering com classificação",
                                    "Achar que PCA cria labels"
                                  ]
                                },
                                {
                                  "stepNumber": 4,
                                  "title": "Caracterizar Vantagens, Desafios e Avaliação",
                                  "subSteps": [
                                    "Liste 3 vantagens: descobre padrões ocultos, lida com dados não rotulados, escalável.",
                                    "Liste 3 desafios: difícil validar resultados, sensível a ruído, escolha de parâmetros.",
                                    "Aprenda métricas: silhouette score para clustering, variância explicada para PCA.",
                                    "Discuta quando usar: dados exploratórios vs preditivos.",
                                    "Escreva um resumo de 1 página caracterizando o aprendizado não-supervisionado."
                                  ],
                                  "verification": "Debata prós/contras em um fórum ou com parceiro e refine o resumo.",
                                  "estimatedTime": "35 minutos",
                                  "materials": [
                                    "Documentos para resumo",
                                    "Referência rápida de métricas ML"
                                  ],
                                  "tips": "Pense em validação interna: resultados devem fazer sentido humano.",
                                  "learningObjective": "Avaliar contextos ideais e limitações para uma caracterização completa.",
                                  "commonMistakes": [
                                    "Superestimar precisão sem métricas",
                                    "Ignorar necessidade de interpretação humana"
                                  ]
                                }
                              ],
                              "practicalExample": "Aplique K-means no dataset de vinhos (sem labels) para clusterizar variedades por características químicas, visualizando clusters com PCA para redução de 13 dimensões para 2D, revelando padrões ocultos de qualidade.",
                              "finalVerifications": [
                                "Defina aprendizado não-supervisionado corretamente sem mencionar labels.",
                                "Dê 2 exemplos precisos de técnicas (clustering e redução).",
                                "Compare com supervisionado em uma frase chave.",
                                "Explique uma métrica de avaliação.",
                                "Descreva um cenário real onde é útil.",
                                "Identifique um desafio comum."
                              ],
                              "assessmentCriteria": [
                                "Precisão e completude da definição (30%)",
                                "Correção nas comparações e técnicas (25%)",
                                "Uso de exemplos e analogias claras (20%)",
                                "Compreensão de vantagens/desafios (15%)",
                                "Capacidade de aplicação prática (10%)"
                              ],
                              "crossCurricularConnections": [
                                "Estatística: medidas de distância e variância",
                                "Programação: implementação em Python (scikit-learn)",
                                "Biologia: clusterização de sequências genéticas",
                                "Negócios: segmentação de mercado",
                                "Visualização de dados: gráficos interativos"
                              ],
                              "realWorldApplication": "Em e-commerce, clustering não-supervisionado segmenta clientes por padrões de navegação em dados não rotulados, permitindo campanhas personalizadas; em saúde, PCA reduz dimensionalidade de imagens médicas para detecção eficiente de anomalias."
                            },
                            "estimatedTime": "1 hora",
                            "difficulty": "beginner",
                            "status": "not_started",
                            "prerequisites": []
                          },
                          {
                            "id": "10.1.2.2.3.3",
                            "name": "Comparar supervisionado e não-supervisionado",
                            "description": "Comparar os dois tipos em termos de requisitos de dados, objetivos e exemplos (ex.: regressão vs. K-means), destacando trade-offs.",
                            "atomicExpansion": {
                              "steps": [
                                {
                                  "stepNumber": 1,
                                  "title": "Entender os Fundamentos do Aprendizado Supervisionado",
                                  "subSteps": [
                                    "Defina aprendizado supervisionado como um método onde o modelo é treinado com dados rotulados (entradas e saídas conhecidas).",
                                    "Identifique objetivos principais: prever valores contínuos (regressão) ou classes discretas (classificação).",
                                    "Liste exemplos: regressão linear para prever preços de casas, classificação para detectar spam em e-mails.",
                                    "Explique o processo: o modelo aprende padrões dos rótulos para generalizar em novos dados.",
                                    "Anote requisitos iniciais: necessidade de dataset grande e rotulado."
                                  ],
                                  "verification": "Crie um diagrama simples mostrando entrada rotulada → modelo → predição, e explique verbalmente.",
                                  "estimatedTime": "20 minutos",
                                  "materials": [
                                    "Notebook para anotações",
                                    "Artigo introdutório sobre ML supervisionado (ex.: Scikit-learn docs)"
                                  ],
                                  "tips": "Use analogias como um professor corrigindo provas (rótulos) para ensinar o aluno (modelo).",
                                  "learningObjective": "Compreender a definição, objetivos e exemplos básicos de aprendizado supervisionado.",
                                  "commonMistakes": [
                                    "Confundir com não-supervisionado achando que não precisa de rótulos",
                                    "Ignorar que regressão é para valores contínuos, não categóricos"
                                  ]
                                },
                                {
                                  "stepNumber": 2,
                                  "title": "Entender os Fundamentos do Aprendizado Não-Supervisionado",
                                  "subSteps": [
                                    "Defina aprendizado não-supervisionado como método com dados não-rotulados, focando em padrões inerentes.",
                                    "Identifique objetivos: descobrir clusters (clustering), reduzir dimensões ou detectar anomalias.",
                                    "Liste exemplos: K-means para segmentar clientes por comportamento, PCA para visualização de dados.",
                                    "Explique o processo: o modelo infere estruturas sem orientação externa.",
                                    "Anote requisitos: datasets grandes, mas sem rótulos, permitindo exploração."
                                  ],
                                  "verification": "Desenhe um fluxograma de dados não-rotulados → algoritmo → grupos/padrões descobertos.",
                                  "estimatedTime": "20 minutos",
                                  "materials": [
                                    "Notebook",
                                    "Vídeo curto sobre K-means (ex.: YouTube Khan Academy)"
                                  ],
                                  "tips": "Pense como um detetive encontrando padrões em evidências sem solução prévia.",
                                  "learningObjective": "Dominar definição, objetivos e exemplos de aprendizado não-supervisionado.",
                                  "commonMistakes": [
                                    "Achar que é 'aleatório' sem estrutura",
                                    "Confundir clustering com classificação"
                                  ]
                                },
                                {
                                  "stepNumber": 3,
                                  "title": "Comparar Requisitos de Dados, Objetivos e Exemplos",
                                  "subSteps": [
                                    "Compare requisitos: supervisionado precisa rótulos (caro de obter); não-supervisionado usa dados brutos (mais acessível).",
                                    "Contraste objetivos: supervisionado prevê saídas específicas; não-supervisionado explora padrões ocultos.",
                                    "Mapeie exemplos lado a lado: regressão (sup.) vs. K-means (não-sup.); classifique trade-offs iniciais.",
                                    "Crie uma tabela comparativa com colunas: Tipo, Dados, Objetivo, Exemplo.",
                                    "Discuta cenários onde um se aplica melhor que o outro."
                                  ],
                                  "verification": "Preencha e revise uma tabela comparativa com pelo menos 4 linhas de diferenças.",
                                  "estimatedTime": "25 minutos",
                                  "materials": [
                                    "Planilha ou papel para tabela",
                                    "Dataset exemplo como Iris (sup.) e Mall Customers (não-sup.)"
                                  ],
                                  "tips": "Use cores na tabela: verde para vantagens de cada tipo.",
                                  "learningObjective": "Realizar comparação direta entre os dois tipos em dimensões chave.",
                                  "commonMistakes": [
                                    "Ignorar custo de rotulagem em dados supervisionados",
                                    "Misturar objetivos, achando clustering prediz valores"
                                  ]
                                },
                                {
                                  "stepNumber": 4,
                                  "title": "Analisar Trade-offs e Sintetizar a Comparação",
                                  "subSteps": [
                                    "Liste trade-offs: supervisionado é preciso mas requer dados rotulados; não-supervisionado é flexível mas interpretativo.",
                                    "Discuta quando usar cada: sup. para tarefas com histórico rotulado; não-sup. para dados novos/exploratórios.",
                                    "Crie um fluxograma de decisão: 'Tenho rótulos? Sim → Sup.; Não → Não-sup.'",
                                    "Resuma prós/contras em bullet points.",
                                    "Aplique a um caso hipotético: análise de vendas."
                                  ],
                                  "verification": "Explique trade-offs para um par (ex.: regressão vs K-means) em 1 minuto.",
                                  "estimatedTime": "25 minutos",
                                  "materials": [
                                    "Ferramenta de fluxograma simples (ex.: Draw.io gratuito)",
                                    "Resumo escrito"
                                  ],
                                  "tips": "Priorize trade-offs quantitativos como 'precisão vs. custo de dados'.",
                                  "learningObjective": "Identificar e justificar trade-offs para seleção adequada de método.",
                                  "commonMistakes": [
                                    "Superestimar precisão não-supervisionado sem validação",
                                    "Não considerar híbridos como semi-supervisionado"
                                  ]
                                }
                              ],
                              "practicalExample": "Em um dataset de e-commerce: Supervisionado usa rótulos de 'compra/não-compra' para prever churn (regressão logística); Não-supervisionado usa K-means para clusterizar clientes por padrões de navegação sem rótulos, revelando segmentos ocultos como 'compradores impulsivos'. Trade-off: sup. prevê exatamente, mas precisa de rótulos históricos; não-sup. descobre novos insights rapidamente.",
                              "finalVerifications": [
                                "Explique diferenças em requisitos de dados sem hesitação.",
                                "Forneça 2 exemplos precisos para cada tipo.",
                                "Descreva 3 trade-offs principais.",
                                "Crie tabela comparativa correta.",
                                "Aplique comparação a um dataset real.",
                                "Justifique escolha de método para um cenário dado."
                              ],
                              "assessmentCriteria": [
                                "Precisão nas definições (supervisionado: rotulado; não-sup: não-rotulado).",
                                "Completude da comparação (dados, objetivos, exemplos, trade-offs).",
                                "Uso de exemplos relevantes (regressão vs. K-means).",
                                "Clareza na tabela ou fluxograma.",
                                "Profundidade nos trade-offs (custo vs. flexibilidade).",
                                "Capacidade de aplicação prática."
                              ],
                              "crossCurricularConnections": [
                                "Estatística: Distribuições e testes de hipóteses para validar modelos.",
                                "Programação: Implementação em Python (Scikit-learn para ambos tipos).",
                                "Negócios: Decisões baseadas em predições (sup.) vs. segmentação de mercado (não-sup.).",
                                "Visualização de Dados: Gráficos de clusters vs. curvas de predição.",
                                "Ética em IA: Viés em dados rotulados vs. interpretação subjetiva em clusters."
                              ],
                              "realWorldApplication": "Em marketing digital, empresas como Netflix usam supervisionado para recomendar filmes baseados em avaliações passadas (rótulos), enquanto usam não-supervisionado para descobrir gêneros emergentes em visualizações anônimas, otimizando estratégias sem custo alto de rotulagem."
                            },
                            "estimatedTime": "1.5 horas",
                            "difficulty": "intermediate",
                            "status": "not_started",
                            "prerequisites": []
                          },
                          {
                            "id": "10.1.2.2.3.4",
                            "name": "Relacionar com etapas da Ciência de Dados",
                            "description": "Mapear os tipos de aprendizado às etapas de ajuste e avaliação de modelos na pipeline de Ciência de Dados.",
                            "atomicExpansion": {
                              "steps": [
                                {
                                  "stepNumber": 1,
                                  "title": "Compreender os tipos básicos de aprendizado de máquina",
                                  "subSteps": [
                                    "Estude as definições de aprendizado supervisionado, não supervisionado e por reforço.",
                                    "Identifique exemplos clássicos para cada tipo: regressão/classificação (supervisionado), clustering/PCA (não supervisionado), agente em ambiente (reforço).",
                                    "Compare as diferenças em termos de dados de entrada (com ou sem rótulos) e objetivos.",
                                    "Anote as características principais de cada um em uma tabela.",
                                    "Revise recursos online como documentação do Scikit-learn para ilustrações."
                                  ],
                                  "verification": "Criar uma tabela resumindo os 3 tipos com definições, exemplos e diferenças.",
                                  "estimatedTime": "45 minutos",
                                  "materials": "Notebook ou papel para tabela, acesso à internet para docs Scikit-learn e vídeos Khan Academy.",
                                  "tips": "Use analogias cotidianas, como ensinar uma criança com exemplos rotulados (supervisionado).",
                                  "learningObjective": "Definir e diferenciar os tipos básicos de aprendizado de máquina.",
                                  "commonMistakes": "Confundir não supervisionado com ausência de dados, ignorando que pode usar dados sem rótulos."
                                },
                                {
                                  "stepNumber": 2,
                                  "title": "Identificar as etapas de ajuste e avaliação na pipeline de Ciência de Dados",
                                  "subSteps": [
                                    "Revise a pipeline completa de DS: coleta, limpeza, feature engineering, modelagem, avaliação e deployment.",
                                    "Foque nas etapas de modelagem: split train/test, fit (ajuste do modelo), validação cruzada, tuning de hiperparâmetros.",
                                    "Aprenda métricas de avaliação: accuracy, precision/recall (supervisionado), silhouette score (não supervisionado).",
                                    "Desenhe um fluxograma da pipeline destacando ajuste e avaliação.",
                                    "Estude casos onde avaliação guia iterações no ajuste."
                                  ],
                                  "verification": "Desenhar e rotular um fluxograma da pipeline com ênfase em ajuste e avaliação.",
                                  "estimatedTime": "50 minutos",
                                  "materials": "Ferramenta de desenho como Draw.io ou papel, vídeo tutorial sobre CRISP-DM ou pipeline DS no YouTube.",
                                  "tips": "Lembre que ajuste é 'treinar o modelo nos dados', avaliação é 'testar em dados não vistos'.",
                                  "learningObjective": "Descrever as etapas específicas de ajuste e avaliação na pipeline de DS.",
                                  "commonMistakes": "Confundir ajuste com feature engineering ou ignorar validação cruzada."
                                },
                                {
                                  "stepNumber": 3,
                                  "title": "Mapear os tipos de aprendizado às etapas de ajuste e avaliação",
                                  "subSteps": [
                                    "Mapeie supervisionado: ajuste com dados rotulados (fit em train set), avaliação com métricas supervisionadas.",
                                    "Mapeie não supervisionado: ajuste em dados não rotulados (clustering), avaliação interna (ex: inertia).",
                                    "Mapeie reforço: usado em tuning hiperparâmetros ou otimização sequencial durante avaliação iterativa.",
                                    "Crie uma matriz de mapeamento: linhas=tipos de aprendizado, colunas=etapas (fit, validação, tuning).",
                                    "Discuta integrações: ensemble methods combinam tipos."
                                  ],
                                  "verification": "Preencher uma matriz de mapeamento com exemplos e justificativas para cada célula.",
                                  "estimatedTime": "60 minutos",
                                  "materials": "Planilha Excel/Google Sheets para matriz, exemplos de código Python simples.",
                                  "tips": "Pense em 'supervisionado precisa de labels no fit', não supervisionado não.",
                                  "learningObjective": "Estabelecer correspondências precisas entre tipos de aprendizado e etapas de DS.",
                                  "commonMistakes": "Aplicar métricas supervisionadas em modelos não supervisionados."
                                },
                                {
                                  "stepNumber": 4,
                                  "title": "Praticar e validar o mapeamento com cenários reais",
                                  "subSteps": [
                                    "Escolha um dataset supervisionado (ex: Iris) e execute fit/avaliação.",
                                    "Aplique clustering (KMeans) em dataset não supervisionado e avalie.",
                                    "Simule tuning com grid search (reforço implícito) e observe iterações.",
                                    "Compare os processos e documente similaridades/diferenças.",
                                    "Responda perguntas: 'Qual tipo para prever churn? Para segmentar clientes?'"
                                  ],
                                  "verification": "Executar códigos Jupyter e gerar relatório com mapeamentos observados.",
                                  "estimatedTime": "90 minutos",
                                  "materials": "Jupyter Notebook, bibliotecas Scikit-learn/Pandas, datasets Iris/Wine do Seaborn.",
                                  "tips": "Comece com códigos prontos e modifique para entender o fit/avaliação.",
                                  "learningObjective": "Aplicar mapeamentos em prática e identificar contextos adequados.",
                                  "commonMistakes": "Overfitting ao não separar train/test adequadamente."
                                }
                              ],
                              "practicalExample": "Em uma empresa de e-commerce: Use aprendizado supervisionado para ajustar um modelo de regressão que prevê vendas futuras (fit nos dados históricos rotulados de vendas passadas, avaliação com RMSE em test set). Para segmentação de clientes sem labels, aplique KMeans não supervisionado (ajuste em features de comportamento, avaliação com silhouette score). Tuning de hiperparâmetros via grid search simula reforço iterativo.",
                              "finalVerifications": [
                                "Explicar verbalmente o mapeamento de supervisionado para fit com labels.",
                                "Identificar métrica correta para avaliação de clustering.",
                                "Desenhar pipeline destacando onde cada tipo se aplica.",
                                "Resolver quiz com 5 cenários de DS mapeando tipos corretamente.",
                                "Criar tabela de mapeamento sem erros.",
                                "Executar código simples de fit/avaliação para dois tipos."
                              ],
                              "assessmentCriteria": [
                                "Precisão conceitual nos tipos de aprendizado (90% correto).",
                                "Correção nos mapeamentos para etapas de ajuste/avaliação.",
                                "Profundidade nos substeps e exemplos práticos fornecidos.",
                                "Qualidade da matriz/fluxograma criada.",
                                "Capacidade de aplicação em cenários reais sem confusões.",
                                "Identificação de erros comuns evitados no prático."
                              ],
                              "crossCurricularConnections": [
                                "Estatística: Métricas de avaliação ligadas a testes de hipótese.",
                                "Programação: Implementação em Python/R para pipelines.",
                                "Matemática: Otimização em reforço e funções de perda.",
                                "Negócios: Aplicação em análise preditiva e segmentação de mercado."
                              ],
                              "realWorldApplication": "Em pipelines de ML de empresas como Netflix (supervisionado para recomendações, ajuste em ratings de usuários) ou bancos (não supervisionado para detecção de fraudes via clustering, avaliação interna), otimizando decisões baseadas em dados com avaliações precisas para produção."
                            },
                            "estimatedTime": "1 hora",
                            "difficulty": "beginner",
                            "status": "not_started",
                            "prerequisites": []
                          }
                        ]
                      }
                    ]
                  },
                  {
                    "id": "10.1.2.3",
                    "name": "Ambiente Independente e Identicamente Distribuído (IID)",
                    "description": "Assunção de que os dados são independentes e seguem a mesma distribuição probabilística.",
                    "individualConcepts": [
                      {
                        "id": "10.1.2.3.1",
                        "name": "Independência dos Dados",
                        "description": "Conceito de que as observações nos dados não influenciam umas às outras, ou seja, o valor de uma observação é independente do valor de qualquer outra.",
                        "specificSkills": [
                          {
                            "id": "10.1.2.3.1.1",
                            "name": "Definir independência estatística",
                            "description": "Explicar formalmente a independência estatística entre variáveis aleatórias, usando probabilidades condicionais: P(X,Y) = P(X)P(Y).",
                            "atomicExpansion": {
                              "steps": [
                                {
                                  "stepNumber": 1,
                                  "title": "Revisar Conceitos Básicos de Probabilidade",
                                  "subSteps": [
                                    "Defina variável aleatória discreta e contínua com exemplos simples.",
                                    "Explique a probabilidade conjunta P(X,Y) como a probabilidade de ambos os eventos ocorrerem simultaneamente.",
                                    "Calcule probabilidades marginais P(X) e P(Y) a partir da conjunta.",
                                    "Discuta o espaço amostral e eventos independentes no senso intuitivo.",
                                    "Resolva um exercício simples de probabilidade conjunta."
                                  ],
                                  "verification": "Responda corretamente a 3 perguntas sobre probabilidades marginais e conjuntas em um quiz.",
                                  "estimatedTime": "45 minutos",
                                  "materials": [
                                    "Livro de probabilidade básica (capítulo 2)",
                                    "Planilha ou calculadora para probabilidades",
                                    "Vídeo introdutório sobre variáveis aleatórias (Khan Academy)"
                                  ],
                                  "tips": "Use tabelas de contingência para visualizar probabilidades conjuntas.",
                                  "learningObjective": "Compreender os fundamentos de probabilidades para variáveis aleatórias múltiplas.",
                                  "commonMistakes": [
                                    "Confundir probabilidade conjunta com condicional",
                                    "Ignorar a soma das probabilidades marginais para 1"
                                  ]
                                },
                                {
                                  "stepNumber": 2,
                                  "title": "Entender Probabilidades Condicionais",
                                  "subSteps": [
                                    "Defina P(X|Y) = P(X,Y)/P(Y) e interprete como 'probabilidade de X dado Y'.",
                                    "Construa uma tabela de probabilidades condicionais a partir de uma conjunta.",
                                    "Diferencie dependência e independência usando exemplos cotidianos (ex: chuva e guarda-chuva).",
                                    "Calcule P(Y|X) e compare com P(X|Y) em cenários assimétricos.",
                                    "Resolva 2 problemas numéricos envolvendo condicionais."
                                  ],
                                  "verification": "Construa corretamente uma tabela condicional a partir de dados fornecidos.",
                                  "estimatedTime": "50 minutos",
                                  "materials": [
                                    "Tabela de contingência em Excel ou papel",
                                    "Exercícios de probabilidade condicional (PDF)",
                                    "Simulador online de probabilidades"
                                  ],
                                  "tips": "Sempre verifique se P(Y) > 0 antes de dividir na fórmula condicional.",
                                  "learningObjective": "Dominar o cálculo e interpretação de probabilidades condicionais.",
                                  "commonMistakes": [
                                    "Dividir por zero em P(Y)",
                                    "Confundir P(X|Y) com P(Y|X)"
                                  ]
                                },
                                {
                                  "stepNumber": 3,
                                  "title": "Formalizar a Definição de Independência Estatística",
                                  "subSteps": [
                                    "Estabeleça a definição: X e Y são independentes se P(X,Y) = P(X)P(Y) para todo x,y.",
                                    "Prove que isso implica P(X|Y) = P(X) e P(Y|X) = P(Y).",
                                    "Derive a equivalência entre as três formas de independência (conjunta, condicional simétrica).",
                                    "Escreva a definição em notação matemática formal.",
                                    "Identifique cenários onde a independência falha."
                                  ],
                                  "verification": "Escreva e prove a equivalência das definições de independência em um parágrafo.",
                                  "estimatedTime": "60 minutos",
                                  "materials": [
                                    "Quadro branco ou papel para derivações",
                                    "Notas de aula sobre probabilidade",
                                    "Artigo introdutório sobre independência (Wikipedia)"
                                  ],
                                  "tips": "Lembre-se: independência é uma propriedade simétrica e forte.",
                                  "learningObjective": "Explicar formalmente a independência usando probabilidades condicionais.",
                                  "commonMistakes": [
                                    "Achar que correlação zero implica independência",
                                    "Esquecer que vale para todas as combinações x,y"
                                  ]
                                },
                                {
                                  "stepNumber": 4,
                                  "title": "Verificar e Aplicar a Independência em Exemplos",
                                  "subSteps": [
                                    "Teste independência em uma tabela de contingência: verifique se P(X,Y) = P(X)P(Y).",
                                    "Simule dados independentes (ex: dois dados justos) e dependentes (ex: dado carregado).",
                                    "Discuta implicações para amostras IID em ciência de dados.",
                                    "Crie um contraexemplo onde intuição falha.",
                                    "Resuma a definição em suas próprias palavras."
                                  ],
                                  "verification": "Analise um dataset pequeno e conclua corretamente se as variáveis são independentes.",
                                  "estimatedTime": "40 minutos",
                                  "materials": [
                                    "Python ou R para simulação (opcional)",
                                    "Datasets de exemplo (CSV simples)",
                                    "Calculadora gráfica"
                                  ],
                                  "tips": "Use testes chi-quadrado intuitivos para verificação prática.",
                                  "learningObjective": "Aplicar a definição para verificar independência em dados reais.",
                                  "commonMistakes": [
                                    "Assumir independência baseada em gráficos visuais apenas",
                                    "Ignorar valores pequenos nas probabilidades"
                                  ]
                                }
                              ],
                              "practicalExample": "Considere dois lançamentos independentes de uma moeda justa: X = cara no primeiro, Y = cara no segundo. P(X=1,Y=1) = 0.25, P(X=1)=0.5, P(Y=1)=0.5. Verifique: 0.25 = 0.5 * 0.5, logo independentes. Agora, se Y depender do primeiro (repetir se cara), P(X=1,Y=1) ≠ P(X)P(Y).",
                              "finalVerifications": [
                                "Explique a definição formal sem olhar notas.",
                                "Prove P(X|Y)=P(X) a partir de P(X,Y)=P(X)P(Y).",
                                "Identifique independência em uma tabela 2x2 fornecida.",
                                "Dê um exemplo real de dependência vs independência.",
                                "Discuta por que independência é crucial para IID.",
                                "Resolva um problema numérico completo."
                              ],
                              "assessmentCriteria": [
                                "Precisão na definição formal (P(X,Y)=P(X)P(Y)).",
                                "Correta derivação de probabilidades condicionais.",
                                "Uso apropriado de exemplos concretos.",
                                "Identificação correta de erros comuns.",
                                "Clareza na explicação intuitiva vs formal.",
                                "Aplicação a contextos de ciência de dados."
                              ],
                              "crossCurricularConnections": [
                                "Estatística: Testes de independência (chi-quadrado).",
                                "Machine Learning: Suposições IID em algoritmos de treinamento.",
                                "Física: Eventos independentes em mecânica quântica.",
                                "Economia: Modelos de risco com variáveis independentes.",
                                "Computação: Geração de dados sintéticos independentes."
                              ],
                              "realWorldApplication": "Em ciência de dados, a independência estatística é assumida em dados IID para treinamento de modelos de ML, como regressão linear ou redes neurais, permitindo que amostras sejam tratadas como representativas do distribuição conjunta sem viés de dependência."
                            },
                            "estimatedTime": "1 hora",
                            "difficulty": "beginner",
                            "status": "not_started",
                            "prerequisites": []
                          },
                          {
                            "id": "10.1.2.3.1.2",
                            "name": "Identificar exemplos de independência e dependência",
                            "description": "Diferenciar cenários reais onde dados são independentes (ex: lançamentos de moedas) versus dependentes (ex: medidas sequenciais de temperatura).",
                            "atomicExpansion": {
                              "steps": [
                                {
                                  "stepNumber": 1,
                                  "title": "Compreender os Conceitos Básicos de Independência e Dependência de Dados",
                                  "subSteps": [
                                    "Defina independência de dados: eventos ou observações onde o resultado de um não afeta o outro.",
                                    "Defina dependência de dados: eventos ou observações onde um influencia o próximo.",
                                    "Compare com exemplos simples: independência como sorteios separados, dependência como cadeia de eventos.",
                                    "Identifique a diferença chave: probabilidade condicional P(A|B) = P(A) para independentes.",
                                    "Registre definições em um glossário pessoal."
                                  ],
                                  "verification": "Escreva definições claras e compare com fontes confiáveis; sem erros conceituais.",
                                  "estimatedTime": "20 minutos",
                                  "materials": "Papel e caneta ou editor de texto; vídeo introdutório sobre probabilidade (ex: Khan Academy).",
                                  "tips": "Use analogias cotidianas como 'lançar moedas' para fixar o conceito.",
                                  "learningObjective": "Dominar definições fundamentais para distinguir independência de dependência.",
                                  "commonMistakes": "Confundir independência com aleatoriedade; assumir que todos os dados reais são independentes."
                                },
                                {
                                  "stepNumber": 2,
                                  "title": "Explorar Exemplos Clássicos de Dados Independentes",
                                  "subSteps": [
                                    "Analise lançamentos de moedas: cada lançamento é independente do anterior.",
                                    "Simule 10 lançamentos e calcule frequências; observe que resultados passados não preveem futuros.",
                                    "Estude sorteios de dados: P(cara|último foi coroa) = P(cara).",
                                    "Crie um diagrama de árvore para 3 lançamentos mostrando multiplicação de probabilidades.",
                                    "Teste com software: gere dados aleatórios em Python (random.choice)."
                                  ],
                                  "verification": "Simulação mostra que probabilidades condicionais igualam marginais.",
                                  "estimatedTime": "30 minutos",
                                  "materials": "Moedas ou dados físicos; Python ou Google Colab com numpy/random.",
                                  "tips": "Repita experimentos múltiplas vezes para ver padrões emergirem.",
                                  "learningObjective": "Reconhecer e simular cenários IID (Independent and Identically Distributed).",
                                  "commonMistakes": "Ignorar a 'identicamente distribuída' parte; focar só em independência."
                                },
                                {
                                  "stepNumber": 3,
                                  "title": "Analisar Exemplos de Dados Dependentes",
                                  "subSteps": [
                                    "Examine medidas sequenciais de temperatura: valor atual depende do anterior devido a inércia climática.",
                                    "Simule uma série temporal: use AR(1) model simples onde X_t = 0.8*X_{t-1} + erro.",
                                    "Compare com draws sem reposição em um baralho: probabilidade muda após cada carta.",
                                    "Plote autocorrelação para dados dependentes vs independentes.",
                                    "Registre como P(atual|passado) ≠ P(atual)."
                                  ],
                                  "verification": "Gráficos mostram correlação serial em dependentes, ausência em independentes.",
                                  "estimatedTime": "35 minutos",
                                  "materials": "Planilha Excel ou Python (pandas, matplotlib); dados de temperatura reais (ex: NOAA).",
                                  "tips": "Use dados reais para tornar concreto; foque em séries temporais.",
                                  "learningObjective": "Identificar dependências em contextos sequenciais ou condicionais.",
                                  "commonMistakes": "Confundir dependência com variabilidade; achar que ruído torna independente."
                                },
                                {
                                  "stepNumber": 4,
                                  "title": "Praticar Diferenciação e Identificação em Cenários Mistos",
                                  "subSteps": [
                                    "Classifique 5 cenários: ex. chutes de pênalti (indep.), preços de ações (dep.).",
                                    "Crie um fluxograma de decisão: 'Resultado passado afeta próximo?'.",
                                    "Debata com parceiro ou autoquestionamento casos ambíguos.",
                                    "Aplique a dados reais: analise dataset IID vs não-IID.",
                                    "Resuma em tabela: cenário, tipo, justificativa."
                                  ],
                                  "verification": "Tabela completa com 100% acurácia em classificações padrão.",
                                  "estimatedTime": "25 minutos",
                                  "materials": "Lista de 10 cenários impressa; Jupyter notebook para análise.",
                                  "tips": "Comece com óbvios, avance para sutis como dados de sensores IoT.",
                                  "learningObjective": "Aplicar conceitos para classificar novos exemplos com confiança.",
                                  "commonMistakes": "Super-generalizar: assumir independência em finanças ou clima."
                                }
                              ],
                              "practicalExample": "Em um experimento: Lance uma moeda 5 vezes (independente: cada cara/coroa é 50%, independente de anteriores). Meça temperatura ambiente a cada minuto por 10 min (dependente: flutuações são correlacionadas devido a tendências térmicas). Classifique e justifique usando probabilidades condicionais.",
                              "finalVerifications": [
                                "Classificar corretamente 8/10 cenários mistos como independente ou dependente.",
                                "Explicar P(A|B) ≠ P(A) para dependentes com exemplo numérico.",
                                "Simular e plotar dados IID vs dependentes mostrando diferenças visuais.",
                                "Identificar IID em contextos de ML como amostras de treinamento padrão.",
                                "Discutir impacto de violar independência em modelos estatísticos.",
                                "Criar 3 exemplos originais com justificativa correta."
                              ],
                              "assessmentCriteria": [
                                "Precisão conceitual: definições sem ambiguidades (30%).",
                                "Exemplos relevantes e variados: pelo menos 4 independentes e 4 dependentes (25%).",
                                "Análise quantitativa: uso correto de probabilidades/simulações (20%).",
                                "Classificação em cenários reais: acurácia >90% (15%).",
                                "Clareza na comunicação: diagramas/tabelas legíveis (10%)."
                              ],
                              "crossCurricularConnections": [
                                "Probabilidade e Estatística: cálculo de probabilidades condicionais.",
                                "Ciência de Dados: premissa IID em machine learning e validação de modelos.",
                                "Física: séries temporais em termodinâmica e dinâmica de sistemas.",
                                "Economia: modelagem de séries financeiras dependentes (ex: ARIMA)."
                              ],
                              "realWorldApplication": "Em machine learning, assumir dados IID permite usar algoritmos como regressão linear; violar (ex: dados temporais de sensores) requer modelos como LSTM, evitando previsões enviesadas em apps de previsão climática ou trading algorítmico."
                            },
                            "estimatedTime": "1.5 horas",
                            "difficulty": "beginner",
                            "status": "not_started",
                            "prerequisites": []
                          },
                          {
                            "id": "10.1.2.3.1.3",
                            "name": "Explicar impactos da violação da independência",
                            "description": "Discutir como a dependência nos dados pode levar a vieses em modelos de aprendizado de máquina e estatísticos.",
                            "atomicExpansion": {
                              "steps": [
                                {
                                  "stepNumber": 1,
                                  "title": "Compreender o Conceito de Independência em Dados IID",
                                  "subSteps": [
                                    "Defina independência estatística: eventos ou observações onde o resultado de uma não afeta a outra.",
                                    "Explique a assunção IID (Independent and Identically Distributed): dados independentes e da mesma distribuição.",
                                    "Discuta por que IID é fundamental para teoremas como Lei dos Grandes Números e Teorema Central do Limite.",
                                    "Compare com dependência: correlação serial ou espacial entre observações.",
                                    "Revise definições formais usando covariância zero entre variáveis aleatórias distintas."
                                  ],
                                  "verification": "Escreva uma definição clara de IID e dê um exemplo simples de violação; revise com um colega ou autoavaliação.",
                                  "estimatedTime": "45 minutos",
                                  "materials": [
                                    "Livro 'Introduction to Statistical Learning' (cap. 2), Notebook Jupyter para simulações básicas",
                                    "Artigo Khan Academy sobre independência probabilística"
                                  ],
                                  "tips": "Use diagramas de Venn para visualizar independência vs. dependência.",
                                  "learningObjective": "Dominar a definição e importância da independência em contextos estatísticos.",
                                  "commonMistakes": "Confundir independência com não-correlação; assumir que dados reais sempre seguem IID perfeitamente."
                                },
                                {
                                  "stepNumber": 2,
                                  "title": "Identificar Tipos Comuns de Violação da Independência",
                                  "subSteps": [
                                    "Classifique violações: autocorrelação temporal (séries temporais), dependência espacial (mapas geográficos), dependência em clusters (redes sociais).",
                                    "Gere dados simulados dependentes usando Python (ex: AR(1) process).",
                                    "Analise testes diagnósticos: Durbin-Watson para autocorrelação, Moran’s I para espacial.",
                                    "Colete exemplos reais: dados de ações (tendências), sensores IoT (ruído correlacionado).",
                                    "Documente cenários onde IID falha em datasets comuns como MNIST ou financeiros."
                                  ],
                                  "verification": "Crie um gráfico de autocorrelação de um dataset simulado e identifique picos significativos.",
                                  "estimatedTime": "60 minutos",
                                  "materials": [
                                    "Python com pandas, statsmodels e matplotlib",
                                    "Dataset de séries temporais do Kaggle (ex: Airline Passengers)"
                                  ],
                                  "tips": "Comece com simulações simples para visualizar dependência antes de dados reais.",
                                  "learningObjective": "Reconhecer e diagnosticar violações de independência em dados práticos.",
                                  "commonMistakes": "Ignorar dependência em amostras pequenas; confundir ruído aleatório com dependência real."
                                },
                                {
                                  "stepNumber": 3,
                                  "title": "Analisar Impactos da Violação em Modelos Estatísticos e de ML",
                                  "subSteps": [
                                    "Explique vieses em inferência: estimadores enviesados, intervalos de confiança inválidos.",
                                    "Discuta overfitting/underfitting em ML: modelos de ML 'aprendem' padrões espúrios da dependência.",
                                    "Simule treinamento de regressão linear em dados dependentes vs. independentes; compare erros.",
                                    "Aborde impactos em validação cruzada: vazamento de informação entre folds.",
                                    "Quantifique com métricas: aumento no erro de predição out-of-sample devido a dependência."
                                  ],
                                  "verification": "Treine dois modelos idênticos em dados IID vs. dependentes e compare MSE em conjunto de teste.",
                                  "estimatedTime": "75 minutos",
                                  "materials": [
                                    "Scikit-learn para regressão",
                                    "Jupyter Notebook com seeds fixas para reprodutibilidade"
                                  ],
                                  "tips": "Use train-test split estratificado e valide com walk-forward validation para séries temporais.",
                                  "learningObjective": "Quantificar e explicar como dependência leva a vieses e desempenho pobre em modelos.",
                                  "commonMistakes": "Avaliar modelos dependentes com validação cruzada padrão; ignorar viés de seleção em testes."
                                },
                                {
                                  "stepNumber": 4,
                                  "title": "Explorar Mitigações e Implicações Práticas",
                                  "subSteps": [
                                    "Liste métodos de correção: differencing para autocorrelação, modelos hierárquicos para clusters.",
                                    "Aplique em ML: TimeSeriesSplit, embeddings para dependência espacial.",
                                    "Discuta implicações éticas: decisões enviesadas em saúde ou finanças.",
                                    "Crie um relatório resumindo impactos e soluções para um caso hipotético.",
                                    "Debata trade-offs: complexidade de modelos vs. assunções relaxadas."
                                  ],
                                  "verification": "Implemente uma mitgação em um dataset dependente e demonstre melhoria no desempenho.",
                                  "estimatedTime": "50 minutos",
                                  "materials": [
                                    "Documentação scikit-learn TimeSeriesSplit",
                                    "Paper 'Pitfalls of IID Assumption in ML' (buscar online)"
                                  ],
                                  "tips": "Sempre reporte diagnósticos de resíduos antes de finalizar um modelo.",
                                  "learningObjective": "Propor soluções acionáveis para violações de independência.",
                                  "commonMistakes": "Aplicar soluções genéricas sem diagnóstico prévio; subestimar custo computacional de mitigações."
                                }
                              ],
                              "practicalExample": "Em um dataset de preços de ações diárias (ex: AAPL), as observações são autocorrelacionadas devido a tendências de mercado. Treinar um modelo de regressão logística IID padrão superestima precisão (MSE baixo em treino, alto em teste futuro), levando a previsões falhas em crashes de mercado.",
                              "finalVerifications": [
                                "Explicar verbalmente três impactos principais da violação de independência.",
                                "Identificar dependência em um gráfico de ACF fornecido.",
                                "Comparar resultados de modelo em dados IID vs. dependentes.",
                                "Propor pelo menos duas mitigações adequadas para um cenário dado.",
                                "Discutir um exemplo real de viés causado por dependência.",
                                "Passar em quiz com 90% de acerto sobre conceitos chave."
                              ],
                              "assessmentCriteria": [
                                "Precisão conceitual: definições corretas de IID e violações (30%)",
                                "Análise quantitativa: simulações e métricas comparativas (25%)",
                                "Exemplos práticos: relevância e concretude (20%)",
                                "Profundidade de impactos: cobertura de vieses, overfitting e inferência (15%)",
                                "Mitigações propostas: viabilidade e adequação (10%)"
                              ],
                              "crossCurricularConnections": [
                                "Estatística: Teoremas assintóticos e inferência clássica.",
                                "Programação: Manipulação de dados com Python/R e visualizações.",
                                "Ética em IA: Vieses em decisões automatizadas.",
                                "Economia/Finanças: Modelagem de séries temporais dependentes."
                              ],
                              "realWorldApplication": "Em sistemas de recomendação do Netflix, ignorar dependência temporal nos cliques de usuários leva a recomendações enviesadas para tendências passageiras, reduzindo retenção; mitigações como modelos LSTM melhoram precisão em 20-30%."
                            },
                            "estimatedTime": "1 hora",
                            "difficulty": "intermediate",
                            "status": "not_started",
                            "prerequisites": []
                          }
                        ]
                      },
                      {
                        "id": "10.1.2.3.2",
                        "name": "Identicamente Distribuídos",
                        "description": "Assunção de que todas as observações são extraídas da mesma distribuição probabilística desconhecida.",
                        "specificSkills": [
                          {
                            "id": "10.1.2.3.2.1",
                            "name": "Definir distribuição probabilística idêntica",
                            "description": "Descrever que variáveis X1, X2, ..., Xn são i.i.d. se cada Xi ~ P, para a mesma distribuição P.",
                            "atomicExpansion": {
                              "steps": [
                                {
                                  "stepNumber": 1,
                                  "title": "Entender o conceito básico de distribuição probabilística",
                                  "subSteps": [
                                    "Revise a definição de uma distribuição probabilística como uma função que atribui probabilidades a possíveis resultados de uma variável aleatória.",
                                    "Identifique exemplos simples, como a distribuição uniforme de um dado justo.",
                                    "Diferencie entre distribuições discretas e contínuas com diagramas.",
                                    "Pratique descrevendo a distribuição de uma moeda viciada.",
                                    "Anote as propriedades essenciais: soma das probabilidades igual a 1."
                                  ],
                                  "verification": "Escreva uma definição própria e um exemplo; compare com fontes confiáveis.",
                                  "estimatedTime": "20 minutos",
                                  "materials": [
                                    "Papel e caneta",
                                    "Livro de probabilidade ou site como Khan Academy"
                                  ],
                                  "tips": "Use analogias cotidianas, como previsão do tempo, para visualizar distribuições.",
                                  "learningObjective": "Compreender o que é uma distribuição probabilística e suas propriedades fundamentais.",
                                  "commonMistakes": "Confundir distribuição com valor esperado; lembre-se que é sobre probabilidades de outcomes."
                                },
                                {
                                  "stepNumber": 2,
                                  "title": "Introduzir variáveis aleatórias múltiplas",
                                  "subSteps": [
                                    "Defina uma variável aleatória Xi como uma função que mapeia outcomes para números.",
                                    "Considere um conjunto X1, X2, ..., Xn de variáveis aleatórias.",
                                    "Descreva que cada Xi tem sua própria distribuição probabilística.",
                                    "Compare distribuições diferentes: Xi ~ Bernoulli(0.5), Xj ~ Bernoulli(0.8).",
                                    "Visualize com histogramas lado a lado para múltiplas amostras."
                                  ],
                                  "verification": "Gere amostras fictícias e plote suas distribuições empíricas.",
                                  "estimatedTime": "25 minutos",
                                  "materials": [
                                    "Calculadora ou Python/Jupyter Notebook",
                                    "Gráficos impressos ou software como Desmos"
                                  ],
                                  "tips": "Pense em Xi como clones potenciais; foque na similaridade das distribuições.",
                                  "learningObjective": "Reconhecer como múltiplas variáveis aleatórias podem compartilhar ou não distribuições.",
                                  "commonMistakes": "Assumir que variáveis com mesmo nome têm mesma distribuição; verifique explicitamente."
                                },
                                {
                                  "stepNumber": 3,
                                  "title": "Definir 'identicamente distribuídas'",
                                  "subSteps": [
                                    "Estabeleça que X1, X2, ..., Xn são identicamente distribuídas se ∀i, Xi ~ P, para a mesma P.",
                                    "Formalize: As funções de distribuição cumulativa (CDF) são iguais para todas Xi.",
                                    "Teste com notação: P(X1 ≤ x) = P(X2 ≤ x) = ... = P(Xn ≤ x).",
                                    "Dê contraexemplos: Alturas de pessoas diferentes vs. clones idênticos.",
                                    "Escreva a definição em palavras e símbolos matemáticos."
                                  ],
                                  "verification": "Crie um exemplo onde distribuições coincidem e prove com cálculos de probabilidade.",
                                  "estimatedTime": "30 minutos",
                                  "materials": [
                                    "Folha de fórmulas de probabilidade",
                                    "Software para simulação como R ou Python"
                                  ],
                                  "tips": "Use a notação ~ P consistentemente para enfatizar identidade.",
                                  "learningObjective": "Dominar a definição precisa de variáveis identicamente distribuídas.",
                                  "commonMistakes": "Confundir com independência; 'identically' é sobre mesma distribuição, não correlação."
                                },
                                {
                                  "stepNumber": 4,
                                  "title": "Aplicar a definição no contexto de i.i.d.",
                                  "subSteps": [
                                    "Integre com independência: i.i.d. significa independentemente E identicamente distribuídas.",
                                    "Foque na parte 'identically': Cada Xi ~ P idêntica.",
                                    "Simule dados i.i.d. de uma distribuição P (ex: normal padrão).",
                                    "Verifique empiricamente: Histogramas de subamostras devem sobrepor.",
                                    "Redija uma descrição completa para X1, ..., Xn ~ P i.i.d."
                                  ],
                                  "verification": "Explique para um par: 'Por quê essas variáveis são identicamente distribuídas?'",
                                  "estimatedTime": "25 minutos",
                                  "materials": [
                                    "Python com NumPy/Matplotlib para simulações",
                                    "Exemplos de datasets"
                                  ],
                                  "tips": "Sempre especifique 'a mesma P' para evitar ambiguidades.",
                                  "learningObjective": "Aplicar a definição de distribuição idêntica em contextos de múltiplas variáveis.",
                                  "commonMistakes": "Ignorar que i.i.d. requer ambas independência e identidade; isole a parte 'identically'."
                                }
                              ],
                              "practicalExample": "Considere o lançamento repetido de um dado justo: X1, X2, ..., X10 onde cada Xi é o resultado do i-ésimo lançamento. Cada Xi ~ Uniforme{1,2,3,4,5,6}, a mesma distribuição P, logo são identicamente distribuídos.",
                              "finalVerifications": [
                                "Pode definir corretamente 'identicamente distribuídas' usando notação Xi ~ P?",
                                "Distingue exemplos verdadeiros de falsos (ex: moedas idênticas vs. moedas com biases diferentes)?",
                                "Simula dados e confirma sobreposição de distribuições empíricas?",
                                "Explica a diferença entre 'identically' e 'independent' em i.i.d.?",
                                "Redige uma descrição verbal precisa sem erros?",
                                "Aplica a definição a um novo cenário real?"
                              ],
                              "assessmentCriteria": [
                                "Precisão na definição matemática (uso correto de ~ P)",
                                "Clareza na distinção de independência",
                                "Uso de exemplos concretos e contraexemplos",
                                "Habilidade em verificação empírica via simulação",
                                "Compreensão contextual em i.i.d. para Ciência de Dados",
                                "Capacidade de comunicação escrita/oral fluida"
                              ],
                              "crossCurricularConnections": [
                                "Estatística: Fundamento para teoremas do limite central",
                                "Ciência de Dados: Assunção padrão em modelagem de ML",
                                "Programação: Geração de dados sintéticos em Python/R",
                                "Física: Modelagem de partículas idênticas em simulações"
                              ],
                              "realWorldApplication": "Em machine learning, assumir que amostras de dados de treinamento são i.i.d. permite generalização de modelos; por exemplo, em análise de imagens de gatos, cada imagem é tratada como amostra idêntica de uma distribuição subjacente de 'imagens de gatos'."
                            },
                            "estimatedTime": "1 hora",
                            "difficulty": "beginner",
                            "status": "not_started",
                            "prerequisites": []
                          },
                          {
                            "id": "10.1.2.3.2.2",
                            "name": "Reconhecer exemplos de dados i.i.d.",
                            "description": "Analisar conjuntos de dados como amostras de uma urna ou dados gerados por RNG para verificar se seguem a mesma distribuição.",
                            "atomicExpansion": {
                              "steps": [
                                {
                                  "stepNumber": 1,
                                  "title": "Compreender os Conceitos Fundamentais de Dados i.i.d.",
                                  "subSteps": [
                                    "Defina 'independente': eventos ou observações onde o resultado de uma não afeta as outras.",
                                    "Defina 'identicamente distribuídos': todas as observações vêm da mesma distribuição de probabilidade.",
                                    "Explique i.i.d. como combinação: independência + mesma distribuição.",
                                    "Diferencie de dados dependentes (ex: séries temporais) e não idênticos (ex: amostras de populações diferentes).",
                                    "Visualize com diagrama: múltiplas moedas idênticas lançadas independentemente."
                                  ],
                                  "verification": "Resuma os conceitos em suas próprias palavras e forneça um exemplo simples de i.i.d. vs. não i.i.d.",
                                  "estimatedTime": "20 minutos",
                                  "materials": [
                                    "Papel e caneta para anotações",
                                    "Vídeo introdutório sobre i.i.d. (ex: Khan Academy)"
                                  ],
                                  "tips": [
                                    "Use analogias cotidianas como lançamentos de moedas para fixar o conceito.",
                                    "Desenhe fluxogramas para visualizar independência."
                                  ],
                                  "learningObjective": "Entender a definição precisa de dados i.i.d. e suas componentes.",
                                  "commonMistakes": [
                                    "Confundir independência com uniformidade.",
                                    "Achar que i.i.d. implica em valores iguais."
                                  ]
                                },
                                {
                                  "stepNumber": 2,
                                  "title": "Identificar Independência em Conjuntos de Dados",
                                  "subSteps": [
                                    "Analise correlações: calcule coeficiente de correlação entre pares consecutivos; deve ser próximo de zero.",
                                    "Teste visual: plote scatterplots; não deve haver padrões lineares ou clusters dependentes.",
                                    "Simule dados dependentes vs. independentes usando RNG em Python (ex: numpy.random).",
                                    "Verifique com testes estatísticos simples como teste de independência qui-quadrado.",
                                    "Compare histogramas de subconjuntos aleatórios; devem ser semelhantes."
                                  ],
                                  "verification": "Gere um conjunto de 100 amostras independentes e dependentes; confirme correlação baixa nos independentes.",
                                  "estimatedTime": "30 minutos",
                                  "materials": [
                                    "Python com NumPy e Matplotlib",
                                    "Jupyter Notebook"
                                  ],
                                  "tips": [
                                    "Comece com amostras pequenas (n=10) para prática rápida.",
                                    "Use seed para reprodutibilidade."
                                  ],
                                  "learningObjective": "Reconhecer padrões de dependência em dados.",
                                  "commonMistakes": [
                                    "Ignorar dependências não lineares.",
                                    "Confundir ruído aleatório com independência verdadeira."
                                  ]
                                },
                                {
                                  "stepNumber": 3,
                                  "title": "Verificar Distribuição Idêntica em Dados",
                                  "subSteps": [
                                    "Divida o conjunto em subamostras iguais e compare histogramas superpostos.",
                                    "Calcule estatísticas descritivas (média, variância, skewness) para cada subamostra; devem ser similares.",
                                    "Aplique teste KS (Kolmogorov-Smirnov) entre subamostras para igualdade de distribuições.",
                                    "Simule urna: extraia bolas coloridas múltiplas vezes sem reposição vs. com reposição.",
                                    "Gere dados RNG de uma distribuição (ex: normal) e valide uniformidade."
                                  ],
                                  "verification": "Para um dataset, prove que subamostras têm KS p-value > 0.05.",
                                  "estimatedTime": "35 minutos",
                                  "materials": [
                                    "Python com SciPy para testes KS",
                                    "Ferramenta online de simulação de urna (ex: Desmos)"
                                  ],
                                  "tips": [
                                    "Use boxplots para comparação rápida de distribuições.",
                                    "Aumente tamanho da amostra para maior confiança."
                                  ],
                                  "learningObjective": "Detectar se dados seguem a mesma distribuição subjacente.",
                                  "commonMistakes": [
                                    "Usar amostras muito pequenas levando a falsos positivos.",
                                    "Confundir variância amostral com diferença de distribuição."
                                  ]
                                },
                                {
                                  "stepNumber": 4,
                                  "title": "Analisar Exemplos Práticos de Dados i.i.d.",
                                  "subSteps": [
                                    "Exemplo urna: simule extrações com reposição; verifique independência e identidade.",
                                    "Exemplo RNG: gere números aleatórios uniformes; teste subamostras.",
                                    "Caso não i.i.d.: simule série temporal (ex: temperatura diária) e identifique falhas.",
                                    "Crie relatório: classifique 3 datasets como i.i.d. ou não, justificando.",
                                    "Pratique com datasets reais (ex: Iris sem shuffle vs. shuffleado)."
                                  ],
                                  "verification": "Classifique corretamente 5 exemplos mistos com justificativa estatística.",
                                  "estimatedTime": "25 minutos",
                                  "materials": [
                                    "Datasets de exemplo (sklearn.datasets)",
                                    "Python ou R para análise"
                                  ],
                                  "tips": [
                                    "Registre p-values de testes para suporte quantitativo.",
                                    "Compare sempre com baseline não i.i.d.."
                                  ],
                                  "learningObjective": "Aplicar reconhecimento de i.i.d. em cenários concretos.",
                                  "commonMistakes": [
                                    "Assumir i.i.d. sem testes.",
                                    "Ignorar viés de seleção em simulações."
                                  ]
                                }
                              ],
                              "practicalExample": "Simule uma urna com 50% bolas vermelhas e 50% azuis. Extraia 100 bolas com reposição (i.i.d.). Plote histogramas de subamostras de 20 extrações cada; eles devem coincidir. Agora, sem reposição (dependente); histogramas divergem progressivamente.",
                              "finalVerifications": [
                                "Correlação entre pares consecutivos < 0.1.",
                                "Teste KS p-value > 0.05 entre todas subamostras.",
                                "Estatísticas descritivas similares (média ±5%, variância ±10%).",
                                "Scatterplot sem padrões visíveis de dependência.",
                                "Classificação correta de 5 exemplos mistos.",
                                "Relatório escrito explicando evidências."
                              ],
                              "assessmentCriteria": [
                                "Precisão na definição de i.i.d. (100%).",
                                "Uso correto de testes estatísticos (pelo menos 2 testes aplicados).",
                                "Análise visual e quantitativa integrada.",
                                "Identificação de contraexemplos não i.i.d.",
                                "Relatório claro com evidências.",
                                "Tempo de execução dentro do estimado."
                              ],
                              "crossCurricularConnections": [
                                "Probabilidade: Modelos de urna e RNG.",
                                "Estatística: Testes de hipótese (KS, correlação).",
                                "Programação: Simulações em Python/NumPy.",
                                "Ciência de Dados: Pressupostos em ML (ex: treinamento/teste IID)."
                              ],
                              "realWorldApplication": "Em Machine Learning, assumir dados i.i.d. garante que modelos treinados em uma amostra generalizem para novas; usado em validação cruzada, previsão de vendas independentes ou detecção de fraudes em transações bancárias simuladas como extrações i.i.d."
                            },
                            "estimatedTime": "1.5 horas",
                            "difficulty": "beginner",
                            "status": "not_started",
                            "prerequisites": []
                          },
                          {
                            "id": "10.1.2.3.2.3",
                            "name": "Testar assunção de distribuição idêntica",
                            "description": "Usar testes estatísticos simples como Kolmogorov-Smirnov para verificar se subamostras têm a mesma distribuição.",
                            "atomicExpansion": {
                              "steps": [
                                {
                                  "stepNumber": 1,
                                  "title": "Compreender os Conceitos de Distribuições Idênticas",
                                  "subSteps": [
                                    "Defina o que significa 'distribuições idênticas' no contexto IID (Independent and Identically Distributed).",
                                    "Explique por que testar essa assunção é crucial em modelagem estatística e machine learning.",
                                    "Diferencie entre hipóteses nula (distribuições iguais) e alternativa (distribuições diferentes).",
                                    "Revise medidas de similaridade de distribuições, como CDFs (Cumulative Distribution Functions).",
                                    "Estude exemplos visuais de distribuições idênticas vs. não idênticas usando histogramas e Q-Q plots."
                                  ],
                                  "verification": "Resuma em suas palavras a importância do teste e desenhe dois histogramas comparativos.",
                                  "estimatedTime": "30 minutos",
                                  "materials": [
                                    "Notebook para anotações",
                                    "Vídeos introdutórios sobre IID (ex: Khan Academy Estatística)"
                                  ],
                                  "tips": "Use analogias como 'duas moedas justas vs. uma viciada' para fixar o conceito.",
                                  "learningObjective": "Entender a base teórica da assunção de distribuições idênticas.",
                                  "commonMistakes": "Confundir independência com identidade de distribuição."
                                },
                                {
                                  "stepNumber": 2,
                                  "title": "Aprender o Teste de Kolmogorov-Smirnov (KS)",
                                  "subSteps": [
                                    "Estude a definição do teste KS: comparação máxima entre duas CDFs empíricas.",
                                    "Aprenda as fórmulas para estatística KS (D = sup |F1(x) - F2(x)|) e distribuição assintótica.",
                                    "Entenda níveis de significância (ex: alpha=0.05) e interpretação de p-value.",
                                    "Compare KS com outros testes (ex: Anderson-Darling, Chi-Quadrado).",
                                    "Pratique cálculos manuais simples com dados pequenos (n=5 por amostra)."
                                  ],
                                  "verification": "Calcule manualmente o D para dois conjuntos de dados pequenos e interprete.",
                                  "estimatedTime": "45 minutos",
                                  "materials": [
                                    "Documentação SciPy.stats.kstest",
                                    "Calculadora ou planilha Excel"
                                  ],
                                  "tips": "Lembre-se: KS é não-paramétrico e bom para qualquer distribuição contínua.",
                                  "learningObjective": "Dominar a mecânica e interpretação do teste KS.",
                                  "commonMistakes": "Ignorar que KS assume continuidade e independência das amostras."
                                },
                                {
                                  "stepNumber": 3,
                                  "title": "Preparar e Simular Dados para Teste",
                                  "subSteps": [
                                    "Gere dados simulados de distribuições idênticas (ex: duas normais N(0,1)).",
                                    "Gere dados de distribuições diferentes (ex: N(0,1) vs. N(1,1)).",
                                    "Divida um dataset real em subamostras (ex: train/test split).",
                                    "Visualize distribuições com histogramas, boxplots e Q-Q plots.",
                                    "Verifique pré-requisitos: independência e tamanho amostral adequado (n>20 por grupo)."
                                  ],
                                  "verification": "Crie plots comparativos e confirme visualmente similaridades/diferenças.",
                                  "estimatedTime": "40 minutos",
                                  "materials": [
                                    "Python com NumPy, Matplotlib, Seaborn",
                                    "Dataset exemplo (ex: Iris ou sintético)"
                                  ],
                                  "tips": "Use np.random.normal() para simulações rápidas; sempre plote antes de testar.",
                                  "learningObjective": "Preparar dados adequadamente para aplicação do teste.",
                                  "commonMistakes": "Usar amostras muito pequenas, levando a baixa potência do teste."
                                },
                                {
                                  "stepNumber": 4,
                                  "title": "Implementar e Executar o Teste KS em Python",
                                  "subSteps": [
                                    "Instale/importe scipy.stats e carregue dados.",
                                    "Execute scipy.stats.ks_2samp(sample1, sample2) e capture estatística D e p-value.",
                                    "Automatize para múltiplas subamostras com loop ou função.",
                                    "Registre resultados em tabela (D, p-value, rejeitar/rejeitar H0).",
                                    "Teste sensibilidade variando alpha e tamanhos de amostra."
                                  ],
                                  "verification": "Rode o código em dados simulados e produza output com interpretação correta.",
                                  "estimatedTime": "50 minutos",
                                  "materials": [
                                    "Jupyter Notebook",
                                    "Bibliotecas: scipy, numpy, pandas"
                                  ],
                                  "tips": "Sempre cheque len(sample1) == len(sample2) não é necessário, mas equilibre tamanhos.",
                                  "learningObjective": "Implementar o teste de forma prática e reprodutível.",
                                  "commonMistakes": "Confundir ks_2samp com kstest (unilateral); usar dados dependentes."
                                },
                                {
                                  "stepNumber": 5,
                                  "title": "Interpretar Resultados e Verificar Assunções",
                                  "subSteps": [
                                    "Interprete p-value: se p > alpha, não rejeite H0 (distribuições idênticas).",
                                    "Discuta limitações: KS sensível a diferenças nas caudas, assume continuidade.",
                                    "Compare com visualizações e outros testes para validação.",
                                    "Documente relatório: conclusão, confiança e implicações para modelo.",
                                    "Aplique correções se assunção falhar (ex: reamostragem, transformações)."
                                  ],
                                  "verification": "Escreva um relatório de 1 parágrafo interpretando um teste real.",
                                  "estimatedTime": "35 minutos",
                                  "materials": [
                                    "Resultados dos passos anteriores",
                                    "Template de relatório"
                                  ],
                                  "tips": "p-value baixo não prova diferença causal; combine com domínio knowledge.",
                                  "learningObjective": "Interpretar e contextualizar resultados do teste.",
                                  "commonMistakes": "Rejeitar H0 baseado só em D grande sem p-value; ignorar múltiplos testes."
                                }
                              ],
                              "practicalExample": "Em um dataset de vendas online, divida dados de 'região Norte' e 'Sul' em subamostras. Use KS para testar se as distribuições de tempo de entrega são idênticas. Se p<0.05, investigue fatores regionais como logística para ajustar o modelo preditivo.",
                              "finalVerifications": [
                                "Explica corretamente hipóteses nula/alternativa do KS.",
                                "Implementa ks_2samp em Python sem erros.",
                                "Interpreta p-value e D em contexto real.",
                                "Identifica e corrige violações de assunções.",
                                "Gera visualizações complementares aos testes.",
                                "Documenta processo em relatório claro."
                              ],
                              "assessmentCriteria": [
                                "Precisão na implementação do teste (código roda e produz valores corretos).",
                                "Correta interpretação de resultados (threshold alpha e implicações).",
                                "Qualidade das visualizações e preparação de dados.",
                                "Profundidade na discussão de limitações e alternativas.",
                                "Criatividade no exemplo prático aplicado.",
                                "Clareza e completude do relatório final."
                              ],
                              "crossCurricularConnections": [
                                "Estatística Inferencial: Hipóteses testing e p-values.",
                                "Programação em Python: Manipulação de dados com SciPy/NumPy.",
                                "Machine Learning: Validação de assunções IID em datasets de treino/teste.",
                                "Visualização de Dados: Histogramas e Q-Q plots com Matplotlib.",
                                "Ciência de Dados: Diagnóstico de dados em pipelines de modelagem."
                              ],
                              "realWorldApplication": "Em machine learning, testar se conjuntos de treino e teste têm distribuições idênticas previne overfitting e melhora generalização; em A/B testing, verifica se grupos de controle/tratamento são comparáveis; em finanças, compara distribuições de retornos de ativos para estratégias de portfólio."
                            },
                            "estimatedTime": "2 horas",
                            "difficulty": "intermediate",
                            "status": "not_started",
                            "prerequisites": []
                          }
                        ]
                      },
                      {
                        "id": "10.1.2.3.3",
                        "name": "Assunção IID em Modelagem e Aprendizado",
                        "description": "Aplicação da assunção IID como base para inferência estatística e treinamento de modelos em ciência de dados.",
                        "specificSkills": [
                          {
                            "id": "10.1.2.3.3.1",
                            "name": "Justificar a importância da IID em ML",
                            "description": "Explicar por que a IID permite generalização de modelos treinados em dados de treino para dados de teste.",
                            "atomicExpansion": {
                              "steps": [
                                {
                                  "stepNumber": 1,
                                  "title": "Compreender a definição de IID",
                                  "subSteps": [
                                    "Ler a definição formal de Independente e Identicamente Distribuído (IID).",
                                    "Identificar os dois componentes: independência (observações não influenciam umas às outras) e distribuição idêntica (mesma distribuição probabilística).",
                                    "Exemplificar com dados simples, como lançamentos de moedas justas.",
                                    "Diferenciar IID de dados dependentes (ex: séries temporais).",
                                    "Resumir em uma frase própria."
                                  ],
                                  "verification": "O aluno define corretamente IID e diferencia de não-IID em um quiz curto.",
                                  "estimatedTime": "20 minutos",
                                  "materials": [
                                    "Artigo introdutório sobre IID (ex: Wikipedia ou capítulo de livro de ML)",
                                    "Notebook para anotações"
                                  ],
                                  "tips": "Use analogias cotidianas, como 'fila de supermercado onde cada cliente é independente'.",
                                  "learningObjective": "Definir IID e seus componentes chave.",
                                  "commonMistakes": [
                                    "Confundir independência com ausência total de correlação",
                                    "Ignorar que IID é uma assunção probabilística"
                                  ]
                                },
                                {
                                  "stepNumber": 2,
                                  "title": "Analisar o papel da IID na generalização de modelos",
                                  "subSteps": [
                                    "Estudar a teoria de aprendizado estatístico: como IID permite que a perda empírica aproxime a perda populacional.",
                                    "Explicar o teorema de Glivenko-Cantelli ou lei dos grandes números sob IID.",
                                    "Visualizar com gráficos: convergência de médias amostrais para expectativa.",
                                    "Discutir como IID justifica o uso de conjuntos de treino e teste separados.",
                                    "Rascunhar um parágrafo justificando isso."
                                  ],
                                  "verification": "O aluno explica verbalmente ou por escrito como IID suporta generalização.",
                                  "estimatedTime": "30 minutos",
                                  "materials": [
                                    "Vídeo Khan Academy sobre Lei dos Grandes Números",
                                    "Gráficos interativos em Python (matplotlib)"
                                  ],
                                  "tips": "Foquem na intuição: IID garante que 'amostra representa população'.",
                                  "learningObjective": "Explicar matematicamente por que IID permite generalização.",
                                  "commonMistakes": [
                                    "Achar que IID garante performance perfeita",
                                    "Confundir IID com normalidade dos dados"
                                  ]
                                },
                                {
                                  "stepNumber": 3,
                                  "title": "Explorar violações da IID e suas consequências",
                                  "subSteps": [
                                    "Identificar cenários reais de violação: dados temporais, agrupados ou com drift.",
                                    "Simular em código: treinar modelo em dados IID vs. dependentes e comparar performance.",
                                    "Analisar overfitting e poor generalization em casos não-IID.",
                                    "Discutir técnicas de mitigação (ex: validação temporal).",
                                    "Listar 3 exemplos de ML onde IID falha."
                                  ],
                                  "verification": "Simulação em código mostra diferença clara de performance.",
                                  "estimatedTime": "40 minutos",
                                  "materials": [
                                    "Python com scikit-learn e numpy",
                                    "Dataset exemplo (ex: air passengers para dependência)"
                                  ],
                                  "tips": "Comece com simulações simples para visualizar o impacto.",
                                  "learningObjective": "Demonstrar empiricamente os riscos de violar IID.",
                                  "commonMistakes": [
                                    "Subestimar violações sutis como concept drift",
                                    "Ignorar que testes ainda assumem IID"
                                  ]
                                },
                                {
                                  "stepNumber": 4,
                                  "title": "Formular justificativas completas da importância da IID",
                                  "subSteps": [
                                    "Combinar conceitos: escrever argumento estruturado (definição + teoria + violações).",
                                    "Criar diagrama ou mindmap ligando IID à pipeline de ML.",
                                    "Praticar explicação oral para um 'público leigo'.",
                                    "Revisar e refinar com feedback autoavaliado.",
                                    "Aplicar a um case study simples."
                                  ],
                                  "verification": "O aluno apresenta justificativa clara e convincente em 2 minutos.",
                                  "estimatedTime": "25 minutos",
                                  "materials": [
                                    "Ferramenta de diagramação (ex: Draw.io)",
                                    "Gravação de áudio para autoavaliação"
                                  ],
                                  "tips": "Estruture como: problema → solução (IID) → evidência → implicações.",
                                  "learningObjective": "Articular justificativa coesa e persuasiva.",
                                  "commonMistakes": [
                                    "Justificativa muito técnica sem intuição",
                                    "Omitir contexto prático"
                                  ]
                                }
                              ],
                              "practicalExample": "Em um modelo de previsão de preços de casas, dados IID permitem treinar em 80% das casas e testar nos 20% restantes, garantindo que o modelo generalize para novas casas não vistas, simulando vendas futuras independentes.",
                              "finalVerifications": [
                                "Define IID com precisão em palavras próprias.",
                                "Explica o link entre IID e generalização usando lei dos grandes números.",
                                "Identifica pelo menos 2 violações comuns e impactos.",
                                "Justifica IID em um contexto de ML real.",
                                "Simula em código a diferença IID vs. não-IID.",
                                "Apresenta argumento sem erros conceituais."
                              ],
                              "assessmentCriteria": [
                                "Clareza e precisão na definição de IID (30%)",
                                "Profundidade na explicação teórica de generalização (25%)",
                                "Uso de exemplos práticos e simulações (20%)",
                                "Análise de violações e limitações (15%)",
                                "Estrutura lógica e persuasiva da justificativa (10%)"
                              ],
                              "crossCurricularConnections": [
                                "Estatística: Lei dos Grandes Números e Teorema Central do Limite.",
                                "Programação: Simulações em Python com numpy/scikit-learn.",
                                "Probabilidade: Conceitos de independência estocástica.",
                                "Filosofia da Ciência: Papel de assunções em modelos empíricos."
                              ],
                              "realWorldApplication": "Na indústria de tech (ex: Google, Netflix), a assunção IID valida modelos de recomendação e detecção de fraudes, permitindo deploy confiável em dados de produção unseen, com validação cruzada e testes estatísticos."
                            },
                            "estimatedTime": "1 hora",
                            "difficulty": "intermediate",
                            "status": "not_started",
                            "prerequisites": []
                          },
                          {
                            "id": "10.1.2.3.3.2",
                            "name": "Analisar violações da IID em cenários reais",
                            "description": "Estudar casos como séries temporais (não-IID) e propor soluções como modelagem de dependências.",
                            "atomicExpansion": {
                              "steps": [
                                {
                                  "stepNumber": 1,
                                  "title": "Compreender os conceitos fundamentais de IID e suas violações comuns",
                                  "subSteps": [
                                    "Defina IID: independência e distribuição idêntica das observações.",
                                    "Liste tipos de violações: dependência temporal (séries temporais), dependência espacial, mudanças de distribuição (drift).",
                                    "Revise assunções em modelos de ML como regressão linear e redes neurais.",
                                    "Compare IID vs. não-IID com diagramas simples.",
                                    "Identifique impactos: viés em estimativas, overfitting/underfitting."
                                  ],
                                  "verification": "Crie um resumo de 1 página explicando IID e 3 violações com exemplos.",
                                  "estimatedTime": "2 horas",
                                  "materials": "Notas de aula sobre IID, artigos introdutórios (ex: 'Assumptions in Machine Learning' do Scikit-learn docs), papel e caneta para diagramas.",
                                  "tips": "Use analogias cotidianas, como moedas justas (IID) vs. dados viciados em sequência (não-IID).",
                                  "learningObjective": "Dominar definições e impactos de violações IID para análise posterior.",
                                  "commonMistakes": "Confundir independência com correlação zero; ignorar violações de distribuição idêntica."
                                },
                                {
                                  "stepNumber": 2,
                                  "title": "Identificar violações IID em cenários reais, focando em séries temporais",
                                  "subSteps": [
                                    "Selecione datasets reais: preços de ações (Yahoo Finance), temperaturas diárias.",
                                    "Calcule autocorrelação (ACF/PACF) para detectar dependência temporal.",
                                    "Visualize séries temporais com plots de linha e histogramas rolantes.",
                                    "Teste estacionariedade com Augmented Dickey-Fuller (ADF).",
                                    "Documente evidências de não-IID: p-value baixo em testes de independência."
                                  ],
                                  "verification": "Gere gráficos e relatórios mostrando autocorrelação significativa em um dataset.",
                                  "estimatedTime": "3 horas",
                                  "materials": "Python (Pandas, Statsmodels, Matplotlib), datasets de séries temporais (ex: AirPassengers do R ou yfinance).",
                                  "tips": "Comece com datasets conhecidos não-estacionários para prática rápida.",
                                  "learningObjective": "Detectar empiricamente violações IID em dados reais.",
                                  "commonMistakes": "Assumir IID sem testes estatísticos; ignorar sazonalidade em séries."
                                },
                                {
                                  "stepNumber": 3,
                                  "title": "Analisar impactos das violações e propor soluções como modelagem de dependências",
                                  "subSteps": [
                                    "Treine modelo IID padrão (ex: regressão linear) em dados não-IID e meça performance (MSE, AIC).",
                                    "Proponha soluções: ARIMA para dependência temporal, embeddings temporais em LSTM.",
                                    "Implemente modelo corrigido e compare métricas pré/pós-correção.",
                                    "Justifique escolhas: por que modelar dependências resolve não-independência.",
                                    "Avalie trade-offs: complexidade vs. ganho em precisão."
                                  ],
                                  "verification": "Produza tabela comparativa de performance entre modelo IID e corrigido.",
                                  "estimatedTime": "4 horas",
                                  "materials": "Jupyter Notebook, bibliotecas (Scikit-learn, Statsmodels, TensorFlow/Keras para LSTM), mesmo dataset do step 2.",
                                  "tips": "Use validação cruzada temporal (TimeSeriesSplit) para evitar leakage.",
                                  "learningObjective": "Quantificar impactos e validar soluções para violações IID.",
                                  "commonMistakes": "Treinar modelos sem split temporal adequado; superestimar melhorias sem baselines."
                                },
                                {
                                  "stepNumber": 4,
                                  "title": "Sintetizar análise e generalizar para outros cenários não-IID",
                                  "subSteps": [
                                    "Resuma achados: violações detectadas, soluções aplicadas, lições aprendidas.",
                                    "Aplique framework a outro cenário (ex: dados espaciais como imagens satélite).",
                                    "Crie checklist genérico para análise de IID em novos datasets.",
                                    "Discuta limitações: quando IID é aproximável vs. modelar explicitamente.",
                                    "Prepare apresentação ou relatório final."
                                  ],
                                  "verification": "Elabore um relatório de 2-3 páginas com checklist e generalizações.",
                                  "estimatedTime": "2 horas",
                                  "materials": "Documento Word/Google Docs, gráficos gerados anteriormente.",
                                  "tips": "Use templates de relatório para estrutura (IMRaD: Intro, Methods, Results, Discussion).",
                                  "learningObjective": "Generalizar habilidades de análise IID para cenários variados.",
                                  "commonMistakes": "Focar só em um caso; não considerar custos computacionais de modelos complexos."
                                }
                              ],
                              "practicalExample": "Analise o dataset de preços diários de ações da AAPL (Apple). Detecte dependência temporal via ACF (lag 1 > 0.8), treine ARIMA(1,1,1) vs. regressão linear simples, mostrando redução de 30% no MSE ao modelar dependências.",
                              "finalVerifications": [
                                "Identificou corretamente pelo menos 3 violações IID em um dataset real.",
                                "Gerou evidências estatísticas (testes ACF/ADF) suportando análise.",
                                "Propôs e implementou solução adequada (ex: ARIMA/LSTM) com comparação quantitativa.",
                                "Criou checklist genérico para detecção de não-IID.",
                                "Documentou impactos em performance de modelos.",
                                "Generalizou para pelo menos um cenário adicional."
                              ],
                              "assessmentCriteria": [
                                "Precisão na detecção de violações (evidências estatísticas claras).",
                                "Qualidade das soluções propostas (justificadas e implementadas corretamente).",
                                "Profundidade da análise comparativa (métricas múltiplas usadas).",
                                "Clareza na documentação e visualizações.",
                                "Capacidade de generalização para outros contextos.",
                                "Consideração de trade-offs e limitações."
                              ],
                              "crossCurricularConnections": [
                                "Estatística: Testes de estacionariedade e autocorrelação.",
                                "Programação: Manipulação de dados temporais em Python/R.",
                                "Matemática Aplicada: Modelos estocásticos como processos ARMA.",
                                "Ciência de Dados: Validação cruzada e avaliação de modelos.",
                                "Economia/Finanças: Aplicações em previsão de séries financeiras."
                              ],
                              "realWorldApplication": "Em finanças, analisa violações IID em dados de mercado para melhorar previsões de preços com modelos como GARCH; em saúde, modela dependências temporais em epidemias para forecasts precisos de surtos."
                            },
                            "estimatedTime": "2 horas",
                            "difficulty": "intermediate",
                            "status": "not_started",
                            "prerequisites": []
                          },
                          {
                            "id": "10.1.2.3.3.3",
                            "name": "Relacionar IID com etapas da ciência de dados",
                            "description": "Conectar a assunção IID às fases de coleta, limpeza e avaliação de modelos, citando referências bibliográficas.",
                            "atomicExpansion": {
                              "steps": [
                                {
                                  "stepNumber": 1,
                                  "title": "Compreender a Definição e Implicações da Assunção IID",
                                  "subSteps": [
                                    "Defina IID como Independent and Identically Distributed: independência entre amostras e mesma distribuição probabilística.",
                                    "Explique independência: ausência de correlação ou dependência entre observações.",
                                    "Descreva distribuição idêntica: todas as amostras vêm da mesma população subjacente.",
                                    "Discuta implicações em ML: permite inferência estatística válida e generalização.",
                                    "Identifique violações comuns, como dados temporais ou agrupados."
                                  ],
                                  "verification": "Escreva um parágrafo resumindo IID e suas implicações, sem erros conceituais.",
                                  "estimatedTime": "30 minutos",
                                  "materials": [
                                    "Capítulo 2 de 'An Introduction to Statistical Learning' (James et al., 2013)",
                                    "Artigo 'Assumptions of Machine Learning' no Towards Data Science"
                                  ],
                                  "tips": "Use analogias como 'lançamentos de moedas justas' para ilustrar independência.",
                                  "learningObjective": "Dominar os conceitos fundamentais de IID para contextualizar suas aplicações.",
                                  "commonMistakes": "Confundir IID com normalidade ou homogeneidade de variância."
                                },
                                {
                                  "stepNumber": 2,
                                  "title": "Relacionar IID à Fase de Coleta de Dados",
                                  "subSteps": [
                                    "Analise como coleta aleatória garante independência entre amostras.",
                                    "Discuta amostragem estratificada para manter distribuição idêntica.",
                                    "Examine riscos de viés de seleção que violam IID.",
                                    "Planeje estratégias de coleta para validar IID, como amostras independentes.",
                                    "Registre fontes de dados e métodos de coleta para auditoria."
                                  ],
                                  "verification": "Crie um fluxograma mostrando como coleta afeta IID.",
                                  "estimatedTime": "45 minutos",
                                  "materials": [
                                    "Seção 5.1 de 'The Elements of Statistical Learning' (Hastie et al., 2009)",
                                    "Notebook Jupyter com simulação de coleta de dados"
                                  ],
                                  "tips": "Sempre pergunte: 'Essa amostra é representativa e independente?'",
                                  "learningObjective": "Conectar práticas de coleta diretamente à validade da assunção IID.",
                                  "commonMistakes": "Ignorar dependências ocultas, como dados de redes sociais."
                                },
                                {
                                  "stepNumber": 3,
                                  "title": "Relacionar IID à Fase de Limpeza de Dados",
                                  "subSteps": [
                                    "Identifique como remoção de outliers pode alterar a distribuição idêntica.",
                                    "Explique preservação de independência durante imputação de missing values.",
                                    "Aplique testes estatísticos (ex: runs test) para checar independência pós-limpeza.",
                                    "Documente transformações e valide se mantêm IID.",
                                    "Use visualizações como ACF plots para detectar violações."
                                  ],
                                  "verification": "Execute limpeza em um dataset pequeno e teste IID antes/depois.",
                                  "estimatedTime": "60 minutos",
                                  "materials": [
                                    "Capítulo 6 de 'Python for Data Analysis' (McKinney, 2017)",
                                    "Biblioteca statsmodels para testes de independência"
                                  ],
                                  "tips": "Limpe minimamente; preserve a estrutura original tanto quanto possível.",
                                  "learningObjective": "Garantir que limpeza de dados não comprometa a assunção IID.",
                                  "commonMistakes": "Sobre-limpar dados, introduzindo artefatos que criam dependências."
                                },
                                {
                                  "stepNumber": 4,
                                  "title": "Relacionar IID à Avaliação de Modelos",
                                  "subSteps": [
                                    "Explique por que IID justifica train-test split e cross-validation.",
                                    "Discuta métricas como MSE que assumem IID para validade estatística.",
                                    "Analise impactos de violações IID em overfitting/underfitting.",
                                    "Implemente k-fold CV assumindo IID e interprete resultados.",
                                    "Compare desempenho em dados IID vs. não-IID simulados."
                                  ],
                                  "verification": "Treine um modelo simples e avalie métricas sob assunção IID.",
                                  "estimatedTime": "60 minutos",
                                  "materials": [
                                    "Capítulo 5 de 'Hands-On Machine Learning with Scikit-Learn' (Géron, 2019)",
                                    "Scikit-learn para cross-validation"
                                  ],
                                  "tips": "Sempre valide IID antes de avaliação para resultados confiáveis.",
                                  "learningObjective": "Entender o papel crítico de IID na confiabilidade de avaliações de modelos.",
                                  "commonMistakes": "Aplicar CV em dados dependentes, levando a estimativas otimistas."
                                },
                                {
                                  "stepNumber": 5,
                                  "title": "Citar Referências Bibliográficas e Sintetizar Conexões",
                                  "subSteps": [
                                    "Liste 3-5 referências chave sobre IID em ciência de dados.",
                                    "Escreva síntese conectando IID às três fases com citações.",
                                    "Crie um mapa mental integrando todos os relacionamentos.",
                                    "Revise por precisão e formatação APA/MLA.",
                                    "Discuta limitações da assunção IID em contextos reais."
                                  ],
                                  "verification": "Produza um relatório de 1 página com citações e síntese.",
                                  "estimatedTime": "30 minutos",
                                  "materials": [
                                    "Google Scholar para referências",
                                    "Zotero ou Mendeley para gerenciamento de citações"
                                  ],
                                  "tips": "Priorize fontes primárias como livros acadêmicos sobre blogs.",
                                  "learningObjective": "Integrar conhecimento com suporte bibliográfico formal.",
                                  "commonMistakes": "Citar fontes irrelevantes ou plágio indireto sem paráfrase."
                                }
                              ],
                              "practicalExample": "No dataset Iris, assuma IID: cada flor é amostrada independentemente da mesma população de íris. Na coleta, evite colher flores próximas (dependência espacial). Na limpeza, remova duplicatas sem alterar distribuição. Na avaliação, use CV para métricas confiáveis, citando James et al. (2013).",
                              "finalVerifications": [
                                "Explicar verbalmente como IID afeta coleta, limpeza e avaliação.",
                                "Identificar violações IID em um dataset dado.",
                                "Citar pelo menos 3 referências corretamente.",
                                "Mapear IID para pipeline completo de ciência de dados.",
                                "Demonstrar teste estatístico para IID em código.",
                                "Discutir cenários onde IID falha e alternativas."
                              ],
                              "assessmentCriteria": [
                                "Precisão conceitual de IID (30%)",
                                "Conexões claras com as três fases (25%)",
                                "Uso adequado de referências bibliográficas (20%)",
                                "Exemplos práticos e verificáveis (15%)",
                                "Clareza e estrutura da explicação (10%)"
                              ],
                              "crossCurricularConnections": [
                                "Estatística: Testes de independência e distribuições.",
                                "Programação: Implementação de pipelines em Python/R.",
                                "Matemática: Teoria de probabilidade e inferência.",
                                "Ética em Dados: Implicações de violações IID em decisões."
                              ],
                              "realWorldApplication": "Em empresas como Google ou bancos, pipelines de ML assumem IID para prever churn de clientes; violações (ex: dados sazonais) levam a modelos falhos, custando milhões – referências como Hastie et al. guiam práticas robustas."
                            },
                            "estimatedTime": "1.5 horas",
                            "difficulty": "intermediate",
                            "status": "not_started",
                            "prerequisites": []
                          }
                        ]
                      }
                    ]
                  },
                  {
                    "id": "10.1.2.4",
                    "name": "Definições de Dados, Informação e Conhecimento",
                    "description": "Distinções hierárquicas entre dados brutos, informação processada e conhecimento derivado.",
                    "individualConcepts": [
                      {
                        "id": "10.1.2.4.1",
                        "name": "Dados Brutos",
                        "description": "Dados brutos referem-se a fatos elementares e isolados, sem contexto ou processamento, como números, símbolos ou registros crus coletados de fontes variadas, representando a base mais fundamental na hierarquia DIK (Dados-Informação-Conhecimento).",
                        "specificSkills": [
                          {
                            "id": "10.1.2.4.1.1",
                            "name": "Definir dados brutos",
                            "description": "Explicar o conceito de dados brutos como elementos crus e não estruturados, sem significado inerente, incluindo exemplos como leituras de sensores, entradas de logs ou valores numéricos isolados.",
                            "atomicExpansion": {
                              "steps": [
                                {
                                  "stepNumber": 1,
                                  "title": "Compreender o Conceito Básico de Dados",
                                  "subSteps": [
                                    "Pesquisar definições padrão de 'dados' em fontes como livros de ciência de dados ou sites educacionais.",
                                    "Explorar a hierarquia dados-informação-conhecimento através de diagramas simples.",
                                    "Anotar o papel fundamental dos dados como base para análise.",
                                    "Discutir em voz alta ou escrever por que os dados precisam de processamento."
                                  ],
                                  "verification": "Criar um diagrama ou parágrafo resumindo a hierarquia dados-informação-conhecimento.",
                                  "estimatedTime": "25 minutos",
                                  "materials": [
                                    "Acesso à internet ou livro de ciência de dados",
                                    "Papel e caneta ou software de diagramação como Draw.io"
                                  ],
                                  "tips": "Comece com exemplos cotidianos para fixar o conceito.",
                                  "learningObjective": "Entender dados como representações simbólicas brutas de fatos do mundo real.",
                                  "commonMistakes": [
                                    "Confundir dados com informação processada",
                                    "Ignorar a hierarquia conceitual"
                                  ]
                                },
                                {
                                  "stepNumber": 2,
                                  "title": "Identificar Características dos Dados Brutos",
                                  "subSteps": [
                                    "Listar traços chave: crus, não estruturados, sem significado inerente.",
                                    "Comparar com dados processados anotando diferenças.",
                                    "Analisar por que dados brutos carecem de contexto isoladamente.",
                                    "Criar uma tabela comparativa: brutos vs. limpos."
                                  ],
                                  "verification": "Tabela comparativa completa com pelo menos 4 características.",
                                  "estimatedTime": "30 minutos",
                                  "materials": [
                                    "Planilha ou editor de texto",
                                    "Exemplos impressos de dados (logs, sensores)"
                                  ],
                                  "tips": "Use analogias como 'ingredientes crus antes da receita'.",
                                  "learningObjective": "Reconhecer atributos essenciais que definem dados brutos.",
                                  "commonMistakes": [
                                    "Atribuir significado prematuro aos dados",
                                    "Confundir não estruturados com aleatórios"
                                  ]
                                },
                                {
                                  "stepNumber": 3,
                                  "title": "Explorar Exemplos Concretos de Dados Brutos",
                                  "subSteps": [
                                    "Coletar exemplos reais: leituras de sensores (ex: temperatura 23.5°C), logs de sistema (ex: IP:192.168.1.1).",
                                    "Classificar 5 exemplos como brutos ou não.",
                                    "Gerar seus próprios exemplos de valores numéricos isolados.",
                                    "Explicar verbalmente por que cada exemplo é bruto."
                                  ],
                                  "verification": "Lista de 5 exemplos classificados com justificativas.",
                                  "estimatedTime": "35 minutos",
                                  "materials": [
                                    "Dispositivo com sensor (termômetro app) ou arquivos de log sample",
                                    "Caderno para anotações"
                                  ],
                                  "tips": "Registre dados reais em tempo real para imersão.",
                                  "learningObjective": "Aplicar o conceito a casos práticos identificando dados brutos.",
                                  "commonMistakes": [
                                    "Usar exemplos processados como 'média de temperatura'",
                                    "Faltar variedade nos exemplos"
                                  ]
                                },
                                {
                                  "stepNumber": 4,
                                  "title": "Diferenciar Dados Brutos de Informação e Conhecimento",
                                  "subSteps": [
                                    "Mapear transformação: dados brutos -> contexto -> informação -> padrões -> conhecimento.",
                                    "Criar fluxograma ilustrando o processo.",
                                    "Discutir cenários onde dados brutos levam a insights.",
                                    "Testar compreensão com quiz autoavaliativo."
                                  ],
                                  "verification": "Fluxograma completo e quiz com 80% acertos.",
                                  "estimatedTime": "30 minutos",
                                  "materials": [
                                    "Ferramenta de fluxograma online",
                                    "Quiz template simples"
                                  ],
                                  "tips": "Pense em uma cadeia: números -> tabela organizada -> previsão.",
                                  "learningObjective": "Distinguir claramente dados brutos no espectro de processamento.",
                                  "commonMistakes": [
                                    "Pular etapas na transformação",
                                    "Superestimar o valor inerente dos dados brutos"
                                  ]
                                },
                                {
                                  "stepNumber": 5,
                                  "title": "Sintetizar e Aplicar o Conceito",
                                  "subSteps": [
                                    "Escrever definição pessoal de dados brutos em 1 parágrafo.",
                                    "Aplicar a um dataset sample identificando brutos.",
                                    "Debater limitações de trabalhar só com dados brutos.",
                                    "Preparar apresentação rápida do aprendizado."
                                  ],
                                  "verification": "Parágrafo de definição e análise de dataset.",
                                  "estimatedTime": "25 minutos",
                                  "materials": [
                                    "Dataset sample (CSV bruto)",
                                    "Gravador de áudio ou vídeo para apresentação"
                                  ],
                                  "tips": "Revise todos os steps anteriores antes de sintetizar.",
                                  "learningObjective": "Consolidar definição com aplicação independente.",
                                  "commonMistakes": [
                                    "Definição vaga ou copiada",
                                    "Ignorar limitações práticas"
                                  ]
                                }
                              ],
                              "practicalExample": "Colete 10 leituras de temperatura de um sensor IoT em um arquivo de texto simples (ex: 22.1, 23.4, 21.8...), sem rótulos ou médias, e explique por que esses são dados brutos aguardando processamento para virar informação como 'temperatura média da sala'.",
                              "finalVerifications": [
                                "Define dados brutos com precisão (crus, não estruturados, sem significado).",
                                "Lista pelo menos 3 exemplos válidos de dados brutos.",
                                "Diferencia corretamente de informação e conhecimento.",
                                "Identifica características chave em exemplos reais.",
                                "Explica transformação de dados brutos para conhecimento.",
                                "Cria fluxograma ou tabela demonstrando compreensão.",
                                "Aplica conceito a um cenário prático."
                              ],
                              "assessmentCriteria": [
                                "Precisão e completude da definição (30%).",
                                "Qualidade e relevância dos exemplos (25%).",
                                "Clareza na diferenciação conceitual (20%).",
                                "Profundidade das características listadas (15%).",
                                "Criatividade na aplicação prática (10%)."
                              ],
                              "crossCurricularConnections": [
                                "Ciência da Computação: Coleta e armazenamento em bancos de dados.",
                                "Estatística: Limpeza e pré-processamento de dados.",
                                "Física/Engenharia: Leituras de sensores em experimentos.",
                                "Informática: Logs de sistemas e debugging.",
                                "Biologia: Sequências genéticas brutas em bioinformática."
                              ],
                              "realWorldApplication": "Em ciência de dados, dados brutos de sensores em dispositivos IoT (como wearables fitness) são processados para gerar insights de saúde, permitindo previsões em machine learning sem as quais modelos não funcionam."
                            },
                            "estimatedTime": "1 hora",
                            "difficulty": "beginner",
                            "status": "not_started",
                            "prerequisites": []
                          },
                          {
                            "id": "10.1.2.4.1.2",
                            "name": "Identificar exemplos de dados brutos",
                            "description": "Reconhecer e listar exemplos reais de dados brutos em contextos de ciência de dados, como planilhas de vendas sem análise ou streams de dados de IoT.",
                            "atomicExpansion": {
                              "steps": [
                                {
                                  "stepNumber": 1,
                                  "title": "Compreender a definição de dados brutos",
                                  "subSteps": [
                                    "Leia a definição oficial de dados brutos: registros não processados, sem análise ou interpretação.",
                                    "Compare com dados processados, como médias ou gráficos derivados.",
                                    "Anote as três características principais: cru, não estruturado ou semi-estruturado, coletado diretamente da fonte.",
                                    "Crie um fluxograma simples mostrando dados brutos → processamento → informação.",
                                    "Discuta com um parceiro ou anote exemplos iniciais de fontes cotidianas."
                                  ],
                                  "verification": "Você pode explicar em suas próprias palavras o que diferencia dados brutos de dados processados.",
                                  "estimatedTime": "20 minutos",
                                  "materials": [
                                    "Definição impressa ou digital de dados brutos",
                                    "Papel e caneta para fluxograma"
                                  ],
                                  "tips": "Use analogias como 'leite cru vs. leite pasteurizado' para fixar o conceito.",
                                  "learningObjective": "Definir precisamente dados brutos e suas características fundamentais.",
                                  "commonMistakes": [
                                    "Confundir dados brutos com dados limpos",
                                    "Ignorar o aspecto 'não interpretado'"
                                  ]
                                },
                                {
                                  "stepNumber": 2,
                                  "title": "Identificar características chave de dados brutos",
                                  "subSteps": [
                                    "Liste atributos: falta de limpeza, ruído possível, formato variado (CSV, JSON raw, logs).",
                                    "Analise um exemplo curto de log de servidor para identificar elementos crus.",
                                    "Classifique 5 conjuntos de dados amostra como 'brutos' ou 'processados' com justificativa.",
                                    "Crie uma tabela comparativa: Colunas para 'Dados Brutos' vs. 'Dados Processados'.",
                                    "Revise e corrija classificações iniciais com base em critérios."
                                  ],
                                  "verification": "Tabela comparativa completa com pelo menos 5 exemplos classificados corretamente.",
                                  "estimatedTime": "25 minutos",
                                  "materials": [
                                    "Amostras de dados em CSV ou texto (logs de servidor, sensores)",
                                    "Planilha ou editor de texto"
                                  ],
                                  "tips": "Procure por 'ruído' como valores faltantes ou erros de digitação como sinal de crueza.",
                                  "learningObjective": "Reconhecer atributos diagnósticos de dados brutos em formatos comuns.",
                                  "commonMistakes": [
                                    "Classificar dados limpos como brutos",
                                    "Overlook formatos semi-estruturados"
                                  ]
                                },
                                {
                                  "stepNumber": 3,
                                  "title": "Explorar exemplos reais em contextos de ciência de dados",
                                  "subSteps": [
                                    "Pesquise e liste 3 exemplos: planilhas de vendas brutas, streams IoT (temperatura de sensores), logs de apps.",
                                    "Baixe um dataset cru público (ex: Kaggle raw sales data) e descreva por que é bruto.",
                                    "Simule coleta: Registre 10 entradas manuais de 'vendas diárias' sem processar.",
                                    "Compare com versões processadas do mesmo dataset.",
                                    "Documente contextos: e-commerce, manufatura IoT, saúde (leituras de wearables)."
                                  ],
                                  "verification": "Lista de pelo menos 5 exemplos reais com descrições e fontes.",
                                  "estimatedTime": "30 minutos",
                                  "materials": [
                                    "Acesso à internet para Kaggle/UCI datasets",
                                    "Planilha para simulação"
                                  ],
                                  "tips": "Foque em fontes primárias: sensores, transações diretas, sem agregação.",
                                  "learningObjective": "Associar dados brutos a cenários reais de ciência de dados.",
                                  "commonMistakes": [
                                    "Usar exemplos processados como gráficos de vendas",
                                    "Ignorar streams em tempo real"
                                  ]
                                },
                                {
                                  "stepNumber": 4,
                                  "title": "Praticar listagem e verificação de exemplos",
                                  "subSteps": [
                                    "Crie uma lista final de 10 exemplos de dados brutos de diversos domínios.",
                                    "Para cada um, escreva uma frase justificando por que é bruto.",
                                    "Teste com quiz autoavaliativo: Classifique 10 itens mistos.",
                                    "Revise erros e refine a lista.",
                                    "Compartilhe lista com feedback de pares ou autoavaliação."
                                  ],
                                  "verification": "Lista final de 10 exemplos com justificativas precisas e quiz com 90% acerto.",
                                  "estimatedTime": "25 minutos",
                                  "materials": [
                                    "Quiz template ou app como Quizlet",
                                    "Lista inicial do step 3"
                                  ],
                                  "tips": "Varie domínios para robustez: finanças, saúde, IoT, redes sociais.",
                                  "learningObjective": "Listar e validar exemplos de dados brutos de forma independente.",
                                  "commonMistakes": [
                                    "Listas genéricas sem contexto específico",
                                    "Falta de justificativa clara"
                                  ]
                                }
                              ],
                              "practicalExample": "Em uma planilha de vendas brutas de uma loja online: colunas com 'ID_Cliente, Produto_ID, Quantidade, Data_Hora, Preco_Unitario' com entradas como '12345, A001, 2, 2023-10-01 14:23:45, 29.99' incluindo valores nulos e duplicatas, sem totais ou categorias agregadas.",
                              "finalVerifications": [
                                "Pode listar pelo menos 8 exemplos corretos de dados brutos de contextos variados.",
                                "Diferencia consistentemente dados brutos de processados/informação.",
                                "Identifica características como ruído e falta de estrutura em amostras reais.",
                                "Justifica exemplos com critérios precisos.",
                                "Aplica conceito a novos cenários como IoT ou logs sem hesitação.",
                                "Quiz de verificação com acurácia >90%."
                              ],
                              "assessmentCriteria": [
                                "Precisão na identificação (exemplos verdadeiramente crus).",
                                "Diversidade de contextos (não só planilhas).",
                                "Justificativas claras e baseadas em definição.",
                                "Ausência de confusão com dados derivados.",
                                "Profundidade: inclui streams e formatos não tabulares.",
                                "Criatividade em exemplos reais e relevantes."
                              ],
                              "crossCurricularConnections": [
                                "Estatística: Reconhecimento de dados para análise descritiva.",
                                "Programação: Leitura de arquivos crus em Python (pandas.read_csv raw).",
                                "Informática: Conceitos de bancos de dados NoSQL semi-estruturados.",
                                "Física/Engenharia: Dados de sensores IoT em experimentos.",
                                "Negócios: Dados de transações para BI."
                              ],
                              "realWorldApplication": "Cientistas de dados usam essa habilidade para iniciar pipelines de ETL (Extract, Transform, Load), identificando dados crus de fontes como sensores industriais para prever falhas em manufatura ou analisar padrões de vendas em e-commerce sem viés de pré-processamento."
                            },
                            "estimatedTime": "0.5 hora",
                            "difficulty": "beginner",
                            "status": "not_started",
                            "prerequisites": []
                          },
                          {
                            "id": "10.1.2.4.1.3",
                            "name": "Caracterizar propriedades dos dados brutos",
                            "description": "Descrever propriedades como volume, variedade e veracidade dos dados brutos, destacando sua falta de contexto e necessidade de processamento.",
                            "atomicExpansion": {
                              "steps": [
                                {
                                  "stepNumber": 1,
                                  "title": "Entender o conceito de dados brutos e suas propriedades fundamentais",
                                  "subSteps": [
                                    "Defina dados brutos como registros não processados coletados diretamente de fontes.",
                                    "Liste as propriedades principais: volume (quantidade), variedade (tipos e formatos) e veracidade (qualidade e confiabilidade).",
                                    "Explique que dados brutos carecem de contexto, significando que são isolados sem interpretação.",
                                    "Discuta a necessidade de processamento para transformá-los em informação útil.",
                                    "Crie um mapa mental conectando esses conceitos."
                                  ],
                                  "verification": "Crie um resumo de 1 parágrafo definindo dados brutos e listando suas 3 propriedades principais com exemplos breves.",
                                  "estimatedTime": "45 minutos",
                                  "materials": "Papel e caneta ou ferramenta digital como MindMeister; artigo introdutório sobre os 3Vs da Big Data.",
                                  "tips": "Use analogias cotidianas, como uma pilha de recibos não organizados, para visualizar dados brutos.",
                                  "learningObjective": "Compreender a definição e as propriedades essenciais dos dados brutos.",
                                  "commonMistakes": "Confundir dados brutos com dados processados; ignorar a veracidade como propriedade chave."
                                },
                                {
                                  "stepNumber": 2,
                                  "title": "Caracterizar o Volume dos dados brutos",
                                  "subSteps": [
                                    "Defina volume como a quantidade massiva de dados gerados (ex: terabytes por dia).",
                                    "Colete um exemplo pequeno de dados (ex: 100 linhas de um CSV) e imagine escalando para milhões.",
                                    "Meça o volume em unidades: bytes, MB, GB, TB.",
                                    "Discuta desafios: armazenamento, transferência e processamento.",
                                    "Calcule o volume aproximado de um dataset real fornecido."
                                  ],
                                  "verification": "Descreva o volume de um dataset exemplo, justificando se é 'alto' ou 'baixo' com números.",
                                  "estimatedTime": "30 minutos",
                                  "materials": "Dataset CSV simples (ex: Kaggle 'Titanic'); calculadora ou planilha Excel/Google Sheets.",
                                  "tips": "Pense em fontes como redes sociais: Twitter gera ~500 milhões de tweets/dia para ilustrar escala.",
                                  "learningObjective": "Identificar e quantificar o volume como propriedade crítica dos dados brutos.",
                                  "commonMistakes": "Focar apenas em tamanho de arquivo sem considerar taxa de geração contínua."
                                },
                                {
                                  "stepNumber": 3,
                                  "title": "Analisar Variedade e Veracidade nos dados brutos",
                                  "subSteps": [
                                    "Defina variedade: dados estruturados (tabelas), semi-estruturados (JSON/XML) e não-estruturados (texto/imagens).",
                                    "Identifique veracidade: precisão, completude, consistência e ausência de erros.",
                                    "Examine um dataset misto para catalogar tipos de variedade.",
                                    "Avalie veracidade verificando duplicatas, valores ausentes ou inconsistências.",
                                    "Compare variedade e veracidade em um exemplo prático."
                                  ],
                                  "verification": "Crie uma tabela classificando 10 amostras de dados por variedade e pontuando veracidade (1-5).",
                                  "estimatedTime": "50 minutos",
                                  "materials": "Dataset misto (ex: logs de web com texto e números); editor de texto como VS Code.",
                                  "tips": "Use cores para marcar variedade em uma planilha para visualização rápida.",
                                  "learningObjective": "Descrever variedade e veracidade, destacando impactos na usabilidade dos dados.",
                                  "commonMistakes": "Ignorar semi-estruturados como JSON; assumir veracidade sem checagem de fontes."
                                },
                                {
                                  "stepNumber": 4,
                                  "title": "Sintetizar falta de contexto e necessidade de processamento",
                                  "subSteps": [
                                    "Explique falta de contexto: dados brutos não têm metadados ou relações interpretáveis.",
                                    "Liste etapas de processamento: limpeza, integração, transformação (ETL).",
                                    "Crie um fluxograma de dados brutos → processamento → informação.",
                                    "Discuta riscos de usar dados brutos sem processamento (viés, erros).",
                                    "Aplique a um caso: por que logs de sensor precisam de contexto temporal?"
                                  ],
                                  "verification": "Escreva um parágrafo explicando por que dados brutos de um exemplo precisam de processamento específico.",
                                  "estimatedTime": "35 minutos",
                                  "materials": "Ferramenta de fluxograma como Draw.io; exemplos de datasets reais (ex: sensores IoT).",
                                  "tips": "Lembre: 'Dados são como ingredientes crus; processamento é a receita para uma refeição'.",
                                  "learningObjective": "Articular como contexto e processamento elevam dados brutos a informação.",
                                  "commonMistakes": "Subestimar o papel do contexto humano na interpretação."
                                }
                              ],
                              "practicalExample": "Ao analisar logs de um e-commerce: alto volume (milhões de cliques/dia), variedade (cliques, compras, textos de busca), veracidade baixa (erros de digitação, bots), sem contexto (sem saber se é pico de vendas ou falha técnica) – requer limpeza e agregação para insights de vendas.",
                              "finalVerifications": [
                                "Liste e defina volume, variedade e veracidade com exemplos precisos.",
                                "Explique em 2 frases por que dados brutos carecem de contexto.",
                                "Identifique 3 desafios de processamento para dados brutos de alto volume.",
                                "Crie uma tabela resumindo propriedades de um dataset exemplo.",
                                "Descreva um risco de ignorar veracidade em análise de dados.",
                                "Monte um fluxograma simples de processamento de dados brutos."
                              ],
                              "assessmentCriteria": [
                                "Precisão na definição das propriedades (volume, variedade, veracidade): 0-4 pontos.",
                                "Uso de exemplos concretos e relevantes: 0-4 pontos.",
                                "Explicação clara da falta de contexto e necessidade de processamento: 0-4 pontos.",
                                "Profundidade nos sub-passos e verificações: 0-4 pontos.",
                                "Integração de dicas e avoidance de erros comuns: 0-4 pontos.",
                                "Coerência geral e estrutura lógica: 0-4 pontos."
                              ],
                              "crossCurricularConnections": [
                                "Estatística: Medição de volume e veracidade via métricas descritivas.",
                                "Programação: Manipulação de variedade com Python/Pandas para ETL.",
                                "Ética e Sociedade: Veracidade e viés em dados para decisões sociais.",
                                "Física/Engenharia: Dados brutos de sensores IoT com falta de contexto temporal."
                              ],
                              "realWorldApplication": "Em empresas como Netflix, caracterizar dados brutos de visualizações (volume alto, variedade de dispositivos, veracidade via logs de usuário) permite processamento para recomendações personalizadas, otimizando retenção de assinantes."
                            },
                            "estimatedTime": "1 hora",
                            "difficulty": "beginner",
                            "status": "not_started",
                            "prerequisites": []
                          }
                        ]
                      },
                      {
                        "id": "10.1.2.4.2",
                        "name": "Informação Processada",
                        "description": "Informação é o resultado do processamento de dados brutos com contexto, análise ou organização, gerando significado e padrões úteis, posicionando-se no segundo nível da hierarquia DIK.",
                        "specificSkills": [
                          {
                            "id": "10.1.2.4.2.1",
                            "name": "Definir informação como dados processados",
                            "description": "Explicar como dados brutos se transformam em informação através de limpeza, agregação ou contextualização, com exemplos como resumos estatísticos de vendas.",
                            "atomicExpansion": {
                              "steps": [
                                {
                                  "stepNumber": 1,
                                  "title": "Compreender Dados Brutos e sua Natureza",
                                  "subSteps": [
                                    "Defina dados brutos como registros não processados e sem contexto, como números isolados ou textos crus.",
                                    "Identifique exemplos reais, como uma lista de valores de vendas diárias sem categorização.",
                                    "Diferencie dados brutos de informação, destacando a ausência de significado ou análise.",
                                    "Registre 3 exemplos pessoais de dados brutos do cotidiano.",
                                    "Discuta por que dados brutos sozinhos não geram insights."
                                  ],
                                  "verification": "Escreva uma definição clara e liste 3 exemplos de dados brutos com contraste à informação.",
                                  "estimatedTime": "20 minutos",
                                  "materials": [
                                    "Papel e caneta",
                                    "Planilha de exemplo com dados de vendas (Excel ou Google Sheets)"
                                  ],
                                  "tips": "Use analogias como 'ingredientes crus' vs. 'prato pronto' para facilitar a compreensão.",
                                  "learningObjective": "Diferenciar conceitualmente dados brutos de informação processada.",
                                  "commonMistakes": "Confundir dados brutos com estatísticas já calculadas, ignorando o estágio inicial."
                                },
                                {
                                  "stepNumber": 2,
                                  "title": "Aplicar Limpeza de Dados",
                                  "subSteps": [
                                    "Identifique problemas comuns em dados brutos: valores faltantes, duplicatas ou erros de digitação.",
                                    "Remova ou corrija duplicatas usando ferramentas como filtros em planilhas.",
                                    "Preencha valores faltantes com médias ou remoções justificadas.",
                                    "Padronize formatos, como datas ou moedas.",
                                    "Salve uma versão limpa e compare com a original."
                                  ],
                                  "verification": "Compare planilha antes/depois da limpeza e documente mudanças realizadas.",
                                  "estimatedTime": "30 minutos",
                                  "materials": [
                                    "Planilha com dados brutos de vendas (ex: 50 linhas com erros)",
                                    "Software de planilhas (Excel/Google Sheets)"
                                  ],
                                  "tips": "Sempre backup os dados originais antes de editar para evitar perdas.",
                                  "learningObjective": "Executar limpeza básica para preparar dados para processamento.",
                                  "commonMistakes": "Ignorar valores outlier como erros, sem análise prévia."
                                },
                                {
                                  "stepNumber": 3,
                                  "title": "Realizar Agregação de Dados",
                                  "subSteps": [
                                    "Agrupe dados por categorias relevantes, como produto ou data.",
                                    "Calcule agregações: somas, médias, contagens ou mín/máx.",
                                    "Crie tabelas resumidas, como total de vendas por produto.",
                                    "Use funções como SUMIF ou PIVOT em planilhas.",
                                    "Valide cálculos com amostras manuais."
                                  ],
                                  "verification": "Gere uma tabela agregada e verifique totais manuais contra automáticos.",
                                  "estimatedTime": "25 minutos",
                                  "materials": [
                                    "Planilha limpa do step anterior",
                                    "Tutoriais rápidos de funções de agregação"
                                  ],
                                  "tips": "Comece com agregações simples para construir confiança antes de complexas.",
                                  "learningObjective": "Transformar dados limpos em resumos quantitativos acionáveis.",
                                  "commonMistakes": "Agregações incorretas por filtros mal aplicados, levando a somas erradas."
                                },
                                {
                                  "stepNumber": 4,
                                  "title": "Contextualizar para Formar Informação",
                                  "subSteps": [
                                    "Adicione contexto: período analisado, unidade de medida ou benchmarks.",
                                    "Interprete resultados, como 'vendas caíram 10% em relação ao mês anterior'.",
                                    "Crie visualizações simples, como gráficos de barras.",
                                    "Explique o significado para um público leigo.",
                                    "Documente o processo completo de transformação."
                                  ],
                                  "verification": "Produza um parágrafo explicativo com gráfico e interpretação contextualizada.",
                                  "estimatedTime": "25 minutos",
                                  "materials": [
                                    "Planilha agregada",
                                    "Ferramentas de gráficos (Excel/Google Sheets)"
                                  ],
                                  "tips": "Pergunte: 'O que isso significa para o negócio?' para guiar a contextualização.",
                                  "learningObjective": "Converter agregações em informação com significado prático.",
                                  "commonMistakes": "Parar na agregação sem interpretação, deixando resumos sem utilidade."
                                }
                              ],
                              "practicalExample": "Pegue uma lista bruta de 100 vendas: datas, produtos, valores (com 10% faltantes e duplicatas). Limpe erros, agregue total por produto (ex: 'Camisetas: R$5.000'), contextualize como 'Vendas de camisetas representam 40% do faturamento mensal, acima da meta de 30%'.",
                              "finalVerifications": [
                                "Explicar verbalmente o pipeline: brutos → limpeza → agregação → contexto.",
                                "Processar um novo conjunto de dados brutos em informação completa.",
                                "Identificar erros em um exemplo dado de transformação incompleta.",
                                "Criar um resumo estatístico de vendas fictícias.",
                                "Diferenciar corretamente dados, informação e conhecimento em um diagrama."
                              ],
                              "assessmentCriteria": [
                                "Precisão conceitual na definição de informação como dados processados.",
                                "Sequência lógica e completa dos passos de transformação.",
                                "Exemplos relevantes e concretos com cálculos verificáveis.",
                                "Clareza na contextualização e interpretação.",
                                "Ausência de erros comuns identificados nos steps.",
                                "Criatividade em conexões reais ao mundo profissional."
                              ],
                              "crossCurricularConnections": [
                                "Estatística: Cálculos de agregação como médias e somas.",
                                "Informática: Manipulação de dados em planilhas e programação básica.",
                                "Negócios: Análise de vendas para decisões gerenciais.",
                                "Língua Portuguesa: Redação clara de relatórios interpretativos."
                              ],
                              "realWorldApplication": "Em e-commerces, analistas limpam logs de transações, agregam por categoria e contextualizam para dashboards que guiam estoques e promoções, otimizando lucros em milhões."
                            },
                            "estimatedTime": "1 hora",
                            "difficulty": "beginner",
                            "status": "not_started",
                            "prerequisites": [
                              "10.1.2.4.1"
                            ]
                          },
                          {
                            "id": "10.1.2.4.2.2",
                            "name": "Exemplificar processos de geração de informação",
                            "description": "Identificar etapas como análise exploratória e visualização que convertem dados em informação, usando ferramentas como gráficos ou tabelas resumidas.",
                            "atomicExpansion": {
                              "steps": [
                                {
                                  "stepNumber": 1,
                                  "title": "Identificar e Descrever Dados Brutos",
                                  "subSteps": [
                                    "Selecionar um dataset simples com dados numéricos ou categóricos.",
                                    "Listar as colunas, tipos de dados e número de registros.",
                                    "Calcular estatísticas básicas como contagem, mínimo e máximo.",
                                    "Documentar observações iniciais sobre a qualidade dos dados.",
                                    "Diferenciar dados brutos (não processados) de informação (processada)."
                                  ],
                                  "verification": "Criar um relatório inicial descrevendo o dataset com estatísticas básicas.",
                                  "estimatedTime": "15-20 minutos",
                                  "materials": [
                                    "Dataset CSV de exemplo (ex: vendas ou notas de alunos)",
                                    "Planilha Excel ou Google Sheets",
                                    "Jupyter Notebook opcional"
                                  ],
                                  "tips": "Comece com datasets pequenos para facilitar a compreensão.",
                                  "learningObjective": "Reconhecer dados brutos como ponto de partida para geração de informação.",
                                  "commonMistakes": "Ignorar valores ausentes ou outliers nos dados iniciais."
                                },
                                {
                                  "stepNumber": 2,
                                  "title": "Realizar Análise Exploratória de Dados (EDA)",
                                  "subSteps": [
                                    "Calcular medidas descritivas: média, mediana, moda e desvio padrão.",
                                    "Identificar padrões, correlações e anomalias nos dados.",
                                    "Agrupar dados por categorias relevantes.",
                                    "Verificar distribuição dos dados com histogramas simples.",
                                    "Registrar insights preliminares que transformam dados em pistas informativas."
                                  ],
                                  "verification": "Gerar tabela de estatísticas descritivas e lista de 3-5 insights.",
                                  "estimatedTime": "20-30 minutos",
                                  "materials": [
                                    "Ferramentas: Excel, Python (Pandas) ou R",
                                    "Dataset do passo 1"
                                  ],
                                  "tips": "Use funções prontas como describe() no Pandas para agilizar.",
                                  "learningObjective": "Aplicar EDA para extrair padrões iniciais dos dados brutos.",
                                  "commonMistakes": "Focar apenas em números sem contexto interpretativo."
                                },
                                {
                                  "stepNumber": 3,
                                  "title": "Criar Visualizações para Converter Dados em Informação",
                                  "subSteps": [
                                    "Escolher tipo de gráfico adequado (barras, linhas, dispersão).",
                                    "Gerar gráficos ou tabelas resumidas destacando padrões.",
                                    "Adicionar rótulos, títulos e legendas claras.",
                                    "Comparar visualizações antes/depois da EDA.",
                                    "Explicar como a visualização facilita a compreensão humana."
                                  ],
                                  "verification": "Produzir pelo menos 2 gráficos ou tabelas com legendas explicativas.",
                                  "estimatedTime": "25-35 minutos",
                                  "materials": [
                                    "Ferramentas de visualização: Matplotlib/Seaborn (Python), Excel Charts, Tableau Public",
                                    "Dataset processado"
                                  ],
                                  "tips": "Mantenha gráficos simples: evite sobrecarga visual.",
                                  "learningObjective": "Usar visualizações para transformar dados em informação acessível.",
                                  "commonMistakes": "Escolher gráfico inadequado que distorce os dados."
                                },
                                {
                                  "stepNumber": 4,
                                  "title": "Interpretar e Exemplificar a Geração de Informação",
                                  "subSteps": [
                                    "Resumir os insights das análises e visualizações.",
                                    "Explicar o fluxo: dados brutos → EDA → visualização → informação.",
                                    "Criar narrativa conectando etapas ao contexto real.",
                                    "Validar se a informação gerada responde a uma pergunta inicial.",
                                    "Documentar o processo completo em um relatório curto."
                                  ],
                                  "verification": "Elaborar um exemplo narrativo completo do processo de geração de informação.",
                                  "estimatedTime": "15-25 minutos",
                                  "materials": [
                                    "Gráficos e tabelas dos passos anteriores",
                                    "Editor de texto ou slides"
                                  ],
                                  "tips": "Use perguntas guiadas como 'O que isso revela?' para interpretação.",
                                  "learningObjective": "Sintetizar o processo de conversão de dados em informação acionável.",
                                  "commonMistakes": "Pular a interpretação, deixando visualizações sem explicação."
                                }
                              ],
                              "practicalExample": "Com um dataset de vendas mensais de uma loja (colunas: mês, produto, valor), realize EDA para calcular médias por produto, crie gráfico de barras mostrando top produtos e tabelas resumidas, gerando informação como 'Produto A representa 40% das vendas, sugerindo foco em estoque'.",
                              "finalVerifications": [
                                "Pode listar e explicar as 4 etapas principais do processo.",
                                "Produz visualizações claras que destacam padrões nos dados.",
                                "Diferencia corretamente dados brutos de informação processada.",
                                "Fornece exemplo prático com dataset real ou simulado.",
                                "Documenta insights interpretativos em narrativa coerente.",
                                "Identifica pelo menos uma conexão com aplicação real."
                              ],
                              "assessmentCriteria": [
                                "Precisão na identificação de etapas de EDA e visualização (30%).",
                                "Qualidade e relevância das visualizações criadas (25%).",
                                "Profundidade da interpretação e narrativa final (20%).",
                                "Uso correto de ferramentas e materiais (15%).",
                                "Identificação de erros comuns evitados (10%)."
                              ],
                              "crossCurricularConnections": [
                                "Estatística: Medidas descritivas e distribuições.",
                                "Programação: Uso de Python/R para EDA e plots.",
                                "Comunicação: Narrativas e relatórios visuais.",
                                "Negócios: Análise de dados para decisões estratégicas."
                              ],
                              "realWorldApplication": "Em empresas de e-commerce, analistas usam EDA e gráficos para transformar logs de vendas em dashboards que informam estratégias de estoque e marketing, otimizando lucros."
                            },
                            "estimatedTime": "1 hora",
                            "difficulty": "beginner",
                            "status": "not_started",
                            "prerequisites": [
                              "10.1.2.4.1"
                            ]
                          },
                          {
                            "id": "10.1.2.4.2.3",
                            "name": "Distinguir informação de dados brutos",
                            "description": "Comparar dados brutos (sem significado) com informação (com relevância contextual), destacando o valor agregado pelo processamento.",
                            "atomicExpansion": {
                              "steps": [
                                {
                                  "stepNumber": 1,
                                  "title": "Compreender a definição e características de dados brutos",
                                  "subSteps": [
                                    "Leia a definição: Dados brutos são fatos isolados, sem contexto ou interpretação, como números ou strings sem significado agregado.",
                                    "Identifique características: Não processados, ambíguos, sem relevância imediata.",
                                    "Liste exemplos: Temperaturas registradas (25, 30, 35), cliques em um site (100, 200, 150).",
                                    "Discuta por que eles são 'brutos': Falta de análise ou contexto.",
                                    "Crie sua própria lista de 5 exemplos de dados brutos do dia a dia."
                                  ],
                                  "verification": "Liste 5 exemplos corretos de dados brutos e explique por que cada um é bruto.",
                                  "estimatedTime": "20 minutos",
                                  "materials": [
                                    "Folha de papel ou editor de texto",
                                    "Acesso a exemplos online de datasets brutos (ex: Kaggle)"
                                  ],
                                  "tips": "Foquem em elementos sem interpretação; evite adicionar contexto prematuramente.",
                                  "learningObjective": "Definir e reconhecer dados brutos com precisão.",
                                  "commonMistakes": [
                                    "Confundir com informação já processada",
                                    "Usar dados com contexto implícito"
                                  ]
                                },
                                {
                                  "stepNumber": 2,
                                  "title": "Compreender a definição e características de informação",
                                  "subSteps": [
                                    "Leia a definição: Informação é dados brutos processados com contexto, análise ou relevância, gerando significado.",
                                    "Identifique características: Contextualizada, interpretada, útil para decisões.",
                                    "Liste exemplos: Média de temperaturas (30°C, indicando clima quente); Taxa de cliques (aumento de 50%, sugerindo campanha eficaz).",
                                    "Analise o valor agregado: Processamento como agregação, categorização ou visualização.",
                                    "Crie sua própria lista de 5 exemplos de informação derivados de dados brutos."
                                  ],
                                  "verification": "Transforme 3 dados brutos em informação adicionando contexto e explique o processo.",
                                  "estimatedTime": "25 minutos",
                                  "materials": [
                                    "Mesmos do step 1",
                                    "Gráficos simples ou calculadora para agregações"
                                  ],
                                  "tips": "Sempre pergunte: 'O que isso significa no contexto?' para validar informação.",
                                  "learningObjective": "Reconhecer como o processamento eleva dados a informação.",
                                  "commonMistakes": [
                                    "Chamar dados brutos de informação sem processamento",
                                    "Ignorar o contexto necessário"
                                  ]
                                },
                                {
                                  "stepNumber": 3,
                                  "title": "Comparar dados brutos e informação lado a lado",
                                  "subSteps": [
                                    "Crie uma tabela comparativa: Colunas para dados brutos vs. informação, linhas para exemplos.",
                                    "Destaque diferenças: Ausência/presença de contexto, utilidade, processamento envolvido.",
                                    "Analise um caso: Pegue dados de vendas brutas (R$100, R$200) e processe para informação (média diária R$150, tendência crescente).",
                                    "Discuta o 'valor agregado': Como a informação suporta decisões.",
                                    "Preencha a tabela com 3 pares de exemplos originais."
                                  ],
                                  "verification": "Apresente tabela comparativa com 3 pares corretos e explicações.",
                                  "estimatedTime": "30 minutos",
                                  "materials": [
                                    "Planilha (Google Sheets ou Excel)",
                                    "Exemplos de datasets reais"
                                  ],
                                  "tips": "Use setas ou destaques visuais para mostrar transformação.",
                                  "learningObjective": "Distinguir visual e conceitualmente os dois conceitos.",
                                  "commonMistakes": [
                                    "Equiparar os dois sem destacar processamento",
                                    "Exemplos inconsistentes"
                                  ]
                                },
                                {
                                  "stepNumber": 4,
                                  "title": "Praticar distinção em cenários reais e verificar compreensão",
                                  "subSteps": [
                                    "Analise cenários mistos: Classifique itens como dados ou informação (ex: lista de IPs vs. relatório de tráfego).",
                                    "Crie fluxos: Desenhe como dados brutos viram informação em 2 contextos (saúde, negócios).",
                                    "Teste-se: Classifique 10 itens aleatórios.",
                                    "Reflita: Escreva um parágrafo sobre importância da distinção.",
                                    "Compartilhe e receba feedback em um fórum ou parceiro."
                                  ],
                                  "verification": "Classifique corretamente 10/10 itens em um quiz autoaplicado.",
                                  "estimatedTime": "35 minutos",
                                  "materials": [
                                    "Quiz online (Quizlet ou similar)",
                                    "Artigos sobre ciência de dados"
                                  ],
                                  "tips": "Pratique com dados reais para fixar; revise erros imediatamente.",
                                  "learningObjective": "Aplicar distinção de forma autônoma em contextos variados.",
                                  "commonMistakes": [
                                    "Subestimar necessidade de contexto",
                                    "Classificações subjetivas sem critérios"
                                  ]
                                }
                              ],
                              "practicalExample": "Dados brutos: Lista de temperaturas diárias [22°C, 28°C, 35°C, 20°C]. Informação: Média semanal de 26.25°C com pico de calor no dia 3, indicando necessidade de alerta climático.",
                              "finalVerifications": [
                                "Define corretamente dados brutos como não processados e sem contexto.",
                                "Explica o papel do processamento na criação de informação.",
                                "Fornece exemplos precisos de ambos com transformação demonstrada.",
                                "Classifica novos itens como dados ou informação com 90% de acerto.",
                                "Descreve valor agregado da informação para decisões.",
                                "Identifica processamento comum (agregação, categorização)."
                              ],
                              "assessmentCriteria": [
                                "Precisão conceitual (definições claras e corretas).",
                                "Qualidade de exemplos (concretos, relevantes, transformados).",
                                "Profundidade de comparação (tabelas ou diagramas eficazes).",
                                "Aplicação prática (classificações autônomas sem erros).",
                                "Reflexão crítica (explicação de implicações).",
                                "Criatividade em cenários (exemplos originais e variados)."
                              ],
                              "crossCurricularConnections": [
                                "Estatística: Agregação e análise descritiva de dados.",
                                "Programação: Processamento de dados em Python (pandas).",
                                "Negócios: Dashboards e relatórios gerenciais.",
                                "Ciências: Observações experimentais vs. conclusões.",
                                "Linguagem: Interpretação textual de dados jornalísticos."
                              ],
                              "realWorldApplication": "Em análise de dados empresariais, distinguir vendas brutas (números diários) de informação (tendências de mercado) permite prever demandas e otimizar estoques, evitando perdas financeiras."
                            },
                            "estimatedTime": "0.5 hora",
                            "difficulty": "beginner",
                            "status": "not_started",
                            "prerequisites": [
                              "10.1.2.4.1"
                            ]
                          }
                        ]
                      },
                      {
                        "id": "10.1.2.4.3",
                        "name": "Conhecimento Derivado",
                        "description": "Conhecimento é a interpretação avançada e aplicação da informação, gerando insights acionáveis, regras ou modelos preditivos, culminando a hierarquia DIK na ciência de dados.",
                        "specificSkills": [
                          {
                            "id": "10.1.2.4.3.1",
                            "name": "Definir conhecimento derivado",
                            "description": "Descrever conhecimento como o nível superior onde informação é sintetizada em entendimentos profundos, como modelos de machine learning ou regras de negócio.",
                            "atomicExpansion": {
                              "steps": [
                                {
                                  "stepNumber": 1,
                                  "title": "Revisar a Hierarquia DIKW: Dados, Informação e Conhecimento",
                                  "subSteps": [
                                    "Estude a pirâmide DIKW: Dados como fatos brutos, Informação como dados processados com contexto, Conhecimento como síntese de informação em padrões compreensíveis.",
                                    "Identifique exemplos simples: Dados (números de vendas), Informação (gráfico de tendências), Conhecimento (regra: 'vendas caem em feriados').",
                                    "Compare os níveis para entender a progressão hierárquica.",
                                    "Anote diferenças chave em um diagrama pessoal.",
                                    "Discuta com um parceiro ou em fórum por que conhecimento requer insight humano ou algorítmico."
                                  ],
                                  "verification": "Crie um diagrama da pirâmide DIKW com pelo menos um exemplo por nível e explique verbalmente ou por escrito.",
                                  "estimatedTime": "45 minutos",
                                  "materials": [
                                    "Artigo 'DIKW Pyramid' no Wikipedia",
                                    "Ferramenta de desenho como Draw.io ou papel e caneta"
                                  ],
                                  "tips": "Use analogias cotidianas, como receita de bolo, para fixar conceitos.",
                                  "learningObjective": "Compreender a progressão de dados para conhecimento como base para derivado.",
                                  "commonMistakes": "Confundir informação com conhecimento; lembre que conhecimento envolve síntese e aplicação."
                                },
                                {
                                  "stepNumber": 2,
                                  "title": "Definir Conhecimento Derivado Especificamente",
                                  "subSteps": [
                                    "Defina conhecimento derivado: Nível superior onde informação é sintetizada em entendimentos profundos, modelos ou regras acionáveis.",
                                    "Destaque características: Abstração, generalização, predição (ex: modelo de ML que prevê churn de clientes).",
                                    "Explore fontes: Inferência humana, algoritmos de aprendizado de máquina ou regras de negócio extraídas de dados.",
                                    "Escreva uma definição pessoal em 2-3 frases.",
                                    "Compare com conhecimento explícito (documentado) vs. tácito (intuitivo)."
                                  ],
                                  "verification": "Escreva e recite uma definição precisa de conhecimento derivado, citando 2 características.",
                                  "estimatedTime": "30 minutos",
                                  "materials": [
                                    "Slides ou vídeo sobre 'Data-Information-Knowledge Pyramid' no YouTube",
                                    "Notebook para anotações"
                                  ],
                                  "tips": "Pense em 'derivado' como derivada em cálculo: algo novo emerge da análise.",
                                  "learningObjective": "Formular uma definição clara e precisa de conhecimento derivado.",
                                  "commonMistakes": "Reduzir a meros fatos; enfatize síntese profunda e utilidade preditiva."
                                },
                                {
                                  "stepNumber": 3,
                                  "title": "Explorar Exemplos em Ciência de Dados",
                                  "subSteps": [
                                    "Analise exemplo: Dados de housing (preços, localização) → Informação (correlações) → Conhecimento derivado (modelo de regressão preditiva).",
                                    "Crie outro exemplo: Dados de tráfego web → Informação (padrões de uso) → Conhecimento (regra de negócio: otimizar ads para horários pico).",
                                    "Use um dataset simples para mapear os níveis.",
                                    "Discuta como ML automatiza derivação de conhecimento.",
                                    "Registre 3 exemplos pessoais de diferentes domínios."
                                  ],
                                  "verification": "Produza um mapa conceitual com 2 exemplos completos da hierarquia culminando em conhecimento derivado.",
                                  "estimatedTime": "60 minutos",
                                  "materials": [
                                    "Dataset de exemplo no Kaggle (Boston Housing)",
                                    "Python/Jupyter para visualização básica"
                                  ],
                                  "tips": "Comece com domínios familiares para tornar abstrato concreto.",
                                  "learningObjective": "Aplicar definição a exemplos reais em ciência de dados.",
                                  "commonMistakes": "Ignorar o 'derivado'; certifique-se de mostrar transformação via síntese."
                                },
                                {
                                  "stepNumber": 4,
                                  "title": "Diferenciar e Sintetizar Aplicações",
                                  "subSteps": [
                                    "Compare conhecimento derivado com outros níveis em uma tabela.",
                                    "Discuta desafios: Viés em derivação, necessidade de validação.",
                                    "Sintetize em um parágrafo: 'Conhecimento derivado é... porque...'.",
                                    "Planeje uma aplicação pessoal: Como derivar conhecimento de seus dados diários.",
                                    "Revise e refine com feedback autoavaliado."
                                  ],
                                  "verification": "Crie uma tabela comparativa e um ensaio curto (200 palavras) sintetizando o conceito.",
                                  "estimatedTime": "45 minutos",
                                  "materials": [
                                    "Planilha Google Sheets para tabela",
                                    "Timer para escrita estruturada"
                                  ],
                                  "tips": "Use bullet points primeiro para organizar pensamentos antes de prose.",
                                  "learningObjective": "Diferenciar e internalizar conhecimento derivado para uso futuro.",
                                  "commonMistakes": "Generalizar demais; foque em precisão hierárquica."
                                }
                              ],
                              "practicalExample": "Em um dataset de e-commerce: Dados (cliques por usuário), Informação (taxa de conversão por categoria), Conhecimento derivado (modelo de recomendação que prevê compras futuras, sintetizando padrões em regras como 'usuários de eletrônicos compram acessórios em 70% dos casos'). Implemente em Python com scikit-learn para gerar predições.",
                              "finalVerifications": [
                                "Explique a hierarquia DIKW sem hesitação.",
                                "Defina conhecimento derivado com exemplo preciso.",
                                "Mapeie um dataset real aos 3 níveis.",
                                "Diferencie de informação em 2 cenários.",
                                "Crie um exemplo original de aplicação em negócios.",
                                "Valide com diagrama ou tabela."
                              ],
                              "assessmentCriteria": [
                                "Precisão na definição (síntese profunda, não factual).",
                                "Uso de exemplos relevantes de ciência de dados.",
                                "Clareza na diferenciação hierárquica.",
                                "Profundidade em verificações e síntese.",
                                "Criatividade em aplicações derivadas.",
                                "Completude de todos os elementos (substeps, tips, etc.)."
                              ],
                              "crossCurricularConnections": [
                                "Filosofia: Epistemologia e níveis de saber (Platão).",
                                "Negócios: Regras de negócio e estratégia (MBA).",
                                "Inteligência Artificial: Modelos de ML como conhecimento derivado.",
                                "Estatística: Inferência e modelagem preditiva."
                              ],
                              "realWorldApplication": "Em empresas de tech, cientistas de dados derivam conhecimento de big data para criar modelos de ML que otimizam lucros, como na Netflix recomendando conteúdos ou no Google Ads prevendo cliques, transformando informação em decisões estratégicas acionáveis."
                            },
                            "estimatedTime": "1 hora",
                            "difficulty": "beginner",
                            "status": "not_started",
                            "prerequisites": [
                              "10.1.2.4.2"
                            ]
                          },
                          {
                            "id": "10.1.2.4.3.2",
                            "name": "Explicar a hierarquia completa DIK",
                            "description": "Mapear a pirâmide hierárquica: dados → informação → conhecimento, ilustrando fluxos em projetos de ciência de dados com exemplos práticos.",
                            "atomicExpansion": {
                              "steps": [
                                {
                                  "stepNumber": 1,
                                  "title": "Entender as Definições Fundamentais de Dados, Informação e Conhecimento",
                                  "subSteps": [
                                    "Pesquise e memorize a definição clássica de Dados: fatos brutos sem contexto (ex: números isolados).",
                                    "Defina Informação: dados processados com contexto, respondendo 'o quê' e 'quando' (ex: médias calculadas).",
                                    "Defina Conhecimento: informação organizada, contextualizada e acionável, respondendo 'por quê' e 'como usar' (ex: insights estratégicos).",
                                    "Compare os três conceitos usando uma tabela simples para destacar diferenças.",
                                    "Leia trechos de fontes como o livro 'Data to Knowledge' ou artigos de Russell Ackoff sobre DIK."
                                  ],
                                  "verification": "Crie um glossário pessoal com definições em suas próprias palavras e exemplos iniciais; revise com um colega ou autoavaliação.",
                                  "estimatedTime": "45 minutos",
                                  "materials": "Notebook, acesso à internet para artigos (Wikipedia DIK, Ackoff pyramid), papel e caneta para tabela.",
                                  "tips": "Use analogias cotidianas, como ingredientes crus (dados) vs. receita pronta (conhecimento).",
                                  "learningObjective": "Dominar as definições precisas e distinções entre Dados, Informação e Conhecimento.",
                                  "commonMistakes": "Confundir informação com conhecimento (ignorar ação) ou dados com informação (faltar contexto)."
                                },
                                {
                                  "stepNumber": 2,
                                  "title": "Construir a Pirâmide Hierárquica DIK",
                                  "subSteps": [
                                    "Desenhe a pirâmide visual: base ampla 'Dados', meio 'Informação', topo estreito 'Conhecimento'.",
                                    "Identifique setas de fluxo: Dados → processados → Informação → analisados → Conhecimento.",
                                    "Adicione atributos a cada nível: volume alto/baixo contexto para Dados; relevância para Informação; utilidade para Conhecimento.",
                                    "Inclua limitações: nem todos os dados viram conhecimento; perda de informação em agregações.",
                                    "Refatore o diagrama digitalmente usando ferramentas como Draw.io ou Lucidchart."
                                  ],
                                  "verification": "Produza um diagrama final da pirâmide DIK e explique verbalmente o fluxo para si mesmo ou grave um vídeo curto.",
                                  "estimatedTime": "60 minutos",
                                  "materials": "Ferramenta de diagramação (Draw.io gratuito), papel para rascunhos iniciais.",
                                  "tips": "Mantenha a pirâmide assimétrica para enfatizar que conhecimento é raro e valioso.",
                                  "learningObjective": "Visualizar e mapear graficamente a hierarquia DIK com fluxos claros.",
                                  "commonMistakes": "Representar como linha reta em vez de pirâmide (ignora volume decrescente) ou inverter níveis."
                                },
                                {
                                  "stepNumber": 3,
                                  "title": "Analisar Fluxos DIK em Projetos de Ciência de Dados",
                                  "subSteps": [
                                    "Escolha um dataset simples (ex: Kaggle Titanic ou vendas hipotéticas).",
                                    "Mapeie: Dados brutos (CSV raw) → ETL para Informação (estatísticas descritivas).",
                                    "Continue: Modelagem ML para Conhecimento (previsões acionáveis).",
                                    "Descreva ferramentas típicas: Pandas para dados→info, Scikit-learn para info→conhecimento.",
                                    "Documente gargalos comuns, como dados sujos impedindo fluxo."
                                  ],
                                  "verification": "Crie um fluxograma de um projeto de dados real, rotulando cada etapa com nível DIK.",
                                  "estimatedTime": "75 minutos",
                                  "materials": "Jupyter Notebook, dataset Kaggle (Titanic.csv), bibliotecas Python (Pandas, Matplotlib).",
                                  "tips": "Comece com dataset pequeno para evitar sobrecarga; foque em 3-5 etapas chave.",
                                  "learningObjective": "Aplicar a hierarquia DIK ao pipeline típico de ciência de dados.",
                                  "commonMistakes": "Pular processamento (assumir dados limpos são informação) ou superestimar modelos como conhecimento direto."
                                },
                                {
                                  "stepNumber": 4,
                                  "title": "Sintetizar e Ilustrar com Exemplos Práticos",
                                  "subSteps": [
                                    "Crie 2-3 exemplos completos: um de saúde (pacientes dados → diagnósticos conhecimento).",
                                    "Desenvolva narrativa: 'Dados de sensores → dashboard info → estratégia de negócios conhecimento'.",
                                    "Teste compreensão ensinando a outro (rubber duck debugging).",
                                    "Revise e refine exemplos para clareza e precisão.",
                                    "Prepare um resumo de 1 página da hierarquia DIK aplicada."
                                  ],
                                  "verification": "Apresente um exemplo completo em um slide ou relatório curto, autoavaliando cobertura de todos níveis.",
                                  "estimatedTime": "50 minutos",
                                  "materials": "Google Slides ou Word para resumo, exemplos de datasets reais.",
                                  "tips": "Use histórias reais de empresas como Netflix (dados de visualização → recomendações conhecimento).",
                                  "learningObjective": "Capacitar-se para explicar DIK com fluência usando exemplos contextualizados.",
                                  "commonMistakes": "Exemplos vagos ou irrelevantes; não ligar de volta à pirâmide."
                                }
                              ],
                              "practicalExample": "Em um projeto de e-commerce: Dados = logs de cliques (10M entradas brutas). Informação = relatórios agregados (taxa de conversão por produto = 2.5%). Conhecimento = Insight acionável (remover produtos com <1% conversão para aumentar receita em 15%). Fluxo: ETL no AWS → dashboard Tableau → decisão executiva.",
                              "finalVerifications": [
                                "Desenhe a pirâmide DIK corretamente de memória em <2 minutos.",
                                "Explique verbalmente o fluxo dados→conhecimento em um projeto de dados com exemplo específico.",
                                "Identifique e corrija um erro comum em um fluxograma DIK fornecido.",
                                "Aplique DIK a um novo dataset simples, rotulando níveis.",
                                "Ensine a hierarquia a um par, respondendo dúvidas com precisão.",
                                "Escreva um parágrafo síntese sem consultar notas."
                              ],
                              "assessmentCriteria": [
                                "Precisão conceitual: Definições e hierarquia exatas (90%+ acurácia).",
                                "Profundidade de exemplos: Pelo menos 2 fluxos práticos em ciência de dados.",
                                "Clareza visual: Diagramas legíveis e hierárquicos corretos.",
                                "Aplicação contextual: Ligações claras a projetos reais de dados.",
                                "Completude: Todos níveis DIK cobertos com fluxos bidirecionais se relevante.",
                                "Originalidade: Exemplos personalizados, não copiados."
                              ],
                              "crossCurricularConnections": [
                                "Filosofia: Epistemologia e teoria do conhecimento (Platão, epistemes).",
                                "Negócios: Tomada de decisões baseadas em dados (Business Intelligence).",
                                "Informática: Bancos de dados e pipelines ETL (SQL, Big Data).",
                                "Estatística: Análise exploratória de dados (EDA) como ponte info-conhecimento.",
                                "Psicologia: Processamento cognitivo humano vs. máquinas."
                              ],
                              "realWorldApplication": "Em empresas como Google ou Amazon, a hierarquia DIK guia desde coleta de dados massivos (logs de usuário) até conhecimento acionável (personalização de anúncios), otimizando receitas bilhões e informando estratégias globais de IA."
                            },
                            "estimatedTime": "1.5 horas",
                            "difficulty": "beginner",
                            "status": "not_started",
                            "prerequisites": [
                              "10.1.2.4.1",
                              "10.1.2.4.2"
                            ]
                          },
                          {
                            "id": "10.1.2.4.3.3",
                            "name": "Aplicar distinções em cenários reais",
                            "description": "Analisar um caso de estudo simples, classificando elementos como dados, informação ou conhecimento, e justificando as transições hierárquicas.",
                            "atomicExpansion": {
                              "steps": [
                                {
                                  "stepNumber": 1,
                                  "title": "Selecionar e Compreender o Caso de Estudo",
                                  "subSteps": [
                                    "Escolha um caso de estudo simples relacionado a um domínio familiar, como vendas em uma loja ou dados de saúde de pacientes.",
                                    "Leia o caso completamente, destacando fatos brutos, processamentos e insights derivados.",
                                    "Identifique o contexto hierárquico: o que é apresentado como entrada bruta versus saída interpretada.",
                                    "Anote o objetivo principal do caso: por exemplo, prever demandas ou diagnosticar problemas.",
                                    "Resuma o caso em 3-5 frases para confirmar compreensão."
                                  ],
                                  "verification": "Confirme se o resumo captura todos os elementos chave sem omissões ou distorções.",
                                  "estimatedTime": "20 minutos",
                                  "materials": "Caso de estudo impresso ou digital, caderno de anotações, caneta.",
                                  "tips": "Escolha um caso curto (1-2 páginas) para evitar sobrecarga inicial.",
                                  "learningObjective": "Dominar a seleção e compreensão inicial de cenários reais para análise hierárquica.",
                                  "commonMistakes": "Escolher casos muito complexos ou irrelevantes, pulando a leitura atenta."
                                },
                                {
                                  "stepNumber": 2,
                                  "title": "Classificar Elementos como Dados, Informação ou Conhecimento",
                                  "subSteps": [
                                    "Liste todos os elementos brutos do caso (ex: números de vendas diárias = dados).",
                                    "Classifique cada elemento: dados (fatos crus), informação (dados processados com contexto), conhecimento (insights acionáveis).",
                                    "Crie uma tabela com colunas: Elemento, Classificação, Justificativa breve.",
                                    "Revise a tabela para garantir que pelo menos 5-10 elementos sejam classificados.",
                                    "Marque elementos ambíguos para refinamento posterior."
                                  ],
                                  "verification": "Verifique se 100% dos elementos listados têm classificação e justificativa plausível.",
                                  "estimatedTime": "30 minutos",
                                  "materials": "Planilha ou tabela em papel/excel, caso de estudo.",
                                  "tips": "Use definições claras: Dados = 'o quê?', Informação = 'o quê + por quê?', Conhecimento = 'o que fazer com isso'.",
                                  "learningObjective": "Aplicar classificações precisas na hierarquia DIK (Dados-Informação-Conhecimento).",
                                  "commonMistakes": "Confundir informação com conhecimento por falta de ação implícita."
                                },
                                {
                                  "stepNumber": 3,
                                  "title": "Justificar Transições Hierárquicas",
                                  "subSteps": [
                                    "Trace caminhos de transição: ex: dados de vendas → média mensal (informação) → estratégia de estoque (conhecimento).",
                                    "Para cada transição, explique o processo (agregação, contexto, inferência).",
                                    "Identifique pelo menos 3 transições principais e justifique com evidências do caso.",
                                    "Avalie gaps: onde uma transição está ausente ou fraca.",
                                    "Documente em diagrama de fluxo: setas conectando níveis hierárquicos."
                                  ],
                                  "verification": "Confirme se todas as transições têm explicação causal e evidência textual.",
                                  "estimatedTime": "25 minutos",
                                  "materials": "Papel para diagrama, software de desenho simples (ex: Draw.io).",
                                  "tips": "Pense em camadas: cada transição adiciona valor (processamento + interpretação).",
                                  "learningObjective": "Explicar mecanicamente como dados evoluem para conhecimento em cenários reais.",
                                  "commonMistakes": "Ignorar o contexto ou pular justificativas para transições implícitas."
                                },
                                {
                                  "stepNumber": 4,
                                  "title": "Sintetizar Análise e Refletir sobre Implicações",
                                  "subSteps": [
                                    "Escreva um parágrafo síntese: classificações principais e transições chave.",
                                    "Discuta implicações: como o conhecimento derivado impacta decisões reais.",
                                    "Compare com outro caso similar para generalização.",
                                    "Autoavalie a análise usando critérios de clareza e completude.",
                                    "Prepare um relatório final de 1 página."
                                  ],
                                  "verification": "O relatório cobre classificação, transições e implicações sem lacunas.",
                                  "estimatedTime": "25 minutos",
                                  "materials": "Processador de texto, relatório anterior.",
                                  "tips": "Use bullet points para síntese clara antes de redigir parágrafos.",
                                  "learningObjective": "Integrar análise em uma narrativa coesa aplicável a cenários reais.",
                                  "commonMistakes": "Focar só em descrição sem reflexão sobre implicações práticas."
                                }
                              ],
                              "practicalExample": "Caso: Uma loja registra vendas diárias (dados: 50 unidades dia 1, 70 dia 2). Média semanal = 60 (informação: tendência estável). Conhecimento: Aumentar estoque em 20% para picos sazonais, baseado em padrões históricos.",
                              "finalVerifications": [
                                "Todos os elementos do caso estão classificados corretamente como dados, informação ou conhecimento.",
                                "Pelo menos 3 transições hierárquicas são explicitamente justificadas com processos claros.",
                                "Diagrama ou tabela visualiza a hierarquia DIK de forma compreensível.",
                                "Síntese identifica implicações práticas derivadas do conhecimento.",
                                "Análise é livre de confusões entre níveis hierárquicos.",
                                "Relatório final é conciso e autoexplicativo."
                              ],
                              "assessmentCriteria": [
                                "Precisão na classificação (90%+ acurácia nas categorias DIK).",
                                "Profundidade das justificativas para transições (causalidade clara).",
                                "Completude da cobertura do caso (todos elementos abordados).",
                                "Clareza e organização visual (tabelas/diagramas eficazes).",
                                "Relevância das implicações ao mundo real.",
                                "Originalidade na reflexão e generalização."
                              ],
                              "crossCurricularConnections": [
                                "Estatística: Agregação de dados em médias e tendências.",
                                "Filosofia: Epistemologia e níveis de saber (DIK pyramid).",
                                "Negócios/Gestão: Tomada de decisões baseada em conhecimento derivado.",
                                "TI/Sistemas de Informação: Bancos de dados e BI tools."
                              ],
                              "realWorldApplication": "Em análise de dados de e-commerce, classificar logs de cliques (dados) em padrões de usuário (informação) leva a recomendações personalizadas (conhecimento), otimizando vendas em plataformas como Amazon."
                            },
                            "estimatedTime": "1 hora",
                            "difficulty": "intermediate",
                            "status": "not_started",
                            "prerequisites": [
                              "10.1.2.4.1",
                              "10.1.2.4.2"
                            ]
                          }
                        ]
                      }
                    ]
                  },
                  {
                    "id": "10.1.2.5",
                    "name": "Introdução ao Aprendizado Supervisionado",
                    "description": "Conceitos iniciais de aprendizado com rótulos, incluindo exemplos como classificação e regressão.",
                    "individualConcepts": [
                      {
                        "id": "10.1.2.5.1",
                        "name": "Definição e Características do Aprendizado Supervisionado",
                        "description": "Conceitos básicos do aprendizado supervisionado, incluindo o uso de dados rotulados, o papel do rótulo na predição e a distinção em relação a outros tipos de aprendizado.",
                        "specificSkills": [
                          {
                            "id": "10.1.2.5.1.1",
                            "name": "Definir aprendizado supervisionado",
                            "description": "Explicar o que é aprendizado supervisionado, destacando que utiliza um conjunto de dados rotulados (pares entrada-saída) para treinar um modelo que generalize predições para novos dados.",
                            "atomicExpansion": {
                              "steps": [
                                {
                                  "stepNumber": 1,
                                  "title": "Entender os Fundamentos do Aprendizado de Máquina",
                                  "subSteps": [
                                    "Revise a definição básica de aprendizado de máquina: sistemas que aprendem padrões de dados sem programação explícita.",
                                    "Diferencie aprendizado de máquina de programação tradicional, usando analogias como 'ensinar uma criança com exemplos'.",
                                    "Identifique os três tipos principais: supervisionado, não-supervisionado e por reforço.",
                                    "Assista a um vídeo introdutório de 5 minutos sobre ML para contextualizar.",
                                    "Anote as diferenças chave entre os tipos de aprendizado."
                                  ],
                                  "verification": "Escreva um parágrafo resumindo os fundamentos do ML e liste os três tipos principais.",
                                  "estimatedTime": "20 minutos",
                                  "materials": [
                                    "Vídeo introdutório no YouTube: 'Machine Learning Basics' (ex: canal de Andrew Ng)",
                                    "Caderno para anotações"
                                  ],
                                  "tips": "Use analogias cotidianas para fixar conceitos abstratos.",
                                  "learningObjective": "Compreender o escopo geral do aprendizado de máquina como base para o supervisionado.",
                                  "commonMistakes": "Confundir ML com programação tradicional; assumir que ML não precisa de dados."
                                },
                                {
                                  "stepNumber": 2,
                                  "title": "Explorar Dados Rotulados e Pares Entrada-Saída",
                                  "subSteps": [
                                    "Defina 'dados rotulados': conjuntos onde cada entrada (features) tem uma saída conhecida (rótulo).",
                                    "Examine exemplos: imagem de gato com rótulo 'gato'; email com rótulo 'spam'.",
                                    "Crie um dataset simples manualmente com 5 exemplos de pares entrada-saída.",
                                    "Discuta por que rótulos são essenciais: fornecem 'supervisão' ao modelo.",
                                    "Compare com dados não-rotulados para destacar a diferença."
                                  ],
                                  "verification": "Crie e liste um dataset de 5 pares entrada-saída para um problema simples, como classificação de frutas.",
                                  "estimatedTime": "25 minutos",
                                  "materials": [
                                    "Papel e caneta ou Google Sheets para criar dataset",
                                    "Artigo: 'What are Labeled Data?' no Towards Data Science"
                                  ],
                                  "tips": "Comece com problemas do dia a dia para tornar os dados relacionáveis.",
                                  "learningObjective": "Dominar o conceito de dados rotulados como pilar do aprendizado supervisionado.",
                                  "commonMistakes": "Ignorar a qualidade dos rótulos; confundir features com rótulos."
                                },
                                {
                                  "stepNumber": 3,
                                  "title": "Descrever o Processo de Treinamento Supervisionado",
                                  "subSteps": [
                                    "Explique o fluxo: dividir dados em treino/teste; alimentar modelo com pares entrada-saída.",
                                    "Descreva como o modelo ajusta parâmetros para minimizar erros (função de perda).",
                                    "Simule um treinamento simples: use um diagrama para mostrar iterações de ajuste.",
                                    "Identifique componentes chave: modelo (ex: regressão linear), otimizador, epochs.",
                                    "Registre o objetivo: aprender a mapear entradas para saídas corretas."
                                  ],
                                  "verification": "Desenhe um fluxograma do processo de treinamento com legendas para cada etapa.",
                                  "estimatedTime": "30 minutos",
                                  "materials": [
                                    "Ferramenta de desenho como Draw.io ou papel",
                                    "Tutorial interativo: Scikit-learn supervised learning intro"
                                  ],
                                  "tips": "Pense no modelo como um aluno recebendo feedback constante dos rótulos.",
                                  "learningObjective": "Articular o mecanismo de treinamento usando dados rotulados.",
                                  "commonMistakes": "Esquecer a divisão treino/teste; superestimar o papel dos rótulos sem ajuste de parâmetros."
                                },
                                {
                                  "stepNumber": 4,
                                  "title": "Compreender Generalização e Predições para Novos Dados",
                                  "subSteps": [
                                    "Defina generalização: capacidade do modelo de predizer corretamente dados não vistos.",
                                    "Discuta overfitting (aprende treino demais, falha em novos) vs underfitting.",
                                    "Teste conceitualmente: use seu dataset para simular predições em 'novos' dados.",
                                    "Explique avaliação: métricas como acurácia em conjunto de teste.",
                                    "Conclua a definição completa: 'Aprendizado supervisionado usa dados rotulados para treinar modelos que generalizam predições.'"
                                  ],
                                  "verification": "Explique em 1 minuto gravado ou escrito como o modelo generaliza para novos dados.",
                                  "estimatedTime": "25 minutos",
                                  "materials": [
                                    "Seu dataset do Step 2",
                                    "Gravação de áudio ou vídeo simples no celular"
                                  ],
                                  "tips": "Sempre valide com dados de teste para medir generalização real.",
                                  "learningObjective": "Integrar todos os elementos na definição final de aprendizado supervisionado.",
                                  "commonMistakes": "Confundir memorização (overfitting) com aprendizado verdadeiro."
                                }
                              ],
                              "practicalExample": "Em um sistema de detecção de spam: dados rotulados incluem emails com rótulos 'spam' ou 'não-spam'. O modelo treina com esses pares (texto do email → rótulo), aprendendo padrões como palavras-chave, e generaliza para classificar novos emails recebidos.",
                              "finalVerifications": [
                                "Explique a definição completa em suas próprias palavras sem consultar notas.",
                                "Diferencie aprendizado supervisionado de não-supervisionado com exemplos.",
                                "Crie um par entrada-saída para um problema real e descreva como treiná-lo.",
                                "Identifique overfitting em um cenário hipotético.",
                                "Desenhe o ciclo de treinamento supervisionado.",
                                "Liste 3 aplicações reais de aprendizado supervisionado."
                              ],
                              "assessmentCriteria": [
                                "Precisão conceitual: definição correta com ênfase em dados rotulados e generalização (30%).",
                                "Clareza na explicação: uso de linguagem simples e analogias (20%).",
                                "Exemplos relevantes: inclusão de pelo menos um exemplo prático concreto (20%).",
                                "Compreensão de processos: descrição precisa de treinamento e avaliação (15%).",
                                "Diferenciação: distinção clara de outros tipos de ML (10%).",
                                "Originalidade: explicação personalizada, não copiada (5%)."
                              ],
                              "crossCurricularConnections": [
                                "Estatística: conceitos de amostragem, funções de perda e testes de hipótese.",
                                "Programação: implementação em Python com bibliotecas como Scikit-learn.",
                                "Matemática: otimização e cálculo (gradiente descendente).",
                                "Ética e Sociedade: viés em dados rotulados e impactos sociais de predições.",
                                "Biologia: classificação de espécies com dados rotulados."
                              ],
                              "realWorldApplication": "Previsão de doenças em medicina (sintomas → diagnóstico), sistemas de recomendação de produtos (histórico de compras → sugestões), detecção de fraudes em bancos (transações → fraudulenta ou não), todas usando dados rotulados para treinar modelos que predizem em novos casos reais."
                            },
                            "estimatedTime": "1 hora",
                            "difficulty": "beginner",
                            "status": "not_started",
                            "prerequisites": []
                          },
                          {
                            "id": "10.1.2.5.1.2",
                            "name": "Identificar dados rotulados",
                            "description": "Reconhecer exemplos de dados rotulados, como imagens com etiquetas de objetos ou valores numéricos associados a entradas, e explicar a importância dos rótulos para o treinamento.",
                            "atomicExpansion": {
                              "steps": [
                                {
                                  "stepNumber": 1,
                                  "title": "Compreender o Conceito Básico de Dados Rotulados",
                                  "subSteps": [
                                    "Defina dados rotulados como conjuntos de dados onde cada entrada está associada a um rótulo ou etiqueta conhecida.",
                                    "Compare com dados não rotulados, onde faltam essas etiquetas.",
                                    "Identifique que rótulos são outputs conhecidos usados para treinar modelos.",
                                    "Leia exemplos simples: par (imagem de gato, rótulo 'gato').",
                                    "Anote a diferença entre input (ex: imagem) e output (rótulo)."
                                  ],
                                  "verification": "Explique em suas palavras o que são dados rotulados e dê um exemplo simples.",
                                  "estimatedTime": "20 minutos",
                                  "materials": [
                                    "Definição impressa ou digital de dados rotulados",
                                    "Notebook para anotações"
                                  ],
                                  "tips": "Use analogias cotidianas, como etiquetar fotos em um álbum familiar.",
                                  "learningObjective": "Definir e diferenciar dados rotulados de não rotulados.",
                                  "commonMistakes": [
                                    "Confundir rótulos com features dos dados",
                                    "Achar que todos os dados precisam de rótulos"
                                  ]
                                },
                                {
                                  "stepNumber": 2,
                                  "title": "Reconhecer Exemplos Visuais de Dados Rotulados",
                                  "subSteps": [
                                    "Examine imagens com bounding boxes ou legendas, como fotos de objetos com nomes.",
                                    "Identifique rótulos em datasets como MNIST (dígitos escritos com labels 0-9).",
                                    "Observe tabelas com colunas: imagem URL e label (ex: 'cachorro').",
                                    "Classifique 5 imagens aleatórias como rotuladas ou não.",
                                    "Descreva verbalmente um exemplo encontrado."
                                  ],
                                  "verification": "Mostre 3 imagens e aponte corretamente os rótulos associados.",
                                  "estimatedTime": "25 minutos",
                                  "materials": [
                                    "Imagens de exemplo de datasets públicos (CIFAR-10 samples)",
                                    "Visualizador de imagens online"
                                  ],
                                  "tips": "Procure por setas ou textos sobrepostos nas imagens como indícios de rótulos.",
                                  "learningObjective": "Identificar visualmente dados rotulados em imagens.",
                                  "commonMistakes": [
                                    "Ignorar rótulos sutis",
                                    "Confundir descrições com rótulos supervisionados"
                                  ]
                                },
                                {
                                  "stepNumber": 3,
                                  "title": "Reconhecer Exemplos Numéricos e Tabulares de Dados Rotulados",
                                  "subSteps": [
                                    "Analise tabelas com entradas numéricas e colunas de target (ex: preço da casa baseado em tamanho).",
                                    "Exemplo: [[idade, salário], label: 'alto_risco'] para crédito.",
                                    "Pratique com CSV simples: features como temperatura, label: 'quente/frio'.",
                                    "Crie uma tabela própria com 5 entradas rotuladas.",
                                    "Explique como os rótulos guiam o aprendizado."
                                  ],
                                  "verification": "Construa e apresente uma tabela de 5 linhas com dados rotulados corretos.",
                                  "estimatedTime": "30 minutos",
                                  "materials": [
                                    "Planilha Excel ou Google Sheets",
                                    "Exemplos de datasets como Iris ou Boston Housing"
                                  ],
                                  "tips": "Sempre verifique se há uma coluna separada para o rótulo/target.",
                                  "learningObjective": "Reconhecer dados rotulados em formatos numéricos e tabulares.",
                                  "commonMistakes": [
                                    "Tratar features como rótulos",
                                    "Omitir o target na tabela"
                                  ]
                                },
                                {
                                  "stepNumber": 4,
                                  "title": "Explicar a Importância dos Rótulos no Treinamento de Modelos",
                                  "subSteps": [
                                    "Descreva como rótulos permitem calcular erros durante o treinamento (loss function).",
                                    "Explique supervisão: modelo aprende mapeando inputs para rótulos conhecidos.",
                                    "Compare cenários: com rótulos (preciso) vs sem (não supervisionado).",
                                    "Discuta custo: coleta manual de rótulos é trabalhosa mas essencial.",
                                    "Resuma em um diagrama simples: input -> modelo -> predição vs rótulo real."
                                  ],
                                  "verification": "Escreva um parágrafo explicando por que rótulos são cruciais, com exemplo.",
                                  "estimatedTime": "25 minutos",
                                  "materials": [
                                    "Papel e caneta para diagrama",
                                    "Vídeo curto sobre supervised learning (opcional)"
                                  ],
                                  "tips": "Pense em um professor corrigindo provas: rótulos são as respostas corretas.",
                                  "learningObjective": "Articular o papel dos rótulos no aprendizado supervisionado.",
                                  "commonMistakes": [
                                    "Subestimar o custo de rotulagem",
                                    "Confundir com validação de modelo"
                                  ]
                                }
                              ],
                              "practicalExample": "Em um dataset de emails, cada email (input: texto) tem rótulo 'spam' ou 'não-spam'. Um modelo treinado com isso aprende a filtrar emails futuros automaticamente, como no Gmail.",
                              "finalVerifications": [
                                "Corretamente identifica 5 exemplos de dados rotulados vs não rotulados.",
                                "Explica a função dos rótulos no treinamento sem erros conceituais.",
                                "Cria um dataset rotulado simples com pelo menos 10 entradas.",
                                "Diferencia imagens, tabelas e textos rotulados.",
                                "Descreve impacto da ausência de rótulos em um modelo."
                              ],
                              "assessmentCriteria": [
                                "Precisão na identificação de rótulos (90%+ acurácia em exemplos).",
                                "Clareza na explicação da importância (cobertura de supervisão e loss).",
                                "Criatividade e correção no practical example criado.",
                                "Profundidade nos substeps completados.",
                                "Capacidade de conectar a conceitos reais de ML.",
                                "Ausência de confusões comuns como features vs labels."
                              ],
                              "crossCurricularConnections": [
                                "Estatística: Conceitos de amostras labeladas em testes de hipótese.",
                                "Programação: Manipulação de datasets com Pandas (colunas target).",
                                "Biologia: Classificação de espécies com imagens rotuladas.",
                                "Linguagem: Análise de sentimento em textos com labels positivos/negativos."
                              ],
                              "realWorldApplication": "Usado em assistentes de voz como Siri (áudio rotulado com comandos), detecção de fraudes em bancos (transações com labels 'fraude/normal') e diagnósticos médicos (imagens de raio-X rotuladas com doenças)."
                            },
                            "estimatedTime": "0.5 hora",
                            "difficulty": "beginner",
                            "status": "not_started",
                            "prerequisites": []
                          },
                          {
                            "id": "10.1.2.5.1.3",
                            "name": "Diferenciar de aprendizado não-supervisionado",
                            "description": "Comparar aprendizado supervisionado (com rótulos) e não-supervisionado (sem rótulos), citando cenários onde cada um é aplicado, como clustering vs. classificação.",
                            "atomicExpansion": {
                              "steps": [
                                {
                                  "stepNumber": 1,
                                  "title": "Revisar Conceitos Básicos do Aprendizado Supervisionado",
                                  "subSteps": [
                                    "Defina aprendizado supervisionado como um método onde o modelo é treinado com dados rotulados (inputs e outputs conhecidos).",
                                    "Identifique características principais: presença de rótulos, objetivo de prever saídas para novos inputs.",
                                    "Liste exemplos iniciais: classificação (ex: spam/não-spam) e regressão (ex: previsão de preços).",
                                    "Desenhe um fluxograma simples mostrando dados rotulados → treinamento → predição.",
                                    "Anote diferenças iniciais intuitivas com métodos sem rótulos."
                                  ],
                                  "verification": "Crie um resumo de 1 parágrafo explicando supervisionado e verifique se menciona rótulos explicitamente.",
                                  "estimatedTime": "20 minutos",
                                  "materials": "Notebook para anotações, vídeo introdutório sobre ML supervisionado (ex: Khan Academy ou YouTube).",
                                  "tips": "Use analogias cotidianas, como um professor corrigindo provas (rótulos = respostas corretas).",
                                  "learningObjective": "Compreender as fundações do aprendizado supervisionado para estabelecer base de comparação.",
                                  "commonMistakes": "Confundir com regressão não-linear sem enfatizar rótulos; ignorar que supervisão implica feedback durante treinamento."
                                },
                                {
                                  "stepNumber": 2,
                                  "title": "Explorar Aprendizado Não-Supervisionado",
                                  "subSteps": [
                                    "Defina aprendizado não-supervisionado como método sem rótulos, onde o modelo encontra padrões nos dados.",
                                    "Descreva técnicas chave: clustering (agrupamento, ex: K-means), redução de dimensionalidade (ex: PCA).",
                                    "Explique objetivos: descobrir estruturas ocultas, anomalias ou compressão de dados.",
                                    "Desenhe fluxograma: dados não-rotulados → análise → padrões emergentes.",
                                    "Compare intuitivamente com supervisionado: 'exploração vs. predição guiada'."
                                  ],
                                  "verification": "Liste 3 técnicas não-supervisionadas com uma frase de exemplo cada; confira precisão.",
                                  "estimatedTime": "25 minutos",
                                  "materials": "Notebook, infográfico comparativo de ML (buscar online ou imprimir de Scikit-learn docs).",
                                  "tips": "Pense em 'caçar tesouros sem mapa' para não-supervisionado vs. 'seguir instruções marcadas'.",
                                  "learningObjective": "Dominar conceitos de não-supervisionado para contraste direto.",
                                  "commonMistakes": "Achar que não-supervisionado é 'aleatório'; esquecer que pode gerar labels implícitos pós-análise."
                                },
                                {
                                  "stepNumber": 3,
                                  "title": "Comparar Diretamente os Dois Abordagens",
                                  "subSteps": [
                                    "Crie uma tabela comparativa: colunas para Supervisionado vs. Não-Supervisionado (linhas: dados, objetivo, exemplos, prós/contras).",
                                    "Destaque diferenças chave: rótulos (sim/não), supervisão (feedback vs. auto-organização), métricas (acurácia vs. silhueta).",
                                    "Discuta cenários: use classificação para emails rotulados; clustering para segmentação de clientes sem labels.",
                                    "Analise trade-offs: supervisionado precisa mais dados preparados; não-supervisionado é flexível mas interpretativo.",
                                    "Preencha tabela com pelo menos 5 linhas de comparação."
                                  ],
                                  "verification": "Compartilhe tabela com colega ou auto-revisão; confirme cobrir rótulos, cenários e exemplos.",
                                  "estimatedTime": "30 minutos",
                                  "materials": "Planilha Google Sheets ou papel para tabela, exemplos de datasets (Iris para ambos).",
                                  "tips": "Use cores na tabela: verde para similaridades, vermelho para diferenças.",
                                  "learningObjective": "Desenvolver habilidade de diferenciação sistemática entre os tipos de aprendizado.",
                                  "commonMistakes": "Ignorar cenários híbridos (semi-supervisionado); superestimar precisão de não-supervisionado."
                                },
                                {
                                  "stepNumber": 4,
                                  "title": "Aplicar em Cenários Práticos e Sintetizar",
                                  "subSteps": [
                                    "Escolha 2 cenários reais: um para cada tipo (ex: detecção de fraudes supervisionada vs. agrupamento de genes não-supervisionada).",
                                    "Justifique por que um método é melhor: custo de labeling, disponibilidade de dados.",
                                    "Crie um mindmap conectando diferenças a aplicações.",
                                    "Resuma em 3 bullet points as lições chave da diferenciação.",
                                    "Teste conhecimento respondendo: 'Por que clustering não é classificação?'."
                                  ],
                                  "verification": "Escreva parágrafo justificando escolha de cenários; verifique lógica.",
                                  "estimatedTime": "25 minutos",
                                  "materials": "Ferramenta de mindmap (ex: MindMeister free), artigos curtos sobre aplicações ML.",
                                  "tips": "Priorize cenários do dia a dia para fixação; grave áudio explicando para si mesmo.",
                                  "learningObjective": "Integrar diferenças teóricas a contextos aplicados.",
                                  "commonMistakes": "Escolher cenários inadequados (ex: usar não-supervisionado onde labels existem); generalizar demais."
                                }
                              ],
                              "practicalExample": "Carregue o dataset Iris (Scikit-learn). Para supervisionado: treine um classificador com species como rótulos para prever flores novas. Para não-supervisionado: aplique K-means sem labels para agrupar flores por características, depois compare clusters com species reais – observe como clusters aproximam mas não são exatos sem supervisão.",
                              "finalVerifications": [
                                "Explique verbalmente as diferenças em rótulos e objetivos sem hesitação.",
                                "Preencha tabela comparativa corretamente em <2 minutos.",
                                "Identifique corretamente 3 cenários para cada tipo.",
                                "Diferencie clustering de classificação com exemplo preciso.",
                                "Justifique trade-offs de dados em um parágrafo coerente.",
                                "Crie fluxograma híbrido mostrando quando alternar métodos."
                              ],
                              "assessmentCriteria": [
                                "Precisão conceitual: definições corretas de rótulos e objetivos (30%).",
                                "Profundidade comparativa: tabela/mindmap com ≥5 diferenças claras (25%).",
                                "Exemplos relevantes: cenários reais e justificados (20%).",
                                "Clareza de síntese: resumo conciso e acionável (15%).",
                                "Aplicação prática: código/exemplo funcional demonstrado (10%)."
                              ],
                              "crossCurricularConnections": [
                                "Estatística: distribuições e testes de hipóteses para validar clusters.",
                                "Programação: implementação em Python (Scikit-learn) para ambos métodos.",
                                "Matemática: vetores/distâncias em clustering vs. funções de perda supervisionadas.",
                                "Ética em Dados: viés em labeling supervisionado vs. interpretação subjetiva não-supervisionada."
                              ],
                              "realWorldApplication": "Em marketing, use supervisionado para prever churn de clientes com histórico rotulado; use não-supervisionado para segmentar audiência sem labels prévios, permitindo campanhas personalizadas sem custo inicial de anotação."
                            },
                            "estimatedTime": "1 hora",
                            "difficulty": "beginner",
                            "status": "not_started",
                            "prerequisites": []
                          }
                        ]
                      },
                      {
                        "id": "10.1.2.5.2",
                        "name": "Classificação no Aprendizado Supervisionado",
                        "description": "Tipo de tarefa supervisionada onde o objetivo é prever uma categoria discreta a partir de dados de entrada, com exemplos práticos.",
                        "specificSkills": [
                          {
                            "id": "10.1.2.5.2.1",
                            "name": "Explicar o problema de classificação",
                            "description": "Descrever classificação como a predição de uma classe categórica (ex: spam/não-spam em e-mails), incluindo conceitos de variáveis dependentes categóricas.",
                            "atomicExpansion": {
                              "steps": [
                                {
                                  "stepNumber": 1,
                                  "title": "Compreender a Definição Básica de Classificação",
                                  "subSteps": [
                                    "Leia a definição: Classificação é uma tarefa de aprendizado supervisionado que visa prever a classe categórica de uma nova instância com base em dados rotulados históricos.",
                                    "Identifique o objetivo principal: Atribuir uma ou mais categorias discretas a entradas desconhecidas.",
                                    "Compare com aprendizado não-supervisionado: Destaque que classificação requer rótulos pré-existentes.",
                                    "Anote os termos chave: 'Predição categórica', 'variável dependente categórica' e 'aprendizado supervisionado'.",
                                    "Resuma em suas próprias palavras a essência do problema."
                                  ],
                                  "verification": "Escreva um parágrafo de 3-5 frases definindo classificação e envie para revisão.",
                                  "estimatedTime": "20 minutos",
                                  "materials": [
                                    "Notebook e caneta",
                                    "Artigo introdutório sobre ML (ex: 'Hands-On Machine Learning' capítulo 1)",
                                    "Vídeo Khan Academy sobre ML básico"
                                  ],
                                  "tips": "Use analogias como 'classificar frutas em maçã ou banana' para fixar o conceito.",
                                  "learningObjective": "Definir precisamente o problema de classificação e diferenciá-lo de outras tarefas de ML.",
                                  "commonMistakes": "Confundir classificação com regressão (que prevê valores contínuos, não categorias)."
                                },
                                {
                                  "stepNumber": 2,
                                  "title": "Identificar Variáveis e Estrutura do Problema",
                                  "subSteps": [
                                    "Defina variável dependente: A classe alvo categórica (ex: 'spam' ou 'não-spam').",
                                    "Liste variáveis independentes: Características numéricas ou categóricas usadas para predição (ex: número de links no e-mail).",
                                    "Entenda o dataset: Conjunto de treinamento com pares (features, rótulo) e teste sem rótulos.",
                                    "Discuta balanceamento: Importância de classes equilibradas para evitar viés.",
                                    "Crie um diagrama simples: Features → Modelo → Classe predita."
                                  ],
                                  "verification": "Desenhe e explique um fluxograma do processo de classificação com variáveis identificadas.",
                                  "estimatedTime": "25 minutos",
                                  "materials": [
                                    "Papel ou ferramenta de diagrama (ex: Draw.io)",
                                    "Dataset exemplo como Iris ou SpamBase do UCI ML Repository"
                                  ],
                                  "tips": "Sempre pergunte: 'O que estamos prevendo? Uma categoria ou um número?'",
                                  "learningObjective": "Mapear corretamente variáveis dependentes e independentes em um problema de classificação.",
                                  "commonMistakes": "Tratar classes categóricas como numéricas, levando a interpretações erradas."
                                },
                                {
                                  "stepNumber": 3,
                                  "title": "Explorar Tipos e Conceitos Chave de Classificação",
                                  "subSteps": [
                                    "Diferencie classificação binária (2 classes, ex: sim/não) de multiclasse (múltiplas, ex: tipos de flor).",
                                    "Introduza multiclasse vs. multilabel: Uma instância pode ter múltiplos rótulos?",
                                    "Discuta métricas básicas: Acurácia, precisão, recall para avaliação.",
                                    "Aborde overfitting/underfitting no contexto de classificação.",
                                    "Pesquise um algoritmo simples: Como KNN funciona para classificação."
                                  ],
                                  "verification": "Classifique 5 cenários reais como binário ou multiclasse e justifique.",
                                  "estimatedTime": "30 minutos",
                                  "materials": [
                                    "Planilha Excel para métricas",
                                    "Tutorial Scikit-learn sobre classificação"
                                  ],
                                  "tips": "Lembre: Número de classes é finito e discreto, não contínuo.",
                                  "learningObjective": "Classificar problemas por tipo e entender métricas iniciais de avaliação.",
                                  "commonMistakes": "Ignorar desbalanceamento de classes, achando que acurácia basta."
                                },
                                {
                                  "stepNumber": 4,
                                  "title": "Sintetizar e Verificar Compreensão com Exemplo",
                                  "subSteps": [
                                    "Escolha um exemplo: E-mails spam/não-spam.",
                                    "Liste 4-5 features (ex: palavras suspeitas, remetente desconhecido).",
                                    "Simule predição: Dado um e-mail novo, atribua classe manualmente.",
                                    "Calcule acurácia hipotética em um mini-dataset de 10 amostras.",
                                    "Explique limitações: Por que precisamos de modelos automáticos."
                                  ],
                                  "verification": "Crie e resolva um problema de classificação fictício com 80% de acurácia simulada.",
                                  "estimatedTime": "25 minutos",
                                  "materials": [
                                    "Editor de texto para simular dataset",
                                    "Python Jupyter se disponível (opcional para plot)"
                                  ],
                                  "tips": "Pratique verbalizando: Explique para um 'amigo' imaginário.",
                                  "learningObjective": "Aplicar conceitos a um caso prático e identificar componentes do problema.",
                                  "commonMistakes": "Omitir o papel dos dados rotulados na supervisão."
                                }
                              ],
                              "practicalExample": "Em um filtro de e-mail, o modelo usa features como 'número de exclamações', 'palavras como \"grátis\"' e 'remetente desconhecido' para prever se uma mensagem é 'spam' (classe 1) ou 'não-spam' (classe 0), treinando com milhares de e-mails rotulados.",
                              "finalVerifications": [
                                "Pode definir classificação como predição de classe categórica?",
                                "Identifica corretamente variável dependente categórica?",
                                "Dá exemplo real de classificação binária?",
                                "Diferencia classificação de regressão?",
                                "Explica fluxo: features → predição de classe?",
                                "Lista pelo menos 3 métricas de avaliação?",
                                "Descreve dataset supervisionado?"
                              ],
                              "assessmentCriteria": [
                                "Definição precisa e completa (20%)",
                                "Uso correto de terminologia (20%)",
                                "Exemplos relevantes e concretos (20%)",
                                "Diferenciação de conceitos relacionados (20%)",
                                "Capacidade de diagramação/visualização (10%)",
                                "Compreensão de avaliação/métricas (10%)"
                              ],
                              "crossCurricularConnections": [
                                "Estatística: Cálculo de probabilidades e distribuições categóricas.",
                                "Programação: Implementação com bibliotecas como Scikit-learn.",
                                "Lógica e Álgebra: Árvores de decisão e funções booleanas.",
                                "Biologia: Classificação taxonômica de espécies.",
                                "Linguística: Análise de sentimento em textos (spam detection)."
                              ],
                              "realWorldApplication": "Classificação é essencial em detecção de spam/fraudes bancárias, diagnóstico médico (benigno/maligno), recomendação de conteúdo (like/dislike) e sistemas autônomos (pedestre/veículo em visão computacional), impactando bilhões de decisões diárias."
                            },
                            "estimatedTime": "1 hora",
                            "difficulty": "beginner",
                            "status": "not_started",
                            "prerequisites": []
                          },
                          {
                            "id": "10.1.2.5.2.2",
                            "name": "Identificar exemplos de classificação",
                            "description": "Listar e analisar exemplos reais, como detecção de fraudes em transações ou diagnóstico médico (benigno/maligno), relacionando entradas e saídas rotuladas.",
                            "atomicExpansion": {
                              "steps": [
                                {
                                  "stepNumber": 1,
                                  "title": "Compreender os Fundamentos da Classificação Supervisionada",
                                  "subSteps": [
                                    "Defina classificação como um tipo de aprendizado supervisionado onde o modelo prevê uma categoria discreta baseada em dados rotulados.",
                                    "Diferencie classificação de regressão: classificação usa saídas categóricas (ex: sim/não), regressão usa contínuas (ex: preço).",
                                    "Explique o papel de entradas (features) e saídas rotuladas (labels) em um dataset de classificação.",
                                    "Identifique componentes chave: dataset de treinamento com pares (input, label), modelo treinado e predições.",
                                    "Revise exemplos simples como 'gato ou cachorro' em imagens."
                                  ],
                                  "verification": "Resuma em 3 frases o que é classificação e dê 1 exemplo básico; confira se menciona inputs e outputs rotulados.",
                                  "estimatedTime": "20 minutos",
                                  "materials": "Notebook, vídeo introdutório sobre ML supervisionado (Khan Academy ou YouTube), quadro branco.",
                                  "tips": "Use analogias cotidianas como 'classificar frutas por cor e forma' para fixar o conceito.",
                                  "learningObjective": "Entender a definição e componentes essenciais da classificação supervisionada.",
                                  "commonMistakes": "Confundir com regressão ou ignorar a necessidade de rótulos nos dados."
                                },
                                {
                                  "stepNumber": 2,
                                  "title": "Identificar Exemplos Cotidianos de Classificação",
                                  "subSteps": [
                                    "Liste 3 exemplos diários: filtro de spam em emails (spam/não spam), recomendação de filmes (gostou/não gostou).",
                                    "Para cada exemplo, descreva inputs (ex: palavras no email) e outputs rotulados (spam ou ham).",
                                    "Classifique-os como binária (2 classes) ou multiclasse (mais de 2).",
                                    "Pesquise 2 exemplos adicionais em apps do dia a dia, como detecção de rostos em fotos.",
                                    "Crie um mapa mental conectando exemplos a cenários reais."
                                  ],
                                  "verification": "Apresente lista de 5 exemplos com inputs/outputs; valide se todos envolvem rótulos categóricos.",
                                  "estimatedTime": "25 minutos",
                                  "materials": "Papel e caneta para brainstorm, acesso a internet para exemplos rápidos.",
                                  "tips": "Pense em decisões 'sim/não' ou categorias limitadas para brainstorm rápido.",
                                  "learningObjective": "Reconhecer padrões de classificação em situações cotidianas.",
                                  "commonMistakes": "Listar exemplos de regressão como 'prever idade' em vez de 'adulto/criança'."
                                },
                                {
                                  "stepNumber": 3,
                                  "title": "Analisar Exemplos Reais Profissionais",
                                  "subSteps": [
                                    "Escolha detecção de fraudes: inputs (valor transação, localização, histórico), output (fraude/legítima).",
                                    "Analise diagnóstico médico: inputs (sintomas, exames), output (benigno/maligno).",
                                    "Descreva como dados rotulados são coletados (ex: transações marcadas por especialistas).",
                                    "Discuta desafios: desbalanceamento de classes (poucas fraudes), importância de precisão.",
                                    "Compare os dois exemplos em similaridades e diferenças."
                                  ],
                                  "verification": "Escreva parágrafo analisando 2 exemplos reais; cheque se relaciona inputs, outputs e contexto.",
                                  "estimatedTime": "30 minutos",
                                  "materials": "Artigos online sobre detecção de fraudes (ex: Towards Data Science), dataset exemplo Kaggle.",
                                  "tips": "Foque em 'por quê' os rótulos são cruciais para treinamento do modelo.",
                                  "learningObjective": "Analisar exemplos reais destacando relação entre entradas rotuladas e saídas.",
                                  "commonMistakes": "Ignorar o aspecto supervisionado ou listar sem análise profunda."
                                },
                                {
                                  "stepNumber": 4,
                                  "title": "Sintetizar e Relacionar Múltiplos Exemplos",
                                  "subSteps": [
                                    "Compile lista final de 5-7 exemplos variados de diferentes domínios.",
                                    "Crie tabela: colunas para domínio, inputs, outputs, tipo (binária/multiclasse).",
                                    "Identifique padrões comuns: necessidade de dados rotulados de qualidade.",
                                    "Discuta aplicações emergentes como classificação de sentimentos em redes sociais.",
                                    "Reflita: como isso difere de outros tipos de ML?"
                                  ],
                                  "verification": "Compartilhe tabela completa; confirme cobertura de domínios diversos e análise.",
                                  "estimatedTime": "25 minutos",
                                  "materials": "Planilha Google Sheets ou Excel para tabela, exemplos anotados anteriores.",
                                  "tips": "Use cores na tabela para destacar binária vs multiclasse visualmente.",
                                  "learningObjective": "Sintetizar conhecimento para listar e analisar exemplos de classificação de forma estruturada.",
                                  "commonMistakes": "Repetir exemplos similares sem diversidade de domínios."
                                }
                              ],
                              "practicalExample": "Em um banco, classificar transações como 'fraude' ou 'legítima': inputs incluem valor, hora, IP e histórico do usuário; outputs rotulados por analistas humanos. O modelo aprende padrões de fraudes raras para alertar em tempo real.",
                              "finalVerifications": [
                                "Pode listar pelo menos 5 exemplos reais de classificação com inputs e outputs?",
                                "Diferencia corretamente classificação de regressão em 2 cenários?",
                                "Analisa desafios como desbalanceamento em exemplos profissionais?",
                                "Cria tabela relacionando múltiplos exemplos?",
                                "Explica o papel essencial dos rótulos supervisionados?",
                                "Identifica aplicações em 3 domínios diferentes?"
                              ],
                              "assessmentCriteria": [
                                "Precisão na identificação de inputs e outputs rotulados (90%+ correto).",
                                "Diversidade de exemplos cobrindo binária e multiclasse.",
                                "Profundidade da análise (desafios e contexto real incluídos).",
                                "Clareza na estruturação (tabelas/mapas mentais usados).",
                                "Relevância aos domínios profissionais como finanças e saúde.",
                                "Capacidade de relacionar a conceitos fundamentais de ML supervisionado."
                              ],
                              "crossCurricularConnections": [
                                "Estatística: Uso de probabilidades em predições de classes.",
                                "Programação: Implementação com bibliotecas como scikit-learn em Python.",
                                "Biologia: Classificação de doenças em diagnósticos médicos.",
                                "Ética: Viés em datasets rotulados e impactos sociais.",
                                "Negócios: Detecção de fraudes em finanças."
                              ],
                              "realWorldApplication": "Na saúde, modelos classificam tumores como benignos ou malignos via imagens de raio-X, salvando vidas com diagnósticos rápidos; em bancos, previnem perdas bilionárias detectando fraudes em transações online em segundos."
                            },
                            "estimatedTime": "0.5 hora",
                            "difficulty": "beginner",
                            "status": "not_started",
                            "prerequisites": []
                          },
                          {
                            "id": "10.1.2.5.2.3",
                            "name": "Entender métricas básicas de classificação",
                            "description": "Introduzir acurácia e confusão matrix como formas simples de avaliar modelos de classificação, sem implementação técnica.",
                            "atomicExpansion": {
                              "steps": [
                                {
                                  "stepNumber": 1,
                                  "title": "Compreender o propósito das métricas de avaliação em classificação",
                                  "subSteps": [
                                    "Discuta o que é um modelo de classificação e seus objetivos principais.",
                                    "Explique por que avaliar um modelo é essencial para validar sua performance.",
                                    "Identifique métricas simples como acurácia e matriz de confusão como ferramentas iniciais.",
                                    "Diferencie avaliação conceitual de implementação técnica.",
                                    "Liste cenários onde a avaliação errada pode levar a decisões ruins."
                                  ],
                                  "verification": "Escreva uma frase resumindo o papel das métricas em um parágrafo curto.",
                                  "estimatedTime": "20 minutos",
                                  "materials": [
                                    "Papel e caneta",
                                    "Quadro branco ou app de desenho simples"
                                  ],
                                  "tips": "Use analogias cotidianas, como julgar um juiz de futebol, para fixar o conceito.",
                                  "learningObjective": "Reconhecer a importância da avaliação para garantir confiabilidade em modelos de classificação.",
                                  "commonMistakes": [
                                    "Confundir avaliação com treinamento do modelo",
                                    "Ignorar que métricas simples têm limitações em dados desbalanceados"
                                  ]
                                },
                                {
                                  "stepNumber": 2,
                                  "title": "Explorar a métrica de Acurácia",
                                  "subSteps": [
                                    "Defina acurácia como a proporção de predições corretas sobre o total de predições.",
                                    "Escreva a fórmula conceitual: (Predições corretas / Total de amostras) x 100%.",
                                    "Calcule acurácia manualmente com um exemplo de 10 itens (ex: 8 corretos = 80%).",
                                    "Discuta vantagens: simplicidade e intuitividade.",
                                    "Identifique limitações: falha em dados desbalanceados (ex: 95% de uma classe domina)."
                                  ],
                                  "verification": "Calcule e explique acurácia para um conjunto de 20 predições com 16 corretas.",
                                  "estimatedTime": "25 minutos",
                                  "materials": [
                                    "Folha de cálculo impressa ou calculadora simples",
                                    "Exemplos numéricos anotados"
                                  ],
                                  "tips": "Sempre relacione com porcentagens do dia a dia, como taxa de acerto em provas.",
                                  "learningObjective": "Calcular e interpretar acurácia de forma precisa em contextos simples.",
                                  "commonMistakes": [
                                    "Confundir acurácia com precisão",
                                    "Aplicar acurácia sem considerar balanceamento de classes"
                                  ]
                                },
                                {
                                  "stepNumber": 3,
                                  "title": "Dominar a Matriz de Confusão",
                                  "subSteps": [
                                    "Desenhe uma matriz 2x2 básica: linhas reais (positivo/negativo), colunas preditas.",
                                    "Identifique os quadrantes: TP (Verdadeiro Positivo), TN, FP (Falso Positivo), FN (Falso Negativo).",
                                    "Preencha uma matriz com dados fictícios (ex: 50 TP, 10 FP, 5 FN, 35 TN).",
                                    "Calcule acurácia a partir da matriz: (TP + TN) / Total.",
                                    "Analise o que cada célula revela sobre erros do modelo."
                                  ],
                                  "verification": "Desenhe e preencha uma matriz de confusão para um exemplo de 100 amostras.",
                                  "estimatedTime": "30 minutos",
                                  "materials": [
                                    "Papel quadriculado",
                                    "Marcadores coloridos para quadrantes"
                                  ],
                                  "tips": "Use cores diferentes para TP/TN (verde) e FP/FN (vermelho) para visualização rápida.",
                                  "learningObjective": "Construir e interpretar uma matriz de confusão para diagnosticar erros de classificação.",
                                  "commonMistakes": [
                                    "Trocar linhas e colunas",
                                    "Esquecer de somar total para acurácia"
                                  ]
                                },
                                {
                                  "stepNumber": 4,
                                  "title": "Comparar e aplicar Acurácia e Matriz de Confusão",
                                  "subSteps": [
                                    "Compare acurácia isolada vs. insights da matriz (ex: alta acurácia mas muitos FP).",
                                    "Aplique ambas em um cenário prático simples.",
                                    "Discuta quando priorizar matriz sobre acurácia (dados desbalanceados).",
                                    "Crie uma tabela resumindo prós e contras de cada métrica.",
                                    "Reflita sobre como elas se complementam na avaliação geral."
                                  ],
                                  "verification": "Explique em 3 frases por que a matriz é superior à acurácia sozinha em certos casos.",
                                  "estimatedTime": "25 minutos",
                                  "materials": [
                                    "Tabela comparativa pré-formatada",
                                    "Exemplo prático impresso"
                                  ],
                                  "tips": "Pratique com dados reais de notícias (ex: testes COVID) para relevância.",
                                  "learningObjective": "Integrar acurácia e matriz de confusão para uma avaliação holística de modelos.",
                                  "commonMistakes": [
                                    "Superestimar acurácia sem matriz",
                                    "Ignorar contexto do problema na escolha de métricas"
                                  ]
                                }
                              ],
                              "practicalExample": "Em um conjunto de 100 e-mails (20 spam reais, 80 não spam), o modelo prevê corretamente 15 spam (TP=15, FN=5), 5 falsos spam (FP=5), 75 não spam corretos (TN=75). Acurácia = (15+75)/100 = 90%. A matriz revela 25% de falsos negativos em spam, útil para ajustar filtros.",
                              "finalVerifications": [
                                "Explicar acurácia em suas próprias palavras com fórmula.",
                                "Desenhar matriz de confusão corretamente identificando TP/TN/FP/FN.",
                                "Calcular acurácia a partir de uma matriz dada.",
                                "Identificar limitação da acurácia em dados desbalanceados.",
                                "Comparar insights de acurácia vs. matriz em um exemplo.",
                                "Listar 2 cenários onde matriz é essencial."
                              ],
                              "assessmentCriteria": [
                                "Definições precisas de acurácia e matriz (sem erros conceituais).",
                                "Cálculos corretos em exemplos numéricos (tolerância de 1%).",
                                "Interpretação qualitativa dos erros via matriz.",
                                "Reconhecimento de limitações da acurácia.",
                                "Integração coerente das duas métricas.",
                                "Uso de linguagem clara e exemplos relevantes."
                              ],
                              "crossCurricularConnections": [
                                "Estatística: Cálculo de proporções e análise de frequências.",
                                "Probabilidade: Conceitos de eventos verdadeiros/falsos.",
                                "Matemática Geral: Frações, porcentagens e tabelas.",
                                "Ciência da Computação: Fundamentos de Machine Learning.",
                                "Negócios: Avaliação de risco em decisões preditivas."
                              ],
                              "realWorldApplication": "Em diagnósticos médicos, como testes de COVID, a matriz de confusão ajuda a minimizar falsos negativos (pacientes infectados não detectados), enquanto acurácia dá visão geral rápida para campanhas de saúde pública."
                            },
                            "estimatedTime": "1 hora",
                            "difficulty": "intermediate",
                            "status": "not_started",
                            "prerequisites": []
                          }
                        ]
                      },
                      {
                        "id": "10.1.2.5.3",
                        "name": "Regressão no Aprendizado Supervisionado",
                        "description": "Tipo de tarefa supervisionada onde o objetivo é prever um valor contínuo, com exemplos e distinções da classificação.",
                        "specificSkills": [
                          {
                            "id": "10.1.2.5.3.1",
                            "name": "Explicar o problema de regressão",
                            "description": "Definir regressão como predição de valores numéricos contínuos (ex: preço de casas baseado em área e localização), contrastando com classificação.",
                            "atomicExpansion": {
                              "steps": [
                                {
                                  "stepNumber": 1,
                                  "title": "Compreender os Fundamentos do Aprendizado Supervisionado",
                                  "subSteps": [
                                    "Definir aprendizado supervisionado como um tipo de machine learning onde o modelo é treinado com dados rotulados (entradas e saídas conhecidas).",
                                    "Explicar que o objetivo é aprender uma função que mapeia entradas para saídas com base em exemplos.",
                                    "Discutir a importância dos dados rotulados e como eles diferem do aprendizado não supervisionado.",
                                    "Identificar exemplos gerais, como previsão de valores ou categorização."
                                  ],
                                  "verification": "Escrever uma definição clara de aprendizado supervisionado e listar 2 exemplos de dados rotulados.",
                                  "estimatedTime": "20 minutos",
                                  "materials": [
                                    "Vídeo introdutório sobre ML supervisionado (ex: Khan Academy)",
                                    "Papel e caneta para anotações"
                                  ],
                                  "tips": "Use analogias cotidianas, como um professor corrigindo provas para guiar o aprendizado.",
                                  "learningObjective": "Entender o contexto geral do aprendizado supervisionado como base para regressão.",
                                  "commonMistakes": "Confundir com aprendizado não supervisionado, achando que não precisa de rótulos."
                                },
                                {
                                  "stepNumber": 2,
                                  "title": "Definir o Problema de Regressão",
                                  "subSteps": [
                                    "Definir regressão como a tarefa de prever valores numéricos contínuos a partir de variáveis de entrada.",
                                    "Explicar que a saída é uma quantidade mensurável em uma escala contínua (ex: preço em reais).",
                                    "Descrever o processo: modelo aprende padrões nos dados para estimar valores futuros.",
                                    "Discutir métricas comuns como erro quadrático médio (MSE) para avaliação."
                                  ],
                                  "verification": "Redigir uma definição precisa de regressão e citar um exemplo de saída contínua.",
                                  "estimatedTime": "25 minutos",
                                  "materials": [
                                    "Artigo ou slide sobre tipos de regressão (ex: Wikipedia - Regression analysis)",
                                    "Exemplo de dataset simples como preços de casas"
                                  ],
                                  "tips": "Pense em regressão como 'prever números exatos', não categorias.",
                                  "learningObjective": "Dominar a definição conceitual de regressão e suas características únicas.",
                                  "commonMistakes": "Confundir valores contínuos com discretos, como idades vs. faixas etárias."
                                },
                                {
                                  "stepNumber": 3,
                                  "title": "Contrastar Regressão com Classificação",
                                  "subSteps": [
                                    "Definir classificação como previsão de categorias discretas (ex: spam ou não-spam).",
                                    "Comparar saídas: regressão (contínuo, ex: 250.000) vs. classificação (discreto, ex: 'alta'/'baixa').",
                                    "Explicar diferenças em métricas: MSE para regressão vs. acurácia para classificação.",
                                    "Ilustrar com o mesmo dataset: prever preço (regressão) vs. classe de luxo (classificação)."
                                  ],
                                  "verification": "Criar uma tabela comparativa entre regressão e classificação com exemplos.",
                                  "estimatedTime": "25 minutos",
                                  "materials": [
                                    "Infográfico comparativo de regressão vs. classificação",
                                    "Notebook Jupyter com exemplos visuais"
                                  ],
                                  "tips": "Use o mesmo problema hipotético para ambos para destacar diferenças.",
                                  "learningObjective": "Diferenciar claramente regressão de classificação.",
                                  "commonMistakes": "Achar que ambos são 'previsões' sem notar a natureza da saída."
                                },
                                {
                                  "stepNumber": 4,
                                  "title": "Aplicar Conceitos com um Exemplo Prático",
                                  "subSteps": [
                                    "Escolher um dataset real, como previsão de preços de casas.",
                                    "Identificar features (área, quartos) e target (preço).",
                                    "Simular treinamento: modelo ajusta linha de tendência nos dados.",
                                    "Prever um novo caso e discutir limitações."
                                  ],
                                  "verification": "Explicar verbalmente ou por escrito como aplicar regressão ao exemplo.",
                                  "estimatedTime": "30 minutos",
                                  "materials": [
                                    "Dataset Boston Housing ou similar (Kaggle)",
                                    "Ferramenta visual como Google Sheets para plotar dados"
                                  ],
                                  "tips": "Visualize dados com gráficos de dispersão para intuitividade.",
                                  "learningObjective": "Integrar conceitos em um exemplo acionável.",
                                  "commonMistakes": "Ignorar que regressão assume relações lineares ou não-lineares adequadamente."
                                }
                              ],
                              "practicalExample": "Prever o preço de venda de uma casa com base em sua área (em m²), número de quartos e localização urbana/rural, usando um modelo de regressão linear treinado em um dataset histórico.",
                              "finalVerifications": [
                                "Definir corretamente regressão como predição de valores contínuos.",
                                "Contrastar com classificação citando diferenças na saída.",
                                "Dar um exemplo real de aplicação de regressão.",
                                "Explicar uma métrica de avaliação como MSE.",
                                "Identificar quando usar regressão vs. classificação."
                              ],
                              "assessmentCriteria": [
                                "Precisão e clareza na definição de regressão (peso: 30%).",
                                "Qualidade do contraste com classificação, incluindo exemplos (peso: 25%).",
                                "Relevância e correção do exemplo prático (peso: 20%).",
                                "Compreensão de métricas e limitações (peso: 15%).",
                                "Capacidade de explicar verbalmente ou por escrito (peso: 10%)."
                              ],
                              "crossCurricularConnections": [
                                "Estatística: Modelos lineares e análise de regressão.",
                                "Programação: Implementação com bibliotecas como scikit-learn em Python.",
                                "Economia: Previsão de preços e demanda de mercado.",
                                "Física: Previsão de trajetórias ou velocidades contínuas."
                              ],
                              "realWorldApplication": "Empresas imobiliárias usam regressão para estimar preços de propriedades com base em features como localização e tamanho, otimizando listagens e negociações; em finanças, para prever retornos de ações; em saúde, para estimar dosagens de medicamentos baseadas em peso e idade."
                            },
                            "estimatedTime": "1 hora",
                            "difficulty": "beginner",
                            "status": "not_started",
                            "prerequisites": []
                          },
                          {
                            "id": "10.1.2.5.3.2",
                            "name": "Identificar exemplos de regressão",
                            "description": "Analisar casos como previsão de temperatura ou vendas futuras, identificando variáveis dependentes contínuas e independentes.",
                            "atomicExpansion": {
                              "steps": [
                                {
                                  "stepNumber": 1,
                                  "title": "Compreender os Conceitos Básicos de Regressão",
                                  "subSteps": [
                                    "Defina regressão como um tipo de aprendizado supervisionado que prevê valores contínuos.",
                                    "Diferencie regressão de classificação, destacando que regressão lida com variáveis dependentes numéricas contínuas.",
                                    "Liste exemplos iniciais como previsão de temperatura ou preço de imóveis.",
                                    "Identifique a estrutura geral: variáveis independentes (features) influenciam a variável dependente (target).",
                                    "Revise definições de variável dependente (y, contínua) e independentes (X, preditoras)."
                                  ],
                                  "verification": "Explique em suas palavras o que é regressão e dê um exemplo simples; confirme com autoavaliação ou quiz rápido.",
                                  "estimatedTime": "20 minutos",
                                  "materials": "Notebook, acesso a Khan Academy ou vídeo introdutório sobre ML supervisionado (ex: YouTube - 'Regressão Linear Explicada').",
                                  "tips": "Use analogias cotidianas, como prever altura de uma planta baseada em água e sol.",
                                  "learningObjective": "Dominar definições fundamentais de regressão e suas distinções de outros métodos.",
                                  "commonMistakes": "Confundir regressão com classificação (ex: prever 'sim/não' em vez de número)."
                                },
                                {
                                  "stepNumber": 2,
                                  "title": "Identificar Variáveis Dependentes e Independentes",
                                  "subSteps": [
                                    "Analise um dataset simples: identifique colunas numéricas contínuas como target (ex: temperatura).",
                                    "Classifique variáveis: dependente = saída contínua; independentes = entradas que afetam a saída.",
                                    "Pratique com exemplos: em previsão de vendas, vendas = dependente, preço/produto = independentes.",
                                    "Crie um diagrama de fluxo: X1, X2 -> y (contínua).",
                                    "Verifique continuidade: valores devem ser numéricos infinitos, não categorias."
                                  ],
                                  "verification": "Rotule corretamente variáveis em um dataset de exemplo com 80% de acerto.",
                                  "estimatedTime": "25 minutos",
                                  "materials": "Datasets CSV simples (ex: Kaggle - Boston Housing ou Temperatura Diária), Google Sheets ou Excel.",
                                  "tips": "Pergunte: 'O que estamos prevendo numericamente?' Isso é y; o resto influenciador é X.",
                                  "learningObjective": "Capacitar identificação precisa de variáveis em contextos de regressão.",
                                  "commonMistakes": "Escolher variáveis categóricas como dependentes (ex: cor como target contínuo)."
                                },
                                {
                                  "stepNumber": 3,
                                  "title": "Analisar Exemplos Reais de Regressão",
                                  "subSteps": [
                                    "Escolha cenários: previsão de temperatura (dependente: °C; independentes: umidade, vento).",
                                    "Analise vendas futuras (dependente: unidades vendidas; independentes: marketing, sazonalidade).",
                                    "Mapeie para cada exemplo: liste 3-5 independentes e justifique por que é regressão.",
                                    "Compare com não-regressão: ex: prever 'venderá ou não' é classificação.",
                                    "Documente em tabela: Exemplo | y | X's | Justificativa."
                                  ],
                                  "verification": "Crie tabela com 3 exemplos corretos, revisada por pares ou auto-checklist.",
                                  "estimatedTime": "30 minutos",
                                  "materials": "Papel/aula digital, exemplos de datasets (ex: UCI ML Repository - regressão tasks).",
                                  "tips": "Busque padrões: regressão sempre prevê 'quanto' ou 'o valor exato', não 'qual categoria'.",
                                  "learningObjective": "Aplicar conceitos a casos reais para reconhecimento intuitivo.",
                                  "commonMistakes": "Ignorar continuidade, tratando contínuo como discreto (ex: notas como categorias)."
                                },
                                {
                                  "stepNumber": 4,
                                  "title": "Diferenciar e Sintetizar Exemplos de Regressão",
                                  "subSteps": [
                                    "Compare 5 cenários mistos: identifique quais são regressão vs. classificação.",
                                    "Sintetize critérios chave: target contínuo, supervisão com labels numéricos.",
                                    "Crie flashcards com exemplos e contraexemplos.",
                                    "Discuta limitações: nem todo problema numérico é regressão (ex: contagens podem ser Poisson).",
                                    "Teste com novo exemplo: prever salário baseado em experiência."
                                  ],
                                  "verification": "Acerte 90% em quiz de 10 itens mistos de identificação.",
                                  "estimatedTime": "25 minutos",
                                  "materials": "Quiz online (ex: Google Forms ou Quizlet), flashcards app.",
                                  "tips": "Pratique diariamente com 2-3 exemplos para fixação.",
                                  "learningObjective": "Consolidar habilidade de identificação rápida e precisa.",
                                  "commonMistakes": "Sobre-generalizar: assumir todo ML supervisionado com números é regressão."
                                }
                              ],
                              "practicalExample": "Em um dataset de casas: prever preço (dependente contínua em dólares) usando tamanho (m²), número de quartos e localização (independentes). Use regressão linear para modelar: Preço = β0 + β1*Tamanho + β2*Quartos + erro.",
                              "finalVerifications": [
                                "Lista corretamente 3 exemplos de regressão com variáveis identificadas.",
                                "Diferencia regressão de classificação em 5 cenários mistos.",
                                "Explica por que target deve ser contínuo.",
                                "Cria diagrama simples para um exemplo.",
                                "Identifica independentes relevantes em dataset real.",
                                "Passa em quiz com 85%+ de acerto."
                              ],
                              "assessmentCriteria": [
                                "Precisão na identificação de variáveis (90%+ correto).",
                                "Profundidade de análise em exemplos (justificativas claras).",
                                "Capacidade de diferenciação de classificação.",
                                "Criatividade em exemplos reais aplicados.",
                                "Completude de documentação (tabelas/diagramas).",
                                "Tempo de resposta intuitiva em quizzes."
                              ],
                              "crossCurricularConnections": [
                                "Estatística: Correlação e causalidade em variáveis.",
                                "Programação: Implementação em Python (scikit-learn LinearRegression).",
                                "Negócios: Previsão de vendas e finanças.",
                                "Ciências: Modelagem climática e biológica."
                              ],
                              "realWorldApplication": "Empresas usam para prever demanda de produtos (ex: Amazon otimiza estoque), meteorologistas preveem temperaturas para alertas, e corretores estimam preços de imóveis para negociações precisas."
                            },
                            "estimatedTime": "0.5 hora",
                            "difficulty": "beginner",
                            "status": "not_started",
                            "prerequisites": []
                          },
                          {
                            "id": "10.1.2.5.3.3",
                            "name": "Entender métricas básicas de regressão",
                            "description": "Apresentar erro quadrático médio (MSE) e R² como métricas para avaliar precisão em predições contínuas.",
                            "atomicExpansion": {
                              "steps": [
                                {
                                  "stepNumber": 1,
                                  "title": "Compreender os fundamentos de erro em predições de regressão",
                                  "subSteps": [
                                    "Revise o conceito de regressão linear supervisionada e predições contínuas.",
                                    "Entenda o que é um resíduo: diferença entre valor real e predito.",
                                    "Discuta por que medir erros é essencial para avaliar modelos.",
                                    "Identifique tipos de erros: absolutos vs. quadráticos.",
                                    "Pratique calculando resíduos manualmente em um pequeno conjunto de dados."
                                  ],
                                  "verification": "Liste corretamente 3 resíduos de um dataset de 5 pontos e explique sua importância.",
                                  "estimatedTime": "20 minutos",
                                  "materials": "Papel e caneta ou planilha Excel; dataset simples de regressão (ex: 5 casas com preço real e predito).",
                                  "tips": "Sempre subtraia predito do real para consistência nos resíduos.",
                                  "learningObjective": "Dominar a base conceitual de erros em predições contínuas.",
                                  "commonMistakes": "Confundir resíduo com erro absoluto; ignorar o sinal dos resíduos."
                                },
                                {
                                  "stepNumber": 2,
                                  "title": "Dominar o Erro Quadrático Médio (MSE)",
                                  "subSteps": [
                                    "Estude a fórmula do MSE: média dos quadrados dos resíduos.",
                                    "Calcule MSE manualmente para um dataset de 5 amostras.",
                                    "Interprete o MSE: unidades quadradas da variável alvo; menor é melhor.",
                                    "Compare MSE de dois modelos no mesmo dataset.",
                                    "Implemente MSE em Python usando NumPy."
                                  ],
                                  "verification": "Calcule MSE corretamente para um dataset dado e interprete se é 'bom' ou 'ruim'.",
                                  "estimatedTime": "30 minutos",
                                  "materials": "Python com NumPy; Jupyter Notebook; dataset de regressão linear simples.",
                                  "tips": "Quadrado penaliza erros grandes mais; normalize dados se escalas variam.",
                                  "learningObjective": "Calcular, interpretar e implementar MSE com precisão.",
                                  "commonMistakes": "Esquecer de dividir pelo número de amostras; confundir com RMSE."
                                },
                                {
                                  "stepNumber": 3,
                                  "title": "Explorar o Coeficiente de Determinação (R²)",
                                  "subSteps": [
                                    "Aprenda a fórmula de R²: 1 - (SS_res / SS_tot), onde SS são somas de quadrados.",
                                    "Calcule R² manualmente usando resíduos e variância total.",
                                    "Interprete R²: proporção de variância explicada (0-1; >0.8 geralmente bom).",
                                    "Compare R² com MSE em cenários diferentes.",
                                    "Implemente R² em Python com scikit-learn."
                                  ],
                                  "verification": "Compute R² para um modelo e explique se ele explica 80% da variância.",
                                  "estimatedTime": "30 minutos",
                                  "materials": "Python com scikit-learn e NumPy; mesmo dataset do Step 2.",
                                  "tips": "R²=0 significa modelo pior que média; pode ser >1 em modelos não-lineares mal ajustados.",
                                  "learningObjective": "Entender e aplicar R² como métrica normalizada de qualidade.",
                                  "commonMistakes": "Interpretar R² como 'precisão percentual'; aplicar em dados não-lineares sem cuidado."
                                },
                                {
                                  "stepNumber": 4,
                                  "title": "Comparar MSE e R² e aplicá-los em prática",
                                  "subSteps": [
                                    "Discuta vantagens/desvantagens: MSE sensível a outliers; R² escala-independente.",
                                    "Treine um modelo de regressão linear e calcule ambas as métricas.",
                                    "Avalie trade-offs: use MSE para otimização, R² para comunicação.",
                                    "Teste em dataset real (ex: Boston Housing).",
                                    "Registre métricas em um relatório simples."
                                  ],
                                  "verification": "Gere relatório comparando MSE e R² de dois modelos, recomendando o melhor.",
                                  "estimatedTime": "40 minutos",
                                  "materials": "Python com pandas, scikit-learn; dataset Boston Housing ou similar.",
                                  "tips": "Sempre valide em conjunto de teste, não treino.",
                                  "learningObjective": "Integrar MSE e R² para avaliação holística de modelos.",
                                  "commonMistakes": "Overfitting: métricas boas no treino, ruins no teste; ignorar escala dos dados."
                                }
                              ],
                              "practicalExample": "Em um dataset de previsão de preços de casas (Boston Housing), treine uma regressão linear. Calcule MSE=25.3 (erro médio de $25k²) e R²=0.74 (74% da variância explicada). Compare com baseline (média dos preços): MSE_baseline=35.6, mostrando melhoria.",
                              "finalVerifications": [
                                "Explicar verbalmente fórmulas de MSE e R² sem consultar notas.",
                                "Calcular MSE e R² manualmente para um dataset de 10 pontos.",
                                "Implementar e plotar ambas as métricas em Python para um modelo treinado.",
                                "Identificar quando usar MSE vs. R² em cenários reais.",
                                "Interpretar valores: MSE=10 é bom? R²=0.5 é aceitável?",
                                "Comparar performance de dois modelos usando as métricas."
                              ],
                              "assessmentCriteria": [
                                "Precisão nos cálculos manuais e código (erro <1%).",
                                "Interpretação correta: unidades, escalas e significados.",
                                "Uso apropriado: distinção entre treino/teste e baselines.",
                                "Clareza na comunicação: gráficos e relatórios explicativos.",
                                "Compreensão de limitações: sensibilidade a outliers e não-linearidades.",
                                "Aplicação prática: escolha correta de métrica por contexto."
                              ],
                              "crossCurricularConnections": [
                                "Estatística: variância, somas de quadrados e distribuições de erros.",
                                "Programação: implementação em Python (NumPy, scikit-learn).",
                                "Matemática: álgebra linear para fórmulas e otimização.",
                                "Ciência de Dados: avaliação de modelos em pipelines ML.",
                                "Negócios: interpretação para decisões baseadas em predições."
                              ],
                              "realWorldApplication": "Em empresas de e-commerce, MSE e R² avaliam modelos de previsão de demanda, otimizando estoque e reduzindo custos; em finanças, medem precisão de forecasts de ações para estratégias de investimento."
                            },
                            "estimatedTime": "1 hora",
                            "difficulty": "intermediate",
                            "status": "not_started",
                            "prerequisites": []
                          }
                        ]
                      }
                    ]
                  },
                  {
                    "id": "10.1.2.6",
                    "name": "Introdução ao Aprendizado Não-Supervisionado",
                    "description": "Conceitos iniciais de aprendizado sem rótulos, como clustering e redução de dimensionalidade.",
                    "individualConcepts": [
                      {
                        "id": "10.1.2.6.1",
                        "name": "Definição e Características do Aprendizado Não-Supervisionado",
                        "description": "Compreensão dos princípios básicos do aprendizado não-supervisionado, que opera sem rótulos nos dados, focando em descobrir padrões intrínsecos, como agrupamentos e simplificações de dados.",
                        "specificSkills": [
                          {
                            "id": "10.1.2.6.1.1",
                            "name": "Diferenciar Aprendizado Supervisionado de Não-Supervisionado",
                            "description": "Explicar as diferenças fundamentais entre aprendizado supervisionado (com rótulos) e não-supervisionado (sem rótulos), incluindo objetivos, tipos de problemas resolvidos e exemplos práticos como classificação vs. clustering.",
                            "atomicExpansion": {
                              "steps": [
                                {
                                  "stepNumber": 1,
                                  "title": "Entender os Fundamentos do Aprendizado Supervisionado",
                                  "subSteps": [
                                    "Leia a definição: Aprendizado supervisionado usa dados rotulados (inputs com outputs conhecidos) para treinar modelos.",
                                    "Identifique os objetivos principais: Fazer previsões ou classificações baseadas em padrões aprendidos dos rótulos.",
                                    "Liste os tipos comuns: Classificação (categorias discretas, ex: spam/não-spam) e Regressão (valores contínuos, ex: preço de casa).",
                                    "Examine um dataset simples com rótulos, como Iris dataset.",
                                    "Anote como o modelo 'aprende' dos rótulos para generalizar."
                                  ],
                                  "verification": "Crie um diagrama simples mostrando input rotulado → modelo treinado → predição em novos dados.",
                                  "estimatedTime": "20 minutos",
                                  "materials": [
                                    "Artigo Khan Academy sobre Machine Learning Supervisionado",
                                    "Dataset Iris do UCI ML Repository",
                                    "Papel e caneta para diagrama"
                                  ],
                                  "tips": "Use analogias cotidianas, como um professor corrigindo provas (rótulos) para ensinar respostas corretas.",
                                  "learningObjective": "Compreender que o aprendizado supervisionado depende de dados rotulados para treinamento supervisionado.",
                                  "commonMistakes": [
                                    "Confundir com reinforcement learning (que usa recompensas, não rótulos fixos)",
                                    "Ignorar a necessidade de dados balanceados nos rótulos"
                                  ]
                                },
                                {
                                  "stepNumber": 2,
                                  "title": "Entender os Fundamentos do Aprendizado Não-Supervisionado",
                                  "subSteps": [
                                    "Leia a definição: Aprendizado não-supervisionado trabalha com dados não rotulados para descobrir padrões inerentes.",
                                    "Identifique os objetivos principais: Encontrar estruturas ocultas, como agrupamentos ou reduções de dimensionalidade.",
                                    "Liste os tipos comuns: Clustering (agrupar similares, ex: K-means), Redução de dimensionalidade (PCA).",
                                    "Examine um dataset sem rótulos, como dados de clientes de compras.",
                                    "Anote como o modelo explora dados sem orientação prévia."
                                  ],
                                  "verification": "Descreva verbalmente ou por escrito três padrões que poderiam emergir de dados não rotulados.",
                                  "estimatedTime": "20 minutos",
                                  "materials": [
                                    "Vídeo de 5min no YouTube sobre Clustering K-means",
                                    "Dataset de vendas anônimo do Kaggle",
                                    "Notebook Jupyter vazio para sketches"
                                  ],
                                  "tips": "Pense como um detetive procurando pistas em dados crus sem dicas prévias.",
                                  "learningObjective": "Compreender que o aprendizado não-supervisionado revela padrões sem orientação de rótulos.",
                                  "commonMistakes": [
                                    "Achar que precisa de rótulos (não precisa)",
                                    "Confundir com semi-supervisionado (que mistura ambos)"
                                  ]
                                },
                                {
                                  "stepNumber": 3,
                                  "title": "Comparar Diferenças Fundamentais Entre os Dois",
                                  "subSteps": [
                                    "Crie uma tabela comparativa: Colunas para Supervisionado vs Não-Supervisionado; Linhas para Dados (rotulados vs não), Objetivos (previsão vs descoberta), Avaliação (acurácia vs silhueta score).",
                                    "Discuta problemas resolvidos: Supervisão para classificação/regressão; Não-supervisionado para clustering/anomalias.",
                                    "Analise prós/contras: Supervisão precisa mais dados rotulados (caro); Não-supervisionado é exploratório mas interpretativo.",
                                    "Identifique cenários híbridos brevemente.",
                                    "Revise exemplos: Classificação de imagens (sup.) vs Segmentação de mercado (não-sup.)."
                                  ],
                                  "verification": "Preencha e explique uma tabela de comparação com pelo menos 5 diferenças chave.",
                                  "estimatedTime": "25 minutos",
                                  "materials": [
                                    "Planilha Google Sheets ou papel para tabela",
                                    "Infográfico comparativo de Scikit-learn docs"
                                  ],
                                  "tips": "Use cores diferentes na tabela para destacar contrastes visuais.",
                                  "learningObjective": "Diferenciar claramente os aspectos de dados, objetivos, métodos e avaliação de cada abordagem.",
                                  "commonMistakes": [
                                    "Misturar objetivos: achar que clustering é classificação sem rótulos",
                                    "Ignorar métricas de avaliação específicas"
                                  ]
                                },
                                {
                                  "stepNumber": 4,
                                  "title": "Aplicar Conhecimento com Exemplos Práticos",
                                  "subSteps": [
                                    "Escolha um problema real: Classifique flores Iris (supervisionado) vs agrupe clientes por compras (não-supervisionado).",
                                    "Simule: Descreva o pipeline para cada (treino com rótulos vs algoritmo de clustering).",
                                    "Justifique escolha: Quando usar qual baseado no problema.",
                                    "Avalie resultados fictícios: Métricas para cada.",
                                    "Reflita: Como isso impacta decisões em projetos de dados."
                                  ],
                                  "verification": "Escreva um parágrafo justificando a escolha de abordagem para dois cenários hipotéticos.",
                                  "estimatedTime": "30 minutos",
                                  "materials": [
                                    "Código Python simples em Google Colab para Iris e K-means",
                                    "Casos de estudo de Towards Data Science"
                                  ],
                                  "tips": "Teste com código real se possível para reforçar intuição.",
                                  "learningObjective": "Aplicar diferenças para selecionar a abordagem certa em problemas reais.",
                                  "commonMistakes": [
                                    "Forçar supervisão em dados não rotulados",
                                    "Subestimar interpretabilidade no não-supervisionado"
                                  ]
                                }
                              ],
                              "practicalExample": "Em marketing, use aprendizado supervisionado para prever churn de clientes com dados históricos rotulados (churn sim/não). Use não-supervisionado para clusterizar clientes por padrões de compra sem rótulos prévios, revelando segmentos como 'compradores impulsivos'.",
                              "finalVerifications": [
                                "Explique verbalmente a diferença em dados de entrada (rótulos vs sem).",
                                "Dê 2 exemplos de problemas para cada tipo.",
                                "Descreva uma métrica de avaliação para supervisão e uma para não-supervisionado.",
                                "Justifique por que não usar supervisão em clustering.",
                                "Crie um fluxograma de decisão: Supervisionado ou Não?",
                                "Identifique um erro comum em cada abordagem."
                              ],
                              "assessmentCriteria": [
                                "Precisão na definição de rótulos e ausência deles (90%+ correto).",
                                "Capacidade de listar tipos e exemplos relevantes (mínimo 2 por tipo).",
                                "Tabela ou comparação clara com pelo menos 5 diferenças chave.",
                                "Justificativa contextual para escolha de abordagem em cenários.",
                                "Uso correto de terminologia (ex: classificação vs clustering).",
                                "Demonstração de compreensão via exemplos práticos originais."
                              ],
                              "crossCurricularConnections": [
                                "Estatística: Métricas de validação e distribuições de dados.",
                                "Programação: Implementação em Python com Scikit-learn.",
                                "Matemática: Álgebra linear em PCA e distâncias em clustering.",
                                "Negócios: Aplicações em análise de mercado e previsão.",
                                "Ética em IA: Viés em dados rotulados vs interpretabilidade em não-supervisionado."
                              ],
                              "realWorldApplication": "Empresas como Netflix usam supervisão para recomendar filmes baseados em ratings passados (rótulos), enquanto usam não-supervisionado para descobrir gêneros emergentes em visualizações sem labels, otimizando conteúdo e retenção de usuários."
                            },
                            "estimatedTime": "1 hora",
                            "difficulty": "beginner",
                            "status": "not_started",
                            "prerequisites": []
                          },
                          {
                            "id": "10.1.2.6.1.2",
                            "name": "Identificar Aplicações do Aprendizado Não-Supervisionado",
                            "description": "Reconhecer cenários reais onde o aprendizado não-supervisionado é aplicado, como detecção de anomalias em transações financeiras ou segmentação de clientes em marketing, sem necessidade de dados rotulados.",
                            "atomicExpansion": {
                              "steps": [
                                {
                                  "stepNumber": 1,
                                  "title": "Revisar Conceitos Fundamentais do Aprendizado Não-Supervisionado",
                                  "subSteps": [
                                    "Defina aprendizado não-supervisionado como técnicas que trabalham com dados não rotulados.",
                                    "Liste características principais: descoberta de padrões, clustering, redução de dimensionalidade e detecção de anomalias.",
                                    "Compare brevemente com aprendizado supervisionado para destacar a ausência de labels.",
                                    "Identifique vantagens: custo baixo em rotulagem, escalabilidade para grandes datasets.",
                                    "Anote 3 exemplos iniciais de problemas sem labels, como agrupamento de documentos."
                                  ],
                                  "verification": "Crie um resumo de 1 parágrafo explicando o que é aprendizado não-supervisionado e suas vantagens.",
                                  "estimatedTime": "20 minutos",
                                  "materials": [
                                    "Notebook para anotações",
                                    "Artigo introdutório sobre ML não-supervisionado (ex: Scikit-learn docs)"
                                  ],
                                  "tips": [
                                    "Use diagramas para visualizar diferenças entre supervisionado e não-supervisionado.",
                                    "Foque em cenários reais onde labels são caros ou impossíveis."
                                  ],
                                  "learningObjective": "Compreender as bases do aprendizado não-supervisionado para identificar contextos aplicáveis.",
                                  "commonMistakes": [
                                    "Confundir com semi-supervisionado.",
                                    "Ignorar a importância da ausência de labels como driver principal."
                                  ]
                                },
                                {
                                  "stepNumber": 2,
                                  "title": "Mapear Características de Problemas Adequados",
                                  "subSteps": [
                                    "Identifique problemas com dados não rotulados: grandes volumes sem categorização prévia.",
                                    "Classifique cenários por tipo: clustering (agrupamento), anomalias (outliers), associação (regras).",
                                    "Analise restrições: quando labels são caros, raros ou subjetivos.",
                                    "Crie uma tabela com colunas: Problema, Tipo de Técnica, Exemplo Domínio.",
                                    "Preencha com 4 entradas iniciais, como 'Transações bancárias' -> Detecção de Anomalias."
                                  ],
                                  "verification": "Produza uma tabela com pelo menos 5 problemas mapeados para técnicas não-supervisionadas.",
                                  "estimatedTime": "30 minutos",
                                  "materials": [
                                    "Planilha Google Sheets ou Excel",
                                    "Lista de datasets públicos (ex: Kaggle)"
                                  ],
                                  "tips": [
                                    "Priorize domínios variados: finanças, saúde, marketing.",
                                    "Pergunte: 'Há labels? Se não, por quê?'"
                                  ],
                                  "learningObjective": "Desenvolver habilidade para mapear problemas reais às características do não-supervisionado.",
                                  "commonMistakes": [
                                    "Aplicar não-supervisionado onde labels existem e são fáceis.",
                                    "Esquecer redução de dimensionalidade para visualização de dados altos dimensionais."
                                  ]
                                },
                                {
                                  "stepNumber": 3,
                                  "title": "Explorar Exemplos Reais em Diferentes Domínios",
                                  "subSteps": [
                                    "Estude caso 1: Detecção de fraudes em transações financeiras usando Isolation Forest.",
                                    "Estude caso 2: Segmentação de clientes em marketing com K-Means clustering.",
                                    "Estude caso 3: Recomendação de produtos via análise de co-ocorrência (Apriori).",
                                    "Estude caso 4: Compressão de imagens ou detecção de tópicos em textos com PCA/LDA.",
                                    "Registre por que cada um usa não-supervisionado: falta de labels históricos suficientes."
                                  ],
                                  "verification": "Escreva descrições curtas (2-3 frases cada) para 4 casos reais, justificando a escolha.",
                                  "estimatedTime": "40 minutos",
                                  "materials": [
                                    "Vídeos tutoriais (ex: YouTube: 'Unsupervised Learning Applications')",
                                    "Artigos de caso (ex: Towards Data Science)"
                                  ],
                                  "tips": [
                                    "Busque cases de empresas reais como Netflix ou bancos.",
                                    "Note métricas de sucesso usadas nesses cenários."
                                  ],
                                  "learningObjective": "Reconhecer aplicações concretas e suas justificativas em contextos profissionais.",
                                  "commonMistakes": [
                                    "Generalizar demais sem contexto específico.",
                                    "Confundir correlação com causalidade em padrões descobertos."
                                  ]
                                },
                                {
                                  "stepNumber": 4,
                                  "title": "Praticar Identificação em Cenários Novos",
                                  "subSteps": [
                                    "Receba 3 cenários hipotéticos (ex: análise de logs de rede, agrupamento de genes).",
                                    "Para cada um, decida se aplica não-supervisionado e qual técnica.",
                                    "Justifique com base em presença/ausência de labels e objetivos.",
                                    "Crie um cenário próprio e autoavalie.",
                                    "Discuta com par ou fórum online para feedback."
                                  ],
                                  "verification": "Submeta relatório com 4 cenários analisados, incluindo decisão e justificativa.",
                                  "estimatedTime": "30 minutos",
                                  "materials": [
                                    "Fórum ou grupo de estudo",
                                    "Datasets de exemplo (ex: UCI ML Repository)"
                                  ],
                                  "tips": [
                                    "Use checklist: Dados rotulados? Objetivo exploratório? Sim -> Não-supervisionado.",
                                    "Varie domínios para robustez."
                                  ],
                                  "learningObjective": "Aplicar conhecimento para identificar aplicações independentes em novos contextos.",
                                  "commonMistakes": [
                                    "Forçar aplicação onde supervisionado é melhor.",
                                    "Ignorar viabilidade computacional para grandes dados."
                                  ]
                                }
                              ],
                              "practicalExample": "Em uma empresa de e-commerce, use K-Means para segmentar 1 milhão de clientes baseados em padrões de compra não rotulados, identificando grupos como 'compradores impulsivos' sem necessidade de surveys prévios, permitindo campanhas personalizadas que aumentam retenção em 15%.",
                              "finalVerifications": [
                                "Liste pelo menos 5 aplicações reais com domínios variados.",
                                "Explique por que não-supervisionado é preferível em cada caso (ausência de labels).",
                                "Mapeie técnicas específicas (ex: clustering, anomalias) para problemas.",
                                "Identifique limitações em 2 cenários onde não se aplica.",
                                "Crie um fluxograma de decisão para escolher não-supervisionado.",
                                "Avalie um case study externo com precisão."
                              ],
                              "assessmentCriteria": [
                                "Precisão na identificação de cenários sem labels (90%+ acurácia).",
                                "Diversidade de domínios cobertos (finanças, marketing, saúde, etc.).",
                                "Justificativas claras e baseadas em características do método.",
                                "Uso correto de terminologia (clustering, PCA, etc.).",
                                "Criatividade em exemplos originais e viáveis.",
                                "Compreensão de trade-offs vs. métodos supervisionados."
                              ],
                              "crossCurricularConnections": [
                                "Estatística: Análise exploratória de dados e testes de hipóteses.",
                                "Negócios: Análise de marketing e CRM para segmentação.",
                                "Computação: Algoritmos de clustering e eficiência computacional.",
                                "Biologia: Bioinformática para clustering de sequências genéticas.",
                                "Economia: Detecção de fraudes e modelagem comportamental."
                              ],
                              "realWorldApplication": "No setor bancário, aprendizado não-supervisionado detecta transações fraudulentas em tempo real analisando padrões anômalos em bilhões de dados diários sem rotulagem manual, salvando milhões em perdas e permitindo foco humano em confirmações críticas."
                            },
                            "estimatedTime": "1 hora",
                            "difficulty": "beginner",
                            "status": "not_started",
                            "prerequisites": []
                          },
                          {
                            "id": "10.1.2.6.1.3",
                            "name": "Compreender o Papel dos Dados i.i.d. no Não-Supervisionado",
                            "description": "Explicar a importância da suposição de dados independentes e identicamente distribuídos (i.i.d.) no contexto de modelagem não-supervisionada e suas implicações para a generalização de padrões descobertos.",
                            "atomicExpansion": {
                              "steps": [
                                {
                                  "stepNumber": 1,
                                  "title": "Definir o Conceito de Dados i.i.d.",
                                  "subSteps": [
                                    "Estude a definição formal: dados independentes (eventos não influenciam uns aos outros) e identicamente distribuídos (mesma distribuição de probabilidade).",
                                    "Analise exemplos simples: lançamento de moedas justas repetidos.",
                                    "Diferencie independência de correlação e distribuição idêntica de variância igual.",
                                    "Crie um diagrama ilustrando dependência vs. independência em sequências de dados.",
                                    "Resuma em uma frase o que viola a suposição i.i.d."
                                  ],
                                  "verification": "Escreva uma definição precisa de i.i.d. e forneça um exemplo correto e um incorreto.",
                                  "estimatedTime": "25 minutos",
                                  "materials": [
                                    "Notebook para anotações",
                                    "Artigo ou vídeo sobre probabilidade básica (ex: Khan Academy)"
                                  ],
                                  "tips": "Use analogias cotidianas, como sorteios independentes em uma loteria.",
                                  "learningObjective": "Compreender e articular a definição exata de dados i.i.d.",
                                  "commonMistakes": [
                                    "Confundir independência estatística com ausência de correlação linear",
                                    "Achar que i.i.d. implica variância zero"
                                  ]
                                },
                                {
                                  "stepNumber": 2,
                                  "title": "Entender a Importância da Suposição i.i.d. no Aprendizado de Máquina",
                                  "subSteps": [
                                    "Revise como i.i.d. fundamenta teoremas como Hoeffding's Inequality para garantias probabilísticas.",
                                    "Compare cenários ML com e sem i.i.d.: convergência de estimadores.",
                                    "Discuta por que violações (ex: dados temporais) quebram modelos.",
                                    "Calcule um exemplo simples de média amostral sob i.i.d. vs. dependente.",
                                    "Liste teoremas ML que dependem de i.i.d. (ex: lei dos grandes números)."
                                  ],
                                  "verification": "Explique verbalmente ou por escrito por que i.i.d. é crucial para a validade estatística de modelos ML.",
                                  "estimatedTime": "30 minutos",
                                  "materials": [
                                    "Python/Jupyter com NumPy para simulações simples",
                                    "Livro 'Elements of Statistical Learning' (cap. 2)"
                                  ],
                                  "tips": "Simule em código para visualizar o impacto na variância.",
                                  "learningObjective": "Reconhecer o papel teórico de i.i.d. na confiabilidade de algoritmos de ML.",
                                  "commonMistakes": [
                                    "Subestimar o impacto em grandes datasets",
                                    "Ignorar que i.i.d. é uma suposição aproximada na prática"
                                  ]
                                },
                                {
                                  "stepNumber": 3,
                                  "title": "Explorar o Papel Específico no Aprendizado Não-Supervisionado",
                                  "subSteps": [
                                    "Analise como i.i.d. permite descobrir padrões globais em clustering (ex: K-means).",
                                    "Estude implicações em PCA ou autoencoders: reconstrução fiel sob i.i.d.",
                                    "Discuta violações comuns em dados reais não-supervisionados (ex: séries temporais).",
                                    "Compare densidades de probabilidade em dados i.i.d. vs. dependentes para modelagem generativa.",
                                    "Avalie como i.i.d. afeta a estabilidade de clusters ou componentes principais."
                                  ],
                                  "verification": "Descreva como a suposição i.i.d. impacta a interpretação de clusters em um dataset.",
                                  "estimatedTime": "35 minutos",
                                  "materials": [
                                    "Scikit-learn para clustering simples",
                                    "Dataset Iris ou similar"
                                  ],
                                  "tips": "Execute K-means em dados i.i.d. e adicione dependência artificial para comparar.",
                                  "learningObjective": "Identificar como i.i.d. suporta a generalização em métodos não-supervisionados.",
                                  "commonMistakes": [
                                    "Achar que não-supervisionado ignora i.i.d.",
                                    "Confundir com necessidade de labels"
                                  ]
                                },
                                {
                                  "stepNumber": 4,
                                  "title": "Analisar Implicações para Generalização de Padrões",
                                  "subSteps": [
                                    "Defina generalização: padrões aplicáveis a novos dados i.i.d. da mesma distribuição.",
                                    "Discuta overfitting em não-supervisionado sem i.i.d. (ex: ruído dependente).",
                                    "Explore técnicas de mitigação: bootstrapping ou validação cruzada adaptada.",
                                    "Crie um contra-exemplo: dados de rede social (dependentes) vs. amostras aleatórias.",
                                    "Sintetize: quando relaxar i.i.d. em aplicações reais."
                                  ],
                                  "verification": "Argumente se um modelo não-supervisionado generaliza sem i.i.d., com justificativa.",
                                  "estimatedTime": "30 minutos",
                                  "materials": [
                                    "Papel e caneta para diagramas",
                                    "Artigo sobre 'i.i.d. assumptions in ML'"
                                  ],
                                  "tips": "Pense em termos de risco empírico vs. risco populacional.",
                                  "learningObjective": "Avaliar riscos à generalização sem i.i.d. no não-supervisionado.",
                                  "commonMistakes": [
                                    "Generalizar i.i.d. como sempre verdadeira",
                                    "Ignorar contextos onde é violada intencionalmente"
                                  ]
                                }
                              ],
                              "practicalExample": "Ao aplicar K-means em um dataset de imagens de dígitos escritos à mão (MNIST subset), assumindo i.i.d., o algoritmo descobre clusters estáveis de dígitos (0-9). Se os dados forem sequenciais (ex: traços de escrita em ordem temporal), violações de i.i.d. levam a clusters instáveis, demonstrando a necessidade da suposição para generalização.",
                              "finalVerifications": [
                                "Explicar i.i.d. em termos probabilísticos simples.",
                                "Identificar violações de i.i.d. em um dataset dado.",
                                "Descrever impacto em clustering sem i.i.d.",
                                "Argumentar por que i.i.d. é essencial para generalização não-supervisionada.",
                                "Propor um teste simples para checar i.i.d. em dados.",
                                "Comparar i.i.d. em supervisionado vs. não-supervisionado."
                              ],
                              "assessmentCriteria": [
                                "Precisão na definição de i.i.d. (90%+ acurácia conceitual).",
                                "Capacidade de ligar i.i.d. a teoremas ML fundamentais.",
                                "Análise correta de implicações em não-supervisionado.",
                                "Uso de exemplos práticos relevantes.",
                                "Identificação de limitações e violações reais.",
                                "Clareza na explicação de generalização."
                              ],
                              "crossCurricularConnections": [
                                "Probabilidade e Estatística: Lei dos Grandes Números e Central Limit Theorem.",
                                "Ciência de Dados: Validação de Modelos e Diagnóstico de Dados.",
                                "Matemática Computacional: Teoria da Informação e Entropia.",
                                "Física: Modelagem de Sistemas Estocásticos (ex: partículas brownianas).",
                                "Engenharia de Software: Testes Unitários para Suposições de Dados."
                              ],
                              "realWorldApplication": "Em detecção de fraudes bancárias não-supervisionada, assumir i.i.d. em transações permite clustering de padrões anômalos que generalizam para novos clientes; violações (ex: tendências sazonais) exigem pré-processamento para manter a eficácia do modelo."
                            },
                            "estimatedTime": "0.5 hora",
                            "difficulty": "intermediate",
                            "status": "not_started",
                            "prerequisites": []
                          }
                        ]
                      },
                      {
                        "id": "10.1.2.6.2",
                        "name": "Clustering",
                        "description": "Introdução ao clustering como técnica principal de aprendizado não-supervisionado para agrupar dados semelhantes sem rótulos prévios, explorando algoritmos e métricas de avaliação.",
                        "specificSkills": [
                          {
                            "id": "10.1.2.6.2.1",
                            "name": "Definir e Exemplificar Clustering",
                            "description": "Descrever o clustering como processo de divisão de dados em grupos baseados em similaridade, com exemplos como agrupamento de clientes por comportamento de compra ou imagens por características visuais.",
                            "atomicExpansion": {
                              "steps": [
                                {
                                  "stepNumber": 1,
                                  "title": "Compreender a Definição Fundamental de Clustering",
                                  "subSteps": [
                                    "Leia a definição oficial: clustering é o processo de dividir um conjunto de dados em grupos (clusters) baseados em similaridades internas, sem rótulos prévios.",
                                    "Identifique componentes chave: dados não rotulados, similaridade, grupos homogêneos.",
                                    "Compare com aprendizado supervisionado: clustering é não-supervisionado, sem 'respostas corretas' dadas.",
                                    "Escreva sua própria definição em 2-3 frases.",
                                    "Pesquise uma definição alternativa em fontes confiáveis como Wikipedia ou livros de ML."
                                  ],
                                  "verification": "Sua definição escrita corresponde à oficial em pelo menos 80% dos elementos chave.",
                                  "estimatedTime": "30 minutos",
                                  "materials": [
                                    "Notebook para anotações",
                                    "Acesso à internet para definições",
                                    "Vídeo introdutório sobre ML não-supervisionado (ex: Khan Academy)"
                                  ],
                                  "tips": "Use analogias cotidianas, como agrupar amigos por hobbies, para fixar o conceito.",
                                  "learningObjective": "Definir clustering com precisão, distinguindo-o de métodos supervisionados.",
                                  "commonMistakes": [
                                    "Confundir com classificação supervisionada",
                                    "Ignorar a ausência de rótulos",
                                    "Pensar que clusters são sempre perfeitamente separados"
                                  ]
                                },
                                {
                                  "stepNumber": 2,
                                  "title": "Explorar Medidas de Similaridade e Distância",
                                  "subSteps": [
                                    "Estude distância euclidiana: fórmula sqrt(sum((x_i - y_i)^2)) e aplique a pontos 2D simples.",
                                    "Aprenda outras métricas: Manhattan, cosseno similarity para dados não-euclidianos.",
                                    "Calcule distâncias manualmente para 3-5 pares de pontos em um dataset fictício.",
                                    "Visualize com gráficos: plote pontos e desenhe linhas de distância.",
                                    "Discuta quando usar cada métrica (ex: euclidiana para coordenadas espaciais)."
                                  ],
                                  "verification": "Calcule corretamente distâncias para pelo menos 3 exemplos e explique escolhas de métrica.",
                                  "estimatedTime": "45 minutos",
                                  "materials": [
                                    "Papel e lápis para cálculos",
                                    "Calculadora ou Google Sheets para fórmulas",
                                    "Ferramenta online como Desmos para plotar pontos"
                                  ],
                                  "tips": "Comece com 2 dimensões para visualizar; evite datasets grandes inicialmente.",
                                  "learningObjective": "Aplicar métricas de similaridade para quantificar quão 'parecidos' são dados.",
                                  "commonMistakes": [
                                    "Esquecer raiz quadrada na euclidiana",
                                    "Usar similaridade em vez de distância sem normalizar",
                                    "Ignorar escala das features"
                                  ]
                                },
                                {
                                  "stepNumber": 3,
                                  "title": "Analisar Exemplos Simples de Clustering",
                                  "subSteps": [
                                    "Exemplo 1: Agrupe 10 frutas por cor e tamanho (ex: maçãs vermelhas grandes em um cluster).",
                                    "Exemplo 2: Clientes por compras: alto valor + frequência vs. baixo valor + raro.",
                                    "Desenhe clusters manualmente em papel para cada exemplo.",
                                    "Identifique o algoritmo implícito (ex: K-means intuitivo).",
                                    "Explique por que esses grupos são úteis (insights gerados)."
                                  ],
                                  "verification": "Crie diagramas de 2 exemplos com clusters claros e justificativas.",
                                  "estimatedTime": "40 minutos",
                                  "materials": [
                                    "Papel quadriculado",
                                    "Marcadores coloridos",
                                    "Dataset simples impresso ou digital"
                                  ],
                                  "tips": "Use cores diferentes para clusters; mire em 2-4 grupos por exemplo.",
                                  "learningObjective": "Exemplificar clustering com casos concretos, destacando padrões emergentes.",
                                  "commonMistakes": [
                                    "Forçar clusters desiguais em tamanho",
                                    "Misturar features irrelevantes",
                                    "Criar mais clusters que dados"
                                  ]
                                },
                                {
                                  "stepNumber": 4,
                                  "title": "Criar e Exemplificar um Clustering Próprio",
                                  "subSteps": [
                                    "Colete um pequeno dataset pessoal (ex: notas de 10 músicas por gênero/duração).",
                                    "Escolha métricas de similaridade e agrupe em 3 clusters.",
                                    "Descreva cada cluster e insights (ex: 'Cluster rock: alta duração, energia').",
                                    "Valide: todos pontos em um cluster são mais similares entre si que com outros?",
                                    "Apresente em um relatório curto com visualização."
                                  ],
                                  "verification": "Relatório com dataset, clusters, métricas e validação coerente.",
                                  "estimatedTime": "50 minutos",
                                  "materials": [
                                    "Planilha Excel/Google Sheets",
                                    "Ferramenta de plotagem como Chart.js ou papel",
                                    "Dados pessoais coletados"
                                  ],
                                  "tips": "Limite a 4-5 features; normalize dados para evitar viés de escala.",
                                  "learningObjective": "Aplicar clustering end-to-end em dados reais para gerar exemplos acionáveis.",
                                  "commonMistakes": [
                                    "Escolher K arbitrário sem elbow method ideia",
                                    "Overfitting a ruído",
                                    "Não documentar decisões"
                                  ]
                                }
                              ],
                              "practicalExample": "Em uma loja online, agrupar 1000 clientes por comportamento de compra: Cluster 1 (frequentes, alto valor: VIPs para promoções premium); Cluster 2 (raros, baixo valor: inativos para reengajamento); Cluster 3 (frequentes, baixo valor: descontos em volume). Usando distância euclidiana em features como frequência, ticket médio e recência.",
                              "finalVerifications": [
                                "Define clustering corretamente como não-supervisionado baseado em similaridade.",
                                "Explica pelo menos 2 métricas de distância com fórmulas ou exemplos.",
                                "Fornece 2 exemplos pessoais válidos com visualizações.",
                                "Distinguir clustering de classificação supervisionada.",
                                "Valida clusters por homogeneidade interna.",
                                "Gera insights úteis de um exemplo prático."
                              ],
                              "assessmentCriteria": [
                                "Precisão da definição (80%+ alinhamento com conceitos padrão).",
                                "Correção matemática em cálculos de similaridade.",
                                "Criatividade e relevância dos exemplos (práticos e ilustrativos).",
                                "Qualidade das visualizações e validações.",
                                "Profundidade de insights gerados dos clusters.",
                                "Clareza na distinção de métodos supervisionados."
                              ],
                              "crossCurricularConnections": [
                                "Estatística: Análise de variância dentro/between clusters.",
                                "Biologia: Filogenética e agrupamento de espécies por DNA.",
                                "Marketing: Segmentação de mercado e personas.",
                                "Informática: Algoritmos de particionamento em grafos."
                              ],
                              "realWorldApplication": "Clustering é usado em recomendadores da Netflix (agrupar usuários por preferências), detecção de fraudes em bancos (grupos anômalos), análise de imagens médicas (tumores por características) e urbanismo (zonas de cidades por densidade demográfica)."
                            },
                            "estimatedTime": "1 hora",
                            "difficulty": "beginner",
                            "status": "not_started",
                            "prerequisites": [
                              "10.1.2.6.1"
                            ]
                          },
                          {
                            "id": "10.1.2.6.2.2",
                            "name": "Entender o Algoritmo K-Means",
                            "description": "Explicar o funcionamento passo a passo do algoritmo K-Means, incluindo inicialização de centróides, atribuição de pontos e atualização iterativa, com discussão sobre escolha do número de clusters (método do cotovelo).",
                            "atomicExpansion": {
                              "steps": [
                                {
                                  "stepNumber": 1,
                                  "title": "Compreender Conceitos Básicos e Inicialização dos Centróides",
                                  "subSteps": [
                                    "Defina clustering como agrupamento de dados semelhantes sem rótulos.",
                                    "Explique K-Means como algoritmo iterativo que particiona dados em K clusters esféricos.",
                                    "Descreva métodos de inicialização: aleatória (Forgy) ou K-Means++ para melhor convergência.",
                                    "Escolha um K inicial e gere centróides iniciais.",
                                    "Visualize centróides iniciais em um espaço 2D com pontos de dados."
                                  ],
                                  "verification": "Desenhe um diagrama com pontos de dados e centróides iniciais, explicando escolhas.",
                                  "estimatedTime": "45 minutos",
                                  "materials": [
                                    "Papel e caneta para diagramas",
                                    "Notebook com Python e matplotlib",
                                    "Dataset simples 2D (ex: iris.csv)"
                                  ],
                                  "tips": "Sempre visualize em 2D primeiro para intuição; use K-Means++ para evitar convergência local ruim.",
                                  "learningObjective": "Entender os fundamentos do K-Means e como inicializar centróides efetivamente.",
                                  "commonMistakes": "Confundir inicialização aleatória com atribuição de clusters; ignorar sensibilidade à inicialização."
                                },
                                {
                                  "stepNumber": 2,
                                  "title": "Atribuição de Pontos aos Clusters",
                                  "subSteps": [
                                    "Calcule a distância euclidiana de cada ponto ao centróide mais próximo.",
                                    "Atribua cada ponto ao cluster cujo centróide minimize a distância.",
                                    "Forme grupos iniciais baseados nessa atribuição.",
                                    "Visualize as atribuições colorindo pontos por cluster.",
                                    "Discuta o papel da métrica de distância (euclidiana padrão)."
                                  ],
                                  "verification": "Implemente ou simule atribuição manual em um dataset pequeno e verifique agrupamentos.",
                                  "estimatedTime": "30 minutos",
                                  "materials": [
                                    "Planilha Excel ou Google Sheets para cálculos manuais",
                                    "Código Python com numpy para distâncias"
                                  ],
                                  "tips": "Use distância euclidiana para dados numéricos; normalize features se escalas diferirem.",
                                  "learningObjective": "Dominar o passo de atribuição baseado em minimização de distâncias.",
                                  "commonMistakes": "Usar distância errada (ex: Manhattan em vez de Euclidiana); esquecer normalização de features."
                                },
                                {
                                  "stepNumber": 3,
                                  "title": "Atualização dos Centróides",
                                  "subSteps": [
                                    "Para cada cluster, calcule o centróide como a média dos pontos atribuídos.",
                                    "Atualize todos os centróides simultaneamente com base nas atribuições atuais.",
                                    "Verifique se os novos centróides mudaram significativamente.",
                                    "Visualize o movimento dos centróides.",
                                    "Explique por que a média minimiza a soma de quadrados das distâncias."
                                  ],
                                  "verification": "Calcule manualmente novos centróides para um cluster de 5 pontos e compare.",
                                  "estimatedTime": "40 minutos",
                                  "materials": [
                                    "Calculadora ou Python para médias",
                                    "Gráficos interativos com plotly"
                                  ],
                                  "tips": "Centróides vazios: reatribua ou ignore; use média ponderada se necessário.",
                                  "learningObjective": "Compreender a atualização como otimização da função objetivo (inércia).",
                                  "commonMistakes": "Atualizar centróides sequencialmente em vez de em batch; dividir por zero em clusters vazios."
                                },
                                {
                                  "stepNumber": 4,
                                  "title": "Processo Iterativo e Critérios de Convergência",
                                  "subSteps": [
                                    "Repita atribuição e atualização até convergência.",
                                    "Defina critérios: mudança máxima em centróides < epsilon, ou máximo de iterações.",
                                    "Monitore a inércia (soma de distâncias quadradas intra-cluster).",
                                    "Identifique convergência local vs. global.",
                                    "Implemente loop em pseudocódigo."
                                  ],
                                  "verification": "Simule 3 iterações em papel e mostre estabilização dos clusters.",
                                  "estimatedTime": "35 minutos",
                                  "materials": [
                                    "Pseudocódigo em editor de texto",
                                    "Jupyter notebook para simulação"
                                  ],
                                  "tips": "Defina epsilon=0.001 e max_iter=100; plote inércia por iteração para diagnóstico.",
                                  "learningObjective": "Entender o loop iterativo e quando parar o algoritmo.",
                                  "commonMistakes": "Não monitorar inércia, levando a loops infinitos; confundir convergência com ótimo global."
                                },
                                {
                                  "stepNumber": 5,
                                  "title": "Escolha do Número de Clusters com Método do Cotovelo",
                                  "subSteps": [
                                    "Execute K-Means para K=1 a 10, calculando inércia para cada.",
                                    "Plote inércia vs. K; identifique 'cotovelo' onde diminuição desacelera.",
                                    "Explique heurística: equilíbrio entre poucos clusters e ajuste fino.",
                                    "Compare com silhouette score como alternativa.",
                                    "Discuta limitações (subjetividade do cotovelo)."
                                  ],
                                  "verification": "Gere gráfico de cotovelo para um dataset e justifique escolha de K.",
                                  "estimatedTime": "50 minutos",
                                  "materials": [
                                    "Python com scikit-learn e matplotlib",
                                    "Dataset como blobs ou moons"
                                  ],
                                  "tips": "Use range amplo de K; automatize com KneeLocator se disponível.",
                                  "learningObjective": "Selecionar K ótimo usando análise visual e métricas.",
                                  "commonMistakes": "Escolher K pelo menor inércia (sobreamostragem); ignorar domínio do problema."
                                }
                              ],
                              "practicalExample": "Em um dataset de clientes de varejo com features como idade e gasto anual, aplique K-Means com K=3: inicialize centróides, atribua clientes a clusters (ex: jovens gastadores, idosos econômicos), atualize iterativamente e use cotovelo para confirmar K=3, segmentando para campanhas direcionadas.",
                              "finalVerifications": [
                                "Explique os 5 passos principais do K-Means sem consultar notas.",
                                "Simule manualmente 2 iterações em um conjunto de 10 pontos 2D.",
                                "Gere e interprete gráfico de cotovelo para dataset padrão.",
                                "Discuta 3 limitações do K-Means (ex: clusters esféricos, sensível a outliers).",
                                "Implemente K-Means básico em Python sem scikit-learn.",
                                "Compare resultados com diferentes inicializações."
                              ],
                              "assessmentCriteria": [
                                "Precisão na descrição dos passos iterativos (atribuição e atualização).",
                                "Correta explicação matemática da distância euclidiana e média como centróide.",
                                "Capacidade de identificar cotovelo em gráfico e justificar K.",
                                "Uso apropriado de visualizações para ilustrar processo.",
                                "Reconhecimento de suposições (ex: clusters globulares) e limitações.",
                                "Clareza na comunicação com exemplos concretos."
                              ],
                              "crossCurricularConnections": [
                                "Estatística: Cálculo de médias, variância e soma de quadrados.",
                                "Programação: Implementação de loops e funções em Python/NumPy.",
                                "Visualização de Dados: Gráficos com Matplotlib/Seaborn para análise.",
                                "Otimização: Minimização de função objetivo iterativa.",
                                "Ciência de Dados: Integração com pré-processamento e avaliação de modelos."
                              ],
                              "realWorldApplication": "Segmentação de clientes em marketing digital para personalização de ofertas; compressão de imagens agrupando cores similares; análise de genes em bioinformática para descobrir padrões de expressão; detecção de comunidades em redes sociais."
                            },
                            "estimatedTime": "2 horas",
                            "difficulty": "intermediate",
                            "status": "not_started",
                            "prerequisites": [
                              "10.1.2.6.1"
                            ]
                          },
                          {
                            "id": "10.1.2.6.2.3",
                            "name": "Avaliar Qualidade de Clusters",
                            "description": "Aplicar métricas de avaliação internas como Silhouette Score e Davies-Bouldin Index para medir a coesão e separação de clusters sem rótulos verdadeiros.",
                            "atomicExpansion": {
                              "steps": [
                                {
                                  "stepNumber": 1,
                                  "title": "Compreender as Métricas de Avaliação Interna de Clusters",
                                  "subSteps": [
                                    "Estude a definição do Silhouette Score: mede quão similar um ponto é ao seu cluster comparado aos vizinhos.",
                                    "Analise o Davies-Bouldin Index (DBI): calcula a razão entre dispersão interna e separação entre clusters.",
                                    "Compare as duas métricas: Silhouette varia de -1 a 1 (maior é melhor), DBI menor é melhor.",
                                    "Revise fórmulas matemáticas básicas para cada métrica.",
                                    "Identifique quando usar cada uma: Silhouette para coesão/separação geral, DBI para robustez em datasets variados."
                                  ],
                                  "verification": "Explique verbalmente ou por escrito as diferenças entre Silhouette Score e DBI, com exemplos numéricos simples.",
                                  "estimatedTime": "45 minutos",
                                  "materials": [
                                    "Documentação Scikit-learn para métricas de clustering",
                                    "Notebook Jupyter",
                                    "Artigos introdutórios sobre avaliação de clusters"
                                  ],
                                  "tips": "Use diagramas visuais para entender coesão (dentro do cluster) vs. separação (entre clusters).",
                                  "learningObjective": "Dominar os conceitos teóricos das métricas Silhouette e DBI para avaliação sem supervisão.",
                                  "commonMistakes": [
                                    "Confundir interpretação: Silhouette alto = bom, DBI baixo = bom.",
                                    "Ignorar que métricas internas não garantem clusters semanticamente corretos."
                                  ]
                                },
                                {
                                  "stepNumber": 2,
                                  "title": "Preparar Dataset e Gerar Clusters",
                                  "subSteps": [
                                    "Carregue um dataset sem rótulos (ex: Iris sem labels) usando Pandas.",
                                    "Pré-processe: normalize/escale features com StandardScaler.",
                                    "Aplique algoritmo de clustering como KMeans com diferentes K (ex: 2-10).",
                                    "Armazene os labels de cluster gerados para cada K.",
                                    "Visualize clusters iniciais com PCA ou t-SNE para intuição."
                                  ],
                                  "verification": "Execute código que gera clusters para K=3 e plote scatter plot colorido por labels de cluster.",
                                  "estimatedTime": "30 minutos",
                                  "materials": [
                                    "Biblioteca Scikit-learn",
                                    "Pandas",
                                    "Matplotlib/Seaborn",
                                    "Dataset Iris ou similar"
                                  ],
                                  "tips": "Sempre escale dados antes de KMeans, pois é sensível a escalas.",
                                  "learningObjective": "Preparar dados reais para aplicação de métricas de cluster.",
                                  "commonMistakes": [
                                    "Esquecer normalização, levando a clusters enviesados.",
                                    "Escolher K fixo sem testar múltiplos."
                                  ]
                                },
                                {
                                  "stepNumber": 3,
                                  "title": "Calcular e Comparar Métricas de Avaliação",
                                  "subSteps": [
                                    "Use silhouette_score(X, labels) do Scikit-learn para cada K.",
                                    "Calcule Davies-Bouldin com davies_bouldin_score(X, labels).",
                                    "Crie um loop para variar K e tabule resultados em DataFrame.",
                                    "Plote gráficos: Silhouette vs K e DBI vs K.",
                                    "Identifique K ótimo: pico no Silhouette ou mínimo no DBI."
                                  ],
                                  "verification": "Gere tabela com scores para K=2 a 10 e identifique o melhor K com justificativa.",
                                  "estimatedTime": "40 minutos",
                                  "materials": [
                                    "Scikit-learn metrics module",
                                    "Jupyter Notebook",
                                    "Dataset preparado do step 2"
                                  ],
                                  "tips": "Use silhouette_analysis() para visualização detalhada por cluster.",
                                  "learningObjective": "Implementar computacionalmente as métricas e automatizar avaliação para múltiplos K.",
                                  "commonMistakes": [
                                    "Passar labels errados para as funções.",
                                    "Interpretar gráficos sem considerar 'elbow' method como complemento."
                                  ]
                                },
                                {
                                  "stepNumber": 4,
                                  "title": "Interpretar Resultados e Validar Qualidade",
                                  "subSteps": [
                                    "Interprete scores: Silhouette >0.5 = clusters fortes; DBI <1 = bom.",
                                    "Compare com método Elbow para validar K ótimo.",
                                    "Analise per-cluster: média Silhouette por cluster para detectar outliers.",
                                    "Discuta limitações: sensibilidade a densidade, não captura hierarquia.",
                                    "Documente relatório com gráficos, scores e recomendações."
                                  ],
                                  "verification": "Escreva um parágrafo resumindo a qualidade dos clusters e sugerindo K final.",
                                  "estimatedTime": "35 minutos",
                                  "materials": [
                                    "Gráficos gerados",
                                    "Notebook com resultados",
                                    "Referências teóricas"
                                  ],
                                  "tips": "Combine múltiplas métricas para decisão robusta, não confie em uma só.",
                                  "learningObjective": "Extrair insights acionáveis das métricas para decisões de modelagem.",
                                  "commonMistakes": [
                                    "Sobre-generalizar: alto score não implica clusters 'corretos' semanticamente.",
                                    "Ignorar computação custosa para grandes datasets."
                                  ]
                                }
                              ],
                              "practicalExample": "Usando o dataset Iris (sem labels verdadeiros), aplique KMeans para K=2 a 10. Calcule Silhouette e DBI: encontre K=3 com Silhouette=0.55 e DBI=0.68, confirmando 3 clusters naturais (setosas, versicolor, virginica). Plote para visualizar separação clara.",
                              "finalVerifications": [
                                "Calcula corretamente Silhouette Score para dataset de teste.",
                                "Interpreta DBI abaixo de 1 como indício de boa separação.",
                                "Identifica K ótimo combinando múltiplas métricas.",
                                "Gera plots de Silhouette por K e por cluster.",
                                "Explica limitações das métricas internas.",
                                "Aplica em dataset novo sem supervisão."
                              ],
                              "assessmentCriteria": [
                                "Precisão nos cálculos das métricas (erro <0.01).",
                                "Interpretação correta: valores altos/baixos e implicações.",
                                "Automação para múltiplos K com visualizações claras.",
                                "Relatório conciso com recomendações baseadas em scores.",
                                "Reconhecimento de limitações e sugestões de complementos.",
                                "Eficiência computacional em datasets médios (>1000 amostras)."
                              ],
                              "crossCurricularConnections": [
                                "Estatística: Distribuições e medidas de dispersão (variância intra-cluster).",
                                "Visualização de Dados: Plots PCA/t-SNE para validação intuitiva.",
                                "Aprendizado Supervisionado: Comparação com métricas externas como ARI.",
                                "Otimização: Elbow method e grid search para hiperparâmetros.",
                                "Ciência de Dados Aplicada: Integração em pipelines de ML não-supervisionado."
                              ],
                              "realWorldApplication": "Em marketing, segmentar clientes por comportamento de compra usando KMeans em dados transacionais; Silhouette/DBI validam segmentos coesos para campanhas targeted, como separar 'compradores fiéis' de 'ocasionalistas' sem labels prévios."
                            },
                            "estimatedTime": "1 hora",
                            "difficulty": "intermediate",
                            "status": "not_started",
                            "prerequisites": [
                              "10.1.2.6.2.1"
                            ]
                          }
                        ]
                      },
                      {
                        "id": "10.1.2.6.3",
                        "name": "Redução de Dimensionalidade",
                        "description": "Conceitos iniciais de técnicas para reduzir o número de features em conjuntos de dados de alta dimensionalidade, preservando informações essenciais, como PCA.",
                        "specificSkills": [
                          {
                            "id": "10.1.2.6.3.1",
                            "name": "Motivação para Redução de Dimensionalidade",
                            "description": "Explicar problemas da 'maldição da dimensionalidade' e benefícios da redução, como visualização de dados, redução de ruído e aceleração de algoritmos de ML.",
                            "atomicExpansion": {
                              "steps": [
                                {
                                  "stepNumber": 1,
                                  "title": "Compreender o Conceito Básico de Dimensionalidade em Dados",
                                  "subSteps": [
                                    "Defina dimensionalidade como o número de features ou variáveis em um dataset.",
                                    "Discuta como dados reais frequentemente têm alta dimensionalidade (ex: imagens com milhares de pixels).",
                                    "Compare baixa vs. alta dimensionalidade usando exemplos simples como 2D (plano) vs. 100D.",
                                    "Explore o volume do espaço de features: demonstre como o volume cresce exponencialmente com dimensões.",
                                    "Calcule distâncias euclidianas em espaços de dimensões crescentes para ilustrar diluição."
                                  ],
                                  "verification": "Crie um gráfico mostrando volume de hipercubo em 1D a 10D e explique o crescimento exponencial.",
                                  "estimatedTime": "45 minutos",
                                  "materials": [
                                    "Python/Jupyter Notebook",
                                    "Bibliotecas: NumPy, Matplotlib",
                                    "Artigo: 'Curse of Dimensionality' de Bellman"
                                  ],
                                  "tips": "Use analogias visuais como 'adicionar andares a um prédio' para intuição geométrica.",
                                  "learningObjective": "Identificar e quantificar o impacto da dimensionalidade no volume do espaço de dados.",
                                  "commonMistakes": [
                                    "Confundir dimensionalidade com tamanho do dataset.",
                                    "Ignorar o crescimento exponencial do volume."
                                  ]
                                },
                                {
                                  "stepNumber": 2,
                                  "title": "Explorar Problemas da Maldição da Dimensionalidade",
                                  "subSteps": [
                                    "Explique a 'maldição' como escassez de dados em espaços de alta dimensão (distâncias se tornam semelhantes).",
                                    "Simule amostragem: mostre como pontos ficam uniformemente espaçados em alta D.",
                                    "Discuta overfitting: modelos memorizam ruído em alta D devido a poucos exemplos por dimensão.",
                                    "Aborde custo computacional: tempo e memória crescem com D para algoritmos como k-NN.",
                                    "Ilustre com experimento: precisão de k-NN caindo com dimensões crescentes em dataset Iris expandido."
                                  ],
                                  "verification": "Execute simulação em código e plote distância média entre pontos vs. dimensões.",
                                  "estimatedTime": "60 minutos",
                                  "materials": [
                                    "Jupyter Notebook",
                                    "Scikit-learn para datasets",
                                    "Notebook exemplo: 'curse_of_dimensionality.ipynb'"
                                  ],
                                  "tips": "Visualize com scatter plots em 2D/3D e generalize para alta D usando métricas.",
                                  "learningObjective": "Reconhecer sintomas práticos da maldição como diluição de distâncias e overfitting.",
                                  "commonMistakes": [
                                    "Pensar que mais dados sempre resolvem alta D sem redução.",
                                    "Subestimar impacto em tempo de computação."
                                  ]
                                },
                                {
                                  "stepNumber": 3,
                                  "title": "Identificar Benefícios da Redução de Dimensionalidade",
                                  "subSteps": [
                                    "Visualização: reduza para 2D/3D para plotar e interpretar clusters.",
                                    "Redução de ruído: elimine features irrelevantes ou correlacionadas.",
                                    "Aceleração de ML: menos features = treinamento mais rápido (ex: PCA antes de SVM).",
                                    "Melhoria de generalização: evita overfitting ao focar em variância principal.",
                                    "Compressão: armazene menos dados mantendo informação essencial."
                                  ],
                                  "verification": "Aplique PCA em dataset MNIST e compare tempo/visualização antes/depois.",
                                  "estimatedTime": "50 minutos",
                                  "materials": [
                                    "Scikit-learn (PCA)",
                                    "Dataset MNIST ou similar",
                                    "Gráficos interativos com Plotly"
                                  ],
                                  "tips": "Sempre compare métricas antes/depois: tempo, acurácia, variância explicada.",
                                  "learningObjective": "Mapear benefícios específicos a problemas reais de ML.",
                                  "commonMistakes": [
                                    "Acreditar que redução sempre melhora performance sem validação.",
                                    "Ignorar perda de informação na redução."
                                  ]
                                },
                                {
                                  "stepNumber": 4,
                                  "title": "Sintetizar Motivações e Aplicações Iniciais",
                                  "subSteps": [
                                    "Resuma maldição vs. benefícios em uma tabela comparativa.",
                                    "Discuta quando usar: alta D > 10-50 features com redundância.",
                                    "Explore trade-offs: perda de interpretabilidade vs. ganhos em performance.",
                                    "Planeje próximo passo: introduzir técnicas como PCA/t-SNE.",
                                    "Crie mindmap conectando conceitos a cenários reais."
                                  ],
                                  "verification": "Escreva um parágrafo explicando por que redução é essencial em Ciência de Dados moderna.",
                                  "estimatedTime": "30 minutos",
                                  "materials": [
                                    "Ferramenta de mindmap (ex: Draw.io)",
                                    "Resumo em Markdown"
                                  ],
                                  "tips": "Use bullet points para motivações chave em apresentações futuras.",
                                  "learningObjective": "Articular motivações de forma concisa para justificar uso em projetos.",
                                  "commonMistakes": [
                                    "Generalizar benefícios sem contexto específico.",
                                    "Esquecer trade-offs como interpretabilidade."
                                  ]
                                }
                              ],
                              "practicalExample": "Em um dataset de genômica com 20.000 genes (features), aplique redução para visualizar clusters de câncer: sem redução, pontos são indistinguíveis em 20kD; com PCA para 2D, revela padrões claros, reduzindo tempo de k-NN de horas para segundos.",
                              "finalVerifications": [
                                "Explicar 'maldição da dimensionalidade' com exemplo matemático (volume ~ 2^d).",
                                "Listar 3 problemas causados por alta D e como redução mitiga cada um.",
                                "Demonstrar em código: impacto de D em distância média de pontos.",
                                "Comparar performance de modelo ML antes/depois de PCA.",
                                "Identificar cenários reais onde redução é crítica (ex: imagens, texto).",
                                "Criar tabela de benefícios vs. maldição."
                              ],
                              "assessmentCriteria": [
                                "Precisão na definição e matemática da maldição (ex: crescimento exponencial).",
                                "Uso de evidências empíricas (gráficos/código) para ilustrar problemas.",
                                "Compreensão clara de benefícios com exemplos quantificáveis (tempo/acurácia).",
                                "Capacidade de conectar a ML prático (overfitting, visualização).",
                                "Articulação concisa de motivações em contexto interdisciplinar.",
                                "Identificação de trade-offs e quando NÃO usar redução."
                              ],
                              "crossCurricularConnections": [
                                "Estatística: variância e correlação de features.",
                                "Computação: complexidade algorítmica O(D^2) em distâncias.",
                                "Visualização de Dados: projeções para 2D/3D.",
                                "Matemática Linear: autovalores em PCA como motivação.",
                                "Inteligência Artificial: pré-processamento em pipelines de ML."
                              ],
                              "realWorldApplication": "Em sistemas de recomendação da Netflix, redução de dimensionalidade em embeddings de usuários/itens (milhares de dims) acelera similaridade, reduz ruído de preferências esparsas e permite visualização de clusters de gosto para personalização eficiente."
                            },
                            "estimatedTime": "1 hora",
                            "difficulty": "beginner",
                            "status": "not_started",
                            "prerequisites": [
                              "10.1.2.6.1"
                            ]
                          },
                          {
                            "id": "10.1.2.6.3.2",
                            "name": "Compreender Análise de Componentes Principais (PCA)",
                            "description": "Descrever o PCA como método estatístico que transforma dados em componentes principais ortogonais, maximizando variância, incluindo passos como centralização e decomposição de autovalores.",
                            "atomicExpansion": {
                              "steps": [
                                {
                                  "stepNumber": 1,
                                  "title": "Entender os Conceitos Fundamentais do PCA",
                                  "subSteps": [
                                    "Defina redução de dimensionalidade e seus benefícios (menor complexidade computacional, visualização).",
                                    "Explique variância como medida de informação nos dados.",
                                    "Descreva componentes principais como direções ortogonais de máxima variância.",
                                    "Diferencie PCA supervisionado de não-supervisionado."
                                  ],
                                  "verification": "Explique verbalmente ou por escrito os conceitos para um colega, garantindo compreensão sem erros conceituais.",
                                  "estimatedTime": "1 hora",
                                  "materials": [
                                    "Notebook para anotações",
                                    "Vídeo introdutório sobre PCA (ex: Khan Academy)",
                                    "Artigo simples sobre dimensionalidade"
                                  ],
                                  "tips": "Use analogias como 'rotacionar dados para capturar mais espalhamento'.",
                                  "learningObjective": "Compreender os princípios teóricos do PCA e sua motivação.",
                                  "commonMistakes": [
                                    "Confundir variância com correlação",
                                    "Ignorar que PCA é linear",
                                    "Pensar que PCA remove ruído automaticamente"
                                  ]
                                },
                                {
                                  "stepNumber": 2,
                                  "title": "Preparar os Dados: Centralização",
                                  "subSteps": [
                                    "Calcule a média de cada feature no dataset.",
                                    "Subtraia a média de cada valor de feature para centralizar os dados em zero.",
                                    "Verifique se a média dos dados centralizados é aproximadamente zero.",
                                    "Discuta por que a centralização é essencial (evita viés na matriz de covariância).",
                                    "Implemente em código simples (NumPy ou manualmente)."
                                  ],
                                  "verification": "Aplique centralização a um dataset 2D pequeno e confirme que as médias são zero.",
                                  "estimatedTime": "45 minutos",
                                  "materials": [
                                    "Python com NumPy",
                                    "Dataset exemplo (ex: 2D com 10 pontos)",
                                    "Calculadora para verificação manual"
                                  ],
                                  "tips": "Sempre centralize antes de PCA; use np.mean() e broadcasting.",
                                  "learningObjective": "Dominar a centralização como primeiro passo do PCA.",
                                  "commonMistakes": [
                                    "Esquecer de centralizar, levando a componentes deslocados",
                                    "Centralizar incorretamente colunas vs linhas"
                                  ]
                                },
                                {
                                  "stepNumber": 3,
                                  "title": "Calcular a Matriz de Covariância",
                                  "subSteps": [
                                    "Entenda a matriz de covariância como medida de relações lineares entre features.",
                                    "Calcule manualmente para dataset 2D: cov(X) = (X^T X)/(n-1).",
                                    "Implemente em Python usando np.cov().",
                                    "Interprete os valores: diagonal (variâncias), off-diagonal (covariâncias).",
                                    "Visualize a matriz com heatmap."
                                  ],
                                  "verification": "Gere matriz de covariância para dataset centralizado e explique cada elemento.",
                                  "estimatedTime": "1 hora",
                                  "materials": [
                                    "Python com NumPy e Matplotlib",
                                    "Dataset 2D ou Iris reduzido"
                                  ],
                                  "tips": "Use dados centralizados; prefira np.cov(ddof=1) para amostra.",
                                  "learningObjective": "Calcular e interpretar a matriz de covariância corretamente.",
                                  "commonMistakes": [
                                    "Usar população vs amostra (dividir por n vs n-1)",
                                    "Não centralizar antes"
                                  ]
                                },
                                {
                                  "stepNumber": 4,
                                  "title": "Decomposição de Autovalores e Autovetores",
                                  "subSteps": [
                                    "Explique autovalores (quantidade de variância) e autovetores (direções).",
                                    "Calcule usando np.linalg.eig() na matriz de covariância.",
                                    "Ordene autovalores decrescentemente e associe autovetores.",
                                    "Selecione top-k componentes baseado em variância explicada (>80-90%).",
                                    "Visualize autovetores como setas em scatter plot."
                                  ],
                                  "verification": "Decompõe matriz 2x2 manualmente e com código, confirmando resultados iguais.",
                                  "estimatedTime": "1 hora e 30 minutos",
                                  "materials": [
                                    "Python com NumPy e SciPy",
                                    "Matriz de covariância exemplo"
                                  ],
                                  "tips": "Autovetores são ortogonais; some autovalores para variância total.",
                                  "learningObjective": "Executar e interpretar a decomposição espectral.",
                                  "commonMistakes": [
                                    "Não ordenar autovalores",
                                    "Confundir autovetor com autovalor"
                                  ]
                                },
                                {
                                  "stepNumber": 5,
                                  "title": "Projeção dos Dados e Interpretação",
                                  "subSteps": [
                                    "Projete dados originais nos top-k autovetores: Z = X @ V_k.",
                                    "Calcule variância explicada cumulativa: lambda_i / sum(lambda).",
                                    "Visualize dados projetados em 2D/3D.",
                                    "Interprete: componentes capturam padrões lineares.",
                                    "Compare antes/depois em termos de perda de informação."
                                  ],
                                  "verification": "Projete dataset Iris para 2D, plote e calcule variância explicada >70%.",
                                  "estimatedTime": "1 hora",
                                  "materials": [
                                    "Python com NumPy, Matplotlib, Scikit-learn (para comparação)",
                                    "Dataset Iris"
                                  ],
                                  "tips": "Use sklearn.decomposition.PCA para validar implementação manual.",
                                  "learningObjective": "Aplicar PCA completo e avaliar resultados.",
                                  "commonMistakes": [
                                    "Projetar dados não centralizados",
                                    "Ignorar normalização de autovetores"
                                  ]
                                }
                              ],
                              "practicalExample": "Carregue o dataset Iris (4 features, 150 amostras). Centralize, compute covariância, decompõe em autovalores/autovetores, projete para 2 componentes principais. Plote scatter original vs projetado e scree plot de variância explicada (deve capturar ~95% em 2D).",
                              "finalVerifications": [
                                "Explicar PCA em 3 frases precisas.",
                                "Centralizar dataset manualmente e verificar médias zero.",
                                "Calcular e decompor matriz de covariância 2x2.",
                                "Projetar dados e calcular % variância explicada.",
                                "Identificar top-2 componentes em Iris e interpretar.",
                                "Comparar PCA manual vs sklearn.PCA."
                              ],
                              "assessmentCriteria": [
                                "Precisão conceitual (sem erros em definições).",
                                "Correção matemática em cálculos manuais/código.",
                                "Interpretação adequada de variância e componentes.",
                                "Visualizações claras e rotuladas.",
                                "Explicação de limitações do PCA (linearidade, não captura não-linearidades).",
                                "Eficiência na seleção de componentes."
                              ],
                              "crossCurricularConnections": [
                                "Estatística: Matrizes de covariância e análise multivariada.",
                                "Machine Learning: Pré-processamento para clustering/classificação.",
                                "Computação: Álgebra linear numérica e otimização.",
                                "Visualização de Dados: Redução para gráficos exploratórios.",
                                "Engenharia: Compressão de sinais em processamento de imagens."
                              ],
                              "realWorldApplication": "No reconhecimento facial (Eigenfaces para identificar rostos em alta dimensionalidade), análise genômica (reduzir milhares de genes para padrões relevantes), recomendação de produtos (Netflix/Amazon para filtrar features de usuários)."
                            },
                            "estimatedTime": "2 horas",
                            "difficulty": "intermediate",
                            "status": "not_started",
                            "prerequisites": [
                              "10.1.2.6.1"
                            ]
                          },
                          {
                            "id": "10.1.2.6.3.3",
                            "name": "Aplicar PCA em Exemplos Simples",
                            "description": "Interpretar resultados de PCA em datasets reais, como redução de dimensões de imagens ou genes, e discutir trade-offs entre perda de informação e simplificação.",
                            "atomicExpansion": {
                              "steps": [
                                {
                                  "stepNumber": 1,
                                  "title": "Preparar o Dataset e Ambiente",
                                  "subSteps": [
                                    "Instale as bibliotecas necessárias: scikit-learn, pandas, matplotlib e seaborn.",
                                    "Carregue um dataset real simples, como o dataset de expressão gênica do câncer de mama (disponível no sklearn ou UCI ML repo).",
                                    "Explore os dados: verifique dimensões, estatísticas descritivas e distribuições com .describe() e histogramas.",
                                    "Trate valores ausentes e outliers iniciais.",
                                    "Padronize os dados usando StandardScaler para garantir média zero e variância unitária."
                                  ],
                                  "verification": "Dataset carregado, explorado e padronizado sem erros; shape e estatísticas impressas corretamente.",
                                  "estimatedTime": "20-30 minutos",
                                  "materials": [
                                    "Python 3.8+",
                                    "Bibliotecas: scikit-learn, pandas, matplotlib, seaborn",
                                    "Dataset: breast_cancer de sklearn.datasets"
                                  ],
                                  "tips": "Use datasets built-in do sklearn para rapidez; sempre padronize antes de PCA para evitar viés de escala.",
                                  "learningObjective": "Compreender a importância da preparação de dados para PCA eficaz.",
                                  "commonMistakes": [
                                    "Esquecer de padronizar variáveis com escalas diferentes",
                                    "Ignorar outliers que distorcem componentes principais",
                                    "Não verificar shape após pré-processamento"
                                  ]
                                },
                                {
                                  "stepNumber": 2,
                                  "title": "Aplicar PCA e Extrair Componentes Principais",
                                  "subSteps": [
                                    "Instancie PCA com n_components=2 ou número desejado e ajuste nos dados padronizados.",
                                    "Transforme os dados para o novo espaço de dimensões reduzidas.",
                                    "Extraia a variância explicada por componente com .explained_variance_ratio_.",
                                    "Gere o scree plot plotando a variância cumulativa.",
                                    "Calcule loadings (importância de features originais nos componentes)."
                                  ],
                                  "verification": "PCA ajustado; variância explicada impressa e scree plot gerado sem erros.",
                                  "estimatedTime": "15-25 minutos",
                                  "materials": [
                                    "Código Python com scikit-learn PCA",
                                    "Jupyter Notebook para visualizações interativas"
                                  ],
                                  "tips": "Comece com n_components=None para ver todas as componentes possíveis antes de escolher.",
                                  "learningObjective": "Executar PCA corretamente e extrair métricas chave de redução dimensional.",
                                  "commonMistakes": [
                                    "Não especificar whiten=True se necessário para normalização",
                                    "Confundir fit() com fit_transform()",
                                    "Interpretar variância sem contexto cumulativo"
                                  ]
                                },
                                {
                                  "stepNumber": 3,
                                  "title": "Visualizar e Interpretar Resultados",
                                  "subSteps": [
                                    "Crie scatter plot dos dados projetados nos 2 primeiros componentes principais, coloridos por classes/labels.",
                                    "Analise clusters visuais e separação de grupos no espaço reduzido.",
                                    "Examine loadings para identificar features dominantes em cada PC.",
                                    "Compare visualizações original vs reduzida para perda de informação.",
                                    "Discuta como os PCs capturam variância (ex: PC1 = tendência geral, PC2 = variação ortogonal)."
                                  ],
                                  "verification": "Plots gerados mostrando clusters claros; loadings interpretados em relatório curto.",
                                  "estimatedTime": "25-35 minutos",
                                  "materials": [
                                    "Matplotlib/Seaborn para plots",
                                    "Dados transformados do Step 2"
                                  ],
                                  "tips": "Use seaborn.pairplot para contexto; rotule eixos com % variância explicada.",
                                  "learningObjective": "Interpretar visual e quantitativamente os resultados de PCA.",
                                  "commonMistakes": [
                                    "Ignorar labels no plot, perdendo interpretação biológica",
                                    "Sobrepor PCs sem verificar ortogonalidade",
                                    "Não escalar eixos para variância"
                                  ]
                                },
                                {
                                  "stepNumber": 4,
                                  "title": "Avaliar Trade-offs e Aplicações",
                                  "subSteps": [
                                    "Calcule perda de informação: 100% - variância cumulativa explicada pelos k componentes.",
                                    "Teste diferentes k e compare métricas (ex: silhouette score para clusters).",
                                    "Discuta trade-offs: simplificação ganha vs perda de detalhes finos.",
                                    "Aplique em contexto real: reduza imagens (ex: pixels de faces) ou genes e compare.",
                                    "Escreva relatório resumindo insights e limitações do PCA."
                                  ],
                                  "verification": "Tabela de trade-offs criada; relatório com discussões claras.",
                                  "estimatedTime": "20-30 minutos",
                                  "materials": [
                                    "Código para múltiplos k",
                                    "Dataset alternativo como digits para imagens"
                                  ],
                                  "tips": "Use elbow method no scree plot para escolher k ótimo.",
                                  "learningObjective": "Criticar PCA balanceando benefícios e limitações.",
                                  "commonMistakes": [
                                    "Subestimar não-linearidades (PCA é linear)",
                                    "Escolher k alto ignorando overfitting",
                                    "Não quantificar perda de informação numericamente"
                                  ]
                                }
                              ],
                              "practicalExample": "Carregue o dataset breast_cancer do sklearn. Aplique PCA para reduzir de 30 features (genes/marcadores) para 2 PCs. Visualize clusters de tumores benignos vs malignos no scatter plot. Note que PC1 captura 44% da variância (tendência de crescimento tumoral), PC2 19% (variações secundárias). Discuta: com 63% cumulativa, perde-se 37% de info fina, mas ganha visualização simples para diagnósticos iniciais.",
                              "finalVerifications": [
                                "Scree plot e scatter plot de PCs gerados corretamente.",
                                "Variância explicada cumulativa calculada e reportada (>60% para 2 PCs).",
                                "Loadings das top features interpretados por PC.",
                                "Trade-offs quantificados (perda % vs simplificação).",
                                "Relatório discute limitações como linearidade.",
                                "Código reproduzível sem erros."
                              ],
                              "assessmentCriteria": [
                                "Precisão na implementação de PCA e pré-processamento (sem erros de escala).",
                                "Qualidade das visualizações (claras, legendadas, informativas).",
                                "Profundidade na interpretação de variância e loadings.",
                                "Análise crítica de trade-offs com exemplos numéricos.",
                                "Relatório conciso conectando resultados a contextos reais.",
                                "Criatividade em extensões (ex: múltiplos datasets)."
                              ],
                              "crossCurricularConnections": [
                                "Estatística: Matrizes de covariância e autovalores.",
                                "Programação: Manipulação de dados com Python/pandas.",
                                "Biologia/Bioinformática: Análise de expressão gênica.",
                                "Visualização de Dados: Técnicas de plotting para insights.",
                                "Machine Learning: Pré-processamento para modelos supervisionados."
                              ],
                              "realWorldApplication": "Na genômica, PCA reduz milhares de genes para poucos PCs, permitindo visualizar subtipos de câncer e acelerar diagnósticos; em visão computacional, comprime imagens de alta dimensão (ex: MNIST) para reconhecimento facial eficiente, equilibrando perda mínima de info com ganhos computacionais massivos."
                            },
                            "estimatedTime": "1.5 horas",
                            "difficulty": "intermediate",
                            "status": "not_started",
                            "prerequisites": [
                              "10.1.2.6.3.2"
                            ]
                          }
                        ]
                      }
                    ]
                  }
                ]
              },
              {
                "id": "10.1.3",
                "name": "Definições de Dados, Informação e Conhecimento",
                "description": "Definições básicas que distinguem dados, informação e conhecimento.",
                "totalSkills": 42,
                "atomicTopics": [
                  {
                    "id": "10.1.3.1",
                    "name": "Definição de Dados",
                    "description": "Conceito básico de dados como fatos brutos, símbolos ou observações sem contexto ou significado inerente.",
                    "specificSkills": [
                      {
                        "id": "10.1.3.1.1",
                        "name": "Definir o conceito de dados",
                        "description": "Articular com precisão a definição de dados como fatos brutos, símbolos ou observações isoladas, sem contexto ou significado inerente, destacando sua natureza raw e ausência de interpretação.",
                        "atomicExpansion": {
                          "steps": [
                            {
                              "stepNumber": 1,
                              "title": "Explorar exemplos cotidianos de dados brutos",
                              "subSteps": [
                                "Colete 5-10 exemplos de itens isolados do dia a dia, como números, símbolos ou observações (ex: '25', 'azul', 'choveu').",
                                "Registre-os em uma lista sem adicionar qualquer descrição ou contexto.",
                                "Observe como esses itens existem sozinhos, sem explicação.",
                                "Discuta em voz alta por que eles não transmitem significado por si só.",
                                "Crie uma tabela simples separando 'dados' de 'interpretações'."
                              ],
                              "verification": "Lista completa de exemplos sem contexto adicionado, confirmada por auto-revisão ou parceiro.",
                              "estimatedTime": "20 minutos",
                              "materials": "Papel, caneta ou editor de texto simples (ex: Notepad, Google Docs).",
                              "tips": "Evite adicionar frases como 'temperatura é 25°C'; mantenha cru.",
                              "learningObjective": "Identificar itens raw como dados através de exemplos concretos.",
                              "commonMistakes": "Adicionar contexto involuntariamente, confundindo dados com informação."
                            },
                            {
                              "stepNumber": 2,
                              "title": "Analisar a definição formal de dados",
                              "subSteps": [
                                "Leia definições padrão: 'Dados são fatos brutos, símbolos ou observações isoladas sem contexto ou significado inerente'.",
                                "Destaque palavras-chave: 'brutos', 'isolados', 'sem contexto', 'raw'.",
                                "Reescreva a definição em suas próprias palavras, enfatizando ausência de interpretação.",
                                "Compare com sinônimos como 'raw data' em inglês.",
                                "Crie um diagrama ou mapa mental da definição."
                              ],
                              "verification": "Definição reescrita com todas as palavras-chave incluídas e diagrama completo.",
                              "estimatedTime": "25 minutos",
                              "materials": "Acesso a definições online (ex: Wikipedia 'Data'), papel para diagrama.",
                              "tips": "Use negrito ou sublinhado para palavras-chave ao reescrever.",
                              "learningObjective": "Compreender e internalizar os componentes precisos da definição de dados.",
                              "commonMistakes": "Ignorar 'ausência de interpretação', achando que dados têm significado implícito."
                            },
                            {
                              "stepNumber": 3,
                              "title": "Diferenciar dados de informação e conhecimento",
                              "subSteps": [
                                "Defina brevemente: Dados (raw), Informação (dados com contexto), Conhecimento (informação aplicada).",
                                "Pegue seus exemplos do Step 1 e adicione contexto para transformá-los em informação (ex: '25°C hoje' = info sobre tempo).",
                                "Crie uma pirâmide: base=dados, meio=informação, topo=conhecimento.",
                                "Teste transformando 3 exemplos: raw → contextualizado → aplicado.",
                                "Explique verbalmente a distinção para um 'público imaginário'."
                              ],
                              "verification": "Pirâmide completa com 3 exemplos transformados e explicação gravada ou escrita.",
                              "estimatedTime": "30 minutos",
                              "materials": "Papel para pirâmide, gravador de voz ou app de notas.",
                              "tips": "Use setas na pirâmide para mostrar progressão.",
                              "learningObjective": "Distinguir dados raw de formas processadas, reforçando sua natureza básica.",
                              "commonMistakes": "Confundir dados com informação, dizendo que dados 'significam algo'."
                            },
                            {
                              "stepNumber": 4,
                              "title": "Articular e praticar a definição completa",
                              "subSteps": [
                                "Escreva uma definição pessoal em 1-2 parágrafos, incorporando todos os elementos chave.",
                                "Pratique verbalizando-a 3 vezes, gravando e ouvindo.",
                                "Crie flashcards: frente='O que são dados?', verso=sua definição.",
                                "Teste-se respondendo perguntas como 'Por que dados são raw?'.",
                                "Revise e refine com base em auto-avaliação."
                              ],
                              "verification": "Definição escrita, flashcards prontos e gravação de prática fluida.",
                              "estimatedTime": "25 minutos",
                              "materials": "Flashcards físicos ou app (ex: Anki), gravador.",
                              "tips": "Tempo-se: mire em 30 segundos para verbalizar perfeitamente.",
                              "learningObjective": "Articular precisamente a definição de dados de forma autônoma e confiante.",
                              "commonMistakes": "Ser vago; sempre inclua 'sem contexto ou significado inerente'."
                            }
                          ],
                          "practicalExample": "Uma planilha com colunas: 'ID: 001, Valor: 42, Cor: #FF0000'. Sem contexto, isso são dados raw – 42 poderia ser idade, temperatura ou código; #FF0000 é apenas um símbolo hexadecimal isolado.",
                          "finalVerifications": [
                            "Pode listar 5 exemplos de dados raw sem adicionar contexto?",
                            "Articula a definição incluindo 'fatos brutos, sem significado inerente'?",
                            "Diferencia corretamente dados de informação em um exemplo?",
                            "Explica por que dados são 'raw' usando a pirâmide DIK?",
                            "Responde fluentemente a perguntas sobre ausência de interpretação?",
                            "Cria um novo exemplo de dados isolados sob demanda?"
                          ],
                          "assessmentCriteria": [
                            "Precisão: Inclui todos elementos chave (raw, isolados, sem contexto/significado)?",
                            "Clareza: Definição é concisa e compreensível em linguagem simples?",
                            "Exemplos: Usa pelo menos 3 exemplos concretos e apropriados?",
                            "Diferenciação: Distingue claramente de informação/conhecimento?",
                            "Fluência: Articula verbalmente sem hesitação ou erros?",
                            "Profundidade: Destaca natureza 'raw' e ausência de interpretação?"
                          ],
                          "crossCurricularConnections": [
                            "Ciência: Observações experimentais raw (ex: medições numéricas).",
                            "Programação: Inputs de usuário como strings/números sem processamento.",
                            "Linguística: Símbolos fonéticos isolados sem gramática.",
                            "Estatística: Conjuntos de dados brutos antes da análise."
                          ],
                          "realWorldApplication": "Em análise de dados (ex: Google Analytics coleta cliques raw como 'user_id:123, action:click'), bancos de dados relacionais armazenam fatos isolados para queries posteriores, ou IoT sensors enviando leituras numéricas sem interpretação para processamento central."
                        },
                        "estimatedTime": "0.5 horas",
                        "difficulty": "beginner",
                        "status": "not_started",
                        "prerequisites": []
                      },
                      {
                        "id": "10.1.3.1.2",
                        "name": "Identificar características principais dos dados",
                        "description": "Reconhecer e listar as propriedades fundamentais dos dados, como falta de contexto, ausência de significado próprio, formato bruto (números, textos, símbolos) e distinção de elementos processados.",
                        "atomicExpansion": {
                          "steps": [
                            {
                              "stepNumber": 1,
                              "title": "Compreender a definição básica de dados",
                              "subSteps": [
                                "Leia a definição de dados como elementos brutos sem interpretação.",
                                "Anote exemplos simples de dados: números (42), textos ('João'), símbolos ('#').",
                                "Compare com objetos cotidianos que não têm significado inerente.",
                                "Discuta em grupo por que dados precisam de contexto para serem úteis.",
                                "Registre sua própria definição em uma frase curta."
                              ],
                              "verification": "Escreva uma definição pessoal de dados e liste 3 exemplos corretos.",
                              "estimatedTime": "20 minutos",
                              "materials": "Papel e caneta ou editor de texto simples; vídeo introdutório sobre ciência de dados (opcional).",
                              "tips": "Use analogias como 'ingredientes crus' para visualizar dados.",
                              "learningObjective": "Definir dados e reconhecer exemplos básicos.",
                              "commonMistakes": "Confundir dados com informação já processada, como 'temperatura média de 25°C'."
                            },
                            {
                              "stepNumber": 2,
                              "title": "Identificar as principais características dos dados",
                              "subSteps": [
                                "Liste as 4 propriedades fundamentais: falta de contexto, ausência de significado próprio, formato bruto, distinção de processados.",
                                "Para cada propriedade, crie um exemplo: '42' (número bruto sem contexto).",
                                "Classifique amostras: é dado ou informação? (ex: '101010' binário vs. 'erro no sistema').",
                                "Crie um fluxograma simples mostrando como dados se tornam informação.",
                                "Revise e refine sua lista com feedback de pares."
                              ],
                              "verification": "Crie uma tabela com 4 características e 2 exemplos cada.",
                              "estimatedTime": "30 minutos",
                              "materials": "Planilha ou tabela em papel; conjunto de 10 amostras de dados/informação.",
                              "tips": "Pergunte sempre: 'Isso tem significado sozinho ou precisa de explicação?'",
                              "learningObjective": "Listar e exemplificar propriedades fundamentais dos dados.",
                              "commonMistakes": "Atribuir significado inerente a dados brutos, ignorando a necessidade de contexto."
                            },
                            {
                              "stepNumber": 3,
                              "title": "Diferenciar dados de informação e conhecimento",
                              "subSteps": [
                                "Estude a hierarquia: Dados → Informação (com contexto) → Conhecimento (interpretação).",
                                "Analise exemplos: Dados ('25, 30, 28'), Informação ('temperaturas diárias'), Conhecimento ('verão quente').",
                                "Transforme dados brutos em informação adicionando contexto manualmente.",
                                "Identifique em textos reais onde dados são mal interpretados por falta de contexto.",
                                "Debata casos reais de 'dados vs. informação' em notícias falsas."
                              ],
                              "verification": "Transforme 5 conjuntos de dados brutos em informação e explique a distinção.",
                              "estimatedTime": "25 minutos",
                              "materials": "Exemplos impressos ou digitais de dados/informação; quadro branco para hierarquia.",
                              "tips": "Visualize como uma pirâmide: base ampla (dados), topo estreito (conhecimento).",
                              "learningObjective": "Distinguir dados de elementos processados.",
                              "commonMistakes": "Considerar qualquer número ou texto como 'informação' sem verificar contexto."
                            },
                            {
                              "stepNumber": 4,
                              "title": "Praticar identificação em conjuntos reais de dados",
                              "subSteps": [
                                "Selecione um dataset simples (ex: lista de vendas: 100, 200, 150).",
                                "Liste características principais aplicadas ao dataset.",
                                "Adicione contexto artificial para transformá-lo em informação.",
                                "Avalie se o dataset original tem significado próprio (resposta: não).",
                                "Crie um relatório curto resumindo as características identificadas."
                              ],
                              "verification": "Produza um relatório de 1 página listando características com evidências.",
                              "estimatedTime": "35 minutos",
                              "materials": "Dataset CSV simples (ex: temperaturas ou vendas); software como Excel ou Google Sheets.",
                              "tips": "Comece com datasets pequenos para evitar sobrecarga.",
                              "learningObjective": "Aplicar identificação de características em dados reais.",
                              "commonMistakes": "Ignorar formatos mistos (números + textos) como dados válidos."
                            }
                          ],
                          "practicalExample": "Dado um arquivo CSV com colunas: 'ID: 001, Valor: 42.5, Unidade: ?'. Identifique: formato bruto (números/textos), falta de contexto (o que é 42.5?), ausência de significado (não sabe se é temperatura, preço etc.), distinto de processados (não é 'preço médio de R$42.5'). Adicione contexto: 'Temperatura em °C do sensor 001' para torná-lo informação.",
                          "finalVerifications": [
                            "Lista corretamente pelo menos 4 características principais dos dados.",
                            "Diferencia dados brutos de informação em 5 exemplos.",
                            "Explica com analogia a falta de contexto em dados.",
                            "Transforma dados em informação adicionando contexto.",
                            "Identifica erros comuns em interpretações de dados reais.",
                            "Cria fluxograma da hierarquia dados-informação-conhecimento."
                          ],
                          "assessmentCriteria": [
                            "Precisão na listagem de propriedades fundamentais (80% correto).",
                            "Qualidade e relevância dos exemplos fornecidos.",
                            "Capacidade de distinção clara entre dados e elementos processados.",
                            "Profundidade na análise de contexto e significado.",
                            "Criatividade em analogias e aplicações práticas.",
                            "Completude do relatório ou tabela de verificação."
                          ],
                          "crossCurricularConnections": [
                            "Programação: Reconhecer variáveis raw em código antes de processamento.",
                            "Estatística: Identificar dados brutos antes de cálculos descritivos.",
                            "Língua Portuguesa: Analisar textos sem contexto vs. com interpretação.",
                            "Ciências: Observações de experimentos como dados brutos (ex: medidas sem hipótese)."
                          ],
                          "realWorldApplication": "Em análise de big data, cientistas de dados primeiro identificam características brutas para limpar e contextualizar, evitando erros em IA, como em sistemas de recomendação da Netflix que processam logs de visualização crus."
                        },
                        "estimatedTime": "0.5 horas",
                        "difficulty": "beginner",
                        "status": "not_started",
                        "prerequisites": []
                      },
                      {
                        "id": "10.1.3.1.3",
                        "name": "Fornecer exemplos de dados brutos",
                        "description": "Selecionar e exemplificar casos concretos de dados, como '25', 'azul', '2023-10-01' ou medições isoladas, explicando por que eles são dados sem contexto inerente.",
                        "atomicExpansion": {
                          "steps": [
                            {
                              "stepNumber": 1,
                              "title": "Compreender o conceito de dados brutos",
                              "subSteps": [
                                "Leia a definição de dados brutos: valores isolados sem contexto, como números, textos ou datas soltos.",
                                "Compare com informação (dados com contexto) e conhecimento (interpretação aplicada).",
                                "Anote as características principais: ausência de significado inerente, bruteza e isolamento.",
                                "Discuta com um parceiro ou em voz alta: 'Por que '25' sozinho não diz nada?'",
                                "Registre 3 diferenças entre dados, informação e conhecimento em um quadro."
                              ],
                              "verification": "Criar um resumo escrito com definição e 3 exemplos iniciais de dados brutos.",
                              "estimatedTime": "20 minutos",
                              "materials": "Papel, caneta ou editor de texto simples",
                              "tips": "Use analogias cotidianas, como 'um ingrediente solto sem receita'.",
                              "learningObjective": "Definir precisamente dados brutos e diferenciá-los de informação e conhecimento.",
                              "commonMistakes": "Confundir dados com informação, como achar que '25 anos' é dado bruto (tem contexto)."
                            },
                            {
                              "stepNumber": 2,
                              "title": "Identificar exemplos comuns de dados brutos",
                              "subSteps": [
                                "Liste categorias: números ('25'), textos ('azul'), datas ('2023-10-01'), símbolos ('#FF0000').",
                                "Colete 10 exemplos de fontes reais: relógios, etiquetas, sensores.",
                                "Classifique cada um: numérico, categórico, temporal.",
                                "Valide: cada exemplo deve ser isolado, sem frase ou explicação.",
                                "Crie uma tabela simples com coluna 'Exemplo' e 'Categoria'."
                              ],
                              "verification": "Apresentar uma lista de 10 exemplos classificados corretamente.",
                              "estimatedTime": "25 minutos",
                              "materials": "Planilha ou papel quadriculado, exemplos impressos de dados reais",
                              "tips": "Pense em telas de coleta de dados, como formulários vazios.",
                              "learningObjective": "Reconhecer e categorizar diversos tipos de dados brutos.",
                              "commonMistakes": "Incluir contextos acidentais, como '25 graus' em vez de '25'."
                            },
                            {
                              "stepNumber": 3,
                              "title": "Explicar a ausência de contexto nos exemplos",
                              "subSteps": [
                                "Para cada exemplo da lista anterior, escreva: 'Este é dado bruto porque...'.",
                                "Destaque: sem 'quem', 'o quê', 'onde' ou 'por quê'.",
                                "Compare: adicione contexto para transformar em informação (ex: '25' -> '25 anos de idade').",
                                "Registre em pares: exemplo bruto vs. com contexto.",
                                "Revise 5 exemplos com um checklist de ausência de contexto."
                              ],
                              "verification": "Produzir 5 pares de explicações (bruto vs. contextualizado).",
                              "estimatedTime": "30 minutos",
                              "materials": "Editor de texto ou caderno com colunas duplas",
                              "tips": "Pergunte sempre: 'Isso faz sentido sozinho?'",
                              "learningObjective": "Justificar por que exemplos são dados sem contexto inerente.",
                              "commonMistakes": "Dar explicações vagas, como 'é só um número', sem enfatizar isolamento."
                            },
                            {
                              "stepNumber": 4,
                              "title": "Praticar fornecendo e validando exemplos originais",
                              "subSteps": [
                                "Invente 5 exemplos novos de dados brutos do dia a dia (ex: medição de sensor '42.5').",
                                "Teste com colegas: peça para interpretarem sem contexto.",
                                "Ajuste exemplos falhos adicionando/removendo detalhes.",
                                "Compile uma apresentação final com 8-10 exemplos explicados.",
                                "Autoavalie usando critérios: isolado, realista, variado."
                              ],
                              "verification": "Demonstrar 5 exemplos originais em uma apresentação curta.",
                              "estimatedTime": "25 minutos",
                              "materials": "Apresentação digital ou quadro branco",
                              "tips": "Use fotos ou capturas de tela de dados reais para autenticidade.",
                              "learningObjective": "Gerar e validar exemplos próprios de dados brutos.",
                              "commonMistakes": "Criar exemplos com contexto implícito, como nomes próprios."
                            }
                          ],
                          "practicalExample": "Exemplo: 'azul' - É um dado bruto porque é apenas uma palavra isolada representando uma cor, sem indicar o objeto colorido (ex: carro azul) ou propósito. Sem contexto, não informa nada útil.",
                          "finalVerifications": [
                            "Lista pelo menos 8 exemplos variados de dados brutos corretos.",
                            "Explica ausência de contexto em todos os exemplos.",
                            "Diferencia dados brutos de informação em comparações paralelas.",
                            "Gera exemplos originais realistas e isolados.",
                            "Classifica exemplos por tipo (numérico, textual, etc.).",
                            "Valida exemplos com auto-checklist de isolamento."
                          ],
                          "assessmentCriteria": [
                            "Precisão na definição e exemplos (sem contextos acidentais).",
                            "Diversidade de tipos de dados (números, textos, datas).",
                            "Clareza nas explicações de ausência de contexto.",
                            "Criatividade em exemplos originais e práticos.",
                            "Completude das justificativas (todas as características cobertas).",
                            "Uso correto de categorias e comparações."
                          ],
                          "crossCurricularConnections": [
                            "Língua Portuguesa: redação clara de explicações e descrições.",
                            "Ciências: medições isoladas de experimentos (ex: temperatura bruta).",
                            "Informática: dados em planilhas ou bancos de dados iniciais.",
                            "Estatística: distinção entre dados crus e processados."
                          ],
                          "realWorldApplication": "Na coleta inicial de dados em pesquisas científicas, sensores IoT ou formulários online, onde valores brutos são armazenados antes de análise, como logs de temperatura '23.4' sem localização, permitindo posterior contextualização em dashboards empresariais."
                        },
                        "estimatedTime": "0.75 horas",
                        "difficulty": "beginner",
                        "status": "not_started",
                        "prerequisites": []
                      },
                      {
                        "id": "10.1.3.1.4",
                        "name": "Diferenciar dados de outros conceitos relacionados",
                        "description": "Comparar dados com informação e conhecimento, enfatizando que dados são o nível mais básico (sem processamento), enquanto informação surge com contexto adicionado.",
                        "atomicExpansion": {
                          "steps": [
                            {
                              "stepNumber": 1,
                              "title": "Definir o conceito de dados",
                              "subSteps": [
                                "Leia a definição padrão: dados são fatos brutos, sem contexto ou interpretação.",
                                "Identifique características: simbólicos, numéricos ou textuais, não processados.",
                                "Anote exemplos cotidianos: números aleatórios como '25, 30, 28'.",
                                "Crie uma lista de 5 itens que são puramente dados.",
                                "Compare com objetos não-dados, como opiniões."
                              ],
                              "verification": "Liste 5 exemplos de dados puros e explique por que não são informação.",
                              "estimatedTime": "20 minutos",
                              "materials": "Papel e caneta ou editor de texto; diagrama da pirâmide DIKW (Dados-Informação-Conhecimento-Sabedoria).",
                              "tips": "Foque no 'bruto': dados sozinhos não dizem nada sem contexto.",
                              "learningObjective": "Compreender dados como o nível mais básico e não interpretado.",
                              "commonMistakes": "Confundir dados com informação ao adicionar contexto prematuramente."
                            },
                            {
                              "stepNumber": 2,
                              "title": "Explorar o conceito de informação",
                              "subSteps": [
                                "Defina informação: dados com contexto adicionado, tornando-os significativos.",
                                "Transforme dados em informação: adicione rótulos, como '25°C, 30°C, 28°C em julho'.",
                                "Liste 3 diferenças chave entre dados e informação.",
                                "Crie um fluxograma simples: Dados → +Contexto → Informação.",
                                "Analise exemplos: um código postal sozinho (dado) vs. com endereço (informação)."
                              ],
                              "verification": "Converta 3 exemplos de dados em informação adicionando contexto.",
                              "estimatedTime": "25 minutos",
                              "materials": "Exemplos impressos de dados; software de diagramação como Draw.io.",
                              "tips": "Pergunte sempre: 'Isso tem significado sem explicação extra?'",
                              "learningObjective": "Distinguir como o contexto eleva dados a informação.",
                              "commonMistakes": "Ignorar que informação ainda não implica análise ou ação."
                            },
                            {
                              "stepNumber": 3,
                              "title": "Compreender o conceito de conhecimento",
                              "subSteps": [
                                "Defina conhecimento: informação processada, organizada e aplicada.",
                                "Exemplo: temperaturas → média de 27.7°C → tendência de aquecimento (conhecimento).",
                                "Identifique etapas: análise, padrões, inferências.",
                                "Crie uma tabela comparativa: Dados | Informação | Conhecimento.",
                                "Discuta limitações: conhecimento é subjetivo e depende de experiência."
                              ],
                              "verification": "Transforme informação em conhecimento com análise em 2 exemplos.",
                              "estimatedTime": "30 minutos",
                              "materials": "Planilha Excel ou Google Sheets para cálculos; tabela comparativa em branco.",
                              "tips": "Pense em 'por quê' e 'como usar': isso indica conhecimento.",
                              "learningObjective": "Reconhecer conhecimento como aplicação interpretativa de informação.",
                              "commonMistakes": "Equiparar informação a conhecimento sem processamento."
                            },
                            {
                              "stepNumber": 4,
                              "title": "Comparar e diferenciar os conceitos",
                              "subSteps": [
                                "Monte a pirâmide DIK: Dados (base) → Informação → Conhecimento (topo).",
                                "Crie 3 cenários mistos e classifique cada elemento.",
                                "Debata com parceiro: 'É dado ou informação?' em casos ambíguos.",
                                "Resuma diferenças em um mapa mental.",
                                "Teste si mesmo com quiz rápido de 10 itens."
                              ],
                              "verification": "Classifique corretamente 10 itens em uma mistura de dados, informação e conhecimento.",
                              "estimatedTime": "25 minutos",
                              "materials": "Mapa mental tool como MindMeister; quiz online criado por você.",
                              "tips": "Use a regra: sem contexto = dados; com contexto = informação; com análise = conhecimento.",
                              "learningObjective": "Aplicar diferenciação prática entre os três níveis.",
                              "commonMistakes": "Subestimar o papel do processamento humano no conhecimento."
                            }
                          ],
                          "practicalExample": "Dados: '101, 102, 99'. Informação: 'Salários em dólares: 101k, 102k, 99k para funcionários A, B, C'. Conhecimento: 'Média salarial de 100.7k indica equilíbrio, mas variação sugere necessidade de revisão de políticas'.",
                          "finalVerifications": [
                            "Pode definir dados sem mencionar contexto?",
                            "Transforma dados em informação corretamente em exemplos dados?",
                            "Identifica quando informação vira conhecimento via análise?",
                            "Classifica 90% dos itens em um quiz misto corretamente?",
                            "Explica a pirâmide DIK verbalmente?",
                            "Diferencia em cenários reais sem hesitação?"
                          ],
                          "assessmentCriteria": [
                            "Precisão nas definições (sem confusão entre níveis)",
                            "Uso correto de exemplos para ilustrar diferenças",
                            "Profundidade na comparação hierárquica",
                            "Capacidade de aplicar em novos contextos",
                            "Clareza na explicação oral ou escrita",
                            "Identificação de erros comuns nos colegas"
                          ],
                          "crossCurricularConnections": [
                            "Filosofia: Epistemologia e níveis de saber (Platão, pirâmide do conhecimento)",
                            "Informática: Processamento de dados em bancos de dados e SQL",
                            "Estatística: Análise descritiva elevando dados a conhecimento",
                            "Linguagem: Semântica e significado contextual em textos"
                          ],
                          "realWorldApplication": "Em ciência de dados, distingue dados brutos de Big Data (ex: sensores IoT geram dados; dashboards criam informação; previsões de IA geram conhecimento para decisões empresariais como otimização de supply chain)."
                        },
                        "estimatedTime": "0.5 horas",
                        "difficulty": "beginner",
                        "status": "not_started",
                        "prerequisites": []
                      }
                    ],
                    "individualConcepts": [
                      {
                        "id": "10.1.3.1.ic1",
                        "name": "Fatos brutos",
                        "description": "Dados como elementos crus e não processados, representando realidades observadas sem qualquer elaboração ou análise inicial.",
                        "specificSkills": [
                          "Identificar e coletar dados crus sem qualquer processamento ou interpretação inicial.",
                          "Reconhecer fatos observados como elementos básicos e não elaborados.",
                          "Diferenciar fatos brutos de informações processadas ou analisadas.",
                          "Registrar realidades observadas de forma objetiva e imparcial.",
                          "Compilar conjuntos iniciais de dados brutos para etapas subsequentes de análise."
                        ]
                      },
                      {
                        "id": "10.1.3.1.ic2",
                        "name": "Símbolos isolados",
                        "description": "Símbolos, números ou caracteres sem ligação ou estrutura, como '42' ou '#FF0000', existindo de forma independente.",
                        "specificSkills": [
                          "Identificar e classificar símbolos isolados, como números ('42') ou códigos hexadecimais ('#FF0000'), em sequências de caracteres.",
                          "Diferenciar símbolos isolados de elementos com ligação ou estrutura sintática.",
                          "Extrair e processar símbolos isolados em contextos de análise lexical ou parsing."
                        ]
                      },
                      {
                        "id": "10.1.3.1.ic3",
                        "name": "Observações sem contexto",
                        "description": "Registros diretos de eventos ou medidas isolados, sem informações sobre origem, propósito ou relações, como 'choveu' ou '25'.",
                        "specificSkills": [
                          "Identificar registros diretos de eventos ou medidas isolados em dados brutos.",
                          "Reconhecer a ausência de informações sobre origem, propósito ou relações em observações como 'choveu' ou '25'.",
                          "Diferenciar observações sem contexto de dados com informações contextuais adicionais.",
                          "Analisar limitações de observações isoladas para inferências ou análises subsequentes."
                        ]
                      },
                      {
                        "id": "10.1.3.1.ic4",
                        "name": "Ausência de significado inerente",
                        "description": "Falta de interpretação ou relevância própria nos dados, exigindo adição de contexto para gerar utilidade ou compreensão.",
                        "specificSkills": [
                          "Reconhecer que dados brutos carecem de significado intrínseco sem contexto adicional.",
                          "Adicionar metadados, anotações ou descrições contextuais para tornar dados interpretáveis.",
                          "Integrar conhecimento de domínio para conferir relevância e utilidade aos dados.",
                          "Avaliar a suficiência de contexto em conjuntos de dados para análise eficaz.",
                          "Desenvolver práticas de documentação que mitiguem a ausência de significado inerente nos dados."
                        ]
                      },
                      {
                        "id": "10.1.3.1.ic5",
                        "name": "Natureza raw",
                        "description": "Estado primitivo e não refinado dos dados, análogo a matérias-primas que necessitam de processamento para transformação em informação.",
                        "specificSkills": [
                          "Identificar o estado primitivo e não refinado dos dados em conjuntos brutos.",
                          "Reconhecer dados raw como análogos a matérias-primas que demandam processamento.",
                          "Diferenciar dados raw de dados processados ou refinados.",
                          "Explicar a transformação de dados raw em informação por meio de etapas de limpeza e processamento."
                        ]
                      }
                    ]
                  },
                  {
                    "id": "10.1.3.2",
                    "name": "Definição de Informação",
                    "description": "Processamento de dados que adiciona contexto, relevância e significado, transformando-os em algo útil.",
                    "individualConcepts": [
                      {
                        "id": "10.1.3.2.1",
                        "name": "Definição Fundamental de Informação",
                        "description": "Compreensão básica de informação como o resultado do processamento de dados brutos, adicionando valor para torná-los úteis.",
                        "specificSkills": [
                          {
                            "id": "10.1.3.2.1.1",
                            "name": "Explicar a definição de informação",
                            "description": "Articular claramente que informação é o processamento de dados que incorpora contexto, relevância e significado, transformando-os em algo utilizável na Ciência de Dados.",
                            "atomicExpansion": {
                              "steps": [
                                {
                                  "stepNumber": 1,
                                  "title": "Revisar o conceito fundamental de dados",
                                  "subSteps": [
                                    "Defina dados como fatos brutos, sem interpretação, como números, símbolos ou registros crus.",
                                    "Liste exemplos de dados: temperaturas registradas (ex: 25°C), cliques em um site (ex: 150).",
                                    "Discuta características dos dados: objetivos, não processados e sem contexto inerente.",
                                    "Crie uma tabela comparativa simples: dados vs não-dados.",
                                    "Explique por que dados sozinhos não são acionáveis."
                                  ],
                                  "verification": "Crie uma lista de 5 exemplos de dados e justifique por que eles são dados brutos.",
                                  "estimatedTime": "20 minutos",
                                  "materials": [
                                    "Papel e caneta ou editor de texto",
                                    "Exemplos de conjuntos de dados online (ex: Kaggle datasets)"
                                  ],
                                  "tips": "Use analogias cotidianas como 'ingredientes crus' para facilitar a compreensão.",
                                  "learningObjective": "Compreender dados como base não interpretada para informação.",
                                  "commonMistakes": "Confundir dados com opiniões ou análises já processadas."
                                },
                                {
                                  "stepNumber": 2,
                                  "title": "Explorar os elementos transformadores: contexto, relevância e significado",
                                  "subSteps": [
                                    "Defina contexto como o ambiente ou situação que dá sentido aos dados (ex: localização geográfica).",
                                    "Explique relevância como a importância dos dados para uma pergunta específica.",
                                    "Descreva significado como a interpretação que torna os dados úteis para decisões.",
                                    "Identifique como esses elementos são aplicados juntos no processamento.",
                                    "Crie diagramas visuais mostrando dados + elementos = informação."
                                  ],
                                  "verification": "Desenhe um fluxograma ilustrando a transformação de dados em informação.",
                                  "estimatedTime": "25 minutos",
                                  "materials": [
                                    "Ferramentas de desenho como Draw.io ou papel",
                                    "Vídeos curtos sobre DIKW pyramid (Data-Information-Knowledge-Wisdom)"
                                  ],
                                  "tips": "Pense em camadas: dados são o 'o quê', elementos transformadores são o 'por quê e como'.",
                                  "learningObjective": "Identificar os componentes chave que elevam dados a informação.",
                                  "commonMistakes": "Ignorar um dos três elementos, tratando-os como intercambiáveis."
                                },
                                {
                                  "stepNumber": 3,
                                  "title": "Articular a definição completa de informação no contexto de Ciência de Dados",
                                  "subSteps": [
                                    "Formule a definição: 'Informação é dados processados com contexto, relevância e significado, tornando-os utilizáveis.'",
                                    "Adapte ao Data Science: limpeza, análise e visualização transformam dados brutos em insights acionáveis.",
                                    "Pratique verbalizando a definição em voz alta, gravando se possível.",
                                    "Compare com definições semelhantes (ex: pirâmide DIKW).",
                                    "Escreva a definição em suas próprias palavras."
                                  ],
                                  "verification": "Escreva e recite a definição sem consultar notas.",
                                  "estimatedTime": "20 minutos",
                                  "materials": [
                                    "Gravador de áudio ou celular",
                                    "Artigo sobre pirâmide DIKW"
                                  ],
                                  "tips": "Mantenha simples e memorável: use acrônimo CRS (Contexto, Relevância, Significado).",
                                  "learningObjective": "Formular uma explicação clara e precisa da definição.",
                                  "commonMistakes": "Usar jargões excessivos sem explicar termos básicos."
                                },
                                {
                                  "stepNumber": 4,
                                  "title": "Praticar explicação com exemplos e diferenças",
                                  "subSteps": [
                                    "Escolha um dataset real e mostre dados vs informação (ex: vendas brutas vs tendências de mercado).",
                                    "Explique a transformação passo a passo para um público leigo.",
                                    "Crie um script de 1 minuto explicando a definição.",
                                    "Receba feedback simulando uma apresentação.",
                                    "Refine com base em erros identificados."
                                  ],
                                  "verification": "Apresente a explicação para um par ou grave e autoavalie clareza.",
                                  "estimatedTime": "30 minutos",
                                  "materials": [
                                    "Dataset simples (ex: CSV de vendas)",
                                    "Timer para prática de fala"
                                  ],
                                  "tips": "Comece com 'Dados são como ingredientes; informação é a receita pronta'.",
                                  "learningObjective": "Aplicar a definição de forma comunicável e contextualizada.",
                                  "commonMistakes": "Pular exemplos concretos, deixando a explicação abstrata."
                                }
                              ],
                              "practicalExample": "Dados: Lista de temperaturas diárias (20°C, 25°C, 18°C). Informação: 'A temperatura média subiu 2°C esta semana devido a uma frente quente, indicando necessidade de ar-condicionado em data centers' – adicionando contexto (semana específica), relevância (impacto em TI) e significado (ação recomendada).",
                              "finalVerifications": [
                                "Pode explicar a definição em menos de 1 minuto sem notas?",
                                "Diferencia corretamente dados de informação com 3 exemplos?",
                                "Identifica contexto, relevância e significado em um dataset dado?",
                                "Aplica a definição a um cenário de Data Science?",
                                "Explica a transformação sem jargões desnecessários?",
                                "Usa analogias para tornar acessível a não-especialistas?"
                              ],
                              "assessmentCriteria": [
                                "Clareza e precisão na articulação da definição (incorpora processamento, contexto, relevância, significado).",
                                "Uso de exemplos concretos para ilustrar transformação de dados em informação.",
                                "Adequação ao contexto de Ciência de Dados (utilizável para análises).",
                                "Estrutura lógica na explicação (definição → elementos → aplicação).",
                                "Comunicação eficaz (linguagem simples, sem ambiguidades).",
                                "Profundidade: conecta a conceitos maiores como DIKW."
                              ],
                              "crossCurricularConnections": [
                                "Filosofia: Pirâmide do Conhecimento (DIKW).",
                                "Comunicação: Técnicas de explicação clara e storytelling.",
                                "Estatística: Processamento inicial de dados em análise.",
                                "Negócios: Transformação de dados em decisões estratégicas."
                              ],
                              "realWorldApplication": "Em Data Science, analistas pegam dados brutos de sensores IoT (ex: tráfego de rede), adicionam contexto (horários de pico), relevância (para otimização de servidores) e significado (previsão de falhas), criando dashboards que previnem downtime em empresas como Google ou AWS."
                            },
                            "estimatedTime": "0.5 horas",
                            "difficulty": "beginner",
                            "status": "not_started",
                            "prerequisites": []
                          },
                          {
                            "id": "10.1.3.2.1.2",
                            "name": "Diferenciar dados de informação",
                            "description": "Identificar e contrastar dados (brutos e sem estrutura) com informação (dados processados com significado adicionado), usando exemplos simples.",
                            "atomicExpansion": {
                              "steps": [
                                {
                                  "stepNumber": 1,
                                  "title": "Entender o conceito de dados",
                                  "subSteps": [
                                    "Leia a definição: Dados são fatos brutos, sem estrutura ou contexto, como números, símbolos ou observações isoladas.",
                                    "Colete exemplos cotidianos de dados, como '25', 'azul', '2023-10-01'.",
                                    "Registre 5 exemplos de dados de diferentes fontes (ex.: temperatura, cor, data).",
                                    "Discuta por que esses itens são 'brutos' e não interpretados.",
                                    "Crie uma lista pessoal de dados observados no seu dia a dia."
                                  ],
                                  "verification": "Lista de pelo menos 5 exemplos de dados brutos criados e explicados corretamente.",
                                  "estimatedTime": "20 minutos",
                                  "materials": [
                                    "Papel e caneta ou editor de texto",
                                    "Exemplos online de dados brutos (opcional)"
                                  ],
                                  "tips": "Foquem em itens isolados sem análise; evite adicionar significado prematuramente.",
                                  "learningObjective": "Definir dados como elementos brutos e identificar exemplos precisos.",
                                  "commonMistakes": [
                                    "Confundir dados com descrições completas",
                                    "Adicionar contexto desnecessário aos exemplos"
                                  ]
                                },
                                {
                                  "stepNumber": 2,
                                  "title": "Entender o conceito de informação",
                                  "subSteps": [
                                    "Leia a definição: Informação é dados processados com contexto, análise ou significado adicionado.",
                                    "Transforme exemplos de dados em informação, ex.: '25°C em São Paulo hoje' a partir de '25'.",
                                    "Identifique o 'processamento' aplicado: contexto, comparação, categorização.",
                                    "Crie 5 exemplos de informação derivados de dados brutos.",
                                    "Explique o valor adicionado em cada transformação."
                                  ],
                                  "verification": "5 exemplos de informação criados, mostrando transformação clara de dados.",
                                  "estimatedTime": "25 minutos",
                                  "materials": [
                                    "Lista de dados do Step 1",
                                    "Dicionário ou glossário online de termos de dados"
                                  ],
                                  "tips": "Sempre pergunte: 'O que torna isso útil ou interpretável?'",
                                  "learningObjective": "Reconhecer informação como dados enriquecidos com significado.",
                                  "commonMistakes": [
                                    "Tratar descrições simples como informação sem processamento",
                                    "Ignorar o papel do contexto"
                                  ]
                                },
                                {
                                  "stepNumber": 3,
                                  "title": "Identificar diferenças chave entre dados e informação",
                                  "subSteps": [
                                    "Crie uma tabela comparativa: colunas para 'Dados' e 'Informação'; linhas para características (estrutura, significado, uso, exemplos).",
                                    "Preencha com pelo menos 5 diferenças (ex.: bruto vs. processado, sem sentido vs. com sentido).",
                                    "Use setas ou fluxogramas para mostrar como dados viram informação.",
                                    "Debata com um parceiro ou anote contra-argumentos potenciais.",
                                    "Resuma as 3 diferenças mais importantes em uma frase cada."
                                  ],
                                  "verification": "Tabela ou fluxograma completo com diferenças claras e exemplos.",
                                  "estimatedTime": "30 minutos",
                                  "materials": [
                                    "Ferramenta de tabela como Google Sheets ou papel",
                                    "Exemplos dos steps anteriores"
                                  ],
                                  "tips": "Visualize o processo: Dados → Processamento → Informação.",
                                  "learningObjective": "Contrastar características fundamentais de dados vs. informação.",
                                  "commonMistakes": [
                                    "Listar sem comparar diretamente",
                                    "Confundir com conhecimento (nível superior)"
                                  ]
                                },
                                {
                                  "stepNumber": 4,
                                  "title": "Praticar diferenciação com exemplos mistos",
                                  "subSteps": [
                                    "Receba ou crie uma lista de 10 itens mistos (dados e informação).",
                                    "Classifique cada um como 'Dado' ou 'Informação' e justifique.",
                                    "Corrija classificações erradas e reescreva justificativas.",
                                    "Crie 3 cenários reais onde confundir os dois causa erro.",
                                    "Teste-se com um quiz autoavaliado de 10 itens."
                                  ],
                                  "verification": "Classificação precisa de 90% dos itens mistos com justificativas.",
                                  "estimatedTime": "25 minutos",
                                  "materials": [
                                    "Lista de itens mistos (ex.: '42', '42 anos é a média de idade')",
                                    "Quiz online gerado"
                                  ],
                                  "tips": "Pergunte: 'Precisa de contexto externo para entender?' Se sim, é dado.",
                                  "learningObjective": "Aplicar diferenciação em contextos variados com precisão.",
                                  "commonMistakes": [
                                    "Classificar tudo como informação por viés de familiaridade",
                                    "Ignorar sutilezas no processamento"
                                  ]
                                }
                              ],
                              "practicalExample": "Dados: Lista de temperaturas diárias [22, 25, 20, 28]. Informação: 'A temperatura média da semana foi 23.75°C, com pico de 28°C na quinta-feira, indicando tendência de aquecimento.'",
                              "finalVerifications": [
                                "Define corretamente dados como brutos e informação como processada.",
                                "Fornece exemplos precisos e transforma dados em informação.",
                                "Cria tabela comparativa com pelo menos 5 diferenças chave.",
                                "Classifica 9/10 itens mistos corretamente em prática.",
                                "Explica impacto de confundir os conceitos em cenários reais.",
                                "Resume diferenças em frases concisas."
                              ],
                              "assessmentCriteria": [
                                "Precisão conceitual (definições exatas: 30%)",
                                "Qualidade de exemplos e transformações (25%)",
                                "Profundidade da comparação (tabela/fluxo: 20%)",
                                "Desempenho em classificação prática (90%+: 15%)",
                                "Criatividade em aplicações e verificações (10%)"
                              ],
                              "crossCurricularConnections": [
                                "Ciência da Computação: Bancos de dados vs. relatórios analíticos.",
                                "Estatística: Dados crus vs. estatísticas descritivas.",
                                "Linguagem: Palavras isoladas (dados) vs. frases com significado (informação).",
                                "Negócios: Vendas brutas (dados) vs. análise de tendências (informação)."
                              ],
                              "realWorldApplication": "Em ciência de dados, analistas transformam logs de usuários (dados) em insights de comportamento (informação) para melhorar apps; em jornalismo, fatos brutos viram reportagens contextualizadas."
                            },
                            "estimatedTime": "0.5 horas",
                            "difficulty": "beginner",
                            "status": "not_started",
                            "prerequisites": []
                          },
                          {
                            "id": "10.1.3.2.1.3",
                            "name": "Reconhecer a importância da informação na Ciência de Dados",
                            "description": "Explicar como a informação serve de base para análises exploratórias e modelagem em projetos de Ciência de Dados.",
                            "atomicExpansion": {
                              "steps": [
                                {
                                  "stepNumber": 1,
                                  "title": "Diferenciar Dados, Informação e Conhecimento na Ciência de Dados",
                                  "subSteps": [
                                    "Defina dados como fatos brutos e não processados (ex: números de vendas diárias).",
                                    "Explique informação como dados processados com contexto (ex: vendas médias por região).",
                                    "Descreva conhecimento como informação aplicada para insights acionáveis (ex: estratégias de marketing baseadas em tendências).",
                                    "Crie um fluxograma ilustrando a transformação: Dados → Informação → Conhecimento.",
                                    "Compare exemplos reais de cada nível em um dataset simples."
                                  ],
                                  "verification": "Criar um diagrama ou tabela comparativa que corretamente diferencia os três conceitos com exemplos precisos.",
                                  "estimatedTime": "45 minutos",
                                  "materials": "Papel e caneta ou ferramenta digital como Draw.io; vídeo introdutório sobre pirâmide DIKW (Dados-Informação-Conhecimento-Sabedoria).",
                                  "tips": "Use analogias cotidianas, como ingredientes (dados), receita (informação) e prato cozido (conhecimento).",
                                  "learningObjective": "Compreender a hierarquia conceitual onde informação emerge de dados processados.",
                                  "commonMistakes": "Confundir informação com dados brutos ou superestimar o papel isolado de cada um sem contexto."
                                },
                                {
                                  "stepNumber": 2,
                                  "title": "Explorar o Papel da Informação em Análises Exploratórias (EDA)",
                                  "subSteps": [
                                    "Revise o que é EDA: processo inicial de visualização e sumarização de dados.",
                                    "Identifique como informação é gerada via estatísticas descritivas (média, mediana, desvios).",
                                    "Analise um dataset exemplo: calcule métricas e interprete padrões emergentes.",
                                    "Discuta como essa informação revela anomalias, correlações e distribuições.",
                                    "Documente insights derivados, como 'vendas caem em fins de semana'."
                                  ],
                                  "verification": "Produzir um relatório curto de EDA com pelo menos 3 insights informacionais de um dataset fornecido.",
                                  "estimatedTime": "60 minutos",
                                  "materials": "Python com Pandas e Matplotlib ou Excel; dataset público como Iris ou Titanic do Kaggle.",
                                  "tips": "Sempre pergunte 'o que isso significa no contexto do problema?' para transformar dados em informação.",
                                  "learningObjective": "Reconhecer como análises exploratórias convertem dados em informação acionável.",
                                  "commonMistakes": "Focar apenas em gráficos sem interpretação, ignorando o contexto que gera informação."
                                },
                                {
                                  "stepNumber": 3,
                                  "title": "Analisar o Papel da Informação na Modelagem de Ciência de Dados",
                                  "subSteps": [
                                    "Explique modelagem como construção de algoritmos preditivos baseados em padrões.",
                                    "Descreva como informação de EDA alimenta feature engineering e seleção de variáveis.",
                                    "Simule um pipeline: use insights informacionais para treinar um modelo simples (ex: regressão linear).",
                                    "Avalie como falta de informação rica leva a modelos enviesados ou ineficazes.",
                                    "Compare métricas de modelo com e sem processamento informativo."
                                  ],
                                  "verification": "Construir e avaliar um modelo básico onde insights informacionais justificam escolhas de features.",
                                  "estimatedTime": "75 minutos",
                                  "materials": "Jupyter Notebook com Scikit-learn; mesmo dataset da EDA.",
                                  "tips": "Priorize features baseadas em informação de EDA para evitar overfitting.",
                                  "learningObjective": "Entender como informação é o alicerce para modelagem robusta e precisa.",
                                  "commonMistakes": "Pular EDA e modelar diretamente dados brutos, resultando em baixa performance."
                                },
                                {
                                  "stepNumber": 4,
                                  "title": "Sintetizar a Importância Geral da Informação em Projetos de Ciência de Dados",
                                  "subSteps": [
                                    "Resuma como informação conecta EDA e modelagem em um ciclo completo de projetos.",
                                    "Discuta impactos: eficiência, precisão e valor de negócio.",
                                    "Crie um mapa mental ligando informação a etapas de um projeto end-to-end.",
                                    "Debata cenários onde ausência de informação falha (ex: decisões erradas em saúde).",
                                    "Formule uma declaração pessoal sobre por que informação é crucial."
                                  ],
                                  "verification": "Elaborar um ensaio ou apresentação de 1 página destacando a centralidade da informação com exemplos.",
                                  "estimatedTime": "45 minutos",
                                  "materials": "Ferramenta de mind mapping como MindMeister; anotações dos steps anteriores.",
                                  "tips": "Pense em termos de ROI: informação boa acelera projetos e reduz erros caros.",
                                  "learningObjective": "Internalizar a importância estratégica da informação como base de todo projeto de CD.",
                                  "commonMistakes": "Subestimar informação como 'apenas intermediária', ignorando seu papel pivotal."
                                }
                              ],
                              "practicalExample": "Em um projeto de previsão de churn de clientes de uma empresa de telecom, dados brutos de uso são processados em informação (taxas de churn por segmento demográfico via EDA), que informa a modelagem de um classificador, prevendo saídas com 85% de acurácia e salvando milhões em retenção.",
                              "finalVerifications": [
                                "Explica corretamente a transformação de dados em informação com exemplos contextualizados.",
                                "Identifica pelo menos 3 usos específicos de informação em EDA e modelagem.",
                                "Demonstra com um exemplo prático como falta de informação impacta projetos.",
                                "Cria um fluxograma ou mapa mostrando o papel central da informação.",
                                "Formula uma declaração clara sobre sua importância estratégica.",
                                "Avalia um modelo simples destacando contribuições informacionais."
                              ],
                              "assessmentCriteria": [
                                "Precisão conceitual: definições e diferenciações corretas (30%).",
                                "Profundidade de análise: conexões claras entre informação, EDA e modelagem (25%).",
                                "Exemplos práticos: relevância e concretude dos casos ilustrados (20%).",
                                "Clareza e estrutura: organização lógica em steps e substeps (15%).",
                                "Criatividade em verificações: verificações acionáveis e mensuráveis (10%)."
                              ],
                              "crossCurricularConnections": [
                                "Estatística: uso de métricas descritivas para gerar informação.",
                                "Programação: implementação de EDA e modelagem em Python/R.",
                                "Gestão de Negócios: aplicação de insights informacionais em decisões estratégicas.",
                                "Ética em TI: importância de informação precisa para evitar vieses em modelos."
                              ],
                              "realWorldApplication": "Na saúde, informação derivada de dados de pacientes (ex: padrões de sintomas via EDA) fundamenta modelos de ML para diagnóstico precoce de doenças, melhorando taxas de sobrevivência em hospitais como o Mayo Clinic."
                            },
                            "estimatedTime": "0.25 horas",
                            "difficulty": "beginner",
                            "status": "not_started",
                            "prerequisites": []
                          }
                        ]
                      },
                      {
                        "id": "10.1.3.2.2",
                        "name": "Elementos Chave da Informação",
                        "description": "Análise dos componentes essenciais que elevam dados a informação: contexto, relevância e significado.",
                        "specificSkills": [
                          {
                            "id": "10.1.3.2.2.1",
                            "name": "Descrever o papel do contexto na informação",
                            "description": "Explicar como o contexto fornece o ambiente ou cenário necessário para interpretar dados, tornando-os compreensíveis.",
                            "atomicExpansion": {
                              "steps": [
                                {
                                  "stepNumber": 1,
                                  "title": "Definir o conceito de contexto na informação",
                                  "subSteps": [
                                    "Pesquisar definições básicas de dados, informação e conhecimento.",
                                    "Identificar o contexto como o 'ambiente' que dá significado aos dados.",
                                    "Explicar que dados isolados são ambíguos sem contexto.",
                                    "Diferenciar contexto interno (fonte dos dados) e externo (cenário de uso).",
                                    "Anotar exemplos iniciais de dados sem contexto."
                                  ],
                                  "verification": "Criar um mapa mental ligando dados, contexto e informação.",
                                  "estimatedTime": "20 minutos",
                                  "materials": [
                                    "Dicionário ou glossário de ciência de dados",
                                    "Papel e caneta ou ferramenta digital como MindMeister"
                                  ],
                                  "tips": "Use analogias cotidianas, como 'número 30 sozinho vs. 30 graus Celsius'.",
                                  "learningObjective": "Compreender o contexto como elemento essencial para transformar dados em informação compreensível.",
                                  "commonMistakes": [
                                    "Confundir contexto com os dados em si",
                                    "Ignorar o contexto cultural ou temporal"
                                  ]
                                },
                                {
                                  "stepNumber": 2,
                                  "title": "Analisar exemplos práticos de contexto ausente e presente",
                                  "subSteps": [
                                    "Selecionar 3 conjuntos de dados ambíguos (ex: '100', 'azul', 'alta').",
                                    "Adicionar contexto a cada um e observar a mudança de significado.",
                                    "Discutir em grupo ou por escrito como o contexto resolve ambiguidades.",
                                    "Registrar variações de interpretação sem contexto.",
                                    "Comparar com informação clara quando contexto é fornecido."
                                  ],
                                  "verification": "Produzir uma tabela comparativa antes/depois do contexto.",
                                  "estimatedTime": "30 minutos",
                                  "materials": [
                                    "Planilha Excel ou Google Sheets",
                                    "Exemplos impressos de dados ambíguos"
                                  ],
                                  "tips": "Escolha dados de fontes reais como notícias ou relatórios para maior relevância.",
                                  "learningObjective": "Demonstrar como o contexto fornece o cenário necessário para interpretação correta.",
                                  "commonMistakes": [
                                    "Usar contexto irrelevante",
                                    "Superestimar o papel dos dados sozinhos"
                                  ]
                                },
                                {
                                  "stepNumber": 3,
                                  "title": "Identificar elementos chave do contexto",
                                  "subSteps": [
                                    "Listar componentes do contexto: quem, quando, onde, por quê, como.",
                                    "Aplicar esses elementos a um dataset específico.",
                                    "Criar uma checklist para avaliar se um dado tem contexto suficiente.",
                                    "Analisar um caso real onde falta de contexto levou a erro.",
                                    "Refinar a descrição do papel do contexto com base na análise."
                                  ],
                                  "verification": "Desenvolver e testar uma checklist em 2 exemplos.",
                                  "estimatedTime": "25 minutos",
                                  "materials": [
                                    "Artigo de notícia com dados",
                                    "Checklist template em documento"
                                  ],
                                  "tips": "Pergunte sempre '5 Ws' (Who, What, When, Where, Why) para enriquecer o contexto.",
                                  "learningObjective": "Mapear os componentes do contexto que tornam dados compreensíveis.",
                                  "commonMistakes": [
                                    "Listar elementos genéricos sem ligação ao dado",
                                    "Omitir contexto dinâmico (mudanças ao longo do tempo)"
                                  ]
                                },
                                {
                                  "stepNumber": 4,
                                  "title": "Praticar a descrição do papel do contexto",
                                  "subSteps": [
                                    "Escolher um cenário da ciência de dados e descrever o contexto necessário.",
                                    "Escrever um parágrafo explicando o impacto do contexto na interpretação.",
                                    "Revisar e melhorar a descrição para clareza e precisão.",
                                    "Apresentar ou compartilhar a descrição para feedback.",
                                    "Sintetizar lições aprendidas em uma definição pessoal."
                                  ],
                                  "verification": "Produzir uma descrição escrita de 200 palavras com exemplos.",
                                  "estimatedTime": "35 minutos",
                                  "materials": [
                                    "Editor de texto",
                                    "Exemplos de cenários da MC-13"
                                  ],
                                  "learningObjective": "Articular verbalmente ou por escrito o papel fundamental do contexto na informação.",
                                  "commonMistakes": [
                                    "Descrições vagas sem exemplos",
                                    "Focar só em benefícios sem mencionar riscos da ausência"
                                  ],
                                  "tips": [
                                    "Escolha cenários da MC-13 que você conheça bem para enriquecer a descrição.",
                                    "Inclua exemplos concretos de como o contexto altera a interpretação dos dados.",
                                    "Mencione explicitamente riscos, como decisões erradas sem contexto.",
                                    "Estruture o parágrafo: introdução ao cenário, papel do contexto, impacto e lições.",
                                    "Leia em voz alta na revisão para detectar ambiguidades."
                                  ]
                                }
                              ],
                              "practicalExample": "Considere o dado '50%'. Sem contexto: pode ser taxa de aprovação, umidade ou desconto. Com contexto (ex: '50% de um teste de Matemática Computacional em 2023'): torna-se informação clara sobre desempenho estudantil, permitindo análise precisa.",
                              "finalVerifications": [
                                "Explicar corretamente o contexto como 'cenário interpretativo' em 3 exemplos diferentes.",
                                "Identificar ambiguidades em dados sem contexto em um novo conjunto.",
                                "Aplicar checklist de elementos contextuais a um caso real.",
                                "Descrever riscos de interpretação errônea sem contexto.",
                                "Produzir uma definição própria alinhada à descrição oficial.",
                                "Discutir em grupo o impacto em ciência de dados."
                              ],
                              "assessmentCriteria": [
                                "Clareza na distinção entre dados e informação contextualizada (30%)",
                                "Precisão na identificação de elementos do contexto (25%)",
                                "Uso de exemplos relevantes e variados (20%)",
                                "Profundidade na análise de ambiguidades resolvidas (15%)",
                                "Estrutura lógica e linguagem acessível (10%)"
                              ],
                              "crossCurricularConnections": [
                                "Linguística: Análise de ambiguidade semântica em textos.",
                                "História: Interpretação de fontes históricas com contexto temporal.",
                                "Ciências: Experimentos onde condições ambientais são o contexto.",
                                "Comunicação: Jornalismo de dados e verificação factual."
                              ],
                              "realWorldApplication": "Em ciência de dados, analistas usam contexto para evitar erros em dashboards (ex: vendas 'alta' sem especificar região/tempo leva a decisões erradas); em IA, modelos precisam de contexto para previsões precisas, como em chatbots interpretando consultas ambíguas."
                            },
                            "estimatedTime": "0.5 horas",
                            "difficulty": "beginner",
                            "status": "not_started",
                            "prerequisites": []
                          },
                          {
                            "id": "10.1.3.2.2.2",
                            "name": "Identificar a relevância na formação de informação",
                            "description": "Compreender como a relevância filtra dados para focar no que é pertinente a um objetivo específico, evitando sobrecarga informacional.",
                            "atomicExpansion": {
                              "steps": [
                                {
                                  "stepNumber": 1,
                                  "title": "Compreender o conceito de relevância na formação de informação",
                                  "subSteps": [
                                    "Defina relevância como a qualidade de dados que se conectam diretamente a um objetivo específico.",
                                    "Analise exemplos de sobrecarga informacional, como feeds de redes sociais cheios de conteúdo aleatório.",
                                    "Diferencie relevância de volume ou novidade de dados.",
                                    "Relacione relevância ao processo de transformação de dados em informação útil.",
                                    "Discuta o impacto da falta de relevância na tomada de decisões."
                                  ],
                                  "verification": "Escreva uma definição pessoal de relevância e cite um exemplo de sobrecarga informacional.",
                                  "estimatedTime": "20 minutos",
                                  "materials": [
                                    "Texto do módulo sobre Definição de Informação",
                                    "Vídeo introdutório sobre ciência de dados (5 min)",
                                    "Papel e caneta para anotações"
                                  ],
                                  "tips": "Use analogias cotidianas, como filtrar e-mails importantes em uma inbox lotada.",
                                  "learningObjective": "Dominar a definição e importância da relevância para filtrar dados.",
                                  "commonMistakes": [
                                    "Confundir relevância com popularidade ou quantidade de dados.",
                                    "Ignorar o papel do contexto objetivo."
                                  ]
                                },
                                {
                                  "stepNumber": 2,
                                  "title": "Definir o objetivo e estabelecer critérios de relevância",
                                  "subSteps": [
                                    "Especifique um objetivo claro e mensurável para a análise de dados.",
                                    "Crie critérios de relevância baseados no objetivo (ex.: temporalidade, geografia, tipo de dado).",
                                    "Liste 3-5 critérios prioritários, como 'dados recentes' ou 'fontes confiáveis'.",
                                    "Valide os critérios com uma pergunta: 'Isso ajuda a responder ao objetivo?'",
                                    "Ajuste critérios com base em feedback inicial."
                                  ],
                                  "verification": "Apresente o objetivo e lista de critérios para um par revisar.",
                                  "estimatedTime": "25 minutos",
                                  "materials": [
                                    "Modelo de objetivo SMART",
                                    "Planilha para listar critérios",
                                    "Exemplos de objetivos do contexto MC-13"
                                  ],
                                  "tips": "Torne critérios específicos e quantificáveis para evitar subjetividade.",
                                  "learningObjective": "Aprender a criar critérios personalizados para filtragem de relevância.",
                                  "commonMistakes": [
                                    "Definir objetivos vagos como 'saber mais sobre X'.",
                                    "Criar critérios excessivamente amplos."
                                  ]
                                },
                                {
                                  "stepNumber": 3,
                                  "title": "Avaliar e filtrar dados disponíveis",
                                  "subSteps": [
                                    "Colete um conjunto inicial de dados brutos relacionados ao objetivo.",
                                    "Aplique cada critério de relevância a cada item de dado individualmente.",
                                    "Classifique dados em: relevante, parcialmente relevante, irrelevante.",
                                    "Registre justificativas para cada classificação.",
                                    "Elimine ou priorize dados com base na classificação."
                                  ],
                                  "verification": "Crie uma tabela de classificação com pelo menos 10 itens de dados e justifique seleções.",
                                  "estimatedTime": "30 minutos",
                                  "materials": [
                                    "Conjunto de dados de exemplo (artigos, estatísticas)",
                                    "Ferramenta de planilha (Google Sheets)",
                                    "Lista de critérios do passo 2"
                                  ],
                                  "tips": "Use cores na planilha para visualização rápida: verde para relevante, vermelho para irrelevante.",
                                  "learningObjective": "Praticar a aplicação prática de critérios para filtrar dados.",
                                  "commonMistakes": [
                                    "Manter dados 'por precaução' sem justificativa.",
                                    "Aplicar critérios de forma inconsistente."
                                  ]
                                },
                                {
                                  "stepNumber": 4,
                                  "title": "Validar a seleção e integrar à formação de informação",
                                  "subSteps": [
                                    "Revise a seleção final para garantir alinhamento total com o objetivo.",
                                    "Teste removendo itens selecionados e verificando se o objetivo ainda é atendido.",
                                    "Integre dados relevantes em uma narrativa ou resumo informativo.",
                                    "Identifique gaps e planeje buscas adicionais se necessário.",
                                    "Documente o processo de filtragem para reproducibilidade."
                                  ],
                                  "verification": "Produza um resumo informativo curto (200 palavras) usando apenas dados relevantes.",
                                  "estimatedTime": "25 minutos",
                                  "materials": [
                                    "Dados filtrados do passo 3",
                                    "Template de resumo informativo",
                                    "Checklist de validação"
                                  ],
                                  "tips": "Pergunte: 'Se eu apresentasse isso a um especialista, faria sentido?'",
                                  "learningObjective": "Consolidar o filtro de relevância em informação acionável.",
                                  "commonMistakes": [
                                    "Sobrecarregar o resumo com detalhes extras.",
                                    "Pular a validação de gaps."
                                  ]
                                }
                              ],
                              "practicalExample": "Ao preparar um relatório sobre 'impacto da pandemia na educação brasileira em 2023', filtre dados: ignore estatísticas globais de 2020 (irrelevantes por temporalidade e geografia), priorize relatórios do INEP e MEC recentes (relevantes por fonte e foco).",
                              "finalVerifications": [
                                "Explica com precisão como relevância evita sobrecarga informacional.",
                                "Aplica critérios corretamente a um conjunto de dados de teste.",
                                "Produz um resumo focado sem elementos irrelevantes.",
                                "Identifica e justifica exclusão de dados em exemplos práticos.",
                                "Demonstra compreensão do processo em uma discussão ou quiz.",
                                "Relaciona relevância à formação de informação no contexto de ciência de dados."
                              ],
                              "assessmentCriteria": [
                                "Precisão na definição e aplicação de relevância (30%)",
                                "Qualidade e especificidade dos critérios criados (25%)",
                                "Efetividade do filtro em exemplos práticos (20%)",
                                "Documentação clara de justificativas (15%)",
                                "Integração coerente em informação final (10%)"
                              ],
                              "crossCurricularConnections": [
                                "Pesquisa Qualitativa em Ciências Humanas: Seleção de fontes bibliográficas relevantes.",
                                "Estatística Descritiva: Filtragem de outliers irrelevantes em datasets.",
                                "Jornalismo Investigativo: Priorização de fatos pertinentes a uma pauta.",
                                "Gestão de Projetos: Identificação de requisitos chave em requisitos de dados."
                              ],
                              "realWorldApplication": "Analistas de dados em empresas como Google ou bancos usam relevância para criar relatórios executivos focados, evitando que decisores se percam em big data e acelerando insights acionáveis."
                            },
                            "estimatedTime": "0.5 horas",
                            "difficulty": "beginner",
                            "status": "not_started",
                            "prerequisites": []
                          },
                          {
                            "id": "10.1.3.2.2.3",
                            "name": "Explicar o significado atribuído à informação",
                            "description": "Detalhar como o significado é adicionado por meio de interpretação humana ou algorítmica, gerando insights acionáveis.",
                            "atomicExpansion": {
                              "steps": [
                                {
                                  "stepNumber": 1,
                                  "title": "Diferenciar Dados de Informação",
                                  "subSteps": [
                                    "Defina dados como fatos brutos sem contexto (ex: números isolados).",
                                    "Defina informação como dados com significado atribuído.",
                                    "Compare exemplos: '25' é dado; '25 vendas em 1 dia' é informação.",
                                    "Discuta por que o contexto adiciona valor aos dados.",
                                    "Crie um diagrama simples de dados → contexto → informação."
                                  ],
                                  "verification": "Crie uma tabela comparando 3 exemplos de dados e sua transformação em informação.",
                                  "estimatedTime": "30 minutos",
                                  "materials": "Papel e caneta ou ferramenta digital como Draw.io; leitura sobre hierarquia DIKW.",
                                  "tips": "Use analogias cotidianas como 'ingredientes crus vs receita pronta'.",
                                  "learningObjective": "Compreender a distinção fundamental entre dados brutos e informação contextualizada.",
                                  "commonMistakes": "Confundir informação com conhecimento; ignorar o papel do contexto."
                                },
                                {
                                  "stepNumber": 2,
                                  "title": "Explorar Atribuição de Significado Humano",
                                  "subSteps": [
                                    "Descreva como humanos interpretam dados baseados em experiência e conhecimento prévio.",
                                    "Analise um caso: temperatura 30°C – significado varia por região (quente no Brasil, ameno na Europa).",
                                    "Pratique interpretando dados ambíguos com diferentes perspectivas culturais.",
                                    "Registre insights gerados pela interpretação humana.",
                                    "Debata limitações: viés subjetivo e inconsistência."
                                  ],
                                  "verification": "Escreva um parágrafo explicando como contexto humano transforma dados em informação acionável.",
                                  "estimatedTime": "45 minutos",
                                  "materials": "Exemplos de datasets reais (ex: CSV de temperaturas); caderno de anotações.",
                                  "tips": "Pergunte 'o que isso significa para o público-alvo?' para guiar a interpretação.",
                                  "learningObjective": "Identificar como a interpretação humana adiciona significado contextual aos dados.",
                                  "commonMistakes": "Superestimar objetividade humana; ignorar viés cognitivo."
                                },
                                {
                                  "stepNumber": 3,
                                  "title": "Analisar Interpretação Algorítmica",
                                  "subSteps": [
                                    "Explique algoritmos como regras pré-definidas para atribuir significado (ex: ML models).",
                                    "Estude exemplo: algoritmo de clustering em dados de clientes gera 'segmentos de mercado'.",
                                    "Compare com humano: algoritmos são consistentes mas dependem de treinamento.",
                                    "Teste um algoritmo simples em Python (ex: pandas para categorizar dados).",
                                    "Avalie limitações: garbage in, garbage out; necessidade de validação humana."
                                  ],
                                  "verification": "Execute um script simples e explique o significado atribuído aos resultados.",
                                  "estimatedTime": "60 minutos",
                                  "materials": "Python com pandas/numpy; dataset sample como Iris ou vendas fictícias.",
                                  "tips": "Comece com bibliotecas prontas para focar no conceito, não na codificação.",
                                  "learningObjective": "Entender como algoritmos automatizam a atribuição de significado aos dados.",
                                  "commonMistakes": "Confundir correlação com causalidade; subestimar necessidade de dados limpos."
                                },
                                {
                                  "stepNumber": 4,
                                  "title": "Gerar Insights Acionáveis",
                                  "subSteps": [
                                    "Integre humano e algorítmico: valide outputs algorítmicos com julgamento humano.",
                                    "Transforme informação em insights: 'vendas caíram 20% → ajustar estoque'.",
                                    "Crie um relatório com 3 insights de um dataset.",
                                    "Teste acionabilidade: o insight leva a uma decisão mensurável?",
                                    "Reflita sobre escalabilidade em cenários reais."
                                  ],
                                  "verification": "Produza um insight acionável de um dataset e proponha uma ação específica.",
                                  "estimatedTime": "45 minutos",
                                  "materials": "Dataset real (ex: Kaggle sales data); template de relatório.",
                                  "tips": "Use o framework 'O que? Por quê? E agora?' para estruturar insights.",
                                  "learningObjective": "Converter informação interpretada em insights práticos e decisórios.",
                                  "commonMistakes": "Gerar insights vagos; falhar em ligar à ação mensurável."
                                }
                              ],
                              "practicalExample": "Em um dataset de e-commerce com 'usuário comprou item X às 14h', um humano atribui significado como 'pico de compras pós-almoço', enquanto um algoritmo de RFM (Recency, Frequency, Monetary) classifica o usuário como 'fiel', gerando insight acionável: 'enviar desconto personalizado para retenção'.",
                              "finalVerifications": [
                                "Explicar diferença entre dados e informação com 2 exemplos precisos.",
                                "Descrever processo de atribuição humana vs algorítmica sem erros conceituais.",
                                "Gerar pelo menos 1 insight acionável de um dataset dado.",
                                "Identificar viés em uma interpretação humana ou algorítmica.",
                                "Criar diagrama da hierarquia DIKW (Dados-Informação-Conhecimento-Sabedoria).",
                                "Propor melhoria em um exemplo de atribuição de significado fraca."
                              ],
                              "assessmentCriteria": [
                                "Clareza na distinção dados vs informação (20%)",
                                "Profundidade na explicação de interpretação humana/algorítmica (30%)",
                                "Qualidade e acionabilidade dos insights gerados (25%)",
                                "Uso preciso de exemplos e verificações (15%)",
                                "Identificação de limitações e viés (10%)"
                              ],
                              "crossCurricularConnections": [
                                "Filosofia: Semântica e epistemologia do conhecimento.",
                                "Computação: Machine Learning e processamento de linguagem natural.",
                                "Negócios: Análise de inteligência de negócios (BI).",
                                "Psicologia: Viés cognitivo na interpretação humana."
                              ],
                              "realWorldApplication": "Em ciência de dados, analistas usam essa habilidade para transformar logs de usuários em recomendações personalizadas no Netflix ou prever churn em bancos, otimizando decisões baseadas em insights acionáveis derivados de dados brutos."
                            },
                            "estimatedTime": "0.5 horas",
                            "difficulty": "beginner",
                            "status": "not_started",
                            "prerequisites": []
                          }
                        ]
                      },
                      {
                        "id": "10.1.3.2.3",
                        "name": "Processo de Transformação de Dados em Informação",
                        "description": "Etapas e exemplos práticos de como dados são processados para se tornarem informação útil.",
                        "specificSkills": [
                          {
                            "id": "10.1.3.2.3.1",
                            "name": "Descrever o processo de transformação",
                            "description": "Mapear as etapas de coleta, limpeza e análise exploratória que adicionam contexto e relevância aos dados.",
                            "atomicExpansion": {
                              "steps": [
                                {
                                  "stepNumber": 1,
                                  "title": "Compreender a Etapa de Coleta de Dados",
                                  "subSteps": [
                                    "Identificar fontes de dados relevantes (bancos de dados, APIs, sensores).",
                                    "Definir critérios de amostragem para representatividade.",
                                    "Registrar metadados sobre origem e timestamp dos dados.",
                                    "Coletar dados brutos em formato estruturado (CSV, JSON).",
                                    "Validar integridade inicial dos dados coletados."
                                  ],
                                  "verification": "Verificar se o dataset coletado possui metadados completos e amostra representativa.",
                                  "estimatedTime": "45 minutos",
                                  "materials": "Computador com acesso à internet, ferramentas como Python (pandas) ou Excel.",
                                  "tips": "Priorize fontes confiáveis para evitar viés desde o início.",
                                  "learningObjective": "Entender como a coleta impacta a qualidade final da informação.",
                                  "commonMistakes": "Ignorar viés de seleção ou coletar dados irrelevantes."
                                },
                                {
                                  "stepNumber": 2,
                                  "title": "Executar a Limpeza de Dados",
                                  "subSteps": [
                                    "Identificar e tratar valores ausentes (imputação ou remoção).",
                                    "Remover duplicatas e corrigir erros de entrada.",
                                    "Padronizar formatos (datas, categorias).",
                                    "Detectar e tratar outliers usando estatísticas descritivas.",
                                    "Documentar todas as transformações aplicadas."
                                  ],
                                  "verification": "Executar script de limpeza e confirmar ausência de NaNs, duplicatas ou inconsistências.",
                                  "estimatedTime": "60 minutos",
                                  "materials": "Python com bibliotecas pandas e numpy, dataset bruto.",
                                  "tips": "Use visualizações iniciais para identificar problemas antes de limpar.",
                                  "learningObjective": "Dominar técnicas para preparar dados confiáveis.",
                                  "commonMistakes": "Remover dados válidos como outliers sem análise."
                                },
                                {
                                  "stepNumber": 3,
                                  "title": "Realizar Análise Exploratória de Dados (EDA)",
                                  "subSteps": [
                                    "Calcular estatísticas descritivas (média, mediana, desvios).",
                                    "Criar visualizações (histogramas, boxplots, correlações).",
                                    "Explorar relações entre variáveis (scatter plots, heatmaps).",
                                    "Identificar padrões, tendências e anomalias.",
                                    "Gerar hipóteses baseadas nos insights iniciais."
                                  ],
                                  "verification": "Produzir relatório com gráficos e estatísticas que revelem padrões nos dados.",
                                  "estimatedTime": "75 minutos",
                                  "materials": "Python com matplotlib/seaborn, Jupyter Notebook.",
                                  "tips": "Comece com univariada antes de multivariada para insights claros.",
                                  "learningObjective": "Aprender a extrair padrões preliminares dos dados limpos.",
                                  "commonMistakes": "Interpretar correlação como causalidade sem contexto."
                                },
                                {
                                  "stepNumber": 4,
                                  "title": "Adicionar Contexto e Relevância para Transformar em Informação",
                                  "subSteps": [
                                    "Integrar dados com conhecimento de domínio (ex: variáveis externas).",
                                    "Mapear padrões encontrados a perguntas de negócio.",
                                    "Criar narrativas que expliquem os insights.",
                                    "Definir relevância com base em objetivos do projeto.",
                                    "Validar transformações com stakeholders."
                                  ],
                                  "verification": "Elaborar um resumo que conecte dados a insights acionáveis.",
                                  "estimatedTime": "45 minutos",
                                  "materials": "Relatório de EDA, documentação de domínio.",
                                  "tips": "Use storytelling para tornar os dados compreensíveis.",
                                  "learningObjective": "Compreender como contexto eleva dados a informação.",
                                  "commonMistakes": "Ignorar o domínio, resultando em insights irrelevantes."
                                }
                              ],
                              "practicalExample": "Colete dados brutos de vendas diárias de uma loja (CSV com datas, produtos, quantidades e valores ausentes). Limpe duplicatas e NaNs. Faça EDA com gráficos de vendas por produto e tendências sazonais. Adicione contexto de feriados para explicar picos, transformando em informação sobre estratégias de estoque.",
                              "finalVerifications": [
                                "Lista completa das 4 etapas com subpassos corretos.",
                                "Exemplo prático demonstrado com dataset real.",
                                "Relatório de EDA com visualizações claras.",
                                "Narrativa que adiciona contexto relevante.",
                                "Ausência de erros comuns identificados.",
                                "Tempo total respeitado com documentação."
                              ],
                              "assessmentCriteria": [
                                "Precisão e sequência lógica das etapas descritas (30%).",
                                "Detalhe e acionabilidade dos subpassos (25%).",
                                "Qualidade das visualizações e análises exploratórias (20%).",
                                "Integração de contexto para relevância (15%).",
                                "Clareza na verificação e dicas práticas (10%)."
                              ],
                              "crossCurricularConnections": [
                                "Estatística: Cálculo de medidas descritivas e testes.",
                                "Programação: Uso de Python/R para manipulação de dados.",
                                "Gestão de Projetos: Documentação e validação com stakeholders.",
                                "Comunicação: Storytelling de dados para relatórios."
                              ],
                              "realWorldApplication": "Em marketing digital, transformar logs de cliques (coleta via API) em insights de comportamento do usuário após limpeza e EDA, adicionando contexto demográfico para campanhas personalizadas e aumento de conversões."
                            },
                            "estimatedTime": "0.75 horas",
                            "difficulty": "beginner",
                            "status": "not_started",
                            "prerequisites": []
                          },
                          {
                            "id": "10.1.3.2.3.2",
                            "name": "Fornecer exemplos de dados virando informação",
                            "description": "Criar ou analisar exemplos reais, como números de vendas (dados) transformados em relatórios com tendências (informação).",
                            "atomicExpansion": {
                              "steps": [
                                {
                                  "stepNumber": 1,
                                  "title": "Identificar e coletar dados brutos relevantes",
                                  "subSteps": [
                                    "Defina o contexto do exemplo (ex: vendas mensais de uma loja).",
                                    "Liste dados brutos numéricos simples, como 'Vendas: Janeiro=100, Fevereiro=150, Março=120'.",
                                    "Garanta que os dados sejam reais ou realistas e mensuráveis.",
                                    "Registre a fonte dos dados para credibilidade.",
                                    "Classifique os dados como 'brutos' explicando por quê (sem processamento)."
                                  ],
                                  "verification": "Verifique se os dados estão listados como números isolados sem análise ou padrões identificados.",
                                  "estimatedTime": "15 minutos",
                                  "materials": "Papel, planilha (Google Sheets ou Excel), exemplos de dados online.",
                                  "tips": "Comece com conjuntos pequenos (5-10 itens) para simplicidade.",
                                  "learningObjective": "Compreender dados como fatos crus e isolados.",
                                  "commonMistakes": "Confundir dados com análises prévias ou usar dados irrelevantes."
                                },
                                {
                                  "stepNumber": 2,
                                  "title": "Escolher e planejar a transformação para informação",
                                  "subSteps": [
                                    "Identifique padrões potenciais: médias, tendências, totais ou comparações.",
                                    "Selecione ferramentas: cálculos manuais, fórmulas em planilhas ou gráficos.",
                                    "Defina o que a informação revelará (ex: 'tendência de aumento nas vendas').",
                                    "Esboce o processo: 'Calcular média mensal e plotar gráfico de linha'.",
                                    "Preveja insights: 'Vendas crescendo 20% ao mês'."
                                  ],
                                  "verification": "Confirme se há um plano claro ligando dados brutos a insights processados.",
                                  "estimatedTime": "20 minutos",
                                  "materials": "Planilha com dados do Step 1, calculadora.",
                                  "tips": "Pergunte: 'O que esses números dizem juntos que isolados não dizem?'",
                                  "learningObjective": "Planejar como agregar valor aos dados brutos.",
                                  "commonMistakes": "Escolher transformações complexas demais sem necessidade."
                                },
                                {
                                  "stepNumber": 3,
                                  "title": "Executar a transformação e gerar informação",
                                  "subSteps": [
                                    "Aplique cálculos: some totais, calcule médias, percentuais de variação.",
                                    "Crie visualizações: gráfico de barras para comparações ou linha para tendências.",
                                    "Forme frases informativas: 'Média de vendas: 123 unidades; tendência de alta'.",
                                    "Valide cálculos com verificações duplas.",
                                    "Organize em relatório simples com título e legendas."
                                  ],
                                  "verification": "Os resultados mostram padrões ou resumos, não apenas números crus.",
                                  "estimatedTime": "25 minutos",
                                  "materials": "Planilha com fórmulas (SOMA, MÉDIA, gráfico), lápis para rascunhos.",
                                  "tips": "Use cores em gráficos para destacar tendências.",
                                  "learningObjective": "Transformar dados quantitativos em insights qualitativos.",
                                  "commonMistakes": "Erros de cálculo ou gráficos mal rotulados."
                                },
                                {
                                  "stepNumber": 4,
                                  "title": "Analisar e validar o exemplo completo",
                                  "subSteps": [
                                    "Explique a transformação: 'Dados de vendas viraram info sobre crescimento'.",
                                    "Compare antes/depois: dados crus vs. relatório.",
                                    "Teste com perguntas: 'Qual a tendência? Por quê?'",
                                    "Refine para clareza e adicione contexto real.",
                                    "Documente o exemplo final para reutilização."
                                  ],
                                  "verification": "Pode articular claramente como dados se tornaram informação útil.",
                                  "estimatedTime": "15 minutos",
                                  "materials": "Relatório gerado, gravador de áudio para explicação oral.",
                                  "tips": "Peça feedback de um par para validar clareza.",
                                  "learningObjective": "Avaliar a efetividade da transformação dados-informação.",
                                  "commonMistakes": "Ignorar contexto ou superestimar a profundidade da informação."
                                }
                              ],
                              "practicalExample": "Dados brutos: Vendas diárias de uma cafeteria - Dia1: 50 cafés, Dia2: 60, Dia3: 55, Dia4: 70, Dia5: 65. Transformação: Calcular média (60 cafés/dia), gráfico de linha mostrando tendência de aumento (pico no Dia4). Informação: 'Vendas crescendo 10% nos últimos dias, sugerindo demanda maior nos fins de semana'.",
                              "finalVerifications": [
                                "Explica diferença entre dados brutos e informação processada.",
                                "Fornece pelo menos um exemplo numérico concreto com transformação.",
                                "Identifica padrões ou tendências corretamente nos dados.",
                                "Apresenta informação de forma visual ou resumida clara.",
                                "Valida que a informação agrega valor aos dados originais.",
                                "Responde perguntas sobre o processo de transformação."
                              ],
                              "assessmentCriteria": [
                                "Relevância e realismo dos dados escolhidos (ex: cotidianos).",
                                "Precisão dos cálculos e transformações aplicadas.",
                                "Clareza na apresentação da informação (gráficos/relatórios).",
                                "Profundidade da análise de padrões identificados.",
                                "Capacidade de articular o 'porquê' da transformação.",
                                "Criatividade e variedade no exemplo fornecido."
                              ],
                              "crossCurricularConnections": [
                                "Estatística: Cálculos de médias e tendências.",
                                "Informática: Uso de planilhas e gráficos em ferramentas como Excel.",
                                "Negócios: Análise de vendas para decisões gerenciais.",
                                "Língua Portuguesa: Redação clara de relatórios informativos."
                              ],
                              "realWorldApplication": "Em empresas de varejo, gerentes transformam dados de vendas diárias em relatórios de tendências para ajustar estoques e promoções, otimizando lucros e evitando perdas."
                            },
                            "estimatedTime": "0.5 horas",
                            "difficulty": "beginner",
                            "status": "not_started",
                            "prerequisites": []
                          },
                          {
                            "id": "10.1.3.2.3.3",
                            "name": "Relacionar informação com conhecimento",
                            "description": "Entender a hierarquia DIK (Dados, Informação, Conhecimento) e como informação é o elo intermediário na Ciência de Dados.",
                            "atomicExpansion": {
                              "steps": [
                                {
                                  "stepNumber": 1,
                                  "title": "Revisar os conceitos fundamentais da hierarquia DIK",
                                  "subSteps": [
                                    "Estude a definição de Dados: fatos brutos sem contexto.",
                                    "Analise a definição de Informação: dados processados com contexto e significado.",
                                    "Compreenda a definição de Conhecimento: informação aplicada, contextualizada e compreendida.",
                                    "Desenhe ou visualize a pirâmide DIK para fixar a hierarquia.",
                                    "Compare os três níveis usando uma tabela comparativa."
                                  ],
                                  "verification": "Recitar as definições de Dados, Informação e Conhecimento com exemplos simples.",
                                  "estimatedTime": "30 minutos",
                                  "materials": "Diagrama da pirâmide DIK (impresso ou digital), vídeo introdutório sobre DIK (YouTube ou Khan Academy).",
                                  "tips": "Use analogias cotidianas, como 'dados são ingredientes crus, informação é a receita pronta'.",
                                  "learningObjective": "Dominar as definições básicas e a estrutura hierárquica DIK.",
                                  "commonMistakes": "Confundir Dados com Informação, ignorando a necessidade de contexto."
                                },
                                {
                                  "stepNumber": 2,
                                  "title": "Analisar o papel da Informação como elo intermediário",
                                  "subSteps": [
                                    "Identifique processos que transformam Dados em Informação (ex: agregação, filtragem).",
                                    "Discuta como a Informação adiciona relevância e estrutura aos Dados.",
                                    "Examine exemplos onde falta de contexto torna Dados inúteis.",
                                    "Crie um fluxograma simples de Dados → Informação.",
                                    "Relacione com Ciência de Dados: limpeza e análise inicial de dados."
                                  ],
                                  "verification": "Explicar verbalmente ou por escrito como Dados se tornam Informação, com um exemplo pessoal.",
                                  "estimatedTime": "45 minutos",
                                  "materials": "Ferramenta de fluxograma (Draw.io ou papel e caneta), conjunto de dados brutos exemplo (planilha Excel).",
                                  "tips": "Pergunte sempre: 'Isso tem contexto? É útil para decisões?'",
                                  "learningObjective": "Entender o mecanismo de transformação Dados → Informação.",
                                  "commonMistakes": "Subestimar o processamento necessário, achando que qualquer dado é informação."
                                },
                                {
                                  "stepNumber": 3,
                                  "title": "Explorar a relação entre Informação e Conhecimento",
                                  "subSteps": [
                                    "Defina como Informação se transforma em Conhecimento via interpretação e experiência.",
                                    "Analise fatores como padrões, inferências e aplicação prática.",
                                    "Diferencie Informação estática de Conhecimento dinâmico e acionável.",
                                    "Crie um mapa mental ligando Informação a insights de Conhecimento.",
                                    "Discuta limitações: nem toda Informação gera Conhecimento sem expertise."
                                  ],
                                  "verification": "Produzir um mapa mental ou parágrafo explicando a ponte Informação → Conhecimento.",
                                  "estimatedTime": "40 minutos",
                                  "materials": "Ferramenta de mind mapping (MindMeister ou papel), exemplos de casos de Ciência de Dados.",
                                  "tips": "Pense em 'sabedoria prática': Informação é 'o que aconteceu?', Conhecimento é 'por quê e o que fazer?'.",
                                  "learningObjective": "Compreender a transição de Informação para Conhecimento.",
                                  "commonMistakes": "Tratar Informação como Conhecimento, sem considerar aplicação ou contexto humano."
                                },
                                {
                                  "stepNumber": 4,
                                  "title": "Sintetizar o elo intermediário da Informação na hierarquia DIK",
                                  "subSteps": [
                                    "Integre os conceitos: monte um diagrama completo DIK destacando Informação no meio.",
                                    "Aplique a um cenário de Ciência de Dados: dados brutos → dashboard → estratégia.",
                                    "Debata cenários onde falha o elo Informação-Conhecimento.",
                                    "Crie uma tabela de exemplos reais para cada transição.",
                                    "Reflita sobre implicações na prática profissional."
                                  ],
                                  "verification": "Apresentar diagrama e tabela, explicando o papel central da Informação.",
                                  "estimatedTime": "35 minutos",
                                  "materials": "Software de diagramação (Lucidchart), exemplos de datasets públicos (Kaggle).",
                                  "tips": "Teste seu entendimento invertendo o fluxo: pode Conhecimento virar Informação?",
                                  "learningObjective": "Relacionar Informação como elo essencial entre Dados e Conhecimento.",
                                  "commonMistakes": "Ignorar o caráter intermediário, vendo Informação como endpoint."
                                }
                              ],
                              "practicalExample": "Em uma empresa de e-commerce: Dados (cliques por hora), Informação (taxa de conversão diária por produto, via agregação), Conhecimento (otimizar estoque de produtos com baixa conversão, baseado em padrões sazonais).",
                              "finalVerifications": [
                                "Explicar a hierarquia DIK em sequência sem erros.",
                                "Dar um exemplo pessoal de Dados → Informação → Conhecimento.",
                                "Identificar o papel intermediário da Informação em um fluxograma.",
                                "Diferenciar Informação de Conhecimento em 3 cenários.",
                                "Aplicar o conceito a um problema de Ciência de Dados simples.",
                                "Criar um diagrama DIK personalizado."
                              ],
                              "assessmentCriteria": [
                                "Precisão nas definições e hierarquia DIK (80% acerto).",
                                "Profundidade na explicação do elo Informação-Conhecimento.",
                                "Qualidade e clareza de diagramas e exemplos criados.",
                                "Capacidade de aplicar conceitos em cenários reais.",
                                "Identificação correta de erros comuns nos fluxos.",
                                "Criatividade em conexões práticas."
                              ],
                              "crossCurricularConnections": [
                                "Filosofia: Epistemologia e teoria do conhecimento.",
                                "Gestão de Negócios: Tomada de decisões baseada em dados.",
                                "Tecnologia da Informação: Processamento e análise de dados.",
                                "Estatística: Agregação e inferência a partir de dados."
                              ],
                              "realWorldApplication": "Na Ciência de Dados corporativa, relacionar Informação com Conhecimento permite transformar relatórios de vendas (Informação) em estratégias preditivas de mercado (Conhecimento), otimizando recursos e competitividade."
                            },
                            "estimatedTime": "0.5 horas",
                            "difficulty": "intermediate",
                            "status": "not_started",
                            "prerequisites": []
                          }
                        ]
                      }
                    ]
                  },
                  {
                    "id": "10.1.3.3",
                    "name": "Definição de Conhecimento",
                    "description": "Informação organizada, analisada e aplicada com experiência, permitindo compreensão e tomada de decisões.",
                    "individualConcepts": [
                      {
                        "id": "10.1.3.3.1",
                        "name": "Organização da Informação",
                        "description": "Compreender como a informação é estruturada e organizada para se transformar em conhecimento, servindo como base para análises mais profundas na Ciência de Dados.",
                        "specificSkills": [
                          {
                            "id": "10.1.3.3.1.1",
                            "name": "Reconhecer informação organizada",
                            "description": "Identificar características de informação organizada, como padrões identificáveis e estruturas lógicas, diferenciando-a de dados brutos ou informação caótica, com exemplos do contexto da Ciência de Dados.",
                            "atomicExpansion": {
                              "steps": [
                                {
                                  "stepNumber": 1,
                                  "title": "Compreender Conceitos Básicos de Dados Brutos e Informação Organizada",
                                  "subSteps": [
                                    "Defina dados brutos como coleções não estruturadas de fatos isolados, sem padrões evidentes.",
                                    "Defina informação organizada como dados processados com estruturas lógicas e padrões identificáveis.",
                                    "Explore analogias cotidianas, como uma pilha de recibos soltos (dados brutos) versus uma planilha categorizada (informação organizada).",
                                    "Registre as diferenças principais em um quadro comparativo.",
                                    "Revise definições com exemplos iniciais de Ciência de Dados, como logs de servidor crus."
                                  ],
                                  "verification": "Crie um quadro comparativo com pelo menos 3 diferenças claras entre dados brutos e informação organizada.",
                                  "estimatedTime": "25 minutos",
                                  "materials": [
                                    "Papel e caneta ou editor de texto",
                                    "Exemplos online de datasets crus (ex: Kaggle)"
                                  ],
                                  "tips": "Use analogias visuais para facilitar a memorização dos conceitos.",
                                  "learningObjective": "Diferenciar conceitualmente dados brutos de informação organizada.",
                                  "commonMistakes": [
                                    "Confundir organização com volume de dados",
                                    "Ignorar o papel dos padrões lógicos"
                                  ]
                                },
                                {
                                  "stepNumber": 2,
                                  "title": "Identificar Características de Informação Organizada",
                                  "subSteps": [
                                    "Liste características chave: padrões repetíveis, hierarquia lógica, categorização e relacionamentos entre elementos.",
                                    "Analise exemplos: uma tabela com colunas padronizadas versus lista aleatória.",
                                    "Identifique padrões como sequências numéricas ou categorias temáticas.",
                                    "Pratique destacando essas características em um texto curto desorganizado.",
                                    "Registre 5 exemplos de estruturas lógicas comuns em Ciência de Dados, como tabelas relacionais."
                                  ],
                                  "verification": "Liste e explique 4 características de informação organizada com exemplos breves.",
                                  "estimatedTime": "30 minutos",
                                  "materials": [
                                    "Amostras de texto ou dados impressos/desorganizados",
                                    "Ferramenta de edição como Google Sheets"
                                  ],
                                  "tips": "Procure por repetições e agrupamentos naturais nos dados para identificar padrões.",
                                  "learningObjective": "Reconhecer padrões e estruturas lógicas em conjuntos de dados.",
                                  "commonMistakes": [
                                    "Focar apenas em ordem alfabética",
                                    "Desconsiderar relacionamentos contextuais"
                                  ]
                                },
                                {
                                  "stepNumber": 3,
                                  "title": "Diferenciar Informação Organizada de Dados Caóticos",
                                  "subSteps": [
                                    "Defina informação caótica como dados com ruído excessivo, sem padrões discerníveis.",
                                    "Compare lado a lado: dados caóticos (ex: logs de erro misturados) vs organizados (ex: relatório filtrado).",
                                    "Classifique 5 exemplos mistos como brutos, caóticos ou organizados.",
                                    "Explique por que a ausência de estrutura impede a análise.",
                                    "Aplique a diferenciação em um cenário de Ciência de Dados, como dados de sensores IoT."
                                  ],
                                  "verification": "Classifique corretamente 5 exemplos fornecidos como organizados, caóticos ou brutos, justificando cada um.",
                                  "estimatedTime": "35 minutos",
                                  "materials": [
                                    "Conjuntos de dados de exemplo (CSV desorganizado e organizado)",
                                    "Navegador para visualizar datasets"
                                  ],
                                  "tips": "Pergunte: 'Posso extrair significado imediato?' para testar organização.",
                                  "learningObjective": "Distinguir informação organizada de formas caóticas ou brutas.",
                                  "commonMistakes": [
                                    "Classificar dados volumosos como organizados automaticamente",
                                    "Ignorar ruído sutil"
                                  ]
                                },
                                {
                                  "stepNumber": 4,
                                  "title": "Aplicar Reconhecimento no Contexto de Ciência de Dados",
                                  "subSteps": [
                                    "Selecione um dataset real de Ciência de Dados (ex: vendas ou saúde).",
                                    "Identifique partes organizadas versus brutas/caóticas.",
                                    "Reorganize uma seção caótica aplicando estruturas lógicas.",
                                    "Avalie o impacto da organização na usabilidade para análise.",
                                    "Documente o processo com antes/depois."
                                  ],
                                  "verification": "Transforme um exemplo caótico em organizado e descreva as melhorias.",
                                  "estimatedTime": "40 minutos",
                                  "materials": [
                                    "Dataset gratuito do Kaggle ou UCI ML Repository",
                                    "Excel ou Python Pandas básico"
                                  ],
                                  "tips": "Comece com datasets pequenos para prática rápida.",
                                  "learningObjective": "Aplicar reconhecimento de informação organizada em cenários profissionais de Dados.",
                                  "commonMistakes": [
                                    "Sobrestimar organização inicial",
                                    "Não considerar contexto domain-specific"
                                  ]
                                }
                              ],
                              "practicalExample": "Em um dataset de vendas de e-commerce: dados brutos são linhas aleatórias como 'ProdutoX, 100, 2023-05'; informação organizada é uma tabela com colunas 'Data', 'Produto', 'Quantidade', 'Valor Total', ordenada por data e com totais por categoria, permitindo análise de tendências.",
                              "finalVerifications": [
                                "Lista pelo menos 5 características de informação organizada com exemplos.",
                                "Diferencia corretamente dados brutos, caóticos e organizados em 4 cenários.",
                                "Reorganiza um dataset pequeno de forma lógica.",
                                "Explica o valor da organização para análise em Ciência de Dados.",
                                "Identifica padrões em um exemplo real sem auxílio."
                              ],
                              "assessmentCriteria": [
                                "Precisão na identificação de padrões e estruturas (80% acerto).",
                                "Qualidade e relevância dos exemplos no contexto de Ciência de Dados.",
                                "Capacidade de diferenciação clara entre tipos de informação.",
                                "Profundidade nas justificativas e verificações.",
                                "Criatividade na aplicação prática e conexões lógicas.",
                                "Completude da documentação de processos."
                              ],
                              "crossCurricularConnections": [
                                "Matemática: Reconhecimento de padrões e sequências lógicas.",
                                "Informática: Estruturas de dados e bancos relacionais.",
                                "Estatística: Preparação de dados para análise descritiva.",
                                "Língua Portuguesa: Organização textual e semântica.",
                                "Ciências: Classificação e categorização de fenômenos."
                              ],
                              "realWorldApplication": "Na Ciência de Dados, reconhecer informação organizada é essencial na etapa de ETL (Extract, Transform, Load), permitindo limpar dados para modelagem preditiva, como em previsões de vendas ou detecção de fraudes em bancos, acelerando insights acionáveis."
                            },
                            "estimatedTime": "0.5 horas",
                            "difficulty": "beginner",
                            "status": "not_started",
                            "prerequisites": []
                          },
                          {
                            "id": "10.1.3.3.1.2",
                            "name": "Exemplificar organização em contextos reais",
                            "description": "Fornecer e analisar exemplos de como organizar informação em conjuntos de dados para gerar conhecimento, relacionando com etapas iniciais da Ciência de Dados como coleta e integração.",
                            "atomicExpansion": {
                              "steps": [
                                {
                                  "stepNumber": 1,
                                  "title": "Selecionar um contexto real e identificar informações brutas",
                                  "subSteps": [
                                    "Escolha um cenário cotidiano, como gerenciamento de estoque de uma loja ou hábitos de estudo diários.",
                                    "Liste fontes de informação não estruturada, como notas em papel, mensagens de texto ou recibos soltos.",
                                    "Registre pelo menos 10-15 itens de informação bruta relevantes ao contexto.",
                                    "Classifique-os inicialmente como 'dados crus' para diferenciar de informação organizada.",
                                    "Documente o problema que a desorganização causa nesse contexto."
                                  ],
                                  "verification": "Verifique se há uma lista clara de dados crus com pelo menos 10 itens e uma descrição do contexto/problema.",
                                  "estimatedTime": "30 minutos",
                                  "materials": "Papel e caneta ou editor de texto simples (Notepad, Google Docs)",
                                  "tips": "Comece com algo familiar para facilitar a identificação de dados reais.",
                                  "learningObjective": "Identificar e documentar dados não estruturados em um contexto prático.",
                                  "commonMistakes": "Escolher contextos muito abstratos; incluir dados irrelevantes desde o início."
                                },
                                {
                                  "stepNumber": 2,
                                  "title": "Organizar a informação em um conjunto de dados estruturado",
                                  "subSteps": [
                                    "Defina categorias/colunas lógicas baseadas nos dados crus (ex: data, item, quantidade, valor).",
                                    "Crie uma tabela simples usando planilha (Excel/Google Sheets) ou lista formatada.",
                                    "Preencha a tabela transferindo e padronizando os dados crus (ex: converter datas para formato YYYY-MM-DD).",
                                    "Adicione uma linha de cabeçalho e valide a consistência de cada entrada.",
                                    "Calcule totais ou resumos básicos para testar a estrutura."
                                  ],
                                  "verification": "Confirme que a tabela tem cabeçalhos claros, dados padronizados e pelo menos uma métrica simples calculada.",
                                  "estimatedTime": "45 minutos",
                                  "materials": "Planilha eletrônica (Google Sheets ou Excel), dados crus do Step 1",
                                  "tips": "Use filtros ou classificação automática para identificar inconsistências rapidamente.",
                                  "learningObjective": "Transformar dados não estruturados em um formato tabular organizado.",
                                  "commonMistakes": "Ignorar padronização de formatos; criar colunas excessivamente complexas."
                                },
                                {
                                  "stepNumber": 3,
                                  "title": "Analisar o conjunto de dados para gerar conhecimento",
                                  "subSteps": [
                                    "Aplique operações básicas: somar valores, contar itens, identificar padrões (ex: mais frequente).",
                                    "Gere insights qualitativos e quantitativos (ex: 'Produto X representa 40% das vendas').",
                                    "Visualize com gráficos simples (pizza, barras) para destacar padrões.",
                                    "Explique como a organização permitiu esses insights que não eram visíveis nos dados crus.",
                                    "Registre 3-5 conhecimentos derivados da análise."
                                  ],
                                  "verification": "Verifique se há pelo menos 3 insights documentados com evidências da tabela e um gráfico simples.",
                                  "estimatedTime": "40 minutos",
                                  "materials": "Planilha com dados do Step 2, funções de gráfico integradas",
                                  "tips": "Pergunte 'O que isso revela?' para cada métrica calculada.",
                                  "learningObjective": "Extrair conhecimento acionável de dados organizados através de análise básica.",
                                  "commonMistakes": "Focar só em números sem explicar o 'porquê' do insight; pular visualizações."
                                },
                                {
                                  "stepNumber": 4,
                                  "title": "Relacionar com etapas iniciais da Ciência de Dados",
                                  "subSteps": [
                                    "Mapeie o processo: Step 1 = Coleta; Step 2 = Integração/Limpeza; Step 3 = Análise inicial.",
                                    "Pesquise definições breves de 'coleta' e 'integração' em Data Science (ex: via Wikipedia ou Khan Academy).",
                                    "Descreva similaridades e diferenças com ferramentas profissionais (ex: Python Pandas vs. Excel).",
                                    "Crie um diagrama de fluxo conectando seu exemplo ao pipeline de Data Science.",
                                    "Conclua com o valor da organização para escalabilidade em grandes datasets."
                                  ],
                                  "verification": "Confirme o mapeamento documentado, definições citadas e diagrama de fluxo.",
                                  "estimatedTime": "35 minutos",
                                  "materials": "Ferramenta de desenho simples (Draw.io, PowerPoint), recursos online sobre Data Science",
                                  "tips": "Use analogias: 'Excel é como Pandas para iniciantes'.",
                                  "learningObjective": "Conectar organização prática às fases fundamentais de Data Science.",
                                  "commonMistakes": "Forçar conexões irrelevantes; não citar fontes para definições."
                                }
                              ],
                              "practicalExample": "Em uma cafeteria, dados crus como recibos soltos (data, item, preço) são organizados em uma tabela no Excel. Análise revela que cafés expressos representam 60% das vendas matinais, gerando conhecimento para otimizar estoque e horários de pico.",
                              "finalVerifications": [
                                "O aluno pode descrever o contexto real e listar dados crus iniciais.",
                                "A tabela organizada está completa, padronizada e livre de erros evidentes.",
                                "Pelo menos 3 insights claros foram extraídos e visualizados.",
                                "O mapeamento para coleta/integração em Data Science é preciso e ilustrado.",
                                "O aluno explica verbalmente como a organização transformou informação em conhecimento.",
                                "Diagrama de fluxo conecta o exemplo ao pipeline de Data Science."
                              ],
                              "assessmentCriteria": [
                                "Clareza e relevância do contexto real escolhido (20%)",
                                "Qualidade da estruturação dos dados (padronização, completude) (25%)",
                                "Profundidade e precisão dos insights gerados (25%)",
                                "Conexão lógica com conceitos de Data Science (20%)",
                                "Uso efetivo de visualizações e documentação (10%)"
                              ],
                              "crossCurricularConnections": [
                                "Estatística: Cálculo de médias e frequências em datasets.",
                                "Programação: Introdução a estruturas de dados como listas e dicionários.",
                                "Negócios/Gestão: Análise de vendas e otimização de recursos.",
                                "Linguagens: Redação de relatórios claros baseados em dados."
                              ],
                              "realWorldApplication": "Profissionais de Data Science em empresas como Amazon ou bancos organizam terabytes de dados de transações diárias em bancos de dados (ex: SQL), permitindo análises que preveem demandas, detectam fraudes e impulsionam decisões estratégicas bilionárias."
                            },
                            "estimatedTime": "0.5 horas",
                            "difficulty": "beginner",
                            "status": "not_started",
                            "prerequisites": []
                          },
                          {
                            "id": "10.1.3.3.1.3",
                            "name": "Diferenciar de informação desorganizada",
                            "description": "Comparar cenários de informação desorganizada (ex.: dados ruídos) com organizada, destacando impactos na formação de conhecimento.",
                            "atomicExpansion": {
                              "steps": [
                                {
                                  "stepNumber": 1,
                                  "title": "Definir Conceitos Básicos de Informação Desorganizada e Organizada",
                                  "subSteps": [
                                    "Pesquise definições de 'informação desorganizada' (ex.: dados ruídos, inconsistentes ou sem estrutura)",
                                    "Liste características chave: ruído, duplicatas, falta de padrões",
                                    "Defina 'informação organizada': dados limpos, estruturados, categorizados",
                                    "Crie um glossário simples com 5 termos relacionados (ex.: ruído, outlier, padronização)",
                                    "Compare definições lado a lado em uma tabela"
                                  ],
                                  "verification": "Glossário e tabela completos com definições precisas e exemplos iniciais",
                                  "estimatedTime": "30 minutos",
                                  "materials": "Notebook, acesso à internet para pesquisa (Wikipedia, Khan Academy), planilha Google Sheets",
                                  "tips": "Use analogias cotidianas como uma gaveta bagunçada vs organizada para fixar conceitos",
                                  "learningObjective": "Compreender definições fundamentais e características distintivas",
                                  "commonMistakes": "Confundir desorganização com volume de dados; foque em estrutura, não quantidade"
                                },
                                {
                                  "stepNumber": 2,
                                  "title": "Identificar Características em Exemplos Simples",
                                  "subSteps": [
                                    "Colete 3 exemplos de dados desorganizados (ex.: lista de temperaturas com erros de digitação)",
                                    "Analise cada exemplo: marque ruídos, inconsistências e impactos potenciais",
                                    "Transforme um exemplo desorganizado em organizado aplicando limpeza básica (remover duplicatas)",
                                    "Documente mudanças em antes/depois com screenshots ou tabelas",
                                    "Repita para dados organizados de fontes confiáveis (ex.: dataset limpo de CSV)"
                                  ],
                                  "verification": "Relatório com 3 exemplos analisados, incluindo tabelas antes/depois",
                                  "estimatedTime": "45 minutos",
                                  "materials": "Datasets sample (Kaggle intro datasets), Excel ou Python Jupyter Notebook básico",
                                  "tips": "Comece com dados pequenos (10-20 linhas) para evitar sobrecarga",
                                  "learningObjective": "Reconhecer visual e analiticamente diferenças entre os tipos de informação",
                                  "commonMistakes": "Ignorar outliers sutis; sempre pergunte 'isso faz sentido no contexto?'"
                                },
                                {
                                  "stepNumber": 3,
                                  "title": "Comparar Cenários e Destacar Impactos na Formação de Conhecimento",
                                  "subSteps": [
                                    "Selecione 2 cenários paralelos: um com dados desorganizados (ex.: vendas com ruídos) e um organizado",
                                    "Simule formação de conhecimento: extraia insights de cada (ex.: média de vendas errada vs correta)",
                                    "Registre impactos: decisões ruins, perda de confiança, tempo desperdiçado",
                                    "Crie um fluxograma mostrando caminho do dado ao conhecimento em ambos casos",
                                    "Discuta em voz alta ou anote 3 lições aprendidas sobre organização"
                                  ],
                                  "verification": "Fluxograma e tabela de impactos completos com pelo menos 3 diferenças chave",
                                  "estimatedTime": "50 minutos",
                                  "materials": "Ferramenta de fluxograma (Draw.io ou papel), exemplos de datasets reais",
                                  "tips": "Pense em 'lixo entra, lixo sai' (GIGO) para enfatizar impactos",
                                  "learningObjective": "Analisar como a organização afeta a extração de conhecimento válido",
                                  "commonMistakes": "Subestimar impactos indiretos como viés em análises futuras"
                                },
                                {
                                  "stepNumber": 4,
                                  "title": "Aplicar e Refletir sobre Diferenciação em um Caso Prático",
                                  "subSteps": [
                                    "Escolha um dataset real desorganizado (ex.: dados de sensores IoT com ruído)",
                                    "Classifique partes como organizadas/desorganizadas e justifique",
                                    "Proponha 3 ações de organização e simule uma",
                                    "Avalie como isso mudaria a formação de conhecimento (ex.: previsão precisa)",
                                    "Escreva um parágrafo resumindo a importância da diferenciação"
                                  ],
                                  "verification": "Relatório final com classificação, ações e reflexão escrita",
                                  "estimatedTime": "40 minutos",
                                  "materials": "Dataset público (UCI ML Repository), ferramenta de análise simples",
                                  "tips": "Use métricas básicas como contagem de valores únicos para quantificar desorganização",
                                  "learningObjective": "Aplicar diferenciação de forma independente em contextos reais",
                                  "commonMistakes": "Não quantificar impactos; inclua números para objetividade"
                                }
                              ],
                              "practicalExample": "Imagine dados de temperatura de uma fazenda: desorganizados (valores como '25C', 'abc', 30) levam a média errada de 15°C (subestimando calor); organizados (todos numéricos limpos: 25, 28, 30) dão média precisa de 27.7°C, permitindo decisões corretas sobre irrigação.",
                              "finalVerifications": [
                                "Explica corretamente 5 características de informação desorganizada",
                                "Compara com precisão um cenário desorganizado vs organizado",
                                "Identifica pelo menos 3 impactos negativos na formação de conhecimento",
                                "Transforma um exemplo desorganizado em organizado com justificativa",
                                "Cria fluxograma mostrando fluxo de dados ao conhecimento",
                                "Reflete sobre aplicação em um dataset real"
                              ],
                              "assessmentCriteria": [
                                "Precisão conceitual (definições corretas: 30%)",
                                "Qualidade da análise comparativa (exemplos e impactos: 25%)",
                                "Profundidade dos substeps e verificações (detalhes acionáveis: 20%)",
                                "Criatividade em exemplos e fluxogramas (originalidade: 15%)",
                                "Reflexão sobre importância (insights pessoais: 10%)"
                              ],
                              "crossCurricularConnections": [
                                "Estatística: Identificação de outliers e limpeza de dados",
                                "Programação: Uso de bibliotecas como Pandas para organização",
                                "Filosofia: Epistemologia e validade do conhecimento",
                                "Gestão de Projetos: Impacto de dados ruins em decisões empresariais"
                              ],
                              "realWorldApplication": "Em ciência de dados, diferenciar informação desorganizada previne modelos de ML falhos, como em saúde (dados de pacientes ruídos levando a diagnósticos errados) ou finanças (previsões de mercado imprecisas por ruído em transações), economizando milhões e salvando vidas."
                            },
                            "estimatedTime": "0.5 horas",
                            "difficulty": "beginner",
                            "status": "not_started",
                            "prerequisites": []
                          }
                        ]
                      },
                      {
                        "id": "10.1.3.3.2",
                        "name": "Análise da Informação",
                        "description": "Explorar o processo de análise que transforma informação organizada em conhecimento acionável, enfatizando técnicas exploratórias e limpeza de dados.",
                        "specificSkills": [
                          {
                            "id": "10.1.3.3.2.1",
                            "name": "Descrever processo de análise",
                            "description": "Explicar como a análise estatística e exploratória (EDA) organiza e interpreta informação para produzir insights, com referência às etapas da Ciência de Dados.",
                            "atomicExpansion": {
                              "steps": [
                                {
                                  "stepNumber": 1,
                                  "title": "Compreender o Contexto e Objetivos da Análise",
                                  "subSteps": [
                                    "Identifique o problema de negócio ou pergunta de pesquisa que a análise visa responder.",
                                    "Defina os objetivos claros da análise, como identificar padrões ou anomalias nos dados.",
                                    "Revise as etapas gerais da Ciência de Dados: coleta, limpeza, EDA, modelagem e deployment.",
                                    "Liste as variáveis relevantes e hipóteses iniciais baseadas no domínio do problema.",
                                    "Documente o escopo da análise para evitar desvios."
                                  ],
                                  "verification": "Crie um documento de planejamento com objetivos e hipóteses listados; revise se cobre o problema principal.",
                                  "estimatedTime": "30 minutos",
                                  "materials": "Notebook para anotações, acesso a documentação do dataset ou problema de negócio.",
                                  "tips": "Sempre comece pelo 'porquê' da análise para manter o foco nos insights acionáveis.",
                                  "learningObjective": "Entender como alinhar a análise com objetivos reais da Ciência de Dados.",
                                  "commonMistakes": "Pular esta etapa e mergulhar nos dados sem contexto, levando a análises irrelevantes."
                                },
                                {
                                  "stepNumber": 2,
                                  "title": "Preparar e Explorar os Dados Inicialmente (Início da EDA)",
                                  "subSteps": [
                                    "Carregue o dataset usando ferramentas como Python (Pandas) ou R.",
                                    "Realize verificações básicas: dimensões, tipos de dados, valores ausentes e duplicatas.",
                                    "Gere resumos estatísticos descritivos (média, mediana, desvio padrão).",
                                    "Crie visualizações iniciais como histogramas e boxplots para distribuição de variáveis.",
                                    "Identifique outliers preliminares usando regras como IQR."
                                  ],
                                  "verification": "Gere um relatório inicial com estatísticas descritivas e gráficos; confirme ausência de erros óbvios de carregamento.",
                                  "estimatedTime": "45 minutos",
                                  "materials": "Python/Jupyter Notebook com bibliotecas Pandas, Matplotlib/Seaborn; dataset de exemplo (ex: Iris ou Titanic).",
                                  "tips": "Use df.describe() no Pandas para um overview rápido e salve gráficos para o relatório.",
                                  "learningObjective": "Dominar as técnicas iniciais de EDA para limpeza e compreensão básica dos dados.",
                                  "commonMistakes": "Ignorar valores ausentes, assumindo dados perfeitos, o que distorce insights."
                                },
                                {
                                  "stepNumber": 3,
                                  "title": "Realizar Análise Estatística e Exploratória Profunda",
                                  "subSteps": [
                                    "Calcule correlações entre variáveis (matriz de correlação e heatmap).",
                                    "Aplique testes estatísticos como t-test ou chi-quadrado para hipóteses.",
                                    "Segmente dados (group by) e compare subgrupos com médias e visualizações.",
                                    "Explore relações multivariadas com scatterplots e pairplots.",
                                    "Documente padrões emergentes, como tendências ou clusters."
                                  ],
                                  "verification": "Produza gráficos de correlação e resultados de testes; interprete se há associações significativas.",
                                  "estimatedTime": "60 minutos",
                                  "materials": "Bibliotecas SciPy/Statsmodels para testes; Seaborn para visualizações avançadas.",
                                  "tips": "Filtre p-values < 0.05 para significância, mas valide com visualizações para evitar falsos positivos.",
                                  "learningObjective": "Aplicar ferramentas estatísticas para extrair padrões e relações nos dados.",
                                  "commonMistakes": "Confundir correlação com causalidade sem testes apropriados."
                                },
                                {
                                  "stepNumber": 4,
                                  "title": "Interpretar Resultados e Gerar Insights",
                                  "subSteps": [
                                    "Sintetize achados da EDA em narrativas claras, ligando de volta aos objetivos.",
                                    "Priorize insights acionáveis, quantificando impactos (ex: 'aumento de 20% em vendas').",
                                    "Valide insights com subamostras ou dados adicionais.",
                                    "Identifique limitações da análise (ex: viés de amostra).",
                                    "Prepare um sumário executivo com recomendações."
                                  ],
                                  "verification": "Escreva um relatório de 1 página com 3-5 insights principais e evidências.",
                                  "estimatedTime": "45 minutos",
                                  "materials": "Ferramenta de relatório como Jupyter ou Google Docs.",
                                  "tips": "Use storytelling: problema > evidência > insight > ação.",
                                  "learningObjective": "Transformar dados analisados em conhecimento prático e comunicável.",
                                  "commonMistakes": "Listar estatísticas sem contexto, resultando em insights vagos."
                                }
                              ],
                              "practicalExample": "Ao analisar um dataset de vendas de uma loja online (colunas: data, produto, vendas, região), aplique EDA para descobrir que vendas caem 30% em regiões sul nas segundas-feiras devido a baixa tráfego de site, sugerindo campanha de marketing direcionada.",
                              "finalVerifications": [
                                "Pode listar e explicar as 4 etapas principais do processo de análise?",
                                "Descreve corretamente o papel da EDA no pipeline de Ciência de Dados?",
                                "Identifica pelo menos 3 técnicas estatísticas usadas na EDA?",
                                "Gera insights interpretáveis de um dataset simples?",
                                "Documenta limitações e próximos passos na análise?",
                                "Comunica o processo de forma clara e sequencial?"
                              ],
                              "assessmentCriteria": [
                                "Clareza e precisão na descrição das etapas (80% cobertura das componentes EDA).",
                                "Uso correto de terminologia estatística e referências à Ciência de Dados.",
                                "Profundidade dos subpassos e exemplos práticos em cada etapa.",
                                "Capacidade de ligar análise a insights acionáveis.",
                                "Completude do relatório com visualizações e validações.",
                                "Identificação de erros comuns e limitações."
                              ],
                              "crossCurricularConnections": [
                                "Estatística: Aplicação de testes de hipótese e medidas descritivas.",
                                "Programação: Uso de Python/R para manipulação e visualização de dados.",
                                "Negócios: Geração de insights para tomada de decisão estratégica.",
                                "Comunicação: Estruturação de relatórios e storytelling de dados."
                              ],
                              "realWorldApplication": "Em empresas como Netflix, EDA é usada para analisar padrões de visualização de usuários, identificando conteúdos populares por região para otimizar recomendações e aumentar retenção em 15-20%."
                            },
                            "estimatedTime": "1 hora",
                            "difficulty": "beginner",
                            "status": "not_started",
                            "prerequisites": []
                          },
                          {
                            "id": "10.1.3.3.2.2",
                            "name": "Aplicar análise básica a exemplos",
                            "description": "Realizar análise simples em conjuntos de dados fictícios para demonstrar a transição de informação para conhecimento.",
                            "atomicExpansion": {
                              "steps": [
                                {
                                  "stepNumber": 1,
                                  "title": "Selecionar e Compreender o Conjunto de Dados Fictício",
                                  "subSteps": [
                                    "Escolha um conjunto de dados fictício simples, como notas de alunos em uma turma (ex: nomes, notas em matemática, ciências e média).",
                                    "Liste as colunas e tipos de dados (numéricos, categóricos).",
                                    "Calcule estatísticas descritivas básicas: média, mediana, mínimo e máximo.",
                                    "Registre observações iniciais sobre a estrutura dos dados.",
                                    "Identifique a 'informação' presente nos dados brutos."
                                  ],
                                  "verification": "Confirme que estatísticas descritivas foram calculadas corretamente e anotadas em um relatório simples.",
                                  "estimatedTime": "20 minutos",
                                  "materials": [
                                    "Conjunto de dados fictício em planilha (Excel ou Google Sheets)",
                                    "Calculadora ou software de planilhas"
                                  ],
                                  "tips": "Comece com dados pequenos (10-20 linhas) para evitar sobrecarga.",
                                  "learningObjective": "Compreender a estrutura de dados como base para análise.",
                                  "commonMistakes": [
                                    "Ignorar tipos de dados",
                                    "Confundir informação bruta com análise",
                                    "Não anotar observações iniciais"
                                  ]
                                },
                                {
                                  "stepNumber": 2,
                                  "title": "Identificar Padrões Básicos na Informação",
                                  "subSteps": [
                                    "Ordene os dados por uma coluna chave (ex: por média de notas).",
                                    "Agrupe dados em categorias (ex: aprovados/reprovados baseados em nota >7).",
                                    "Calcule proporções ou percentuais (ex: % de alunos com média acima de 8).",
                                    "Crie uma tabela resumida destacando padrões.",
                                    "Questione: 'O que esses padrões revelam além dos números crus?'"
                                  ],
                                  "verification": "Tabela de padrões criada e padrões explicados em 2-3 frases.",
                                  "estimatedTime": "25 minutos",
                                  "materials": [
                                    "Planilha com dados do Step 1",
                                    "Papel e caneta para esboços"
                                  ],
                                  "tips": "Use gráficos simples como barras para visualizar padrões intuitivamente.",
                                  "learningObjective": "Detectar padrões que transformam dados em insights iniciais.",
                                  "commonMistakes": [
                                    "Focar só em números absolutos sem proporções",
                                    "Não agrupar dados logicamente",
                                    "Ignorar outliers"
                                  ]
                                },
                                {
                                  "stepNumber": 3,
                                  "title": "Realizar Análise Simples e Gerar Insights",
                                  "subSteps": [
                                    "Aplique uma análise básica: correlação simples (ex: notas em matemática vs. média geral).",
                                    "Crie um gráfico de dispersão ou histograma para visualizar relações.",
                                    "Interprete: 'Essa correlação sugere que boas notas em matemática preveem médias altas?'",
                                    "Documente 2-3 insights derivados da análise.",
                                    "Diferencie: informação (fatos) vs. conhecimento (interpretação)."
                                  ],
                                  "verification": "Gráficos criados e insights anotados com distinção clara entre informação e conhecimento.",
                                  "estimatedTime": "30 minutos",
                                  "materials": [
                                    "Software de planilhas com gráficos (Excel/Google Sheets)",
                                    "Tutorial rápido de gráficos se necessário"
                                  ],
                                  "tips": "Mantenha análises univariadas ou bivariadas para simplicidade.",
                                  "learningObjective": "Aplicar ferramentas básicas para extrair conhecimento da informação.",
                                  "commonMistakes": [
                                    "Sobreinterpretar correlações como causalidade",
                                    "Gráficos mal rotulados",
                                    "Pular interpretação"
                                  ]
                                },
                                {
                                  "stepNumber": 4,
                                  "title": "Concluir e Demonstrar Transição para Conhecimento",
                                  "subSteps": [
                                    "Resuma os insights em um parágrafo coeso.",
                                    "Explique como a análise transformou dados (raw) → informação (descritivos) → conhecimento (insights acionáveis).",
                                    "Proponha uma recomendação baseada no conhecimento (ex: focar em matemática para melhorar médias).",
                                    "Revise o relatório completo para clareza.",
                                    "Compartilhe com um par para feedback inicial."
                                  ],
                                  "verification": "Relatório final com diagrama da transição e recomendação clara.",
                                  "estimatedTime": "15 minutos",
                                  "materials": [
                                    "Relatório dos steps anteriores",
                                    "Ferramenta de texto (Word/Google Docs)"
                                  ],
                                  "tips": "Use um fluxograma simples: Dados → Informação → Análise → Conhecimento.",
                                  "learningObjective": "Demonstrar a cadeia de valor da análise de dados.",
                                  "commonMistakes": [
                                    "Não fechar o loop com recomendação",
                                    "Confundir etapas da transição",
                                    "Relatório desorganizado"
                                  ]
                                }
                              ],
                              "practicalExample": "Analise um conjunto fictício de vendas de uma cafeteria: colunas (data, item, quantidade, preço). Calcule total diário, item mais vendido e identifique padrão sazonal (ex: mais café pela manhã), transformando vendas brutas em insight: 'Café da manhã impulsiona 60% das receitas'.",
                              "finalVerifications": [
                                "Pode distinguir dados brutos de informação e conhecimento com exemplos do relatório.",
                                "Relatório contém pelo menos 3 insights derivados de análise básica.",
                                "Gráficos ou tabelas resumem padrões corretamente.",
                                "Recomendação prática baseada em conhecimento é acionável.",
                                "Transição Dados→Informação→Conhecimento é explicitamente mapeada.",
                                "Análise aplicada sem erros matemáticos básicos."
                              ],
                              "assessmentCriteria": [
                                "Precisão das estatísticas descritivas e cálculos (30%)",
                                "Clareza na identificação e interpretação de padrões (25%)",
                                "Qualidade visual e explicativa dos gráficos/tabelas (20%)",
                                "Demonstração explícita da transição para conhecimento (15%)",
                                "Recomendação prática e realista (10%)"
                              ],
                              "crossCurricularConnections": [
                                "Estatística: Cálculos descritivos e visualizações.",
                                "Programação: Uso de planilhas como introdução a scripts de dados.",
                                "Negócios: Insights para decisões gerenciais.",
                                "Linguagem: Redação clara de relatórios e insights."
                              ],
                              "realWorldApplication": "Em empresas, analistas usam isso para transformar logs de vendas em estratégias de estoque; em saúde, dados de pacientes viram recomendações preventivas; em educação, notas viram planos de melhoria curricular."
                            },
                            "estimatedTime": "1 hora",
                            "difficulty": "intermediate",
                            "status": "not_started",
                            "prerequisites": [
                              "10.1.3.3.1"
                            ]
                          },
                          {
                            "id": "10.1.3.3.2.3",
                            "name": "Identificar ferramentas de análise",
                            "description": "Listar e descrever ferramentas iniciais como visualização e estatísticas descritivas usadas na análise de informação na Ciência de Dados.",
                            "atomicExpansion": {
                              "steps": [
                                {
                                  "stepNumber": 1,
                                  "title": "Compreender o Conceito de Ferramentas de Análise de Informação",
                                  "subSteps": [
                                    "Defina análise de informação como o processo de transformar dados brutos em insights úteis.",
                                    "Explique a diferença entre dados, informação e conhecimento no contexto da Ciência de Dados.",
                                    "Identifique categorias iniciais de ferramentas: estatísticas descritivas e visualização.",
                                    "Pesquise exemplos básicos de cada categoria em fontes confiáveis.",
                                    "Anote pelo menos 3 definições chave relacionadas."
                                  ],
                                  "verification": "Crie um mapa mental ou resumo escrito com definições e categorias corretas.",
                                  "estimatedTime": "30 minutos",
                                  "materials": "Notebook, acesso à internet para leitura de artigos introdutórios (ex: Wikipedia Ciência de Dados, Khan Academy).",
                                  "tips": "Use analogias cotidianas, como resumir notas de uma aula para estudar.",
                                  "learningObjective": "Entender o papel fundamental das ferramentas de análise na conversão de dados em informação.",
                                  "commonMistakes": "Confundir análise descritiva com preditiva; ignorar o contexto hierárquico (dados > informação > conhecimento)."
                                },
                                {
                                  "stepNumber": 2,
                                  "title": "Explorar Estatísticas Descritivas como Ferramenta Inicial",
                                  "subSteps": [
                                    "Liste medidas centrais: média, mediana, moda.",
                                    "Descreva medidas de dispersão: variância, desvio padrão, amplitude.",
                                    "Calcule exemplos simples manualmente com um dataset pequeno (ex: 10 números de idades).",
                                    "Explique como essas estatísticas resumem dados para análise de informação.",
                                    "Compare usos em cenários reais, como análise de salários."
                                  ],
                                  "verification": "Resolva 3 exercícios de cálculo e explique resultados em um parágrafo.",
                                  "estimatedTime": "45 minutos",
                                  "materials": "Calculadora, planilha Excel ou Google Sheets, dataset de exemplo (ex: CSV com dados de vendas).",
                                  "tips": "Sempre verifique unidades e outliers antes de calcular.",
                                  "learningObjective": "Dominar estatísticas descritivas para resumir e descrever conjuntos de dados.",
                                  "commonMistakes": "Usar média em dados assimétricos sem mediana; esquecer de interpretar os resultados."
                                },
                                {
                                  "stepNumber": 3,
                                  "title": "Investigar Visualização de Dados como Ferramenta de Análise",
                                  "subSteps": [
                                    "Identifique tipos comuns: histogramas, boxplots, gráficos de barras e dispersão.",
                                    "Descreva o propósito de cada um (ex: histograma para distribuição).",
                                    "Crie visualizações simples usando ferramentas gratuitas.",
                                    "Analise como visualizações revelam padrões não visíveis em números crus.",
                                    "Compare visualização com estatísticas descritivas."
                                  ],
                                  "verification": "Gere 3 gráficos de um dataset e descreva insights obtidos.",
                                  "estimatedTime": "50 minutos",
                                  "materials": "Google Sheets, Tableau Public (gratuito) ou Python com Matplotlib (se disponível), dataset exemplo.",
                                  "tips": "Escolha o gráfico certo para o tipo de dados (categórico vs numérico).",
                                  "learningObjective": "Aplicar visualizações para identificar padrões e tendências em dados.",
                                  "commonMistakes": "Sobrecarregar gráficos com dados excessivos; ignorar escalas inadequadas."
                                },
                                {
                                  "stepNumber": 4,
                                  "title": "Listar e Descrever Ferramentas Iniciais Integradas",
                                  "subSteps": [
                                    "Compile uma lista de 5-7 ferramentas: média, mediana, histograma, boxplot, etc.",
                                    "Para cada uma, escreva: definição, uso na análise de informação, exemplo.",
                                    "Crie uma tabela comparativa destacando forças e limitações.",
                                    "Simule uma análise completa usando pelo menos 3 ferramentas em um dataset.",
                                    "Reflita sobre como essas ferramentas constroem conhecimento a partir de informação."
                                  ],
                                  "verification": "Produza uma tabela ou relatório com lista completa e análise simulada.",
                                  "estimatedTime": "40 minutos",
                                  "materials": "Planilha ou documento Google Docs, datasets de prática (ex: Iris dataset).",
                                  "tips": "Priorize ferramentas acessíveis para iniciantes; foque em descrições concisas.",
                                  "learningObjective": "Integrar ferramentas para uma análise inicial abrangente de informação.",
                                  "commonMistakes": "Listar ferramentas avançadas como PCA; não conectar de volta à definição de conhecimento."
                                }
                              ],
                              "practicalExample": "Analise um dataset de vendas de uma loja (colunas: produto, quantidade, preço). Calcule média de vendas, crie histograma de quantidades e boxplot de preços para identificar produtos mais vendidos e variações de preço, gerando insights como 'Produto A tem alta dispersão, sugerindo estoque irregular'.",
                              "finalVerifications": [
                                "Lista pelo menos 6 ferramentas iniciais com descrições precisas.",
                                "Explica corretamente o papel de estatísticas descritivas e visualização na análise.",
                                "Demonstra cálculos ou visualizações corretas em um dataset de exemplo.",
                                "Identifica limitações das ferramentas descritivas (ex: não preditivas).",
                                "Conecta ferramentas ao fluxo dados > informação > conhecimento.",
                                "Produz uma tabela ou resumo organizado."
                              ],
                              "assessmentCriteria": [
                                "Precisão conceitual (correção de definições e descrições): 30%",
                                "Completude da lista e exemplos práticos: 25%",
                                "Qualidade de visualizações e cálculos: 20%",
                                "Clareza e organização na apresentação: 15%",
                                "Conexões com contexto da Ciência de Dados: 10%"
                              ],
                              "crossCurricularConnections": [
                                "Estatística: Fundamentos de medidas descritivas.",
                                "Programação: Uso de bibliotecas como Pandas e Matplotlib em Python.",
                                "Matemática: Cálculos básicos de média e variância.",
                                "Informática: Manipulação de dados em planilhas e software de visualização."
                              ],
                              "realWorldApplication": "Em empresas de e-commerce, usar estatísticas descritivas e gráficos para analisar padrões de vendas diárias, identificando produtos populares e otimizando estoques; em saúde, resumir dados de pacientes para detectar tendências epidemiológicas iniciais."
                            },
                            "estimatedTime": "0.5 horas",
                            "difficulty": "beginner",
                            "status": "not_started",
                            "prerequisites": []
                          }
                        ]
                      },
                      {
                        "id": "10.1.3.3.3",
                        "name": "Aplicação com Experiência e Tomada de Decisões",
                        "description": "Integrar experiência humana ou contextual à informação analisada para gerar conhecimento que suporte compreensão e decisões estratégicas.",
                        "specificSkills": [
                          {
                            "id": "10.1.3.3.3.1",
                            "name": "Integrar experiência ao conhecimento",
                            "description": "Explicar o papel da experiência (contexto domain-specific) na aplicação de informação analisada, ilustrando com casos de Ciência de Dados.",
                            "atomicExpansion": {
                              "steps": [
                                {
                                  "stepNumber": 1,
                                  "title": "Identificar Experiência Domain-Specific Relevante",
                                  "subSteps": [
                                    "Revise seu background em Ciência de Dados: liste projetos passados, problemas resolvidos e lições aprendidas.",
                                    "Defina o contexto específico: por exemplo, previsão de churn em telecomunicações ou detecção de fraudes em finanças.",
                                    "Documente 3-5 experiências chave que influenciaram decisões anteriores em cenários semelhantes."
                                  ],
                                  "verification": "Lista documentada de experiências com descrições breves (mínimo 3 itens).",
                                  "estimatedTime": "20 minutos",
                                  "materials": [
                                    "Notebook ou documento digital",
                                    "Histórico pessoal de projetos em Ciência de Dados"
                                  ],
                                  "tips": "Foquem em experiências que envolvam incertezas ou ambiguidades nos dados.",
                                  "learningObjective": "Reconhecer como experiências passadas fornecem contexto intuitivo para dados.",
                                  "commonMistakes": "Ignorar experiências irrelevantes ou superestimar o impacto de uma única experiência."
                                },
                                {
                                  "stepNumber": 2,
                                  "title": "Analisar Informação e Dados Disponíveis",
                                  "subSteps": [
                                    "Colete e resuma a informação analisada: estatísticas descritivas, visualizações e insights iniciais.",
                                    "Identifique gaps na informação onde intuição domain-specific pode ajudar.",
                                    "Compare a informação atual com padrões observados em experiências passadas."
                                  ],
                                  "verification": "Resumo escrito da informação com gaps destacados e comparações iniciais.",
                                  "estimatedTime": "30 minutos",
                                  "materials": [
                                    "Dataset exemplo (ex: Kaggle churn dataset)",
                                    "Ferramentas: Jupyter Notebook, Pandas"
                                  ],
                                  "tips": "Use gráficos para visualizar padrões e facilitar comparações.",
                                  "learningObjective": "Diferenciar informação pura de conhecimento contextualizado.",
                                  "commonMistakes": "Sobrecarregar com dados irrelevantes, perdendo foco nos gaps acionáveis."
                                },
                                {
                                  "stepNumber": 3,
                                  "title": "Integrar Experiência à Informação para Formar Conhecimento Aplicado",
                                  "subSteps": [
                                    "Mapeie conexões: para cada gap na informação, aplique insights da experiência.",
                                    "Crie hipóteses baseadas na integração, como 'baseado na experiência X, priorizar feature Y'.",
                                    "Refine o conhecimento: ajuste modelos ou decisões com base nessa fusão."
                                  ],
                                  "verification": "Diagrama ou tabela mostrando mapeamentos e hipóteses geradas.",
                                  "estimatedTime": "45 minutos",
                                  "materials": [
                                    "Ferramentas de diagramação (Draw.io ou papel)",
                                    "Código em Python para prototipagem rápida"
                                  ],
                                  "tips": "Use analogias de casos passados para validar hipóteses intuitivamente.",
                                  "learningObjective": "Aplicar experiência para transformar informação em decisões acionáveis.",
                                  "commonMistakes": "Forçar integrações sem evidência, levando a vieses confirmationais."
                                },
                                {
                                  "stepNumber": 4,
                                  "title": "Ilustrar e Validar a Integração com Casos de Ciência de Dados",
                                  "subSteps": [
                                    "Escolha um caso real: ex. integração de experiência em tuning de hiperparâmetros para evitar overfitting.",
                                    "Desenvolva narrativa: explique antes/depois da integração.",
                                    "Teste em mini-projeto: rode modelo com e sem experiência integrada e compare métricas."
                                  ],
                                  "verification": "Relato ilustrativo com métricas comparativas (ex: AUC melhorada em 5%).",
                                  "estimatedTime": "60 minutos",
                                  "materials": [
                                    "Dataset público de Ciência de Dados",
                                    "Scikit-learn para modelagem"
                                  ],
                                  "tips": "Registre métricas quantitativas para objetividade.",
                                  "learningObjective": "Demonstrar valor prático da experiência na Ciência de Dados.",
                                  "commonMistakes": "Usar exemplos genéricos em vez de domain-specific, reduzindo credibilidade."
                                }
                              ],
                              "practicalExample": "Em um projeto de previsão de vendas, dados mostram sazonalidade, mas experiência passada revela impacto de promoções locais não capturadas; integrar ajusta modelo para +15% precisão.",
                              "finalVerifications": [
                                "Lista de experiências domain-specific documentada.",
                                "Gaps na informação identificados e mapeados à experiência.",
                                "Hipóteses testadas com métricas melhoradas.",
                                "Narrativa ilustrativa completa com caso de Ciência de Dados."
                              ],
                              "assessmentCriteria": [
                                "Clareza na distinção entre informação e experiência (nota 1-5).",
                                "Profundidade das conexões mapeadas (mín. 3 exemplos concretos).",
                                "Melhoria mensurável em aplicação prática (ex: métrica de modelo).",
                                "Narrativa coerente e ilustrativa.",
                                "Ausência de vieses não mitigados."
                              ],
                              "crossCurricularConnections": [
                                "Estatística: Uso de priors bayesianos inspirados em experiência.",
                                "Programação: Implementação de features baseadas em domínio.",
                                "Negócios: Alinhamento de modelos com estratégia empresarial.",
                                "Psicologia Cognitiva: Reconhecimento de heurísticas expertais."
                              ],
                              "realWorldApplication": "Data scientists em empresas como Netflix integram experiência de padrões de usuário para refinar recomendações, superando limitações de dados puros e acelerando decisões em produção."
                            },
                            "estimatedTime": "0.5 horas",
                            "difficulty": "beginner",
                            "status": "not_started",
                            "prerequisites": [
                              "10.1.3.3.2"
                            ]
                          },
                          {
                            "id": "10.1.3.3.3.2",
                            "name": "Relacionar conhecimento à tomada de decisões",
                            "description": "Demonstrar como conhecimento permite decisões informadas, com exemplos de modelagem e avaliação em projetos de dados.",
                            "atomicExpansion": {
                              "steps": [
                                {
                                  "stepNumber": 1,
                                  "title": "Revisar Conceitos Fundamentais de Dados, Informação e Conhecimento",
                                  "subSteps": [
                                    "Defina dados como fatos brutos e não processados.",
                                    "Explique informação como dados processados com contexto.",
                                    "Descreva conhecimento como informação aplicada com experiência e insight.",
                                    "Diferencie os três com um diagrama simples.",
                                    "Discuta como o conhecimento emerge da experiência acumulada."
                                  ],
                                  "verification": "Crie um fluxograma mostrando a progressão de dados para conhecimento e compartilhe para revisão.",
                                  "estimatedTime": "30 minutos",
                                  "materials": "Papel e caneta ou ferramenta digital como Draw.io; acesso a definições online de ciência de dados.",
                                  "tips": "Use analogias cotidianas, como receita de bolo (dados=ingredientes, informação=proporções, conhecimento=ajustes baseados em testes).",
                                  "learningObjective": "Compreender a hierarquia de dados, informação e conhecimento como base para decisões.",
                                  "commonMistakes": "Confundir informação com conhecimento, ignorando o papel da experiência."
                                },
                                {
                                  "stepNumber": 2,
                                  "title": "Analisar Exemplos de Conhecimento Informando Decisões",
                                  "subSteps": [
                                    "Selecione um caso real de projeto de dados, como previsão de vendas.",
                                    "Identifique dados brutos (vendas passadas) e como viram conhecimento (padrões sazonais).",
                                    "Descreva a decisão tomada (aumentar estoque em dezembro).",
                                    "Compare decisão com e sem conhecimento.",
                                    "Registre insights sobre o impacto da experiência."
                                  ],
                                  "verification": "Escreva um parágrafo resumindo um exemplo onde conhecimento evitou uma má decisão.",
                                  "estimatedTime": "45 minutos",
                                  "materials": "Artigos ou vídeos curtos sobre casos de ciência de dados (ex: Kaggle datasets); notebook para anotações.",
                                  "tips": "Pergunte 'E se não tivéssemos esse conhecimento?' para destacar valor.",
                                  "learningObjective": "Reconhecer como conhecimento transforma dados em ações informadas.",
                                  "commonMistakes": "Focar apenas em dados sem ligar à experiência prática."
                                },
                                {
                                  "stepNumber": 3,
                                  "title": "Modelar um Projeto de Dados Simples",
                                  "subSteps": [
                                    "Escolha um dataset simples (ex: vendas mensais).",
                                    "Limpe e processe dados para gerar informação (médias, tendências).",
                                    "Aplique experiência para criar conhecimento (considerar fatores externos como feriados).",
                                    "Construa um modelo básico de regressão ou visualização.",
                                    "Documente suposições baseadas em conhecimento prévio."
                                  ],
                                  "verification": "Gere um gráfico ou tabela mostrando o modelo e explique-o oralmente.",
                                  "estimatedTime": "60 minutos",
                                  "materials": "Python com pandas e matplotlib ou Google Sheets; dataset sample de vendas (ex: de UCI ML Repository).",
                                  "tips": "Comece pequeno para evitar sobrecarga; valide modelo com dados conhecidos.",
                                  "learningObjective": "Aplicar modelagem para converter informação em conhecimento acionável.",
                                  "commonMistakes": "Sobrecarregar o modelo com variáveis desnecessárias sem justificativa experencial."
                                },
                                {
                                  "stepNumber": 4,
                                  "title": "Avaliar Modelo e Simular Tomada de Decisão",
                                  "subSteps": [
                                    "Avalie precisão do modelo com métricas (ex: RMSE).",
                                    "Simule cenários: decisão baseada em dados vs. conhecimento.",
                                    "Escolha uma decisão (ex: investir em marketing) e justifique com conhecimento.",
                                    "Reflita sobre incertezas e como experiência mitiga riscos.",
                                    "Atualize o modelo com novos insights."
                                  ],
                                  "verification": "Produza um relatório de 1 página com decisão recomendada e racional.",
                                  "estimatedTime": "45 minutos",
                                  "materials": "Ferramentas de modelagem usadas no step 3; template de relatório simples.",
                                  "tips": "Use 'what-if' analysis para testar robustez da decisão.",
                                  "learningObjective": "Demonstrar avaliação crítica levando a decisões informadas.",
                                  "commonMistakes": "Ignorar vieses ou incertezas no conhecimento derivado."
                                }
                              ],
                              "practicalExample": "Em um projeto de e-commerce, dados de vendas passadas (brutos) são analisados para identificar padrões sazonais (informação). Com experiência de mercado, isso vira conhecimento: 'aumentar estoque de eletrônicos em Black Friday'. Decisão: alocar 20% mais orçamento, resultando em 15% de aumento nas vendas vs. perda sem esse insight.",
                              "finalVerifications": [
                                "Explicar verbalmente a diferença entre dados, informação e conhecimento com exemplo próprio.",
                                "Apresentar um modelo simples que leva a uma decisão informada.",
                                "Identificar pelo menos dois riscos mitigados pelo conhecimento.",
                                "Simular uma decisão alternativa sem conhecimento e discutir consequências.",
                                "Criar um fluxograma pessoal de como conhecimento guia decisões em projetos de dados.",
                                "Avaliar um modelo de dados de colega usando critérios de conhecimento."
                              ],
                              "assessmentCriteria": [
                                "Clareza na distinção entre dados, informação e conhecimento (20%)",
                                "Profundidade dos exemplos de modelagem e avaliação (25%)",
                                "Precisão na ligação de conhecimento à tomada de decisões (20%)",
                                "Criatividade e realismo no exemplo prático (15%)",
                                "Completude da reflexão sobre erros comuns e tips (10%)",
                                "Qualidade das verificações e conexões interdisciplinares (10%)"
                              ],
                              "crossCurricularConnections": [
                                "Estatística: Uso de métricas para validar conhecimento derivado de dados.",
                                "Programação: Implementação de modelos em Python/R para gerar insights.",
                                "Ética: Consideração de vieses no conhecimento para decisões justas.",
                                "Negócios: Aplicação em análise estratégica e previsão de mercado."
                              ],
                              "realWorldApplication": "Em saúde, conhecimento de dados epidemiológicos permite decisões como alocação de vacinas durante pandemias; em finanças, modela riscos para investimentos informados, reduzindo perdas em 30% em fundos de hedge."
                            },
                            "estimatedTime": "1 hora",
                            "difficulty": "intermediate",
                            "status": "not_started",
                            "prerequisites": [
                              "10.1.3.3.1",
                              "10.1.3.3.2"
                            ]
                          },
                          {
                            "id": "10.1.3.3.3.3",
                            "name": "Avaliar impactos éticos na aplicação",
                            "description": "Discutir considerações éticas na aplicação de conhecimento derivado de dados, alinhando com princípios da Ciência de Dados.",
                            "atomicExpansion": {
                              "steps": [
                                {
                                  "stepNumber": 1,
                                  "title": "Identificar Princípios Éticos Relevantes",
                                  "subSteps": [
                                    "Estude os principais princípios éticos em Ciência de Dados, como privacidade, transparência, justiça e não-maleficência.",
                                    "Revise frameworks como os da ACM Code of Ethics ou GDPR para dados.",
                                    "Liste 4-6 princípios aplicáveis ao contexto específico de dados e conhecimento.",
                                    "Relacione cada princípio à definição de conhecimento derivado de dados (dados + experiência + decisões).",
                                    "Documente em uma tabela: Princípio | Definição | Relevância para aplicação."
                                  ],
                                  "verification": "Verifique se a tabela está completa com pelo menos 4 princípios explicados e relacionados ao contexto.",
                                  "estimatedTime": "45 minutos",
                                  "materials": "Documentos éticos (ACM Ethics, GDPR summary), caderno ou Google Docs para tabela.",
                                  "tips": "Comece com princípios universais e adapte ao domínio específico para maior relevância.",
                                  "learningObjective": "Compreender e catalogar princípios éticos fundamentais aplicáveis a aplicações de dados.",
                                  "commonMistakes": "Ignorar princípios culturais ou contextuais, focando apenas em aspectos técnicos."
                                },
                                {
                                  "stepNumber": 2,
                                  "title": "Analisar Cenários de Aplicação Potencial",
                                  "subSteps": [
                                    "Descreva o conhecimento derivado: origem dos dados, experiência usada e decisões envolvidas.",
                                    "Mapeie cenários reais de aplicação (ex: uso em hiring, saúde, finanças).",
                                    "Identifique stakeholders afetados (usuários, sociedade, empresa).",
                                    "Para cada cenário, liste riscos éticos potenciais (viés, discriminação, invasão de privacidade).",
                                    "Classifique riscos por gravidade (alta/média/baixa) com justificativa."
                                  ],
                                  "verification": "Confira se há pelo menos 3 cenários mapeados com stakeholders e riscos classificados.",
                                  "estimatedTime": "60 minutos",
                                  "materials": "Diagrama de fluxograma (Draw.io ou papel), exemplos de casos reais de ética em dados.",
                                  "tips": "Use mind maps para visualizar conexões entre dados, conhecimento e impactos.",
                                  "learningObjective": "Mapear cenários de aplicação e identificar riscos éticos iniciais.",
                                  "commonMistakes": "Focar apenas em benefícios, negligenciando impactos negativos ou em minorias."
                                },
                                {
                                  "stepNumber": 3,
                                  "title": "Avaliar Impactos Positivos e Negativos",
                                  "subSteps": [
                                    "Para cada risco identificado, avalie impactos: probabilidade, magnitude e duração.",
                                    "Pondere benefícios éticos (ex: transparência melhora confiança) vs. malefícios.",
                                    "Use uma matriz de decisão: Impacto Positivo/Negativo | Probabilidade | Mitigação Inicial.",
                                    "Considere trade-offs, como precisão vs. privacidade.",
                                    "Quantifique onde possível (ex: % de viés potencial em dados)."
                                  ],
                                  "verification": "Matriz preenchida com análises qualitativas/quantitativas para todos riscos.",
                                  "estimatedTime": "75 minutos",
                                  "materials": "Planilha Excel/Google Sheets para matriz, calculadora para probabilidades.",
                                  "tips": "Inclua perspectivas múltiplas (jurídica, social) para avaliação equilibrada.",
                                  "learningObjective": "Realizar avaliação balanceada de impactos éticos em aplicações de dados.",
                                  "commonMistakes": "Superestimar benefícios sem evidências ou subestimar riscos indiretos."
                                },
                                {
                                  "stepNumber": 4,
                                  "title": "Propor Estratégias de Mitigação e Documentar",
                                  "subSteps": [
                                    "Desenvolva ações de mitigação para cada risco alto (ex: auditorias de viés, consentimento explícito).",
                                    "Priorize estratégias por custo-benefício e viabilidade.",
                                    "Crie um plano de monitoramento contínuo pós-aplicação.",
                                    "Redija um relatório ético resumindo análise, avaliação e recomendações.",
                                    "Discuta com pares para feedback e refine."
                                  ],
                                  "verification": "Relatório completo com plano de mitigação e seção de monitoramento.",
                                  "estimatedTime": "60 minutos",
                                  "materials": "Template de relatório ético, ferramenta de colaboração (Google Docs).",
                                  "tips": "Torne recomendações SMART (Específicas, Mensuráveis, Alcançáveis, Relevantes, Temporais).",
                                  "learningObjective": "Formular e documentar estratégias éticas acionáveis para aplicações de conhecimento.",
                                  "commonMistakes": "Propor soluções genéricas sem ligação específica aos riscos identificados."
                                }
                              ],
                              "practicalExample": "Ao aplicar um modelo de ML para recrutamento baseado em dados históricos de contratações (conhecimento derivado: padrões de sucesso preditos), avalie: viés de gênero nos dados pode discriminar candidatas mulheres (risco alto); mitigue com rebalanceamento de dados e auditoria transparente.",
                              "finalVerifications": [
                                "Lista completa de princípios éticos relevantes ao contexto.",
                                "Matriz de riscos e impactos preenchida com classificações.",
                                "Relatório ético documentado com recomendações acionáveis.",
                                "Evidência de análise de trade-offs (benefícios vs. riscos).",
                                "Plano de monitoramento pós-aplicação definido.",
                                "Feedback de pares incorporado nas revisões."
                              ],
                              "assessmentCriteria": [
                                "Profundidade na identificação de princípios (cobertura ampla e contextualizada).",
                                "Precisão na análise de riscos e impactos (qualitativa e quantitativa).",
                                "Criatividade e viabilidade das estratégias de mitigação.",
                                "Clareza e estrutura do relatório final.",
                                "Integração de múltiplas perspectivas (social, legal, técnica).",
                                "Demonstração de reflexão crítica sobre trade-offs éticos."
                              ],
                              "crossCurricularConnections": [
                                "Filosofia: Debates sobre utilitarismo vs. deontologia em decisões de dados.",
                                "Direito: Aplicação de leis de proteção de dados (LGPD/GDPR).",
                                "Sociologia: Impactos em desigualdades sociais perpetuadas por algoritmos.",
                                "Psicologia: Viés cognitivo na interpretação de dados e conhecimento."
                              ],
                              "realWorldApplication": "Em empresas como Google ou bancos, avaliar éticamente modelos de crédito que usam dados pessoais para evitar discriminação racial, garantindo compliance regulatório e confiança pública, como no caso do algoritmo COMPAS nos EUA que foi contestado por viés."
                            },
                            "estimatedTime": "0.5 horas",
                            "difficulty": "intermediate",
                            "status": "not_started",
                            "prerequisites": []
                          }
                        ]
                      }
                    ]
                  },
                  {
                    "id": "10.1.3.4",
                    "name": "Distinções entre Dados, Informação e Conhecimento",
                    "description": "Hierarquia e diferenças fundamentais que distinguem os três conceitos na pirâmide DIK.",
                    "individualConcepts": [
                      {
                        "id": "10.1.3.4.1",
                        "name": "Dados",
                        "description": "Dados são elementos brutos, observações ou fatos isolados sem contexto, estrutura ou interpretação, representando a base da pirâmide DIK.",
                        "specificSkills": [
                          {
                            "id": "10.1.3.4.1.1",
                            "name": "Definir dados na pirâmide DIK",
                            "description": "Explicar dados como símbolos, sinais ou valores numéricos/qualitativos sem significado inerente, como números crus ou strings isoladas, diferenciando de informação e conhecimento.",
                            "atomicExpansion": {
                              "steps": [
                                {
                                  "stepNumber": 1,
                                  "title": "Compreender a Pirâmide DIK",
                                  "subSteps": [
                                    "Pesquise a origem e estrutura da pirâmide DIK (Data, Information, Knowledge).",
                                    "Identifique os níveis hierárquicos: Dados na base, seguidos de Informação e Conhecimento.",
                                    "Desenhe ou esboce a pirâmide para visualizar a progressão.",
                                    "Leia definições básicas de cada nível de fontes confiáveis.",
                                    "Anote as setas de fluxo que indicam transformação entre níveis."
                                  ],
                                  "verification": "Crie um diagrama simples da pirâmide DIK rotulado corretamente e explique verbalmente ou por escrito.",
                                  "estimatedTime": "20 minutos",
                                  "materials": "Papel e caneta ou ferramenta digital como Draw.io; artigos online sobre pirâmide DIK.",
                                  "tips": "Use cores diferentes para cada nível para facilitar a memorização visual.",
                                  "learningObjective": "Entender a estrutura hierárquica da pirâmide DIK como base para definições.",
                                  "commonMistakes": "Confundir a ordem dos níveis ou ignorar a progressão ascendente."
                                },
                                {
                                  "stepNumber": 2,
                                  "title": "Definir Dados Precisamente",
                                  "subSteps": [
                                    "Defina dados como símbolos, sinais ou valores numéricos/qualitativos sem significado inerente.",
                                    "Liste sinônimos: números crus, strings isoladas, fatos brutos.",
                                    "Explique ausência de contexto ou interpretação.",
                                    "Compare com exemplos cotidianos: '25' sozinho vs. interpretado.",
                                    "Escreva uma definição em suas próprias palavras."
                                  ],
                                  "verification": "Escreva uma definição de dados em 1-2 frases e cite 3 exemplos sem contexto.",
                                  "estimatedTime": "15 minutos",
                                  "materials": "Caderno de anotações; exemplos de dados reais como listas de números.",
                                  "tips": "Sempre pergunte: 'Isso tem significado sozinho?' para validar se é dado.",
                                  "learningObjective": "Formular uma definição clara e precisa de dados na pirâmide DIK.",
                                  "commonMistakes": "Atribuir significado prematuro, confundindo com informação."
                                },
                                {
                                  "stepNumber": 3,
                                  "title": "Explorar Exemplos e Características de Dados",
                                  "subSteps": [
                                    "Colete exemplos: temperatura '30°C' isolada, ID de usuário '12345', string 'azul'.",
                                    "Classifique cada exemplo como dado explicando por quê (sem contexto).",
                                    "Crie uma tabela com colunas: Exemplo, Tipo (numérico/qualitativo), Sem Significado?.",
                                    "Discuta variações: dados brutos em logs de sensores ou bancos de dados.",
                                    "Gere 5 exemplos originais de dados."
                                  ],
                                  "verification": "Apresente uma tabela com 5 exemplos validados como dados puros.",
                                  "estimatedTime": "25 minutos",
                                  "materials": "Planilha Excel ou Google Sheets; banco de dados de exemplo online.",
                                  "tips": "Use dados reais de APIs públicas para autenticidade.",
                                  "learningObjective": "Reconhecer e gerar exemplos concretos de dados sem interpretação.",
                                  "commonMistakes": "Incluir contexto acidentalmente nos exemplos."
                                },
                                {
                                  "stepNumber": 4,
                                  "title": "Diferenciar Dados de Informação e Conhecimento",
                                  "subSteps": [
                                    "Defina Informação como dados com contexto (ex: '30°C em São Paulo hoje').",
                                    "Defina Conhecimento como informação aplicada e compreendida (ex: 'Usar ar-condicionado').",
                                    "Crie um fluxograma: Dados → +Contexto = Informação → +Experiência = Conhecimento.",
                                    "Compare pares de exemplos em cada nível da pirâmide.",
                                    "Teste transformações: adicione contexto a dados para formar informação."
                                  ],
                                  "verification": "Explique diferenças com 3 pares de exemplos progressivos na pirâmide.",
                                  "estimatedTime": "20 minutos",
                                  "materials": "Ferramenta de fluxograma como Lucidchart; exemplos preparados.",
                                  "tips": "Pense em camadas: dados são blocos de Lego soltos; informação é montada.",
                                  "learningObjective": "Distinguir dados de níveis superiores na pirâmide DIK.",
                                  "commonMistakes": "Subestimar a necessidade de contexto para sair do nível de dados."
                                }
                              ],
                              "practicalExample": "Em um log de sensores de um app de fitness: '175', '82', '25' são dados (altura, peso, idade isolados). Adicionando contexto: '175cm altura, 82kg peso, 25 anos idade de um usuário' vira informação; analisando IMC=26.7 (sobrepeso) vira conhecimento.",
                              "finalVerifications": [
                                "Pode definir dados sem mencionar contexto ou significado?",
                                "Lista pelo menos 5 exemplos puros de dados?",
                                "Explica corretamente a posição de dados na base da pirâmide DIK?",
                                "Diferencia dados de informação com exemplos concretos?",
                                "Desenha a pirâmide DIK com rótulos precisos?",
                                "Identifica erros comuns em definições de pares?"
                              ],
                              "assessmentCriteria": [
                                "Precisão da definição de dados (80% sem ambiguidades).",
                                "Qualidade e quantidade de exemplos (mínimo 5 variados).",
                                "Clareza na diferenciação com informação/conhecimento.",
                                "Completude do diagrama ou fluxograma da pirâmide.",
                                "Profundidade das explicações em sub-steps.",
                                "Ausência de confusões comuns identificadas."
                              ],
                              "crossCurricularConnections": [
                                "Ciência da Computação: Bancos de dados e big data.",
                                "Filosofia: Epistemologia e ontologia da informação.",
                                "Estatística: Dados brutos em análise estatística.",
                                "Negócios: Gerenciamento de dados em BI (Business Intelligence)."
                              ],
                              "realWorldApplication": "Em análise de dados empresariais, reconhecer dados brutos em planilhas de vendas permite limpar e contextualizar para gerar insights acionáveis, como prever demandas de mercado."
                            },
                            "estimatedTime": "0.5 horas",
                            "difficulty": "beginner",
                            "status": "not_started",
                            "prerequisites": []
                          },
                          {
                            "id": "10.1.3.4.1.2",
                            "name": "Identificar exemplos de dados",
                            "description": "Reconhecer exemplos reais de dados, como '25', 'azul' ou '2023-10-01', destacando sua falta de contexto e como eles não transmitem significado sozinhos.",
                            "atomicExpansion": {
                              "steps": [
                                {
                                  "stepNumber": 1,
                                  "title": "Compreender a definição de dados brutos",
                                  "subSteps": [
                                    "Leia a definição oficial de dados como valores isolados sem contexto, como números, textos ou datas.",
                                    "Anote as características principais: falta de interpretação, ausência de relações ou significado.",
                                    "Compare com exemplos cotidianos, como um número aleatório em um papel.",
                                    "Registre em suas palavras: 'Dados são brutos e precisam de contexto para virar informação.'",
                                    "Discuta com um parceiro ou reflita sobre por que '25' sozinho não diz nada."
                                  ],
                                  "verification": "Escreva uma definição pessoal de dados brutos em 2-3 frases e verifique se menciona 'falta de contexto'.",
                                  "estimatedTime": "15 minutos",
                                  "materials": "Folha de papel, caneta, acesso à definição de dados (livro ou site educacional).",
                                  "tips": "Use analogia: dados são como ingredientes crus; sem receita, não formam um prato.",
                                  "learningObjective": "Internalizar que dados são elementos isolados sem significado inerente.",
                                  "commonMistakes": "Confundir dados com informação, assumindo que qualquer valor tem significado automático."
                                },
                                {
                                  "stepNumber": 2,
                                  "title": "Reconhecer exemplos clássicos de dados",
                                  "subSteps": [
                                    "Analise exemplos dados: '25' (número), 'azul' (texto), '2023-10-01' (data).",
                                    "Classifique cada um como dado bruto e justifique: 'Não há quem, quando ou por quê'.",
                                    "Liste 3 variações: numérico (42), categórico (vermelho), temporal (14:30).",
                                    "Crie uma tabela simples: Exemplo | Tipo | Por que é dado?",
                                    "Teste-se: Cubra a justificativa e reescreva."
                                  ],
                                  "verification": "Crie uma tabela com 5 exemplos corretamente classificados como dados brutos.",
                                  "estimatedTime": "20 minutos",
                                  "materials": "Planilha ou papel quadriculado, exemplos impressos ou digitais.",
                                  "tips": "Pense em dados como 'pixels soltos' de uma imagem; sozinhos, são ruído.",
                                  "learningObjective": "Identificar padrões em exemplos reais de dados numéricos, textuais e temporais.",
                                  "commonMistakes": "Incluir contexto acidental, como assumir '25' é idade sem especificar."
                                },
                                {
                                  "stepNumber": 3,
                                  "title": "Destacar a ausência de contexto nos dados",
                                  "subSteps": [
                                    "Pegue um exemplo como '25' e pergunte: Quem mediu? Unidade? Propósito?",
                                    "Adicione contexto gradualmente: '25°C em SP hoje' → vira informação.",
                                    "Para cada exemplo anterior, liste 3 perguntas que revelam a falta de contexto.",
                                    "Desenhe um fluxograma: Dados → +Contexto → Informação.",
                                    "Aplique a 2 novos exemplos seus, enfatizando o 'vazio semântico'."
                                  ],
                                  "verification": "Para 3 exemplos, liste perguntas de contexto não respondidas e explique o impacto.",
                                  "estimatedTime": "25 minutos",
                                  "materials": "Papel para fluxograma, marcadores coloridos.",
                                  "tips": "Use a regra '5 Ws': Who, What, When, Where, Why – dados respondem zero.",
                                  "learningObjective": "Demonstrar como dados isolados carecem de significado sem contexto.",
                                  "commonMistakes": "Adicionar contexto imaginário, perdendo o foco no 'bruto'."
                                },
                                {
                                  "stepNumber": 4,
                                  "title": "Praticar identificação em cenários mistos",
                                  "subSteps": [
                                    "Receba uma lista mista: 'João tem 25 anos' (info), '25' (dado), 'azul' (dado).",
                                    "Classifique 10 itens como 'dado' ou 'não dado', justificando.",
                                    "Crie seus próprios 5 itens mistos e classifique.",
                                    "Revise com um checklist: Tem contexto? Transmite significado sozinho?",
                                    "Registre acertos/erros para autoavaliação."
                                  ],
                                  "verification": "Classifique corretamente 90% de uma lista de 10 itens mistos.",
                                  "estimatedTime": "20 minutos",
                                  "materials": "Lista de itens mistos (preparada ou online), checklist impresso.",
                                  "tips": "Dica mnemônica: DADO = Desconectado, Ambíguo, Desprovido de Ordenação.",
                                  "learningObjective": "Diferenciar dados de informação em contextos reais.",
                                  "commonMistakes": "Classificar frases completas como dados, ignorando o contexto embutido."
                                }
                              ],
                              "practicalExample": "Considere a sequência '36.5, 37.2, 36.8'. São medições de temperatura corporal (dados brutos). Sem saber o paciente, data, unidade ou contexto clínico, não transmitem febre ou normalidade – puramente números isolados, inúteis para decisão.",
                              "finalVerifications": [
                                "Listar 5 exemplos corretos de dados brutos sem contexto.",
                                "Explicar verbalmente por que '2023-10-01' sozinho não é informação.",
                                "Classificar 8/10 itens em uma lista mista como dados ou não.",
                                "Identificar falta de contexto em novos exemplos fornecidos.",
                                "Criar um exemplo próprio e justificar como dado bruto.",
                                "Diferenciar dados de informação em um par de exemplos contrastantes."
                              ],
                              "assessmentCriteria": [
                                "Precisão na identificação de dados brutos (sem adicionar contexto).",
                                "Explicação clara da falta de significado isolado.",
                                "Uso correto de exemplos variados (numérico, textual, temporal).",
                                "Capacidade de gerar e justificar exemplos próprios.",
                                "Diferenciação consistente entre dados e informação.",
                                "Completude nas justificativas (menciona contexto explicitamente)."
                              ],
                              "crossCurricularConnections": [
                                "Informática: Entrada de dados em bancos de dados relacionais.",
                                "Estatística: Dados crus antes de análise descritiva.",
                                "Língua Portuguesa: Palavras isoladas vs. frases com sentido.",
                                "Ciências: Medições experimentais sem hipótese ou método."
                              ],
                              "realWorldApplication": "Em sensores IoT de uma fazenda, leituras como '24.5' (umidade) são dados brutos; sem timestamp, localização ou norma de referência, agricultores não podem irrigar adequadamente – contexto transforma em ação decisória."
                            },
                            "estimatedTime": "0.5 horas",
                            "difficulty": "beginner",
                            "status": "not_started",
                            "prerequisites": []
                          },
                          {
                            "id": "10.1.3.4.1.3",
                            "name": "Listar características fundamentais dos dados",
                            "description": "Descrever propriedades como bruteza, ausência de relações, volume potencialmente alto e necessidade de processamento para gerar valor.",
                            "atomicExpansion": {
                              "steps": [
                                {
                                  "stepNumber": 1,
                                  "title": "Identificar e definir dados brutos",
                                  "subSteps": [
                                    "Leia a definição de dados como fatos brutos sem contexto.",
                                    "Anote exemplos de dados brutos, como números isolados ou strings sem significado.",
                                    "Pesquise fontes confiáveis sobre 'raw data' em ciência de dados.",
                                    "Registre a característica principal: bruteza (não processados).",
                                    "Crie uma frase definindo dados brutos em suas próprias palavras."
                                  ],
                                  "verification": "Verifique se você tem uma lista com pelo menos 3 exemplos de dados brutos e uma definição escrita.",
                                  "estimatedTime": "20 minutos",
                                  "materials": [
                                    "Acesso à internet para pesquisa",
                                    "Caderno ou editor de texto",
                                    "Exemplos de conjuntos de dados simples (ex: CSV com números aleatórios)"
                                  ],
                                  "tips": "Use analogias como 'ingredientes crus' para facilitar a compreensão.",
                                  "learningObjective": "Compreender que dados são brutos e carecem de processamento inicial.",
                                  "commonMistakes": "Confundir dados com informação já processada, como relatórios prontos."
                                },
                                {
                                  "stepNumber": 2,
                                  "title": "Explorar ausência de relações e contexto",
                                  "subSteps": [
                                    "Analise um conjunto de dados sem contexto, como uma lista de números sem rótulos.",
                                    "Explique por que esses dados não têm significado inerente ou relações explícitas.",
                                    "Liste 3 cenários onde dados isolados levam a interpretações erradas.",
                                    "Descreva a característica: ausência de relações ou estrutura pré-definida.",
                                    "Compare com dados estruturados para destacar a diferença."
                                  ],
                                  "verification": "Confirme se você pode explicar verbalmente ou por escrito por que dados crus não têm contexto.",
                                  "estimatedTime": "25 minutos",
                                  "materials": [
                                    "Conjunto de dados exemplo (ex: lista de 100 números sem headers)",
                                    "Papel e caneta para diagramas"
                                  ],
                                  "tips": "Desenhe setas entre dados para mostrar como relações precisam ser adicionadas.",
                                  "learningObjective": "Reconhecer que dados requerem contexto externo para ganhar relevância.",
                                  "commonMistakes": "Assumir que volume alto implica relações automáticas entre itens."
                                },
                                {
                                  "stepNumber": 3,
                                  "title": "Analisar volume potencialmente alto e escalabilidade",
                                  "subSteps": [
                                    "Discuta como dados podem gerar volumes massivos (ex: sensores IoT).",
                                    "Calcule ou estime o volume de um exemplo real, como logs diários de um site.",
                                    "Identifique desafios de armazenamento e processamento em alto volume.",
                                    "Registre a característica: volume potencialmente alto e necessidade de ferramentas escaláveis.",
                                    "Pesquise ferramentas como Big Data (Hadoop) para lidar com volume."
                                  ],
                                  "verification": "Verifique se você listou 2 exemplos de alto volume e 2 desafios associados.",
                                  "estimatedTime": "30 minutos",
                                  "materials": [
                                    "Internet para exemplos reais (ex: Kaggle datasets)",
                                    "Calculadora ou planilha para estimativas"
                                  ],
                                  "tips": "Comece com volumes pequenos e escale mentalmente para bilhões de registros.",
                                  "learningObjective": "Entender implicações do volume alto na manipulação de dados.",
                                  "commonMistakes": "Ignorar que volume baixo ainda pode ter as outras características."
                                },
                                {
                                  "stepNumber": 4,
                                  "title": "Compreender necessidade de processamento para valor",
                                  "subSteps": [
                                    "Descreva o pipeline: dados -> processamento -> informação -> conhecimento.",
                                    "Aplique processamento simples a dados brutos (ex: média de números).",
                                    "Liste métodos comuns: limpeza, agregação, análise estatística.",
                                    "Explique por que processamento é essencial para gerar valor.",
                                    "Crie um fluxograma resumindo as 4 características principais."
                                  ],
                                  "verification": "Avalie se seu fluxograma cobre todas as características com exemplos.",
                                  "estimatedTime": "25 minutos",
                                  "materials": [
                                    "Ferramenta de desenho (ex: Draw.io ou papel)",
                                    "Exemplo de dados para processar (Excel ou Python básico)"
                                  ],
                                  "tips": "Sempre pergunte: 'O que esse dado vira após processamento?'",
                                  "learningObjective": "Perceber que valor emerge apenas após transformação dos dados.",
                                  "commonMistakes": "Pensar que dados têm valor intrínseco sem intervenção humana/máquina."
                                }
                              ],
                              "practicalExample": "Dado um arquivo CSV com 1.000 linhas de temperaturas brutas (apenas números como 23.5, 24.1 sem datas ou locais): liste bruteza (sem rótulos), ausência de relações (não se sabe de onde vêm), volume alto (1.000+ entradas), e processe calculando média para gerar informação útil como 'temperatura média de 23°C'.",
                              "finalVerifications": [
                                "O aluno lista corretamente as 4 características principais sem erros.",
                                "Pode fornecer exemplos reais para cada característica.",
                                "Explica diferenças entre dados brutos e informação processada.",
                                "Cria um fluxograma ou lista visual das características.",
                                "Identifica desafios de volume em um cenário dado.",
                                "Demonstra processamento simples de dados brutos."
                              ],
                              "assessmentCriteria": [
                                "Completude: Todas as características fundamentais cobertas (bruteza, ausência de relações, volume, processamento).",
                                "Precisão: Definições alinhadas com conceitos de ciência de dados.",
                                "Profundidade: Exemplos concretos e explicações detalhadas por característica.",
                                "Clareza: Linguagem simples e organizada, sem jargões desnecessários.",
                                "Aplicação: Capacidade de aplicar a exemplos reais.",
                                "Criatividade: Uso de analogias ou visuais para ilustrar conceitos."
                              ],
                              "crossCurricularConnections": [
                                "Estatística: Análise de volume e variabilidade de dados.",
                                "Programação: Manipulação de dados brutos com Python/Pandas.",
                                "Negócios: Uso de dados em decisões empresariais após processamento.",
                                "Filosofia: Discussão sobre conhecimento vs. informação raw."
                              ],
                              "realWorldApplication": "Em empresas de e-commerce, logs de cliques (dados brutos de alto volume sem contexto) são processados para gerar relatórios de comportamento do usuário, otimizando recomendações e aumentando vendas."
                            },
                            "estimatedTime": "0.5 horas",
                            "difficulty": "beginner",
                            "status": "not_started",
                            "prerequisites": []
                          }
                        ]
                      },
                      {
                        "id": "10.1.3.4.2",
                        "name": "Informação",
                        "description": "Informação surge da organização e contextualização dos dados, adicionando significado e relações, posicionando-se no nível intermediário da pirâmide DIK.",
                        "specificSkills": [
                          {
                            "id": "10.1.3.4.2.1",
                            "name": "Definir informação na pirâmide DIK",
                            "description": "Explicar informação como dados processados com contexto, respondendo a perguntas como 'o quê?', 'quem?' ou 'quando?', transformando bruteza em padrões úteis.",
                            "atomicExpansion": {
                              "steps": [
                                {
                                  "stepNumber": 1,
                                  "title": "Compreender a Pirâmide DIK",
                                  "subSteps": [
                                    "Estude a estrutura hierárquica da pirâmide DIK: Dados (base bruta), Informação (dados contextualizados), Conhecimento (padrões interpretados) e Sabedoria (aplicação ética).",
                                    "Identifique as setas de transformação: dados crus → processamento com contexto → informação útil.",
                                    "Anote definições chave: foque na transição de dados para informação.",
                                    "Desenhe um diagrama simples da pirâmide para visualização.",
                                    "Compare com analogias cotidianas, como ingredientes crus vs. receita pronta."
                                  ],
                                  "verification": "Desenhe e explique a pirâmide DIK corretamente para um parceiro de estudo.",
                                  "estimatedTime": "20 minutos",
                                  "materials": "Papel e caneta ou ferramenta digital como Draw.io; vídeo introdutório sobre pirâmide DIK (ex: Khan Academy ou YouTube).",
                                  "tips": "Use cores diferentes para cada nível da pirâmide para facilitar a memorização visual.",
                                  "learningObjective": "Visualizar e internalizar a hierarquia DIK como base conceitual.",
                                  "commonMistakes": "Confundir informação com conhecimento; lembre que informação é o primeiro nível de processamento."
                                },
                                {
                                  "stepNumber": 2,
                                  "title": "Definir Informação na Pirâmide DIK",
                                  "subSteps": [
                                    "Leia a definição: Informação é dados processados com contexto, respondendo 'o quê?', 'quem?', 'quando?' ou 'onde?'.",
                                    "Escreva sua própria definição em 1-2 frases, incorporando 'transformar bruteza em padrões úteis'.",
                                    "Liste 3 perguntas chave que informação responde (ex: 'O quê aconteceu?', 'Quem participou?').",
                                    "Diferencie de dados: dados são brutos (ex: '25'), informação é contextualizada (ex: '25 vendas em 2023').",
                                    "Crie flashcards com definição vs. não-definição."
                                  ],
                                  "verification": "Recite a definição sem consultar notas e dê um exemplo correto.",
                                  "estimatedTime": "15 minutos",
                                  "materials": "Flashcards físicos ou app como Anki; texto de referência sobre pirâmide DIK.",
                                  "tips": "Associe 'contexto' a adicionar 'porquê' parcial, mas sem interpretação profunda (isso é conhecimento).",
                                  "learningObjective": "Formular uma definição precisa e consistente de informação.",
                                  "commonMistakes": "Tratar informação como sinônimo de dados; sempre adicione contexto explícito."
                                },
                                {
                                  "stepNumber": 3,
                                  "title": "Explorar Exemplos de Transformação Dados → Informação",
                                  "subSteps": [
                                    "Pegue dados brutos (ex: lista de números de temperatura: 20, 25, 30).",
                                    "Adicione contexto: 'Temperaturas em São Paulo, Janeiro 2023: média 25°C'.",
                                    "Aplique perguntas: 'O quê?' (temperaturas), 'Quando?' (Janeiro 2023), 'Onde?' (São Paulo).",
                                    "Crie 3 exemplos pessoais: transforme dados do dia a dia em informação.",
                                    "Discuta com um colega: valide se o contexto transforma realmente em informação."
                                  ],
                                  "verification": "Produza 3 pares de dados/informação corretos e explique a transformação.",
                                  "estimatedTime": "25 minutos",
                                  "materials": "Planilha Excel ou Google Sheets para dados; exemplos reais de datasets públicos (ex: Kaggle).",
                                  "tips": "Pergunte sempre: 'Isso responde a uma pergunta factual?' para validar informação.",
                                  "learningObjective": "Praticar a adição de contexto para gerar informação acionável.",
                                  "commonMistakes": "Adicionar interpretação subjetiva cedo demais; mantenha factual."
                                },
                                {
                                  "stepNumber": 4,
                                  "title": "Aplicar e Sintetizar o Conceito",
                                  "subSteps": [
                                    "Analise um dataset real: pegue dados brutos e gere informação respondendo perguntas DIK.",
                                    "Escreva um parágrafo explicando como informação emerge na pirâmide.",
                                    "Crie um quiz autoavaliativo com 5 perguntas sobre definições e exemplos.",
                                    "Compartilhe em fórum ou grupo: receba feedback.",
                                    "Revise erros e refine compreensão."
                                  ],
                                  "verification": "Responda corretamente a um quiz de 5 itens sobre informação na DIK.",
                                  "estimatedTime": "20 minutos",
                                  "materials": "Dataset simples (ex: CSV de vendas); plataforma de quiz como Quizlet.",
                                  "tips": "Use abreviações como 'DIK' em resumos para reforço rápido.",
                                  "learningObjective": "Integrar definição e exemplos em aplicação independente.",
                                  "commonMistakes": "Ignorar o 'contexto relevante'; certifique-se de que seja útil e específico."
                                }
                              ],
                              "practicalExample": "Dados brutos: 'João, 1500'. Informação: 'João vendeu 1500 unidades em Dezembro 2023 na loja X, respondendo 'quem?', 'o quê?' e 'quando?', revelando padrão de vendas sazonais.",
                              "finalVerifications": [
                                "Explicar verbalmente a definição de informação na pirâmide DIK sem erros.",
                                "Transformar 3 conjuntos de dados brutos em informação contextualizada.",
                                "Diferenciar corretamente dados de informação em exemplos dados.",
                                "Identificar perguntas respondidas por informação ('o quê?', 'quem?', etc.).",
                                "Desenhar pirâmide DIK com rótulos precisos.",
                                "Aplicar conceito a um dataset real com contexto adequado."
                              ],
                              "assessmentCriteria": [
                                "Precisão na definição: inclui contexto e perguntas chave (30%).",
                                "Qualidade de exemplos: transformação clara de dados para informação (25%).",
                                "Diferenciação conceitual: distingue de dados/conhecimento (20%).",
                                "Aplicação prática: usa em cenários reais (15%).",
                                "Clareza e originalidade na explicação (10%)."
                              ],
                              "crossCurricularConnections": [
                                "Estatística: Processamento de dados em análise descritiva.",
                                "Filosofia: Epistemologia e níveis de compreensão humana.",
                                "Ciência da Computação: Bancos de dados relacionais adicionando metadados.",
                                "Negócios: Relatórios gerenciais transformando métricas em insights."
                              ],
                              "realWorldApplication": "Em análise de dados empresariais, transforma logs de vendas brutos ('1000') em informação ('1000 acessos ao site em Black Friday 2023 por usuários brasileiros'), permitindo decisões como otimizar estoque."
                            },
                            "estimatedTime": "0.5 horas",
                            "difficulty": "beginner",
                            "status": "not_started",
                            "prerequisites": [
                              "10.1.3.4.1"
                            ]
                          },
                          {
                            "id": "10.1.3.4.2.2",
                            "name": "Explicar a transformação de dados em informação",
                            "description": "Descrever processos como agregação, filtragem ou categorização que adicionam relevância e estrutura aos dados brutos.",
                            "atomicExpansion": {
                              "steps": [
                                {
                                  "stepNumber": 1,
                                  "title": "Compreender a diferença entre dados brutos e informação",
                                  "subSteps": [
                                    "Defina dados brutos como fatos isolados sem contexto, como números ou textos crus.",
                                    "Defina informação como dados processados que ganham relevância e estrutura.",
                                    "Compare exemplos: uma lista de números (dados) vs. média calculada (informação).",
                                    "Discuta por que contexto é essencial na transformação.",
                                    "Crie um diagrama simples de dados → processamento → informação."
                                  ],
                                  "verification": "Crie um diagrama ou tabela comparando 3 exemplos de dados brutos e sua forma como informação.",
                                  "estimatedTime": "30 minutos",
                                  "materials": "Papel, caneta, acesso a um editor de texto ou ferramenta de desenho online (ex: Draw.io).",
                                  "tips": "Use analogias cotidianas, como ingredientes crus vs. receita pronta.",
                                  "learningObjective": "Diferenciar conceitualmente dados brutos de informação processada.",
                                  "commonMistakes": "Confundir informação com conhecimento (que adiciona interpretação humana)."
                                },
                                {
                                  "stepNumber": 2,
                                  "title": "Explorar o processo de filtragem",
                                  "subSteps": [
                                    "Identifique dados irrelevantes em um conjunto bruto.",
                                    "Aplique critérios de filtro, como remover valores fora de um intervalo.",
                                    "Exemplo: filtrar vendas abaixo de R$100 de uma lista de transações.",
                                    "Registre os dados filtrados e explique a relevância adicionada.",
                                    "Teste com diferentes critérios para observar impactos."
                                  ],
                                  "verification": "Filtre um conjunto de 20 dados fornecidos e justifique as escolhas em um relatório curto.",
                                  "estimatedTime": "45 minutos",
                                  "materials": "Planilha Excel ou Google Sheets, conjunto de dados de exemplo (ex: lista de temperaturas).",
                                  "tips": "Sempre documente o critério de filtro para rastreabilidade.",
                                  "learningObjective": "Aplicar filtragem para eliminar ruído e adicionar relevância aos dados.",
                                  "commonMistakes": "Filtrar demais, perdendo dados úteis (over-filtering)."
                                },
                                {
                                  "stepNumber": 3,
                                  "title": "Dominar agregação e categorização",
                                  "subSteps": [
                                    "Aprenda agregação: somar, médias ou contagens em grupos.",
                                    "Exemplo: agregar vendas por mês em uma lista diária.",
                                    "Pratique categorização: agrupar dados em classes (ex: alta/baixa venda).",
                                    "Combine: categorize e agregue (ex: média de vendas por categoria de produto).",
                                    "Visualize resultados com tabelas ou gráficos simples."
                                  ],
                                  "verification": "Agregue e categorize um dataset de 50 itens, produzindo uma tabela resumida.",
                                  "estimatedTime": "60 minutos",
                                  "materials": "Planilha Excel/Google Sheets, dados de exemplo (ex: vendas fictícias em CSV).",
                                  "tips": "Escolha funções de agregação adequadas ao objetivo (média para tendências, soma para totais).",
                                  "learningObjective": "Usar agregação e categorização para estruturar dados brutos.",
                                  "commonMistakes": "Ignorar outliers que distorcem agregações."
                                },
                                {
                                  "stepNumber": 4,
                                  "title": "Integrar processos em um fluxo completo de transformação",
                                  "subSteps": [
                                    "Monte um pipeline: filtre → categorize → agregue dados brutos.",
                                    "Aplique a um caso real: transforme logs de acesso em relatório de uso.",
                                    "Explique como cada passo adiciona relevância e estrutura.",
                                    "Avalie o resultado: os dados agora são informação útil?",
                                    "Refine o processo com base em feedback autoavaliado."
                                  ],
                                  "verification": "Desenvolva um fluxo completo para um dataset dado e apresente o antes/depois.",
                                  "estimatedTime": "45 minutos",
                                  "materials": "Planilha ou Python simples (opcional), dataset exemplo.",
                                  "tips": "Pense no 'porquê' de cada processo para manter foco na relevância.",
                                  "learningObjective": "Sintetizar processos para transformar dados em informação acionável.",
                                  "commonMistakes": "Pular etapas, resultando em informação incompleta."
                                }
                              ],
                              "practicalExample": "Dada uma lista bruta de 100 temperaturas diárias de várias cidades (dados): filtre temperaturas acima de 20°C, categorize por cidade, agregue médias mensais. Resultado: informação sobre tendências climáticas por região, útil para planejamento agrícola.",
                              "finalVerifications": [
                                "Pode definir e exemplificar dados brutos vs. informação.",
                                "Demonstra filtragem correta em um dataset com justificativa.",
                                "Executa agregação e categorização com resultados precisos.",
                                "Integra processos em um pipeline lógico.",
                                "Explica como estrutura e relevância são adicionadas.",
                                "Identifica limitações do processo."
                              ],
                              "assessmentCriteria": [
                                "Clareza na distinção conceitual (20%)",
                                "Precisão nos processos de filtragem/agregação (30%)",
                                "Criatividade e relevância nos exemplos (20%)",
                                "Qualidade da integração e visualização (20%)",
                                "Profundidade na explicação de relevância (10%)"
                              ],
                              "crossCurricularConnections": [
                                "Estatística: Uso de medidas resumidas como médias.",
                                "Programação: Implementação em scripts (Python Pandas).",
                                "Negócios: Análise de dados para relatórios gerenciais.",
                                "Ciências: Processamento de dados experimentais."
                              ],
                              "realWorldApplication": "Em empresas de e-commerce, transformar logs de cliques brutos (dados) via filtragem de usuários ativos, categorização por produto e agregação de taxas de conversão em dashboards de performance, guiando decisões de estoque e marketing."
                            },
                            "estimatedTime": "1 hora",
                            "difficulty": "beginner",
                            "status": "not_started",
                            "prerequisites": [
                              "10.1.3.4.1"
                            ]
                          },
                          {
                            "id": "10.1.3.4.2.3",
                            "name": "Identificar exemplos de informação",
                            "description": "Reconhecer casos como 'A temperatura média em SP é 25°C em outubro', contrastando com dados crus e destacando o contexto adicionado.",
                            "atomicExpansion": {
                              "steps": [
                                {
                                  "stepNumber": 1,
                                  "title": "Entender a definição de dados crus versus informação",
                                  "subSteps": [
                                    "Leia a definição de dados crus: valores isolados sem contexto, como '25°C' ou '1000'.",
                                    "Leia a definição de informação: dados com contexto que adicionam significado, como 'A temperatura média em SP é 25°C em outubro'.",
                                    "Anote as diferenças principais: ausência vs. presença de contexto (quem, o quê, onde, quando, por quê).",
                                    "Crie um diagrama simples comparando os dois conceitos.",
                                    "Discuta com um parceiro ou anote exemplos cotidianos de cada."
                                  ],
                                  "verification": "Confirme se você pode explicar a diferença em suas próprias palavras sem consultar notas.",
                                  "estimatedTime": "20 minutos",
                                  "materials": "Papel e caneta ou editor de texto digital; exemplos impressos ou tela com definições.",
                                  "tips": "Use analogias: dados são como ingredientes soltos; informação é a receita completa.",
                                  "learningObjective": "Diferenciar conceitualmente dados crus de informação contextualizada.",
                                  "commonMistakes": "Confundir qualquer número com informação; ignorar que contexto pode ser implícito mas deve adicionar relevância."
                                },
                                {
                                  "stepNumber": 2,
                                  "title": "Analisar exemplos concretos de informação",
                                  "subSteps": [
                                    "Examine o exemplo dado: 'A temperatura média em SP é 25°C em outubro' – identifique o contexto (média, localização SP, mês outubro).",
                                    "Compare com dados crus: '25°C' sozinho – explique por que não é informação.",
                                    "Encontre 3 outros exemplos: e.g., 'João tem 1.75m' vs. 'João, com 30 anos, tem altura média de 1.75m para homens brasileiros'.",
                                    "Classifique cada exemplo em uma tabela: coluna 'É informação?' e justificativa.",
                                    "Destaque elementos contextuais comuns: unidades, fontes, comparações, propósitos."
                                  ],
                                  "verification": "Crie uma tabela com pelo menos 5 exemplos corretamente classificados.",
                                  "estimatedTime": "25 minutos",
                                  "materials": "Planilha ou papel para tabela; lista de 10 frases mistas (dados e informação).",
                                  "tips": "Pergunte sempre: 'Isso responde a quê/onde/quando?' para detectar contexto.",
                                  "learningObjective": "Reconhecer padrões de contexto que transformam dados em informação.",
                                  "commonMistakes": "Considerar frases longas automaticamente como informação sem verificar relevância contextual."
                                },
                                {
                                  "stepNumber": 3,
                                  "title": "Praticar identificação em cenários variados",
                                  "subSteps": [
                                    "Receba 10 frases aleatórias e classifique cada uma como 'dado cru', 'informação' ou 'nem um nem outro'.",
                                    "Para cada 'informação', liste os elementos contextuais explicitamente.",
                                    "Crie seus próprios 5 exemplos de informação a partir de dados crus simples (e.g., '50' → '50% das vendas em 2023').",
                                    "Teste com um quiz online ou autoavaliação, corrigindo erros imediatamente.",
                                    "Discuta casos ambíguos: e.g., '25°C' em um gráfico com legenda – conta como informação?"
                                  ],
                                  "verification": "Acertar pelo menos 90% em um quiz de 10 itens independentes.",
                                  "estimatedTime": "30 minutos",
                                  "materials": "Quiz impresso ou ferramenta online como Google Forms; timer.",
                                  "tips": "Foque no 'porquê' do contexto: ele torna o dado útil e interpretável?",
                                  "learningObjective": "Aplicar a distinção de forma fluida em exemplos diversos.",
                                  "commonMistakes": "Ignorar contexto implícito em listas ou tabelas; tratar opiniões como informação sem base factual."
                                },
                                {
                                  "stepNumber": 4,
                                  "title": "Consolidar e contrastar com conhecimento",
                                  "subSteps": [
                                    "Compare informação com 'conhecimento': e.g., informação + interpretação = '25°C indica clima ameno em SP'.",
                                    "Revise todos os exemplos anteriores e reclassifique se necessário.",
                                    "Crie um fluxograma de decisão: 'Tem contexto relevante? → Sim: Informação'.",
                                    "Aplique em um texto real: extraia dados e informação de um artigo de notícia.",
                                    "Autoavalie fraquezas e planeje prática adicional."
                                  ],
                                  "verification": "Produzir um fluxograma funcional e analisar corretamente um parágrafo de texto real.",
                                  "estimatedTime": "20 minutos",
                                  "materials": "Artigo de jornal ou site de dados (e.g., IBGE); software de diagrama como Draw.io.",
                                  "tips": "Lembre: informação é o primeiro passo para conhecimento – foque na ponte do contexto.",
                                  "learningObjective": "Integrar o conceito na hierarquia dados-informação-conhecimento.",
                                  "commonMistakes": "Confundir informação com conhecimento subjetivo; superestimar dados com labels mínimas."
                                }
                              ],
                              "practicalExample": "Dado cru: '15 vendas'. Informação: 'A loja teve 15 vendas no dia 10/10/2023, 20% acima da média semanal, devido à promoção de Black Friday antecipada'. Aqui, contexto (data, comparação, causa) adiciona significado acionável.",
                              "finalVerifications": [
                                "Classificar corretamente 10/10 exemplos mistos em um teste final.",
                                "Explicar verbalmente a diferença com um exemplo original.",
                                "Identificar contexto em 5 frases reais de fontes variadas.",
                                "Criar 3 exemplos próprios de informação a partir de dados crus.",
                                "Aplicar fluxograma de decisão sem erros em novos casos.",
                                "Diferenciar informação de conhecimento em 3 pares de exemplos."
                              ],
                              "assessmentCriteria": [
                                "Precisão na identificação de contexto (90%+ acurácia).",
                                "Capacidade de justificar classificações com elementos específicos.",
                                "Criatividade em gerar exemplos originais e relevantes.",
                                "Compreensão da hierarquia dados-informação-conhecimento.",
                                "Aplicação consistente em contextos reais e ambíguos.",
                                "Clareza na explicação oral ou escrita dos conceitos."
                              ],
                              "crossCurricularConnections": [
                                "Língua Portuguesa: Análise de textos informativos e redação contextualizada.",
                                "Ciências: Interpretação de dados experimentais em relatórios.",
                                "História: Distinção entre fatos crus e narrativas contextualizadas.",
                                "Educação Financeira: Análise de relatórios econômicos com contexto."
                              ],
                              "realWorldApplication": "Em ciência de dados, analistas transformam dados crus de sensores (e.g., temperaturas isoladas) em informações acionáveis para previsões climáticas ou negócios, evitando decisões baseadas em números sem contexto, como em dashboards de empresas ou relatórios governamentais."
                            },
                            "estimatedTime": "0.5 horas",
                            "difficulty": "beginner",
                            "status": "not_started",
                            "prerequisites": [
                              "10.1.3.4.1"
                            ]
                          }
                        ]
                      },
                      {
                        "id": "10.1.3.4.3",
                        "name": "Conhecimento",
                        "description": "Conhecimento é a interpretação e aplicação da informação por meio de experiência e regras, permitindo decisões e ações, no topo da pirâmide DIK.",
                        "specificSkills": [
                          {
                            "id": "10.1.3.4.3.1",
                            "name": "Definir conhecimento na pirâmide DIK",
                            "description": "Explicar conhecimento como informação compreendida, internalizada e acionável, envolvendo padrões, causalidade e expertise humana ou modelos.",
                            "atomicExpansion": {
                              "steps": [
                                {
                                  "stepNumber": 1,
                                  "title": "Revisar a Pirâmide DIK",
                                  "subSteps": [
                                    "Estude a hierarquia: Dados (fatos brutos), Informação (dados contextualizados), Conhecimento (informação compreendida e acionável).",
                                    "Identifique exemplos simples para cada nível: Dados = '25°C', Informação = 'Temperatura atual é 25°C em São Paulo', Conhecimento = 'Usar ar-condicionado pois 25°C é quente para o local'.",
                                    "Desenhe ou anote a pirâmide visualmente para fixar a estrutura."
                                  ],
                                  "verification": "Crie um diagrama da pirâmide DIK com exemplos corretos para cada nível.",
                                  "estimatedTime": "20 minutos",
                                  "materials": [
                                    "Papel e caneta ou ferramenta digital como Draw.io",
                                    "Artigo ou vídeo introdutório sobre Pirâmide DIK (ex: Wikipedia ou Khan Academy)"
                                  ],
                                  "tips": "Use analogias cotidianas para tornar abstrato em concreto.",
                                  "learningObjective": "Compreender a hierarquia sequencial da Pirâmide DIK.",
                                  "commonMistakes": "Confundir informação com conhecimento, achando que contexto basta sem internalização."
                                },
                                {
                                  "stepNumber": 2,
                                  "title": "Explorar a Definição de Conhecimento",
                                  "subSteps": [
                                    "Leia definições: Conhecimento é informação compreendida, internalizada e acionável.",
                                    "Analise 'compreendida' (entender significado), 'internalizada' (absorver como própria), 'acionável' (aplicar em decisões).",
                                    "Escreva sua própria definição em 1-2 frases incorporando esses elementos."
                                  ],
                                  "verification": "Sua definição escrita deve incluir compreensão, internalização e ação, sem mencionar apenas 'saber fatos'.",
                                  "estimatedTime": "25 minutos",
                                  "materials": [
                                    "Texto fonte sobre Pirâmide DIK (ex: livro 'Data to Wisdom' ou slides de aula)",
                                    "Notebook para anotações"
                                  ],
                                  "tips": "Pense em como você 'sabe' andar de bicicleta: não é só informação, é internalizado.",
                                  "learningObjective": "Formular uma definição precisa de conhecimento alinhada à Pirâmide DIK.",
                                  "commonMistakes": "Reduzir conhecimento a mera memorização de fatos, ignorando a ação."
                                },
                                {
                                  "stepNumber": 3,
                                  "title": "Analisar Componentes Chave do Conhecimento",
                                  "subSteps": [
                                    "Identifique padrões: Reconhecer repetições e tendências em dados/informação.",
                                    "Explore causalidade: Entender 'por quê' e relações causa-efeito.",
                                    "Discuta expertise: Envolve julgamento humano ou modelos treinados (ex: IA).",
                                    "Liste 2 exemplos para cada componente."
                                  ],
                                  "verification": "Crie uma tabela com componentes (padrões, causalidade, expertise) e exemplos verificáveis.",
                                  "estimatedTime": "30 minutos",
                                  "materials": [
                                    "Planilha Excel ou Google Sheets para tabela",
                                    "Vídeos curtos sobre causalidade em dados (ex: TED Talk)"
                                  ],
                                  "tips": "Use o exemplo de previsão do tempo: padrões em dados, causalidade em física, expertise em meteorologistas.",
                                  "learningObjective": "Mapear padrões, causalidade e expertise como pilares do conhecimento.",
                                  "commonMistakes": "Ignorar causalidade, confundindo correlação com causa."
                                },
                                {
                                  "stepNumber": 4,
                                  "title": "Diferenciar Conhecimento de Níveis Inferiores",
                                  "subSteps": [
                                    "Compare com dados: Conhecimento adiciona compreensão e ação.",
                                    "Compare com informação: Vai além do contexto para internalização.",
                                    "Crie cenários onde informação falha em virar conhecimento.",
                                    "Debata com um parceiro ou anote contra-argumentos."
                                  ],
                                  "verification": "Escreva 3 pares de exemplos: um de informação sem conhecimento, transformado em conhecimento.",
                                  "estimatedTime": "25 minutos",
                                  "materials": [
                                    "Fórum online ou parceiro de estudo",
                                    "Templates de comparação (PDF ou Word)"
                                  ],
                                  "tips": "Pergunte: 'Isso guia uma decisão? Se não, não é conhecimento.'",
                                  "learningObjective": "Distinguir claramente conhecimento de dados e informação.",
                                  "commonMistakes": "Equiparar conhecimento a informação avançada, sem ênfase na expertise humana/modelos."
                                }
                              ],
                              "practicalExample": "Em ciência de dados, dados brutos de vendas ('100 unidades vendidas') viram informação ('100 unidades em julho, +20% vs junho'), e conhecimento ('Padrão sazonal indica pico no verão devido a causalidade climática; expertise sugere estoque extra baseado em modelo preditivo').",
                              "finalVerifications": [
                                "Pode definir conhecimento incluindo compreensão, internalização e ação?",
                                "Identifica corretamente padrões, causalidade e expertise em exemplos?",
                                "Diferencia conhecimento de informação/dados em cenários reais?",
                                "Explica papel de humanos/modelos na geração de conhecimento?",
                                "Aplica definição em um exemplo de ciência de dados?",
                                "Desenha pirâmide DIK com exemplos precisos?"
                              ],
                              "assessmentCriteria": [
                                "Precisão na definição (alinhada à Pirâmide DIK): 0-30%",
                                "Profundidade nos componentes chave (padrões/causalidade/expertise): 0-25%",
                                "Clareza em diferenciações: 0-20%",
                                "Uso de exemplos concretos e relevantes: 0-15%",
                                "Capacidade de aplicação acionável: 0-10%"
                              ],
                              "crossCurricularConnections": [
                                "Filosofia: Epistemologia e teoria do conhecimento (Platão, Gettier).",
                                "Inteligência Artificial: Modelos de ML como 'expertise' simulada.",
                                "Negócios: Tomada de decisão baseada em conhecimento (estratégia).",
                                "Psicologia: Aprendizado e internalização cognitiva.",
                                "Estatística: Inferência causal vs correlação."
                              ],
                              "realWorldApplication": "Em análise de dados empresariais, transforma relatórios de vendas (informação) em estratégias de estoque (conhecimento), usando padrões sazonais, causalidade econômica e modelos preditivos para otimizar lucros e reduzir desperdícios."
                            },
                            "estimatedTime": "0.5 horas",
                            "difficulty": "intermediate",
                            "status": "not_started",
                            "prerequisites": [
                              "10.1.3.4.2"
                            ]
                          },
                          {
                            "id": "10.1.3.4.3.2",
                            "name": "Diferenciar conhecimento de informação",
                            "description": "Comparar os dois níveis, enfatizando que conhecimento requer compreensão profunda, previsão e uso prático, não apenas descrição.",
                            "atomicExpansion": {
                              "steps": [
                                {
                                  "stepNumber": 1,
                                  "title": "Definir e Exemplificar Informação",
                                  "subSteps": [
                                    "Leia a definição: Informação é dados organizados que descrevem fatos ou eventos sem contexto profundo.",
                                    "Analise exemplos: 'A temperatura é 25°C' ou 'Vendas foram 100 unidades'.",
                                    "Identifique características: Descritiva, estática, não preditiva.",
                                    "Crie 3 exemplos pessoais de informação cotidiana.",
                                    "Registre em um quadro comparativo."
                                  ],
                                  "verification": "Liste 3 exemplos claros de informação sem elementos preditivos ou aplicáveis.",
                                  "estimatedTime": "20 minutos",
                                  "materials": "Papel e caneta, quadro branco ou app de notas como Notion.",
                                  "tips": "Pense em manchetes de jornal: elas informam, mas não explicam o 'porquê'.",
                                  "learningObjective": "Distinguir informação como nível descritivo básico.",
                                  "commonMistakes": "Confundir informação com opinião ou interpretação pessoal."
                                },
                                {
                                  "stepNumber": 2,
                                  "title": "Definir e Exemplificar Conhecimento",
                                  "subSteps": [
                                    "Leia a definição: Conhecimento é informação compreendida profundamente, permitindo previsão, explicação e aplicação prática.",
                                    "Analise exemplos: 'Vendas caíram porque concorrente lançou produto similar; preveremos aumento ajustando preços'.",
                                    "Identifique características: Preditiva, contextual, acionável.",
                                    "Crie 3 exemplos onde informação vira conhecimento via compreensão.",
                                    "Compare com exemplos do Step 1."
                                  ],
                                  "verification": "Explique como um fato simples se transforma em conhecimento com contexto.",
                                  "estimatedTime": "25 minutos",
                                  "materials": "Vídeos curtos sobre pirâmide DIKW (Dados-Informação-Conhecimento-Sabedoria), papel para diagramas.",
                                  "tips": "Pergunte: 'Isso permite prever ou agir?' Se sim, é conhecimento.",
                                  "learningObjective": "Reconhecer conhecimento como nível superior com compreensão e utilidade.",
                                  "commonMistakes": "Reduzir conhecimento a mera memorização de fatos."
                                },
                                {
                                  "stepNumber": 3,
                                  "title": "Comparar Diferenças Chave entre Informação e Conhecimento",
                                  "subSteps": [
                                    "Crie uma tabela comparativa: colunas para definição, exemplos, características (estática vs dinâmica), uso.",
                                    "Enfatize: Informação descreve 'o que'; conhecimento responde 'por quê, como e o que fazer'.",
                                    "Discuta limitações: Informação é superficial; conhecimento requer experiência e raciocínio.",
                                    "Preencha tabela com 5 pares de exemplos contrastantes.",
                                    "Debata com parceiro ou auto-reflita."
                                  ],
                                  "verification": "Complete tabela com pelo menos 4 diferenças claras e justificadas.",
                                  "estimatedTime": "30 minutos",
                                  "materials": "Planilha Excel ou Google Sheets para tabela, exemplos impressos de dados de vendas.",
                                  "tips": "Use setas na tabela para mostrar progressão: Informação → Conhecimento.",
                                  "learningObjective": "Mapear distinções precisas entre os dois níveis.",
                                  "commonMistakes": "Ignorar o aspecto preditivo e prático do conhecimento."
                                },
                                {
                                  "stepNumber": 4,
                                  "title": "Praticar Diferenciação em Cenários de Ciência de Dados",
                                  "subSteps": [
                                    "Pegue cenários: 'Dataset mostra 80% churn rate' (info) vs 'Churn devido a suporte ruim; implementar chatbots' (conhecimento).",
                                    "Classifique 5 cenários como info ou conhecimento, justificando.",
                                    "Transforme 3 itens de info em conhecimento adicionando análise.",
                                    "Aplique a um dataset simples (ex: Iris dataset).",
                                    "Revise e refine classificações."
                                  ],
                                  "verification": "Classifique corretamente 5/5 cenários e transforme 3 infos em conhecimentos válidos.",
                                  "estimatedTime": "35 minutos",
                                  "materials": "Dataset sample (Kaggle intro datasets), Python Jupyter Notebook opcional.",
                                  "tips": "Em ciência de dados, conhecimento gera insights acionáveis para negócios.",
                                  "learningObjective": "Aplicar diferenciação em contextos reais de dados.",
                                  "commonMistakes": "Classificar análise superficial como conhecimento verdadeiro."
                                }
                              ],
                              "practicalExample": "Informação: 'O modelo de ML tem 85% accuracy'. Conhecimento: 'Accuracy de 85% é baixa para imbalanced data; usar F1-score e oversampling para melhorar previsões de churn, prevendo +15% retenção'.",
                              "finalVerifications": [
                                "Explique verbalmente 3 diferenças chave sem hesitação.",
                                "Classifique 5 novos exemplos como info ou conhecimento com justificativa.",
                                "Transforme uma informação dada em conhecimento acionável.",
                                "Crie diagrama da pirâmide DIKW destacando info vs conhecimento.",
                                "Debata por que conhecimento é essencial em ciência de dados.",
                                "Autoavalie compreensão em escala 1-10 com evidências."
                              ],
                              "assessmentCriteria": [
                                "Precisão nas definições (sem confusão com dados).",
                                "Exemplos relevantes e contrastantes.",
                                "Ênfase em previsão e aplicação prática para conhecimento.",
                                "Uso correto de tabela comparativa.",
                                "Capacidade de transformar info em conhecimento.",
                                "Integração ao contexto de ciência de dados."
                              ],
                              "crossCurricularConnections": [
                                "Filosofia: Epistemologia e níveis de saber (Platão).",
                                "Psicologia: Teoria de Bloom (compreensão vs memorização).",
                                "Negócios: Tomada de decisão baseada em dados vs insights.",
                                "Linguística: Semântica e níveis de significado."
                              ],
                              "realWorldApplication": "Em equipes de dados, analistas fornecem informação (relatórios); cientistas de dados geram conhecimento (previsões e estratégias), impulsionando decisões como otimizar campanhas de marketing baseadas em padrões preditivos de comportamento do cliente."
                            },
                            "estimatedTime": "1 hora",
                            "difficulty": "intermediate",
                            "status": "not_started",
                            "prerequisites": [
                              "10.1.3.4.1",
                              "10.1.3.4.2"
                            ]
                          },
                          {
                            "id": "10.1.3.4.3.3",
                            "name": "Exemplificar a aplicação de conhecimento",
                            "description": "Fornecer exemplos como 'A alta temperatura em SP indica risco de chuvas; ativar alertas', mostrando transição hierárquica completa.",
                            "atomicExpansion": {
                              "steps": [
                                {
                                  "stepNumber": 1,
                                  "title": "Compreender a Hierarquia Dados-Informação-Conhecimento",
                                  "subSteps": [
                                    "Revise as definições: Dados são fatos brutos (ex: 35°C em SP); Informação é dados processados com contexto (ex: temperatura acima da média); Conhecimento é informação aplicada com insight (ex: risco de chuvas).",
                                    "Crie um diagrama simples da pirâmide hierárquica.",
                                    "Identifique exemplos cotidianos para cada nível.",
                                    "Compare e contraste as diferenças entre os níveis.",
                                    "Explique verbalmente ou por escrito a transição completa."
                                  ],
                                  "verification": "Desenhe e explique o diagrama da hierarquia sem erros conceituais.",
                                  "estimatedTime": "25 minutos",
                                  "materials": [
                                    "Papel e caneta ou ferramenta digital como Draw.io",
                                    "Notas de aula sobre Ciência de Dados"
                                  ],
                                  "tips": "Use analogias como 'dados=ingredientes, informação=receita, conhecimento=prato pronto'.",
                                  "learningObjective": "Dominar as definições e transições da hierarquia para basear exemplos precisos.",
                                  "commonMistakes": "Confundir informação com conhecimento, ignorando o aspecto de aplicação prática."
                                },
                                {
                                  "stepNumber": 2,
                                  "title": "Selecionar e Coletar Dados Relevantes",
                                  "subSteps": [
                                    "Escolha um cenário real, como previsão do tempo em uma cidade.",
                                    "Colete dados brutos reais de fontes confiáveis (ex: temperatura, umidade via app meteorológico).",
                                    "Liste 5-10 dados numéricos ou categóricos relacionados.",
                                    "Documente a fonte e o contexto temporal/geográfico.",
                                    "Valide a precisão dos dados consultando múltiplas fontes."
                                  ],
                                  "verification": "Apresente uma tabela com dados coletados e fontes citadas corretamente.",
                                  "estimatedTime": "30 minutos",
                                  "materials": [
                                    "Acesso à internet (sites como INMET ou AccuWeather)",
                                    "Planilha Google Sheets ou Excel"
                                  ],
                                  "tips": "Priorize dados recentes e locais para maior relevância.",
                                  "learningObjective": "Aprender a identificar e validar dados brutos como base para análise.",
                                  "commonMistakes": "Usar dados fictícios ou não verificados, levando a exemplos irreais."
                                },
                                {
                                  "stepNumber": 3,
                                  "title": "Transformar Dados em Informação e Conhecimento",
                                  "subSteps": [
                                    "Processe os dados: calcule médias, compare com normas (ex: 35°C > 30°C médio em SP).",
                                    "Gere informação: 'Alta temperatura indica instabilidade atmosférica'.",
                                    "Contextualize para conhecimento: 'Risco elevado de chuvas fortes; preparar alertas'.",
                                    "Crie uma narrativa fluida mostrando a transição completa.",
                                    "Teste a lógica com perguntas: 'Por quê isso leva à ação?'."
                                  ],
                                  "verification": "Escreva um parágrafo descrevendo a transição, com setas lógicas (dados → info → conhecimento).",
                                  "estimatedTime": "40 minutos",
                                  "materials": [
                                    "Planilha para cálculos simples",
                                    "Gráfico de barras ou linha para visualização"
                                  ],
                                  "tips": "Use regras if-then para automatizar a transição mentalmente.",
                                  "learningObjective": "Construir cadeias lógicas de processamento hierárquico.",
                                  "commonMistakes": "Pular etapas, indo direto de dados para ação sem informação intermediária."
                                },
                                {
                                  "stepNumber": 4,
                                  "title": "Formular e Apresentar o Exemplo de Aplicação",
                                  "subSteps": [
                                    "Escreva o exemplo completo: 'Dados: 35°C em SP → Info: Acima da média → Conhecimento: Ativar alertas de chuva'.",
                                    "Inclua ação prática: 'Defesa Civil emite alertas preventivos'.",
                                    "Crie uma visualização (infográfico ou slide).",
                                    "Apresente para um par e receba feedback.",
                                    "Refine com base no feedback para clareza."
                                  ],
                                  "verification": "Produza um infográfico ou slide pronto com exemplo validado por feedback.",
                                  "estimatedTime": "35 minutos",
                                  "materials": [
                                    "Ferramenta Canva ou PowerPoint",
                                    "Exemplo impresso para revisão"
                                  ],
                                  "tips": "Mantenha o exemplo conciso, mas completo, com no máximo 100 palavras.",
                                  "learningObjective": "Exemplificar aplicação de conhecimento de forma clara e acionável.",
                                  "commonMistakes": "Exemplos vagos sem ação específica ou sem mostrar hierarquia explícita."
                                }
                              ],
                              "practicalExample": "Dados: Temperatura de 35°C e umidade 60% em São Paulo às 14h (fonte: INMET). Informação: Condições acima da média histórica, indicando instabilidade. Conhecimento: Alto risco de chuvas intensas à tarde; ativar alertas de emergência e orientar população a evitar áreas de risco.",
                              "finalVerifications": [
                                "O exemplo demonstra transição clara de dados para conhecimento.",
                                "Inclui ação prática e acionável baseada no conhecimento.",
                                "Usa dados reais e contextualizados.",
                                "Hierarquia é visual ou explicitamente mapeada.",
                                "Exemplo é conciso e compreensível para leigos."
                              ],
                              "assessmentCriteria": [
                                "Clareza da hierarquia (0-3 pontos: completa/incompleta).",
                                "Relevância e realismo dos dados (0-3 pontos).",
                                "Profundidade da aplicação prática (0-3 pontos).",
                                "Uso de evidências e fontes (0-3 pontos).",
                                "Criatividade e impacto do exemplo (0-3 pontos)."
                              ],
                              "crossCurricularConnections": [
                                "Ciências da Natureza: Meteorologia e análise climática.",
                                "Geografia: Impactos regionais de fenômenos climáticos.",
                                "Programação: Processamento de dados em Python para previsões.",
                                "Educação Financeira: Aplicações em seguros contra desastres."
                              ],
                              "realWorldApplication": "Em centros de meteorologia como o INMET, essa transição permite emitir alertas precoces, salvando vidas e reduzindo danos econômicos; em empresas de logística, otimiza rotas evitando chuvas, minimizando atrasos e custos."
                            },
                            "estimatedTime": "0.5 horas",
                            "difficulty": "intermediate",
                            "status": "not_started",
                            "prerequisites": [
                              "10.1.3.4.1",
                              "10.1.3.4.2"
                            ]
                          }
                        ]
                      },
                      {
                        "id": "10.1.3.4.4",
                        "name": "Pirâmide DIK e Distinções Fundamentais",
                        "description": "Hierarquia DIK (Dados-Informação-Conhecimento) ilustra o fluxo ascendente de valor, com diferenças em contexto, processamento e utilidade.",
                        "specificSkills": [
                          {
                            "id": "10.1.3.4.4.1",
                            "name": "Descrever a pirâmide DIK",
                            "description": "Representar visual e conceitualmente a pirâmide, mostrando fluxo unidirecional: dados (base ampla) -> informação -> conhecimento (topo estreito).",
                            "atomicExpansion": {
                              "steps": [
                                {
                                  "stepNumber": 1,
                                  "title": "Entender as Definições Individuais de Dados, Informação e Conhecimento",
                                  "subSteps": [
                                    "Pesquise e memorize a definição de Dados: fatos brutos, sem contexto (ex: números isolados).",
                                    "Estude Informação: dados processados com contexto, respondendo 'quem, o quê, onde, quando'.",
                                    "Analise Conhecimento: informação aplicada, com entendimento e insight, respondendo 'por quê e como'.",
                                    "Compare exemplos reais para cada nível.",
                                    "Crie flashcards com definições e exemplos."
                                  ],
                                  "verification": "Recite as definições sem consultar notas e forneça um exemplo para cada.",
                                  "estimatedTime": "30 minutos",
                                  "materials": [
                                    "Livro ou site sobre Ciência de Dados",
                                    "Flashcards ou app como Anki"
                                  ],
                                  "tips": "Use analogias cotidianas, como ingredientes crus (dados) vs. receita pronta (conhecimento).",
                                  "learningObjective": "Diferenciar precisamente os três conceitos fundamentais da pirâmide DIK.",
                                  "commonMistakes": "Confundir informação com conhecimento, ignorando a necessidade de insight e aplicação."
                                },
                                {
                                  "stepNumber": 2,
                                  "title": "Compreender a Hierarquia e o Fluxo Unidirecional da Pirâmide",
                                  "subSteps": [
                                    "Visualize a pirâmide: base larga para Dados (volume alto, valor baixo), meio para Informação, topo estreito para Conhecimento (volume baixo, valor alto).",
                                    "Descreva o fluxo: Dados → processados para Informação → analisados para Conhecimento (irrevogável).",
                                    "Explique por que é unidirecional: conhecimento não reverte para dados sem perda de valor.",
                                    "Estude exceções raras, como refinamento iterativo.",
                                    "Anote a pirâmide em palavras antes de desenhar."
                                  ],
                                  "verification": "Explique o fluxo em voz alta, justificando a forma da pirâmide.",
                                  "estimatedTime": "45 minutos",
                                  "materials": [
                                    "Diagrama da pirâmide DIK impresso ou online",
                                    "Caderno para anotações"
                                  ],
                                  "tips": "Pense em uma ampulheta: base larga de areia (dados) afunilando para o topo.",
                                  "learningObjective": "Graspar a estrutura visual e o processo transformacional da pirâmide DIK.",
                                  "commonMistakes": "Representar como fluxo bidirecional, subestimando a perda de valor reversa."
                                },
                                {
                                  "stepNumber": 3,
                                  "title": "Representar Visual e Conceitualmente a Pirâmide DIK",
                                  "subSteps": [
                                    "Desenhe a pirâmide do zero: base ampla rotulada 'Dados', meio 'Informação', topo 'Conhecimento'.",
                                    "Adicione setas unidirecionais ascendentes indicando transformação.",
                                    "Inclua rótulos de características: 'Bruto/Alto Volume' na base, 'Contextual/Médio Valor' no meio, 'Insight/Alto Valor' no topo.",
                                    "Crie uma versão digital usando ferramentas como Draw.io ou PowerPoint.",
                                    "Compare seu desenho com referências padrão."
                                  ],
                                  "verification": "Mostre o desenho a alguém e peça feedback sobre clareza e precisão.",
                                  "estimatedTime": "40 minutos",
                                  "materials": [
                                    "Papel e caneta ou software de diagramação (Draw.io, Canva)"
                                  ],
                                  "tips": "Use cores diferentes para cada camada para reforçar distinções visuais.",
                                  "learningObjective": "Criar uma representação visual precisa e conceitual da pirâmide.",
                                  "commonMistakes": "Fazer a base estreita ou setas bidirecionais, invertendo a hierarquia."
                                },
                                {
                                  "stepNumber": 4,
                                  "title": "Praticar a Descrição Completa e Integrada da Pirâmide DIK",
                                  "subSteps": [
                                    "Escreva um parágrafo descrevendo a pirâmide, cobrindo visual, fluxo e distinções.",
                                    "Grave um vídeo ou áudio de 1-2 minutos explicando para um público iniciante.",
                                    "Aplique a um exemplo prático: transforme dados em conhecimento.",
                                    "Revise e refine com base em autoavaliação.",
                                    "Ensine a um colega ou responda perguntas hipotéticas."
                                  ],
                                  "verification": "Sua descrição deve ser compreensível por um leigo sem erros conceituais.",
                                  "estimatedTime": "50 minutos",
                                  "materials": [
                                    "Gravador de voz/câmera no celular",
                                    "Exemplo prático anotado"
                                  ],
                                  "tips": "Estruture: introdução → visual → fluxo → exemplo → conclusão.",
                                  "learningObjective": "Descrever fluentemente a pirâmide DIK de forma visual e conceitual.",
                                  "commonMistakes": "Omitir o aspecto unidirecional ou usar jargão sem explicar."
                                }
                              ],
                              "practicalExample": "Dados: Lista de temperaturas diárias (25°C, 28°C, 22°C). Informação: Média semanal de 25°C com pico em terça. Conhecimento: Tendência de aquecimento devido a padrões sazonais, sugerindo ação climática.",
                              "finalVerifications": [
                                "Desenho da pirâmide é preciso com base larga e topo estreito.",
                                "Explicação verbal cobre definições, fluxo unidirecional e distinções.",
                                "Exemplo prático demonstra transformação de dados para conhecimento.",
                                "Identifica corretamente por que o fluxo é irrevogável.",
                                "Representação visual inclui setas e rótulos de características.",
                                "Pode ensinar o conceito a outro sem hesitação."
                              ],
                              "assessmentCriteria": [
                                "Precisão conceitual: Definições e hierarquia corretas (30%).",
                                "Clareza visual: Diagrama bem estruturado e legível (25%).",
                                "Completude da descrição: Inclui fluxo, distinções e exemplo (20%).",
                                "Fluência na explicação: Verbal e escrita sem erros (15%).",
                                "Criatividade no exemplo: Aplicação relevante e concreta (10%)."
                              ],
                              "crossCurricularConnections": [
                                "Filosofia: Epistemologia e níveis de compreensão humana.",
                                "Gestão de Negócios: Gestão do Conhecimento em organizações.",
                                "Informática: Processamento de Big Data em pipelines ETL.",
                                "Estatística: Transformação de dados brutos em insights analíticos."
                              ],
                              "realWorldApplication": "Em análise de dados empresariais, transforma logs de vendas (dados) em relatórios de tendências (informação) e estratégias de mercado (conhecimento), otimizando decisões em empresas como Google ou Amazon."
                            },
                            "estimatedTime": "1 hora",
                            "difficulty": "intermediate",
                            "status": "not_started",
                            "prerequisites": [
                              "10.1.3.4.1",
                              "10.1.3.4.2",
                              "10.1.3.4.3"
                            ]
                          },
                          {
                            "id": "10.1.3.4.4.2",
                            "name": "Analisar distinções entre os três conceitos",
                            "description": "Comparar diferenças chave: dados (sem contexto), informação (com contexto), conhecimento (com ação), usando tabela ou diagrama para síntese.",
                            "atomicExpansion": {
                              "steps": [
                                {
                                  "stepNumber": 1,
                                  "title": "Compreender as Definições Individuais de Dados, Informação e Conhecimento",
                                  "subSteps": [
                                    "Leia e anote a definição de dados: fatos brutos sem contexto (ex: números isolados).",
                                    "Estude informação: dados com contexto adicionado (ex: números com significado).",
                                    "Analise conhecimento: informação aplicada com ação ou insight (ex: usar informação para decidir).",
                                    "Crie flashcards com definições e exemplos simples para cada termo.",
                                    "Repita as definições em voz alta para fixação."
                                  ],
                                  "verification": "Escreva definições próprias sem consultar materiais e compare com fontes originais (acurácia >90%).",
                                  "estimatedTime": "30 minutos",
                                  "materials": [
                                    "Artigos ou slides sobre Pirâmide DIK",
                                    "Flashcards ou app como Anki"
                                  ],
                                  "tips": "Use analogias cotidianas, como 'dados é como ingredientes crus, informação é a receita, conhecimento é o prato cozido'.",
                                  "learningObjective": "Definir com precisão os três conceitos fundamentais da Pirâmide DIK.",
                                  "commonMistakes": "Confundir informação com conhecimento ignorando o elemento de ação; tratar dados como sempre numéricos."
                                },
                                {
                                  "stepNumber": 2,
                                  "title": "Identificar Diferenças Chave Entre os Conceitos",
                                  "subSteps": [
                                    "Liste atributos de cada: dados (sem estrutura/contexto), informação (processada/organizada), conhecimento (aplicável/experiencial).",
                                    "Compare pares: dados vs. informação (contexto adicionado?), informação vs. conhecimento (ação envolvida?).",
                                    "Registre 3-5 diferenças chave por par em uma lista.",
                                    "Pesquise exemplos reais para ilustrar cada diferença.",
                                    "Discuta com um parceiro ou grave um áudio explicando as distinções."
                                  ],
                                  "verification": "Crie uma lista de pelo menos 9 diferenças chave (3 por par) e valide com referência externa.",
                                  "estimatedTime": "45 minutos",
                                  "materials": [
                                    "Folha de papel ou Google Docs",
                                    "Vídeos introdutórios sobre DIK (ex: YouTube Khan Academy)"
                                  ],
                                  "tips": "Pergunte 'O que transforma dados em informação?' e 'O que eleva informação a conhecimento?' para guiar.",
                                  "learningObjective": "Discernir e articular diferenças precisas entre dados, informação e conhecimento.",
                                  "commonMistakes": "Ignorar hierarquia da pirâmide; superpor conceitos sem diferenciar níveis de abstração."
                                },
                                {
                                  "stepNumber": 3,
                                  "title": "Criar Tabela ou Diagrama para Síntese Comparativa",
                                  "subSteps": [
                                    "Escolha formato: tabela (colunas: Conceito, Definição, Exemplo, Características) ou diagrama pirâmide.",
                                    "Preencha com definições, diferenças e exemplos de cada step anterior.",
                                    "Adicione setas ou camadas mostrando progressão: dados → informação → conhecimento.",
                                    "Revise para clareza e completude (todas diferenças incluídas?).",
                                    "Digitalize ou fotografe o artefato final."
                                  ],
                                  "verification": "Apresente o diagrama/tabela a alguém e pergunte se as distinções ficam claras (feedback positivo).",
                                  "estimatedTime": "40 minutos",
                                  "materials": [
                                    "Ferramentas como Draw.io, Canva, Excel ou papel/caneta"
                                  ],
                                  "tips": "Use cores diferentes para cada conceito e ícones visuais para reforçar memória.",
                                  "learningObjective": "Sintetizar distinções visuais de forma clara e hierárquica.",
                                  "commonMistakes": "Tabela/diagrama muito genérico sem exemplos específicos; desequilíbrio nas colunas."
                                },
                                {
                                  "stepNumber": 4,
                                  "title": "Aplicar e Verificar Compreensão com Exemplos Práticos",
                                  "subSteps": [
                                    "Pegue um cenário real (ex: leituras de temperatura) e classifique: dados, info, conhecimento.",
                                    "Use sua tabela/diagrama para mapear o exemplo.",
                                    "Identifique gaps: onde falha a transformação entre níveis?",
                                    "Escreva um parágrafo sintetizando análise do exemplo.",
                                    "Teste-se com quiz online sobre DIK."
                                  ],
                                  "verification": "Classifique corretamente 5 exemplos independentes usando sua síntese.",
                                  "estimatedTime": "35 minutos",
                                  "materials": [
                                    "Exemplos de cenários reais impressos ou digitais",
                                    "Quiz online sobre Pirâmide DIK"
                                  ],
                                  "tips": "Comece com exemplos familiares para construir confiança antes de cenários complexos.",
                                  "learningObjective": "Aplicar análise de distinções em contextos práticos.",
                                  "commonMistakes": "Forçar um exemplo em múltiplos níveis sem justificar; pular verificação prática."
                                }
                              ],
                              "practicalExample": "Analise leituras de temperatura de uma cidade: Dados = '25°C, 28°C, 22°C'; Informação = 'Temperatura média de 25°C indica dia quente'; Conhecimento = 'Levar roupas leves e hidratar-se com base na tendência de aquecimento'. Use tabela para mapear transformações.",
                              "finalVerifications": [
                                "Define corretamente dados, informação e conhecimento sem erros.",
                                "Lista pelo menos 3 diferenças chave por par de conceitos.",
                                "Diagrama/tabela sintetiza hierarquia DIK de forma visualmente clara.",
                                "Classifica exemplos reais nos três níveis com justificativa.",
                                "Explica verbalmente distinções em 2 minutos.",
                                "Identifica erros comuns em classificações alheias."
                              ],
                              "assessmentCriteria": [
                                "Precisão conceitual (definições e diferenças exatas: 30%)",
                                "Qualidade da síntese visual (clareza/organização da tabela/diagrama: 25%)",
                                "Profundidade de exemplos práticos (relevância e variedade: 20%)",
                                "Capacidade de aplicação (classificação correta em cenários: 15%)",
                                "Completude da expansão (todos elementos presentes: 10%)"
                              ],
                              "crossCurricularConnections": [
                                "Filosofia: Epistemologia e teoria do conhecimento.",
                                "Gestão de Negócios: Sistemas de informação gerencial (SIG).",
                                "Tecnologia da Informação: Bancos de dados vs. Business Intelligence.",
                                "Estatística: Processamento de dados brutos em insights acionáveis."
                              ],
                              "realWorldApplication": "Em ciência de dados, profissionais usam essa distinção para transformar dados brutos de sensores IoT em dashboards informativos e recomendações de ação em machine learning, como em previsão de demanda em e-commerce."
                            },
                            "estimatedTime": "1 hora",
                            "difficulty": "intermediate",
                            "status": "not_started",
                            "prerequisites": [
                              "10.1.3.4.1",
                              "10.1.3.4.2",
                              "10.1.3.4.3"
                            ]
                          }
                        ]
                      }
                    ]
                  },
                  {
                    "id": "10.1.3.5",
                    "name": "Exemplos Ilustrativos de Dados, Informação e Conhecimento",
                    "description": "Casos práticos que demonstram a transformação de dados em informação e, subsequentemente, em conhecimento.",
                    "individualConcepts": [
                      {
                        "id": "10.1.3.5.1",
                        "name": "Exemplo Ilustrativo 1: Dados de Vendas em uma Loja",
                        "description": "Caso prático com registros de vendas diárias (ex.: 10, 15, 20 unidades vendidas por hora), demonstrando a transformação de dados brutos em informação (totais e médias) e em conhecimento (padrões de demanda).",
                        "specificSkills": [
                          {
                            "id": "10.1.3.5.1.1",
                            "name": "Identificar dados brutos no contexto de vendas",
                            "description": "Reconhecer elementos isolados como números de vendas por hora (ex.: 10, 15, 20) como dados brutos, sem contexto, análise ou significado agregado.",
                            "atomicExpansion": {
                              "steps": [
                                {
                                  "stepNumber": 1,
                                  "title": "Compreender o conceito de dados brutos",
                                  "subSteps": [
                                    "Leia a definição de dados brutos: elementos isolados sem contexto ou análise.",
                                    "Compare com informação (dados com contexto) e conhecimento (análise interpretada).",
                                    "Anote exemplos simples de dados brutos em outras áreas, como temperatura (25°C) sem data.",
                                    "Registre diferenças chave em um quadro comparativo.",
                                    "Repita a definição em suas próprias palavras."
                                  ],
                                  "verification": "Criar um quadro comparativo com pelo menos 3 exemplos corretos de dados brutos.",
                                  "estimatedTime": "20 minutos",
                                  "materials": [
                                    "Papel e caneta ou editor de texto",
                                    "Material didático sobre tipos de dados"
                                  ],
                                  "tips": "Use analogias cotidianas, como ingredientes crus vs. receita pronta.",
                                  "learningObjective": "Definir e diferenciar dados brutos de informação e conhecimento.",
                                  "commonMistakes": "Confundir dados brutos com totais ou médias calculadas."
                                },
                                {
                                  "stepNumber": 2,
                                  "title": "Explorar o contexto de vendas em uma loja",
                                  "subSteps": [
                                    "Descreva um cenário de vendas: loja com registros horários de vendas.",
                                    "Liste elementos típicos: hora do dia, número de itens vendidos, valor total.",
                                    "Identifique o que é registrado isoladamente vs. com significado agregado.",
                                    "Desenhe um fluxograma simples do processo de registro de vendas.",
                                    "Discuta por que registros isolados são necessários inicialmente."
                                  ],
                                  "verification": "Produzir um fluxograma ou descrição do cenário com 5 elementos identificados.",
                                  "estimatedTime": "25 minutos",
                                  "materials": [
                                    "Papel para fluxograma",
                                    "Exemplo textual de contexto de vendas"
                                  ],
                                  "tips": "Pense em um caixa registrador: ele só anota números crus primeiro.",
                                  "learningObjective": "Contextualizar dados brutos no ambiente de vendas varejistas.",
                                  "commonMistakes": "Adicionar contexto prematuramente aos dados isolados."
                                },
                                {
                                  "stepNumber": 3,
                                  "title": "Analisar exemplo ilustrativo de dados de vendas",
                                  "subSteps": [
                                    "Examine o exemplo: Vendas por hora - 9h:10, 10h:15, 11h:20.",
                                    "Marque cada número como isolado e sem análise.",
                                    "Explique por que esses números são brutos (falta hora, loja, período).",
                                    "Compare com versão contextualizada: 'Média de 15 vendas/hora pela manhã'.",
                                    "Classifique 5 números semelhantes como brutos ou não."
                                  ],
                                  "verification": "Classificar corretamente todos os itens do exemplo como dados brutos.",
                                  "estimatedTime": "30 minutos",
                                  "materials": [
                                    "Exemplo impresso ou digital de dados de vendas",
                                    "Marcadores coloridos"
                                  ],
                                  "tips": "Destaque números isolados em vermelho para visualização clara.",
                                  "learningObjective": "Reconhecer números de vendas isolados como dados brutos no exemplo dado.",
                                  "commonMistakes": "Interpretar os números como informação sem contexto explícito."
                                },
                                {
                                  "stepNumber": 4,
                                  "title": "Praticar identificação em cenários variados",
                                  "subSteps": [
                                    "Crie seu próprio exemplo com 5 registros de vendas por hora.",
                                    "Identifique e isole os dados brutos em uma lista.",
                                    "Transforme-os em informação adicionando contexto (ex.: média).",
                                    "Avalie um exemplo de colega ou gerado aleatoriamente.",
                                    "Registre 3 casos onde confundiu dados brutos inicialmente."
                                  ],
                                  "verification": "Produzir lista com 5 dados brutos identificados corretamente e transformados.",
                                  "estimatedTime": "35 minutos",
                                  "materials": [
                                    "Planilha ou papel para prática",
                                    "Gerador de números aleatórios online"
                                  ],
                                  "tips": "Varie os cenários para reforçar (ex.: vendas online vs. física).",
                                  "learningObjective": "Aplicar identificação de dados brutos de forma independente.",
                                  "commonMistakes": "Incluir cálculos ou totais como dados brutos."
                                }
                              ],
                              "practicalExample": "Em uma loja de roupas, o caixa registra: 9h-10h: 10 vendas; 10h-11h: 15 vendas; 11h-12h: 20 vendas. Esses números (10, 15, 20) são dados brutos porque estão isolados, sem indicação de valor médio, tendência ou comparação com dias anteriores.",
                              "finalVerifications": [
                                "Lista corretamente os dados brutos do exemplo sem adicionar contexto.",
                                "Diferencia dados brutos de informação em pelo menos 3 cenários.",
                                "Explica verbalmente por que 10, 15, 20 são brutos no contexto de vendas.",
                                "Identifica dados brutos em um novo exemplo criado pelo aluno.",
                                "Cria um quadro comparativo preciso entre dados, informação e conhecimento."
                              ],
                              "assessmentCriteria": [
                                "Precisão na identificação de dados brutos (90% de acertos).",
                                "Compreensão conceitual demonstrada em explicações claras.",
                                "Uso correto de exemplos contextualizados em vendas.",
                                "Capacidade de distinguir brutos de agregados sem erros comuns.",
                                "Criatividade e variedade em práticas independentes.",
                                "Clareza na documentação de fluxogramas e listas."
                              ],
                              "crossCurricularConnections": [
                                "Estatística: Base para coleta inicial de dados em análises descritivas.",
                                "Negócios: Registro de vendas em contabilidade e relatórios gerenciais.",
                                "Programação: Entrada de dados brutos em scripts de processamento.",
                                "Língua Portuguesa: Definições e descrições precisas de conceitos técnicos."
                              ],
                              "realWorldApplication": "Em empresas de varejo, como redes de supermercados, identificar dados brutos de vendas por hora permite alimentar sistemas de ERP para análises posteriores, otimizando estoques e turnos de funcionários sem interpretações prematuras."
                            },
                            "estimatedTime": "0.25 horas",
                            "difficulty": "beginner",
                            "status": "not_started",
                            "prerequisites": []
                          },
                          {
                            "id": "10.1.3.5.1.2",
                            "name": "Transformar dados em informação através de agregação",
                            "description": "Aplicar operações como soma (total de 150 unidades no dia) e média (média de 18,75 unidades/hora) para converter dados brutos em informação acionável sobre desempenho diário.",
                            "atomicExpansion": {
                              "steps": [
                                {
                                  "stepNumber": 1,
                                  "title": "Coletar e Organizar Dados Brutos",
                                  "subSteps": [
                                    "Liste todos os dados brutos relevantes, como unidades vendidas por hora em uma loja.",
                                    "Registre os valores em uma tabela ou lista ordenada cronologicamente.",
                                    "Identifique o período total (ex: 8 horas de operação).",
                                    "Conte o número total de observações para agregações futuras.",
                                    "Verifique a completude dos dados, removendo ou marcando valores ausentes."
                                  ],
                                  "verification": "Confira se a tabela/lista está completa e sem erros de transcrição.",
                                  "estimatedTime": "15 minutos",
                                  "materials": "Papel, planilha (Excel/Google Sheets) ou editor de texto.",
                                  "tips": "Use colunas para hora e unidades vendidas para facilitar visualização.",
                                  "learningObjective": "Entender a importância de dados organizados como base para agregação.",
                                  "commonMistakes": "Ignorar valores ausentes ou transcrever incorretamente os números."
                                },
                                {
                                  "stepNumber": 2,
                                  "title": "Identificar Operações de Agregação Necessárias",
                                  "subSteps": [
                                    "Defina o objetivo: ex: total diário para estoque e média por hora para eficiência.",
                                    "Escolha soma para total acumulado (∑ unidades).",
                                    "Escolha média para tendência (média = total / número de horas).",
                                    "Liste fórmulas: Soma = Σxi; Média = Σxi / n.",
                                    "Priorize agregações baseadas no contexto (desempenho diário)."
                                  ],
                                  "verification": "Escreva as fórmulas escolhidas e explique por que cada uma é relevante.",
                                  "estimatedTime": "10 minutos",
                                  "materials": "Planilha ou calculadora para rascunho.",
                                  "tips": "Pergunte: 'Que insight acionável isso gera?' para guiar escolhas.",
                                  "learningObjective": "Selecionar agregações adequadas para transformar dados em informação.",
                                  "commonMistakes": "Confundir soma com média ou escolher agregações irrelevantes."
                                },
                                {
                                  "stepNumber": 3,
                                  "title": "Calcular as Agregações",
                                  "subSteps": [
                                    "Some todos os valores: ex: 10+20+15+25+30+25+25 = 150 unidades.",
                                    "Calcule a média: 150 / 8 horas = 18,75 unidades/hora.",
                                    "Use funções automáticas em planilhas (=SOMA() e =MÉDIA()).",
                                    "Arredonde resultados conforme precisão necessária (ex: 2 casas decimais).",
                                    "Registre cálculos passo a passo para rastreabilidade."
                                  ],
                                  "verification": "Recalcule manualmente e compare com ferramenta automatizada.",
                                  "estimatedTime": "20 minutos",
                                  "materials": "Planilha com fórmulas (Excel/Google Sheets), calculadora.",
                                  "tips": "Teste com subconjuntos pequenos primeiro para validar.",
                                  "learningObjective": "Executar cálculos precisos de soma e média em conjuntos de dados.",
                                  "commonMistakes": "Erros aritméticos ou divisão por zero (n=0)."
                                },
                                {
                                  "stepNumber": 4,
                                  "title": "Interpretar Resultados como Informação Acionável",
                                  "subSteps": [
                                    "Analise o total: '150 unidades vendidas indicam necessidade de reabastecimento.'",
                                    "Analise a média: '18,75/hora sugere pico em horários específicos.'",
                                    "Compare com benchmarks: ex: meta de 20/hora não atingida.",
                                    "Gere insights: 'Aumentar equipe em horários de pico.'",
                                    "Documente a transformação: dados brutos → informação estratégica."
                                  ],
                                  "verification": "Escreva 2-3 ações baseadas nos resultados.",
                                  "estimatedTime": "15 minutos",
                                  "materials": "Planilha com resumo e notas textuais.",
                                  "tips": "Use linguagem não técnica: foque em 'o que isso significa?'",
                                  "learningObjective": "Converter números agregados em decisões práticas.",
                                  "commonMistakes": "Parar nos números sem interpretação contextual."
                                },
                                {
                                  "stepNumber": 5,
                                  "title": "Visualizar e Comunicar a Informação",
                                  "subSteps": [
                                    "Crie um gráfico de barras para dados por hora e destaque total/média.",
                                    "Adicione rótulos: 'Total: 150 unid.; Média: 18,75/hora'.",
                                    "Prepare um relatório curto: introdução, cálculos, insights.",
                                    "Compartilhe com stakeholders fictícios (ex: gerente de loja).",
                                    "Revise para clareza e impacto visual."
                                  ],
                                  "verification": "Peça feedback simulado: 'Os insights são claros?'",
                                  "estimatedTime": "20 minutos",
                                  "materials": "Planilha com gráficos ou ferramenta como Google Data Studio.",
                                  "tips": "Mantenha simples: evite sobrecarga visual.",
                                  "learningObjective": "Apresentar informação agregada de forma eficaz.",
                                  "commonMistakes": "Gráficos confusos ou falta de contexto nos rótulos."
                                }
                              ],
                              "practicalExample": "Dados de vendas por hora em uma loja: Hora 1:10un, H2:20un, H3:15un, H4:25un, H5:30un, H6:25un, H7:25un, H8:25un. Agregue: Total=150un (soma para estoque); Média=18.75un/h (para eficiência operacional). Insight: Reforçar vendas nas primeiras horas.",
                              "finalVerifications": [
                                "Cálculos de soma e média estão precisos e documentados.",
                                "Interpretação gera pelo menos 2 ações acionáveis.",
                                "Visualização destaca total e média claramente.",
                                "Transformação de dados para informação é explicitada.",
                                "Todos os passos foram seguidos sem erros comuns.",
                                "Resultados comparam com benchmarks ou metas."
                              ],
                              "assessmentCriteria": [
                                "Precisão matemática nos cálculos (100% correto).",
                                "Profundidade da interpretação (insights relevantes e acionáveis).",
                                "Organização e clareza na documentação/visualização.",
                                "Seleção adequada de agregações ao contexto.",
                                "Completude: todos substeps executados.",
                                "Criatividade em aplicações reais."
                              ],
                              "crossCurricularConnections": [
                                "Estatística: Conceitos de medidas de tendência central.",
                                "Programação: Uso de funções em Python (sum(), mean()) ou SQL (SUM, AVG).",
                                "Negócios: Análise de desempenho e tomada de decisões.",
                                "Visualização de Dados: Gráficos em ferramentas como Tableau.",
                                "Matemática Financeira: Agregações em relatórios contábeis."
                              ],
                              "realWorldApplication": "Em uma loja de varejo, agregar vendas diárias permite gerentes otimizarem estoque (baseado em totais) e escalonarem turnos (baseado em médias por hora), aumentando eficiência e lucros."
                            },
                            "estimatedTime": "0.5 horas",
                            "difficulty": "beginner",
                            "status": "not_started",
                            "prerequisites": []
                          },
                          {
                            "id": "10.1.3.5.1.3",
                            "name": "Derivar conhecimento de padrões na informação",
                            "description": "Analisar a informação para identificar padrões, como picos de vendas à tarde, gerando conhecimento sobre hábitos de consumo para decisões estratégicas.",
                            "atomicExpansion": {
                              "steps": [
                                {
                                  "stepNumber": 1,
                                  "title": "Preparar e Organizar a Informação de Vendas",
                                  "subSteps": [
                                    "Reúna dados brutos de vendas, incluindo data, hora, produto e quantidade vendida.",
                                    "Classifique os dados por intervalos de tempo (ex: manhã, tarde, noite).",
                                    "Crie uma tabela ou planilha resumindo totais de vendas por horário.",
                                    "Limpe dados inconsistentes, como entradas duplicadas ou faltantes.",
                                    "Calcule médias simples por período para visualização inicial."
                                  ],
                                  "verification": "Verifique se a tabela está completa sem erros e as somas batem com os dados originais.",
                                  "estimatedTime": "30 minutos",
                                  "materials": "Planilha (Excel/Google Sheets), dados de vendas simulados ou reais.",
                                  "tips": "Use filtros na planilha para agilizar a organização.",
                                  "learningObjective": "Entender como transformar dados brutos em informação organizada.",
                                  "commonMistakes": "Ignorar valores atípicos que distorcem as médias."
                                },
                                {
                                  "stepNumber": 2,
                                  "title": "Identificar Padrões Visuais e Estatísticos",
                                  "subSteps": [
                                    "Crie um gráfico de barras ou linha mostrando vendas por horário.",
                                    "Observe picos e vales nos gráficos (ex: pico à tarde).",
                                    "Calcule estatísticas básicas: média, mediana e desvio padrão por período.",
                                    "Compare períodos para confirmar padrões recorrentes.",
                                    "Anote padrões potenciais, como 'vendas 40% maiores à tarde'."
                                  ],
                                  "verification": "O gráfico destaca claramente um pico à tarde com dados quantificados.",
                                  "estimatedTime": "45 minutos",
                                  "materials": "Planilha com gráficos, calculadora ou funções STAT no Excel.",
                                  "tips": "Escolha escalas adequadas no gráfico para não mascarar padrões.",
                                  "learningObjective": "Desenvolver habilidade em visualização e detecção de padrões na informação.",
                                  "commonMistakes": "Confundir correlação com causalidade sem análise adicional."
                                },
                                {
                                  "stepNumber": 3,
                                  "title": "Interpretar Padrões para Gerar Conhecimento",
                                  "subSteps": [
                                    "Explique o padrão: 'Pico à tarde indica maior fluxo de clientes nesse horário'.",
                                    "Relacione com fatores contextuais (ex: horário de almoço ou fim de expediente).",
                                    "Formule hipótese de conhecimento: 'Consumidores compram mais à tarde devido a hábitos pós-almoço'.",
                                    "Valide com subconjuntos de dados (ex: dias úteis vs. fins de semana).",
                                    "Documente o conhecimento derivado em uma declaração clara."
                                  ],
                                  "verification": "Produza uma declaração de conhecimento apoiada por evidências dos dados.",
                                  "estimatedTime": "40 minutos",
                                  "materials": "Gráficos e tabelas do passo anterior, caderno para anotações.",
                                  "tips": "Pergunte 'por quê?' repetidamente para aprofundar a interpretação.",
                                  "learningObjective": "Transformar padrões identificados em insights acionáveis sobre hábitos.",
                                  "commonMistakes": "Generalizar padrões de poucos dados sem validação."
                                },
                                {
                                  "stepNumber": 4,
                                  "title": "Aplicar Conhecimento para Decisões Estratégicas",
                                  "subSteps": [
                                    "Proponha ações baseadas no conhecimento (ex: reforçar estoque à tarde).",
                                    "Simule impacto: 'Aumentar equipe à tarde pode elevar vendas em 20%'.",
                                    "Crie um plano simples com metas mensuráveis.",
                                    "Discuta limitações e testes futuros para refinar o conhecimento.",
                                    "Registre o plano em formato executável."
                                  ],
                                  "verification": "O plano inclui pelo menos 3 ações específicas ligadas ao padrão.",
                                  "estimatedTime": "35 minutos",
                                  "materials": "Declaração de conhecimento, template de plano de ação.",
                                  "tips": "Priorize ações de baixo custo para testes rápidos.",
                                  "learningObjective": "Conectar conhecimento derivado a decisões práticas e estratégicas.",
                                  "commonMistakes": "Propor ações sem considerar viabilidade operacional."
                                }
                              ],
                              "practicalExample": "Em uma loja de varejo, dados de 30 dias mostram 150 unidades vendidas pela manhã, 350 à tarde e 100 à noite. O pico à tarde revela hábito de compras pós-trabalho, levando à decisão de promoções nesse horário.",
                              "finalVerifications": [
                                "Gráficos e tabelas mostram padrões claros nos dados.",
                                "Declaração de conhecimento explica o 'porquê' do padrão.",
                                "Plano de ação inclui 3+ estratégias baseadas no insight.",
                                "Evidências quantitativas suportam todas as interpretações.",
                                "Limitações do análise foram identificadas.",
                                "Simulação de impacto é realista e mensurável."
                              ],
                              "assessmentCriteria": [
                                "Precisão na identificação de padrões (80%+ de acerto nos picos).",
                                "Profundidade da interpretação (conexão clara com hábitos de consumo).",
                                "Criatividade e viabilidade das decisões estratégicas.",
                                "Uso correto de ferramentas de visualização e estatística básica.",
                                "Documentação completa e organizada.",
                                "Capacidade de evitar vieses comuns na análise."
                              ],
                              "crossCurricularConnections": [
                                "Estatística: Cálculo de médias e visualizações gráficas.",
                                "Negócios: Análise de mercado e planejamento estratégico.",
                                "Informática: Manipulação de dados em planilhas e programação básica.",
                                "Comportamento do Consumidor: Psicologia aplicada a hábitos."
                              ],
                              "realWorldApplication": "Em e-commerce como Amazon, análise de padrões de tráfego à tarde otimiza recomendações e logística, aumentando conversões em 15-25%; em redes de varejo, ajusta turnos de funcionários e estoques para maximizar lucros."
                            },
                            "estimatedTime": "0.5 horas",
                            "difficulty": "intermediate",
                            "status": "not_started",
                            "prerequisites": [
                              "10.1.3.5.1.2"
                            ]
                          }
                        ]
                      },
                      {
                        "id": "10.1.3.5.2",
                        "name": "Exemplo Ilustrativo 2: Dados de Temperatura Diária",
                        "description": "Caso prático com medições de temperatura (ex.: 25°C, 28°C, 22°C em diferentes dias), ilustrando dados, informação (tendências médias) e conhecimento (previsões climáticas).",
                        "specificSkills": [
                          {
                            "id": "10.1.3.5.2.1",
                            "name": "Reconhecer dados brutos de medições ambientais",
                            "description": "Identificar valores isolados de temperatura (ex.: 25°C, 28°C, 22°C) como dados brutos crus, sem processamento ou interpretação.",
                            "atomicExpansion": {
                              "steps": [
                                {
                                  "stepNumber": 1,
                                  "title": "Entender o conceito de dados brutos",
                                  "subSteps": [
                                    "Defina dados brutos como valores coletados diretamente de uma medição, sem análise ou processamento.",
                                    "Leia exemplos simples: um termômetro marcando 25°C é um dado bruto.",
                                    "Discuta a diferença entre dado bruto e informação (ex.: média de temperaturas é informação).",
                                    "Anote a definição em suas próprias palavras.",
                                    "Crie um fluxograma simples: medição → dado bruto → processamento → informação."
                                  ],
                                  "verification": "Escreva uma definição clara de dados brutos e dê 2 exemplos corretos de medições ambientais.",
                                  "estimatedTime": "15 minutos",
                                  "materials": "Papel e caneta ou editor de texto digital; vídeo curto sobre tipos de dados (opcional).",
                                  "tips": "Use analogias cotidianas, como pesos em uma balança sem calcular IMC.",
                                  "learningObjective": "Compreender a definição fundamental de dados brutos em contextos de medição.",
                                  "commonMistakes": "Confundir dados brutos com resultados calculados, como médias ou gráficos."
                                },
                                {
                                  "stepNumber": 2,
                                  "title": "Explorar medições ambientais como dados brutos",
                                  "subSteps": [
                                    "Colete exemplos de medições: temperaturas como 25°C, 28°C, 22°C de um termômetro.",
                                    "Identifique fontes ambientais: termômetros, higrômetros para umidade bruta.",
                                    "Liste 5 valores isolados de temperatura diária de uma estação meteorológica fictícia.",
                                    "Registre-os sem adicionar comentários ou cálculos.",
                                    "Compare com não-exemplos: 'temperatura média de 25°C' não é bruto."
                                  ],
                                  "verification": "Liste 5 valores de temperatura isolados e explique por que são dados brutos.",
                                  "estimatedTime": "20 minutos",
                                  "materials": "Lista impressa ou digital de medições ambientais; termômetro simples (opcional).",
                                  "tips": "Foque em valores 'crus' como lidos diretamente no instrumento.",
                                  "learningObjective": "Reconhecer medições ambientais isoladas como dados brutos de temperatura.",
                                  "commonMistakes": "Incluir interpretações como 'quente' ou médias no lugar de valores puros."
                                },
                                {
                                  "stepNumber": 3,
                                  "title": "Praticar identificação em conjuntos de dados",
                                  "subSteps": [
                                    "Analise um conjunto misto: [25°C (medição), 28°C (medição), média 25.5°C (processado)].",
                                    "Marque apenas os dados brutos com um símbolo (ex.: *).",
                                    "Crie seu próprio conjunto com 10 entradas ambientais e classifique.",
                                    "Troque com um parceiro para verificação mútua.",
                                    "Registre acertos e erros em um log."
                                  ],
                                  "verification": "Classifique corretamente 8/10 itens em um conjunto de dados mistos.",
                                  "estimatedTime": "25 minutos",
                                  "materials": "Planilha ou papel com conjuntos de dados de temperatura; parceiro de estudo (opcional).",
                                  "tips": "Pergunte: 'Isso veio direto da medição ou foi calculado?'",
                                  "learningObjective": "Aplicar reconhecimento de dados brutos em contextos reais de dados ambientais.",
                                  "commonMistakes": "Marcar valores processados (somatórios, médias) como brutos."
                                },
                                {
                                  "stepNumber": 4,
                                  "title": "Diferenciar dados brutos de informação e conhecimento",
                                  "subSteps": [
                                    "Estude a hierarquia: dados brutos → informação (ex.: gráfico de temp.) → conhecimento (previsão climática).",
                                    "Transforme dados brutos de temp. em informação (calcule média).",
                                    "Discuta como dados brutos de temp. levam a alertas de calor.",
                                    "Crie um diagrama da pirâmide de dados.",
                                    "Teste-se com quiz de 5 perguntas mistas."
                                  ],
                                  "verification": "Explique a hierarquia com exemplos corretos de temperatura ambiental.",
                                  "estimatedTime": "20 minutos",
                                  "materials": "Diagrama em branco; quiz online sobre dados/informação.",
                                  "tips": "Lembre: dados brutos são o 'ingrediente cru', não a 'receita pronta'.",
                                  "learningObjective": "Distinguir dados brutos ambientais de etapas superiores de processamento.",
                                  "commonMistakes": "Equiparar qualquer número ambiental a dado bruto, ignorando processamento."
                                }
                              ],
                              "practicalExample": "Em uma estação meteorológica, os valores lidos diretamente do termômetro às 8h (25°C), 12h (28°C) e 18h (22°C) são dados brutos. Sem calcular média ou traçar gráfico, esses números isolados representam medições ambientais cruas.",
                              "finalVerifications": [
                                "Identifica corretamente 100% de 10 valores brutos de temperatura em um conjunto misto.",
                                "Explica a diferença entre 25°C (bruto) e 'média de 25°C' (processado).",
                                "Lista 5 fontes ambientais que geram dados brutos de temperatura.",
                                "Cria um exemplo próprio de dados brutos ambientais sem processamento.",
                                "Descreve verbalmente a hierarquia dados → informação usando temperaturas.",
                                "Passa em um quiz de 5 perguntas sobre reconhecimento de dados brutos."
                              ],
                              "assessmentCriteria": [
                                "Precisão na identificação de dados brutos (90%+ acertos).",
                                "Clareza na explicação conceitual sem confusão com processamento.",
                                "Uso correto de exemplos ambientais reais ou simulados.",
                                "Capacidade de diferenciar hierarquia de dados em contextos práticos.",
                                "Criatividade em exemplos próprios e diagramas.",
                                "Completude das verificações em steps individuais."
                              ],
                              "crossCurricularConnections": [
                                "Ciências: Monitoramento climático e coleta de dados em ecologia.",
                                "Geografia: Análise de padrões ambientais regionais via medições brutas.",
                                "Informática: Entrada de dados brutos em planilhas para ciência de dados.",
                                "Estatística: Base para cálculos posteriores em conjuntos de medições."
                              ],
                              "realWorldApplication": "Em estações meteorológicas ou projetos de monitoramento ambiental, reconhecer dados brutos de temperatura permite cientistas iniciarem análises precisas, evitando erros em previsões climáticas ou alertas de desastres como ondas de calor."
                            },
                            "estimatedTime": "0.25 horas",
                            "difficulty": "beginner",
                            "status": "not_started",
                            "prerequisites": []
                          },
                          {
                            "id": "10.1.3.5.2.2",
                            "name": "Gerar informação por meio de estatísticas descritivas",
                            "description": "Calcular média (média de 25°C) e variação para transformar dados em informação sobre comportamento térmico semanal.",
                            "atomicExpansion": {
                              "steps": [
                                {
                                  "stepNumber": 1,
                                  "title": "Coletar e Organizar Dados de Temperatura Semanal",
                                  "subSteps": [
                                    "Registre as temperaturas diárias de uma semana (ex: Seg=24°C, Ter=26°C, Qua=25°C, Qui=23°C, Sex=27°C, Sáb=25°C, Dom=24°C).",
                                    "Crie uma tabela ou lista com os dias da semana e valores correspondentes.",
                                    "Verifique se todos os 7 dias estão representados sem duplicatas ou valores ausentes.",
                                    "Ordene os dados cronologicamente para facilitar cálculos.",
                                    "Calcule a contagem total de observações (n=7)."
                                  ],
                                  "verification": "Confirme que a tabela/lista tem 7 entradas válidas e ordenadas corretamente.",
                                  "estimatedTime": "15 minutos",
                                  "materials": "Papel, planilha (Excel/Google Sheets) ou editor de texto simples.",
                                  "tips": "Use uma planilha para facilitar somas futuras; rotule colunas como 'Dia' e 'Temperatura (°C)'",
                                  "learningObjective": "Organizar dados brutos de forma estruturada para análise estatística.",
                                  "commonMistakes": "Ignorar valores ausentes ou registrar temperaturas em escalas diferentes (ex: °F em vez de °C)."
                                },
                                {
                                  "stepNumber": 2,
                                  "title": "Calcular a Média Aritmética das Temperaturas",
                                  "subSteps": [
                                    "Some todas as temperaturas: 24+26+25+23+27+25+24 = 174°C.",
                                    "Divida a soma pelo número de dias: 174 / 7 = 24.86°C (arredonde para 25°C).",
                                    "Registre o resultado como 'Média semanal: 25°C'.",
                                    "Use fórmula: Média = Σx / n, onde x são os valores e n=7.",
                                    "Verifique o cálculo manualmente ou com calculadora/planilha."
                                  ],
                                  "verification": "A média calculada deve ser aproximadamente 25°C para os dados exemplo.",
                                  "estimatedTime": "10 minutos",
                                  "materials": "Calculadora, planilha com função AVERAGE ou soma manual.",
                                  "tips": "Em planilhas, selecione o intervalo e use =AVERAGE(A2:A8) para automação.",
                                  "learningObjective": "Aplicar a fórmula de média para resumir dados centrais.",
                                  "commonMistakes": "Esquecer de dividir pela contagem correta ou arredondar prematuramente."
                                },
                                {
                                  "stepNumber": 3,
                                  "title": "Calcular a Variação (Variância) das Temperaturas",
                                  "subSteps": [
                                    "Calcule a diferença de cada temperatura em relação à média (ex: 24-25=-1, 26-25=1, etc.).",
                                    "Eleve cada diferença ao quadrado: (-1)^2=1, 1^2=1, etc., somando: 1+1+0+4+4+0+1=11.",
                                    "Divida pela contagem (amostral: n-1=6): 11/6 ≈ 1.83°C² (variância).",
                                    "Registre como 'Variação: 1.83°C², indicando baixa flutuação'.",
                                    "Opcional: Calcule desvio padrão = √variância ≈ 1.35°C."
                                  ],
                                  "verification": "Variância deve ser cerca de 1.83°C²; verifique soma de quadrados.",
                                  "estimatedTime": "20 minutos",
                                  "materials": "Calculadora científica para raízes, planilha com funções VAR ou VAR.S.",
                                  "tips": "Use variância amostral (n-1) para dados reais; planilhas facilitam com =VAR.S(A2:A8).",
                                  "learningObjective": "Medir a dispersão dos dados usando variância para entender estabilidade.",
                                  "commonMistakes": "Usar n em vez de n-1 para amostra ou erros em quadrados negativos."
                                },
                                {
                                  "stepNumber": 4,
                                  "title": "Interpretar Resultados e Gerar Informação Sobre Comportamento Térmico",
                                  "subSteps": [
                                    "Resuma: 'Temperatura média de 25°C com variação baixa (1.83°C²), indicando comportamento térmico estável.'",
                                    "Compare com benchmarks: 'Estável para clima tropical; variação <2°C² sugere pouca oscilação.'",
                                    "Crie uma frase informativa: 'Semana teve temperatura consistente em torno de 25°C.'",
                                    "Visualize opcionalmente com gráfico de linha ou boxplot.",
                                    "Documente insights para conhecimento futuro."
                                  ],
                                  "verification": "A interpretação transforma números em declaração clara sobre estabilidade térmica.",
                                  "estimatedTime": "15 minutos",
                                  "materials": "Papel para resumo, software de gráficos (Excel/Google Sheets).",
                                  "tips": "Ligue estatísticas a contexto: baixa variação = previsível; alta = volátil.",
                                  "learningObjective": "Transformar estatísticas descritivas em informação acionável.",
                                  "commonMistakes": "Interpretar apenas números sem contexto real (ex: ignorar unidade °C)."
                                }
                              ],
                              "practicalExample": "Dados de temperatura em São Paulo: Seg 24°C, Ter 26°C, Qua 25°C, Qui 23°C, Sex 27°C, Sáb 25°C, Dom 24°C. Média=25°C, variância=1.83°C² → 'Clima estável, ideal para atividades ao ar livre sem extremos.'",
                              "finalVerifications": [
                                "Média calculada corretamente como 25°C.",
                                "Variância computada com fórmula precisa (≈1.83°C²).",
                                "Dados organizados em tabela com 7 entradas.",
                                "Interpretação inclui estabilidade térmica semanal.",
                                "Verificação manual dos cálculos soma e divisões.",
                                "Unidades consistentes (°C) em todos os resultados."
                              ],
                              "assessmentCriteria": [
                                "Precisão nos cálculos de média e variância (erro <1%).",
                                "Organização clara dos dados brutos.",
                                "Interpretação contextualizada e informativa.",
                                "Uso correto de fórmulas estatísticas.",
                                "Documentação completa com verificações.",
                                "Eficiência temporal dentro dos estimates."
                              ],
                              "crossCurricularConnections": [
                                "Ciências: Meteorologia e análise climática.",
                                "Computação: Programação em Python/R para automação (pandas.describe()).",
                                "Geografia: Padrões climáticos regionais.",
                                "Física: Termodinâmica e flutuações térmicas."
                              ],
                              "realWorldApplication": "Em estações meteorológicas, calcular média e variação diária/semanal ajuda previsores a alertar sobre ondas de calor/frio, otimizar consumo de energia em edifícios ou planejar agricultura com base em estabilidade térmica."
                            },
                            "estimatedTime": "0.5 horas",
                            "difficulty": "beginner",
                            "status": "not_started",
                            "prerequisites": []
                          },
                          {
                            "id": "10.1.3.5.2.3",
                            "name": "Extrair conhecimento sobre tendências climáticas",
                            "description": "Interpretar a informação para compreender tendências de aquecimento, permitindo conhecimento para planejamento agrícola ou energético.",
                            "atomicExpansion": {
                              "steps": [
                                {
                                  "stepNumber": 1,
                                  "title": "Preparar e Explorar os Dados de Temperatura",
                                  "subSteps": [
                                    "Coletar dados de temperatura diária de uma fonte confiável (ex: NOAA ou estação local) por pelo menos 10 anos.",
                                    "Importar os dados para uma ferramenta como Python (Pandas) ou Excel, organizando em colunas: data e temperatura média.",
                                    "Limpar os dados: remover valores ausentes, outliers e formatar datas corretamente.",
                                    "Criar uma visualização inicial: gráfico de linha de temperatura vs. tempo.",
                                    "Calcular estatísticas descritivas básicas: média, mediana, desvio padrão por ano."
                                  ],
                                  "verification": "Gráfico de linha gerado mostrando variações anuais e tabela de estatísticas descritivas completa.",
                                  "estimatedTime": "45 minutos",
                                  "materials": "Computador com Python (Pandas, Matplotlib) ou Excel; dataset de temperaturas (ex: CSV de temperaturas diárias de uma cidade).",
                                  "tips": "Use funções como pd.read_csv() e plt.plot() para eficiência; agrupe por ano para visão geral rápida.",
                                  "learningObjective": "Entender a estrutura dos dados climáticos e prepará-los para análise.",
                                  "commonMistakes": "Ignorar valores ausentes levando a médias enviesadas; não converter datas para formato datetime."
                                },
                                {
                                  "stepNumber": 2,
                                  "title": "Calcular Tendências e Estatísticas de Longo Prazo",
                                  "subSteps": [
                                    "Agrupar dados por ano ou década e calcular médias anuais de temperatura.",
                                    "Aplicar regressão linear simples (ex: scipy.linregress) para estimar a taxa de mudança (°C/ano).",
                                    "Computar anomalias de temperatura (desvio da média histórica).",
                                    "Identificar períodos de aquecimento acelerado usando médias móveis (janela de 5 anos).",
                                    "Visualizar tendência: gráfico com linha de regressão sobreposta aos dados."
                                  ],
                                  "verification": "Gráfico com linha de tendência e relatório com inclinação da regressão (ex: +0.02°C/ano).",
                                  "estimatedTime": "60 minutos",
                                  "materials": "Python com SciPy e Matplotlib; dataset preparado do passo 1.",
                                  "tips": "Use rolling mean para suavizar ruído sazonal; teste significância da regressão (p-value < 0.05).",
                                  "learningObjective": "Quantificar tendências climáticas a partir de dados de temperatura.",
                                  "commonMistakes": "Confundir correlação com causalidade; ignorar sazonalidade ao calcular tendências."
                                },
                                {
                                  "stepNumber": 3,
                                  "title": "Interpretar Tendências de Aquecimento e Extrair Conhecimento",
                                  "subSteps": [
                                    "Analisar a inclinação da regressão: interpretar se indica aquecimento (positivo) e magnitude.",
                                    "Comparar com benchmarks globais (ex: +0.08°C/década do IPCC).",
                                    "Identificar impactos potenciais: dias mais quentes afetando agricultura ou energia.",
                                    "Criar narrativa: 'Temperatura média subiu X°C em Y anos, sugerindo aquecimento local.'",
                                    "Documentar incertezas: intervalos de confiança da regressão."
                                  ],
                                  "verification": "Relatório escrito com interpretação clara, incluindo gráfico e narrativa de conhecimento extraído.",
                                  "estimatedTime": "45 minutos",
                                  "materials": "Resultados dos passos anteriores; template de relatório.",
                                  "tips": "Contextualize com dados regionais; use frases como 'evidência sugere' para incertezas.",
                                  "learningObjective": "Transformar informação estatística em conhecimento acionável sobre clima.",
                                  "commonMistakes": "Superestimar certeza sem intervalos de confiança; ignorar fatores locais como urbanização."
                                },
                                {
                                  "stepNumber": 4,
                                  "title": "Aplicar Conhecimento para Planejamento Prático",
                                  "subSteps": [
                                    "Simular planejamento agrícola: prever mudanças em ciclos de plantio baseadas na tendência.",
                                    "Modelar demanda energética: estimar aumento em uso de ar-condicionado com temperaturas mais altas.",
                                    "Propor recomendações: ex: variedades de culturas resistentes ao calor ou expansão de energia renovável.",
                                    "Criar dashboard interativo resumindo tendências e aplicações.",
                                    "Avaliar cenários futuros projetando tendência por mais 10 anos."
                                  ],
                                  "verification": "Dashboard ou relatório com recomendações práticas e projeções.",
                                  "estimatedTime": "50 minutos",
                                  "materials": "Ferramentas como Tableau Public ou Python Plotly; dados analisados.",
                                  "tips": "Use projeções lineares conservadoras; integre com dados de precipitação se disponível.",
                                  "learningObjective": "Aplicar conhecimento climático a decisões reais em agricultura e energia.",
                                  "commonMistakes": "Fazer projeções sem base em incertezas; ignorar adaptações locais existentes."
                                }
                              ],
                              "practicalExample": "Usando dados diários de temperatura de São Paulo (2000-2020), calcule a tendência de +0.15°C/década via regressão linear. Interprete como evidência de aquecimento urbano, recomendando plantio antecipado de soja em 5-10 dias e aumento de 10% na capacidade de refrigeração elétrica.",
                              "finalVerifications": [
                                "Gráficos de tendência gerados corretamente com regressão linear.",
                                "Interpretação escrita identifica aquecimento e magnitude quantitativa.",
                                "Recomendações práticas para agricultura ou energia baseadas na tendência.",
                                "Incertezas (ex: intervalo de confiança) documentadas.",
                                "Projeção futura plausível calculada.",
                                "Narrativa conecta dados a conhecimento acionável."
                              ],
                              "assessmentCriteria": [
                                "Precisão na preparação e limpeza de dados (sem erros em estatísticas básicas).",
                                "Correta aplicação e interpretação de regressão linear (inclinação e significância).",
                                "Profundidade na extração de conhecimento (além de números, impactos discutidos).",
                                "Criatividade e relevância nas aplicações práticas.",
                                "Clareza e completude do relatório/dashboard.",
                                "Uso adequado de visualizações para comunicação."
                              ],
                              "crossCurricularConnections": [
                                "Geografia: Análise de mudanças climáticas regionais e impactos espaciais.",
                                "Biologia: Efeitos no ecossistema e adaptação de espécies agrícolas.",
                                "Economia: Planejamento de recursos energéticos e produtividade agrícola.",
                                "Estatística: Modelos de regressão e análise de séries temporais."
                              ],
                              "realWorldApplication": "Agricultores usam tendências de aquecimento para ajustar datas de semeadura, evitando perdas por calor extremo; empresas energéticas preveem picos de demanda de resfriamento, otimizando redes e investindo em solares para mitigar riscos climáticos."
                            },
                            "estimatedTime": "0.5 horas",
                            "difficulty": "intermediate",
                            "status": "not_started",
                            "prerequisites": [
                              "10.1.3.5.2.2"
                            ]
                          }
                        ]
                      },
                      {
                        "id": "10.1.3.5.3",
                        "name": "Exemplo Ilustrativo 3: Dados de Registros Médicos",
                        "description": "Caso prático com medidas de pressão arterial (ex.: 120/80, 130/85, 115/75), mostrando transformação em informação (faixas médias) e conhecimento (riscos à saúde).",
                        "specificSkills": [
                          {
                            "id": "10.1.3.5.3.1",
                            "name": "Identificar dados brutos em registros clínicos",
                            "description": "Classificar leituras individuais de pressão (ex.: 120/80) como dados brutos, sem análise ou contexto paciente.",
                            "atomicExpansion": {
                              "steps": [
                                {
                                  "stepNumber": 1,
                                  "title": "Compreender o Conceito de Dados Brutos",
                                  "subSteps": [
                                    "Defina dados brutos como observações isoladas e não processadas, sem interpretação ou contexto agregado.",
                                    "Compare com informação (dados processados) e conhecimento (insights derivados).",
                                    "Estude exemplos simples: temperatura de 37°C vs. 'febre diagnosticada'.",
                                    "Anote características chave: numérico, categórico, sem análise.",
                                    "Crie um fluxograma mental distinguindo dados brutos de outros níveis."
                                  ],
                                  "verification": "Explique em suas palavras o que são dados brutos e dê 2 exemplos não médicos.",
                                  "estimatedTime": "15 minutos",
                                  "materials": "Folha de papel, caneta, acesso a definição online de dados brutos (opcional).",
                                  "tips": "Use analogias cotidianas, como 'peso na balança' vs. 'IMC calculado'.",
                                  "learningObjective": "Diferenciar dados brutos de informação e conhecimento conceitualmente.",
                                  "commonMistakes": "Confundir dados brutos com diagnósticos ou médias calculadas."
                                },
                                {
                                  "stepNumber": 2,
                                  "title": "Analisar Estrutura de um Registro Clínico",
                                  "subSteps": [
                                    "Obtenha um exemplo de registro médico fictício ou anonimizado.",
                                    "Identifique seções: dados vitais, histórico, diagnósticos.",
                                    "Liste todos os valores numéricos ou categóricos presentes.",
                                    "Marque itens isolados como leituras de pressão (ex.: 120/80).",
                                    "Ignore seções com interpretações como 'hipertensão controlada'."
                                  ],
                                  "verification": "Desenhe um mapa do registro destacando apenas itens potenciais de dados brutos.",
                                  "estimatedTime": "20 minutos",
                                  "materials": "Exemplo de registro clínico impresso ou digital (fornecido ou criado).",
                                  "tips": "Procure por abreviações como BP (blood pressure) ou valores isolados.",
                                  "learningObjective": "Reconhecer componentes estruturais de registros clínicos.",
                                  "commonMistakes": "Incluir notas narrativas ou conclusões médicas como dados brutos."
                                },
                                {
                                  "stepNumber": 3,
                                  "title": "Identificar Leituras Individuais de Pressão como Dados Brutos",
                                  "subSteps": [
                                    "Localize entradas de pressão arterial: sistólica/diastólica (ex.: 120/80 mmHg).",
                                    "Classifique cada leitura como dado bruto por ser uma medição pontual.",
                                    "Verifique ausência de contexto: não é média, tendência ou diagnóstico.",
                                    "Compare múltiplas leituras: cada uma é bruta individualmente.",
                                    "Documente por que não é informação (falta agregação ou análise)."
                                  ],
                                  "verification": "Liste 3 leituras de pressão de um registro e justifique cada uma como bruta.",
                                  "estimatedTime": "25 minutos",
                                  "materials": "Registro clínico exemplo com múltiplas leituras de pressão.",
                                  "tips": "Foquem em 'leituras individuais'; ignore gráficos ou resumos.",
                                  "learningObjective": "Classificar especificamente leituras de pressão como dados brutos.",
                                  "commonMistakes": "Interpretar 120/80 como 'normal' em vez de valor bruto isolado."
                                },
                                {
                                  "stepNumber": 4,
                                  "title": "Praticar Classificação em Registros Completos",
                                  "subSteps": [
                                    "Examine 2-3 registros clínicos variados.",
                                    "Extraia e classifique todos dados brutos relacionados a vitais.",
                                    "Crie uma tabela: coluna 'Dado Bruto' vs. 'Não Bruto'.",
                                    "Justifique classificações com critérios de isolamento e falta de análise.",
                                    "Revise com um parceiro ou autoavaliação."
                                  ],
                                  "verification": "Apresente tabela completa com pelo menos 5 itens corretamente classificados.",
                                  "estimatedTime": "30 minutos",
                                  "materials": "3 exemplos de registros clínicos fictícios.",
                                  "tips": "Use cores para destacar: verde para brutos, vermelho para processados.",
                                  "learningObjective": "Aplicar identificação de dados brutos em contextos clínicos reais.",
                                  "commonMistakes": "Classificar sequências de leituras como informação sem agregação explícita."
                                }
                              ],
                              "practicalExample": "Em um registro clínico: 'Paciente: João Silva. Data: 01/10/2023. Pressão Arterial: 120/80 mmHg (medida às 09:00). Peso: 75kg. Diagnóstico: Hipertensão estágio 1.' Aqui, '120/80 mmHg' é dado bruto (leitura isolada), enquanto 'Hipertensão estágio 1' é conhecimento derivado.",
                              "finalVerifications": [
                                "Corretamente identifica 120/80 como dado bruto em 90% dos exemplos.",
                                "Explica ausência de contexto paciente na classificação.",
                                "Distingue de outros vitais como temperatura ou pulso.",
                                "Justifica por que não é informação (sem cálculo ou interpretação).",
                                "Aplica a múltiplos registros sem inclusão de notas médicas.",
                                "Cria exemplos próprios de dados brutos clínicos."
                              ],
                              "assessmentCriteria": [
                                "Precisão na identificação de dados brutos (sem falsos positivos).",
                                "Compreensão conceitual demonstrada em justificativas.",
                                "Cobertura completa de elementos em registros fornecidos.",
                                "Uso correto de terminologia (dados brutos vs. informação).",
                                "Capacidade de generalizar para novos registros clínicos.",
                                "Clareza na documentação e tabelas criadas."
                              ],
                              "crossCurricularConnections": [
                                "Ciência de Dados: Pré-processamento de dados em pipelines.",
                                "Biologia/Medicina: Interpretação de sinais vitais.",
                                "Estatística: Dados observacionais vs. resumidos.",
                                "Informática: Extração de dados de EHRs (Electronic Health Records)."
                              ],
                              "realWorldApplication": "Em hospitais, cientistas de dados identificam leituras brutas de pressão em milhões de registros para alimentar modelos de ML que preveem riscos cardíacos, garantindo que apenas dados não processados sejam usados na etapa inicial de limpeza."
                            },
                            "estimatedTime": "0.25 horas",
                            "difficulty": "beginner",
                            "status": "not_started",
                            "prerequisites": []
                          },
                          {
                            "id": "10.1.3.5.3.2",
                            "name": "Converter dados em informação agregada de saúde",
                            "description": "Computar médias sistólica (121,67) e diastólica (80) para obter informação sobre evolução da pressão do paciente.",
                            "atomicExpansion": {
                              "steps": [
                                {
                                  "stepNumber": 1,
                                  "title": "Coletar e organizar os dados brutos de pressão arterial",
                                  "subSteps": [
                                    "Identifique as medições sistólica e diastólica de cada registro médico (ex: Dia 1: 120/80, Dia 2: 130/85, Dia 3: 115/75, Dia 4: 125/82).",
                                    "Liste separadamente as valores sistólicos (120, 130, 115, 125) e diastólicos (80, 85, 75, 82) em colunas ou tabelas.",
                                    "Registre a data ou período de cada medição para contextualizar a evolução temporal.",
                                    "Verifique a completude dos dados, descartando registros incompletos ou outliers evidentes.",
                                    "Formate os dados em uma tabela simples (ex: Excel ou papel) com cabeçalhos: Data, Sistólica, Diastólica."
                                  ],
                                  "verification": "Confira se a tabela está completa, com todos os valores listados corretamente e sem erros de transcrição.",
                                  "estimatedTime": "15 minutos",
                                  "materials": "Papel, planilha Excel/Google Sheets, registros médicos fictícios ou reais.",
                                  "tips": "Use vírgula como separador decimal no Brasil para consistência.",
                                  "learningObjective": "Compreender a importância de organizar dados brutos para análise agregada.",
                                  "commonMistakes": "Confundir sistólica com diastólica; ignorar unidades (mmHg)."
                                },
                                {
                                  "stepNumber": 2,
                                  "title": "Calcular a média da pressão sistólica",
                                  "subSteps": [
                                    "Some todos os valores sistólicos: 120 + 130 + 115 + 125 = 490.",
                                    "Conte o número de medições: 4.",
                                    "Divida a soma pelo número de medições: 490 / 4 = 122,5.",
                                    "Arredonde para duas casas decimais se necessário (ex: 122,50).",
                                    "Registre o resultado como 'Média Sistólica: 122,50 mmHg'."
                                  ],
                                  "verification": "Recalcule a soma e divisão manualmente ou com calculadora para confirmar o valor exato.",
                                  "estimatedTime": "10 minutos",
                                  "materials": "Calculadora, planilha com função AVERAGE() ou papel e caneta.",
                                  "tips": "Em planilhas, use =SOMA(B2:B5)/CONT.VALORES(B2:B5) para automação.",
                                  "learningObjective": "Dominar o cálculo de média aritmética simples para agregação de dados.",
                                  "commonMistakes": "Esquecer de dividir pela contagem correta; erro na soma."
                                },
                                {
                                  "stepNumber": 3,
                                  "title": "Calcular a média da pressão diastólica",
                                  "subSteps": [
                                    "Some todos os valores diastólicos: 80 + 85 + 75 + 82 = 322.",
                                    "Conte o número de medições: 4.",
                                    "Divida a soma pelo número: 322 / 4 = 80,5.",
                                    "Arredonde adequadamente e registre: 'Média Diastólica: 80,50 mmHg'.",
                                    "Compare visualmente com a média sistólica para nota inicial."
                                  ],
                                  "verification": "Verifique o cálculo replicando em outra ferramenta (ex: Python ou app de calculadora).",
                                  "estimatedTime": "10 minutos",
                                  "materials": "Calculadora, planilha Excel/Google Sheets.",
                                  "tips": "Mantenha consistência no arredondamento para evitar discrepâncias.",
                                  "learningObjective": "Aplicar fórmula de média a conjuntos de dados separados.",
                                  "commonMistakes": "Usar valores errados da lista; confundir com cálculo sistólico."
                                },
                                {
                                  "stepNumber": 4,
                                  "title": "Interpretar as médias para avaliar a evolução da saúde do paciente",
                                  "subSteps": [
                                    "Compare as médias (122,50/80,50) com valores normais (ex: <120/80 ideal, 120-129/80 hipertensão estágio 1).",
                                    "Analise tendências: se médias indicam estabilização ou piora em relação a médias anteriores.",
                                    "Crie um gráfico simples de linha com médias vs. tempo para visualização.",
                                    "Registre insights: 'Evolução estável, mas sistólica elevada sugere monitoramento'.",
                                    "Documente em relatório: 'Informação agregada: Média PA 122,50/80,50 mmHg'."
                                  ],
                                  "verification": "O relatório inclui médias corretas, comparação com normas e conclusão sobre evolução.",
                                  "estimatedTime": "20 minutos",
                                  "materials": "Planilha com gráficos, tabela de referência de PA (OMS).",
                                  "tips": "Use cores no gráfico para destacar valores acima do normal.",
                                  "learningObjective": "Transformar informação agregada em conhecimento sobre saúde do paciente.",
                                  "commonMistakes": "Ignorar contexto clínico; interpretar sem referência a normas."
                                }
                              ],
                              "practicalExample": "Dados de paciente: Dia 1: 120/80, Dia 2: 130/85, Dia 3: 115/75, Dia 4: 125/82. Cálculos: Média sistólica = (120+130+115+125)/4 = 122,50 mmHg; Média diastólica = (80+85+75+82)/4 = 80,50 mmHg. Interpretação: Pressão arterial média elevada na sistólica, indicando necessidade de monitoramento para evolução hipertensiva.",
                              "finalVerifications": [
                                "Médias sistólica e diastólica calculadas corretamente com valores exatos.",
                                "Tabela de dados organizada e verificada sem erros.",
                                "Interpretação inclui comparação com normas de saúde (ex: OMS).",
                                "Gráfico ou visualização criada mostrando evolução.",
                                "Relatório final resume dados → informação → insight de saúde.",
                                "Todos os cálculos replicáveis por terceiros."
                              ],
                              "assessmentCriteria": [
                                "Precisão matemática nos cálculos de média (erro <1%).",
                                "Organização clara dos dados brutos em tabela.",
                                "Interpretação contextualizada com referências clínicas.",
                                "Uso adequado de ferramentas (planilha/gráficos).",
                                "Relatório completo e acionável.",
                                "Tempo respeitado e passos lógicos seguidos."
                              ],
                              "crossCurricularConnections": [
                                "Biologia/Saúde: Entender fisiologia da pressão arterial.",
                                "Estatística: Conceitos de média e agregação de dados.",
                                "Informática: Uso de planilhas e gráficos para análise.",
                                "Ética Médica: Privacidade e uso responsável de dados de saúde."
                              ],
                              "realWorldApplication": "Em clínicas ou hospitais, médicos usam médias de PA de múltiplas medições para diagnosticar hipertensão, ajustar tratamentos e monitorar evolução de pacientes crônicos, evitando decisões baseadas em leituras isoladas."
                            },
                            "estimatedTime": "0.5 horas",
                            "difficulty": "beginner",
                            "status": "not_started",
                            "prerequisites": []
                          },
                          {
                            "id": "10.1.3.5.3.3",
                            "name": "Produzir conhecimento para diagnóstico preventivo",
                            "description": "Usar informação para detectar tendências de hipertensão, gerando conhecimento para recomendações médicas personalizadas.",
                            "atomicExpansion": {
                              "steps": [
                                {
                                  "stepNumber": 1,
                                  "title": "Coleta e Preparação de Dados Médicos",
                                  "subSteps": [
                                    "Identifique fontes de dados como registros de pressão arterial, idade, IMC e hábitos alimentares.",
                                    "Importe os dados para uma ferramenta como Python (Pandas) ou Excel.",
                                    "Limpe os dados removendo valores ausentes, duplicatas e outliers.",
                                    "Padronize variáveis como categorizar faixas etárias e níveis de hipertensão.",
                                    "Crie um dataset limpo focado em hipertensão."
                                  ],
                                  "verification": "Dataset limpo exportado com shape, estatísticas descritivas e sem erros de importação.",
                                  "estimatedTime": "2-3 horas",
                                  "materials": "Dataset de registros médicos (ex: Kaggle Hypertension Dataset), Python/Jupyter Notebook ou Excel.",
                                  "tips": "Use funções como df.dropna() e df.fillna() para limpeza eficiente.",
                                  "learningObjective": "Preparar dados brutos para análise confiável.",
                                  "commonMistakes": "Ignorar outliers que distorcem tendências ou não documentar transformações."
                                },
                                {
                                  "stepNumber": 2,
                                  "title": "Análise Exploratória para Detectar Tendências",
                                  "subSteps": [
                                    "Calcule estatísticas descritivas (média, mediana de pressão arterial por grupo etário).",
                                    "Crie visualizações como histogramas de pressão e boxplots por IMC.",
                                    "Identifique correlações entre variáveis (ex: correlação entre idade e hipertensão).",
                                    "Aplique testes estatísticos simples como chi-quadrado para associações.",
                                    "Documente tendências emergentes, como maior risco em adultos acima de 50 anos."
                                  ],
                                  "verification": "Relatório com gráficos e métricas mostrando pelo menos 3 tendências claras de hipertensão.",
                                  "estimatedTime": "3-4 horas",
                                  "materials": "Python (Matplotlib, Seaborn), ou Google Sheets para gráficos.",
                                  "tips": "Use pairplot() no Seaborn para visualizações rápidas de múltiplas variáveis.",
                                  "learningObjective": "Transformar informação em padrões detectáveis.",
                                  "commonMistakes": "Sobrepor gráficos ilegíveis ou ignorar causalidade vs. correlação."
                                },
                                {
                                  "stepNumber": 3,
                                  "title": "Geração de Conhecimento a partir de Tendências",
                                  "subSteps": [
                                    "Agrupe dados em clusters (ex: K-means para perfis de risco de hipertensão).",
                                    "Derive regras de conhecimento, como 'pacientes com IMC>30 e idade>60 têm 70% risco'.",
                                    "Use agregações para criar scores de risco personalizados.",
                                    "Valide clusters com métricas como silhouette score.",
                                    "Compile conhecimento em formato legível (tabela de regras)."
                                  ],
                                  "verification": "Tabela de regras ou modelo com scores de risco para subgrupos de pacientes.",
                                  "estimatedTime": "4-5 horas",
                                  "materials": "Python (Scikit-learn para clustering), dataset preparado.",
                                  "tips": "Comece com 3-5 clusters para simplicidade e interprete biologicamente.",
                                  "learningObjective": "Extrair conhecimento acionável de padrões informacionais.",
                                  "commonMistakes": "Criar clusters sem validação ou regras não generalizáveis."
                                },
                                {
                                  "stepNumber": 4,
                                  "title": "Criação de Recomendações Médicas Personalizadas",
                                  "subSteps": [
                                    "Mapeie clusters para recomendações (ex: dieta baixa sódio para alto IMC).",
                                    "Personalize por paciente usando scores de risco.",
                                    "Gere relatórios com recomendações preventivas preventivas.",
                                    "Inclua alertas para casos de alto risco.",
                                    "Teste recomendações em cenários simulados."
                                  ],
                                  "verification": "Relatório com 5+ recomendações personalizadas linkadas a tendências.",
                                  "estimatedTime": "2-3 horas",
                                  "materials": "Python para scripts de mapeamento, templates de relatório (Word/Google Docs).",
                                  "tips": "Use if-else ou funções para automação de recomendações.",
                                  "learningObjective": "Aplicar conhecimento para ações preventivas personalizadas.",
                                  "commonMistakes": "Recomendações genéricas sem base em dados ou ignorar contraindicações."
                                },
                                {
                                  "stepNumber": 5,
                                  "title": "Validação e Apresentação do Conhecimento",
                                  "subSteps": [
                                    "Compare conhecimento gerado com literatura médica sobre hipertensão.",
                                    "Simule aplicação em novos dados para verificar precisão.",
                                    "Crie dashboard visual com tendências e recomendações.",
                                    "Documente limitações e próximos passos.",
                                    "Apresente em formato shareable."
                                  ],
                                  "verification": "Dashboard ou relatório validado com métricas de acurácia >80%.",
                                  "estimatedTime": "2 horas",
                                  "materials": "Dash (Plotly), literatura médica (PubMed resumos).",
                                  "tips": "Use cross-validation para robustez em novos dados.",
                                  "learningObjective": "Garantir confiabilidade do conhecimento produzido.",
                                  "commonMistakes": "Não validar contra evidências externas ou sobrecarregar com detalhes técnicos."
                                }
                              ],
                              "practicalExample": "Usando um dataset de 1000 pacientes com pressão arterial, idade e IMC: Detecta-se que 65% dos >55 anos com IMC>28 têm hipertensão pré-clínica. Gera-se conhecimento: 'Recomendar monitoramento semanal e dieta DASH para esse grupo', resultando em relatórios personalizados como 'Paciente João, 62 anos, IMC 30: Inicie exercícios 30min/dia'.",
                              "finalVerifications": [
                                "Tendências de hipertensão identificadas com suporte estatístico/visual.",
                                "Conhecimento gerado em regras claras e quantificáveis.",
                                "Recomendações personalizadas para pelo menos 3 perfis de pacientes.",
                                "Validação cruzada com dados simulados ou literatura.",
                                "Dashboard ou relatório completo e interpretável.",
                                "Documentação de limitações éticas e de dados."
                              ],
                              "assessmentCriteria": [
                                "Precisão na detecção de tendências (correlações >0.5 suportadas).",
                                "Profundidade do conhecimento gerado (regras acionáveis e personalizadas).",
                                "Clareza e usabilidade das recomendações médicas.",
                                "Qualidade das visualizações e validações.",
                                "Integração de múltiplas variáveis sem vieses.",
                                "Eficiência temporal e uso de recursos."
                              ],
                              "crossCurricularConnections": [
                                "Biologia/Saúde: Entender fisiopatologia da hipertensão.",
                                "Estatística: Testes de hipótese e modelagem preditiva.",
                                "Programação: Manipulação de dados em Python/R.",
                                "Ética: Privacidade de dados médicos (GDPR/HIPAA).",
                                "Comunicação: Apresentação de insights para não-especialistas."
                              ],
                              "realWorldApplication": "Em clínicas ou apps como Apple Health/Google Fit, onde dados de wearables detectam tendências precoces de hipertensão, gerando alertas personalizados para prevenir AVCs, reduzindo custos hospitalares em 20-30% via prevenção."
                            },
                            "estimatedTime": "0.5 horas",
                            "difficulty": "intermediate",
                            "status": "not_started",
                            "prerequisites": [
                              "10.1.3.5.3.2"
                            ]
                          }
                        ]
                      }
                    ]
                  }
                ]
              },
              {
                "id": "10.1.4",
                "name": "Etapas da Ciência de Dados",
                "description": "Processo completo incluindo coleta, integração e armazenamento de dados; análise exploratória e visualização; limpeza de dados; ajuste e avaliação de modelos, com exemplos e estudos de caso.",
                "totalSkills": 57,
                "atomicTopics": [
                  {
                    "id": "10.1.4.1",
                    "name": "Coleta de Dados",
                    "description": "Processos e fontes para aquisição de dados brutos na ciência de dados.",
                    "individualConcepts": [
                      {
                        "id": "10.1.4.1.1",
                        "name": "Fontes de Dados",
                        "description": "Classificação e identificação das principais fontes de dados brutos utilizados na ciência de dados, incluindo fontes internas, externas, públicas e proprietárias.",
                        "specificSkills": [
                          {
                            "id": "10.1.4.1.1.1",
                            "name": "Identificar fontes internas de dados",
                            "description": "Reconhecer e descrever fontes de dados geradas internamente pela organização, como bancos de dados transacionais, logs de sistemas e arquivos de CRM.",
                            "atomicExpansion": {
                              "steps": [
                                {
                                  "stepNumber": 1,
                                  "title": "Compreender o conceito de fontes internas de dados",
                                  "subSteps": [
                                    "Defina fontes internas de dados como informações geradas dentro da organização.",
                                    "Diferencie fontes internas de externas (ex: dados de clientes vs. dados públicos).",
                                    "Estude exemplos iniciais: bancos de dados transacionais, logs de sistemas e CRM.",
                                    "Analise por que essas fontes são valiosas para a ciência de dados.",
                                    "Registre definições em um glossário pessoal."
                                  ],
                                  "verification": "Crie um resumo de 100 palavras explicando fontes internas e liste 3 exemplos.",
                                  "estimatedTime": "20 minutos",
                                  "materials": "Notebook, acesso à internet para artigos introdutórios sobre ciência de dados.",
                                  "tips": "Use analogias como 'dados internos são como o diário da empresa'.",
                                  "learningObjective": "Entender e definir fontes internas de dados com precisão.",
                                  "commonMistakes": "Confundir fontes internas com externas, como dados de redes sociais."
                                },
                                {
                                  "stepNumber": 2,
                                  "title": "Identificar tipos comuns de fontes internas",
                                  "subSteps": [
                                    "Liste bancos de dados transacionais (ex: SQL para vendas).",
                                    "Explore logs de sistemas (ex: logs de servidores web, erros de aplicação).",
                                    "Descreva arquivos de CRM (ex: Salesforce com histórico de interações).",
                                    "Identifique outros tipos: planilhas internas, repositórios de documentos.",
                                    "Classifique cada tipo por formato e uso comum."
                                  ],
                                  "verification": "Crie uma tabela com 5 tipos de fontes, incluindo descrição e exemplo.",
                                  "estimatedTime": "25 minutos",
                                  "materials": "Planilha (Google Sheets ou Excel), documentação online de ferramentas como MySQL e Salesforce.",
                                  "tips": "Pesquise diagramas de arquitetura de dados para visualização.",
                                  "learningObjective": "Reconhecer e categorizar fontes internas padrão.",
                                  "commonMistakes": "Ignorar logs como fontes viáveis por parecerem 'técnicos demais'."
                                },
                                {
                                  "stepNumber": 3,
                                  "title": "Mapear fontes internas em uma organização simulada",
                                  "subSteps": [
                                    "Escolha uma organização fictícia (ex: e-commerce).",
                                    "Mapeie fluxos de dados: de onde vêm transações, logs e CRM.",
                                    "Entreviste 'stakeholders' simulados ou pesquise casos reais.",
                                    "Crie um diagrama de fontes de dados internas.",
                                    "Avalie acessibilidade e qualidade de cada fonte."
                                  ],
                                  "verification": "Desenhe e descreva um mapa de 4-6 fontes internas para a organização.",
                                  "estimatedTime": "30 minutos",
                                  "materials": "Ferramenta de diagramação (Draw.io ou papel e caneta), exemplos de casos de estudo online.",
                                  "tips": "Comece com departamentos chave: vendas, TI, marketing.",
                                  "learningObjective": "Aplicar identificação de fontes em contexto organizacional.",
                                  "commonMistakes": "Focar apenas em uma fonte, ignorando diversidade."
                                },
                                {
                                  "stepNumber": 4,
                                  "title": "Documentar e descrever fontes identificadas",
                                  "subSteps": [
                                    "Para cada fonte, descreva: origem, formato, volume aproximado e uso.",
                                    "Escreva uma ficha técnica por fonte (metadata básica).",
                                    "Avalie privacidade e conformidade (ex: LGPD/GDPR).",
                                    "Priorize fontes por relevância para análises de dados.",
                                    "Compile em um relatório final."
                                  ],
                                  "verification": "Produza um documento com descrições detalhadas de pelo menos 5 fontes.",
                                  "estimatedTime": "25 minutos",
                                  "materials": "Editor de texto (Google Docs), templates de documentação de dados.",
                                  "tips": "Use bullet points para clareza e inclua diagramas.",
                                  "learningObjective": "Documentar fontes de forma profissional e acionável.",
                                  "commonMistakes": "Omitir detalhes de qualidade ou acessibilidade das fontes."
                                }
                              ],
                              "practicalExample": "Em uma empresa de e-commerce como a 'ShopFast', identifique: (1) Banco de dados PostgreSQL de transações (pedidos diários), (2) Logs Apache de tráfego web (páginas visitadas), (3) Arquivos HubSpot CRM (leads e histórico de emails). Descreva cada um para planejar uma análise de churn de clientes.",
                              "finalVerifications": [
                                "Lista e descreve corretamente pelo menos 5 fontes internas com exemplos reais.",
                                "Diferencia fontes internas de externas em um diagrama.",
                                "Documenta metadata básica (formato, origem, uso) para cada fonte.",
                                "Avalia acessibilidade e relevância das fontes identificadas.",
                                "Produz um relatório coeso com mapa de fontes.",
                                "Identifica potenciais issues de qualidade ou privacidade."
                              ],
                              "assessmentCriteria": [
                                "Precisão na identificação (90% das fontes corretas como internas).",
                                "Completude das descrições (inclui origem, formato e uso).",
                                "Profundidade do mapeamento (conexões entre fontes e processos).",
                                "Clareza e organização do documento final.",
                                "Criatividade em exemplos e conexões práticas.",
                                "Atenção a aspectos éticos e de qualidade de dados."
                              ],
                              "crossCurricularConnections": [
                                "Administração e Negócios: Uso de dados internos para KPIs e relatórios gerenciais.",
                                "Tecnologia da Informação: Gerenciamento de bancos de dados e logs de sistemas.",
                                "Estatística: Preparação de dados para análise quantitativa.",
                                "Gestão de Projetos: Mapeamento de dados em fluxos de trabalho ágeis."
                              ],
                              "realWorldApplication": "Em empresas como Amazon ou bancos, identificar fontes internas permite extrair insights de logs e CRM para otimizar operações, prever demandas e personalizar serviços, impulsionando decisões baseadas em dados e competitividade."
                            },
                            "estimatedTime": "1 hora",
                            "difficulty": "beginner",
                            "status": "not_started",
                            "prerequisites": []
                          },
                          {
                            "id": "10.1.4.1.1.2",
                            "name": "Identificar fontes externas de dados",
                            "description": "Listar e exemplificar fontes externas, incluindo APIs públicas, datasets abertos como Kaggle e UCI Repository, e serviços de terceiros.",
                            "atomicExpansion": {
                              "steps": [
                                {
                                  "stepNumber": 1,
                                  "title": "Compreender Conceitos Básicos de Fontes Externas de Dados",
                                  "subSteps": [
                                    "Defina fontes externas de dados como recursos disponíveis fora do controle direto do usuário, como repositórios públicos e APIs.",
                                    "Classifique fontes em categorias: datasets abertos (ex: Kaggle, UCI), APIs públicas (ex: OpenWeatherMap) e serviços de terceiros (ex: Google Maps API).",
                                    "Discuta vantagens (gratuito, atualizado) e desvantagens (qualidade variável, termos de uso).",
                                    "Revise exemplos iniciais: Kaggle para competições de ML, UCI para datasets clássicos de ML.",
                                    "Anote diferenças entre fontes estruturadas (CSV, JSON) e não estruturadas (imagens, texto)."
                                  ],
                                  "verification": "Crie um mapa mental ou tabela resumindo 3 categorias com 2 exemplos cada.",
                                  "estimatedTime": "20 minutos",
                                  "materials": [
                                    "Notebook ou ferramenta de mind mapping (ex: Draw.io, papel e caneta)",
                                    "Acesso à internet para consulta rápida de definições"
                                  ],
                                  "tips": "Use analogias: pense em fontes externas como 'bibliotecas públicas' vs. dados internos como 'seus próprios livros'.",
                                  "learningObjective": "Diferenciar e categorizar fontes externas de dados com exemplos precisos.",
                                  "commonMistakes": [
                                    "Confundir fontes externas com internas (ex: banco de dados próprio)",
                                    "Ignorar restrições de licença ou privacidade"
                                  ]
                                },
                                {
                                  "stepNumber": 2,
                                  "title": "Explorar Repositórios de Datasets Abertos",
                                  "subSteps": [
                                    "Acesse Kaggle.com e busque datasets populares (ex: Titanic, Housing Prices).",
                                    "Navegue pelo UCI Machine Learning Repository (archive.ics.uci.edu) e selecione 3 datasets clássicos (ex: Iris, Wine).",
                                    "Registre metadados: tamanho, formato, descrição e uso comum.",
                                    "Compare Kaggle (comunidade ativa, notebooks) vs. UCI (acadêmico, histórico).",
                                    "Baixe um dataset pequeno para inspeção local (ex: CSV do Iris)."
                                  ],
                                  "verification": "Compile uma lista de 5 datasets com links, tamanhos e descrições breves.",
                                  "estimatedTime": "30 minutos",
                                  "materials": [
                                    "Navegador web",
                                    "Conta gratuita no Kaggle (opcional)",
                                    "Editor de texto ou Excel para visualizar CSV"
                                  ],
                                  "tips": "Use filtros de busca no Kaggle por 'usability' e 'size' para encontrar datasets acessíveis.",
                                  "learningObjective": "Identificar e descrever datasets de repositórios públicos como Kaggle e UCI.",
                                  "commonMistakes": [
                                    "Baixar datasets muito grandes sem verificar requisitos de hardware",
                                    "Não ler a seção de licença"
                                  ]
                                },
                                {
                                  "stepNumber": 3,
                                  "title": "Descobrir e Exemplificar APIs Públicas e Serviços de Terceiros",
                                  "subSteps": [
                                    "Pesquise APIs públicas em sites como publicapis.org ou ProgrammableWeb.",
                                    "Teste uma API simples (ex: JSONPlaceholder para dados fake, ou REST Countries para dados de países).",
                                    "Explore serviços de terceiros: Google Datasets Search, Quandl para finanças.",
                                    "Registre endpoints, autenticação (chave API gratuita?) e exemplos de resposta JSON.",
                                    "Integre um exemplo básico em código (ex: fetch no JavaScript ou requests em Python)."
                                  ],
                                  "verification": "Execute uma chamada de API e capture a resposta em um arquivo JSON ou screenshot.",
                                  "estimatedTime": "40 minutos",
                                  "materials": [
                                    "Navegador com Postman ou Insomnia para testar APIs",
                                    "Ambiente de programação básico (ex: Jupyter Notebook, VS Code)"
                                  ],
                                  "tips": "Comece com APIs sem autenticação para testes rápidos; use ferramentas como curl no terminal.",
                                  "learningObjective": "Listar e testar APIs públicas e serviços de terceiros para coleta de dados.",
                                  "commonMistakes": [
                                    "Esquecer rate limits (limites de chamadas)",
                                    "Não tratar erros de autenticação ou parsing de JSON"
                                  ]
                                },
                                {
                                  "stepNumber": 4,
                                  "title": "Sintetizar e Documentar Fontes Externas para um Caso de Uso",
                                  "subSteps": [
                                    "Escolha um tema (ex: análise climática) e liste 3-5 fontes relevantes.",
                                    "Crie uma tabela comparativa: fonte, tipo, prós, contras, link.",
                                    "Avalie critérios: relevância, frescor dos dados, facilidade de acesso.",
                                    "Documente considerações éticas: atribuição, privacidade (GDPR).",
                                    "Prepare um relatório curto com exemplos de uso."
                                  ],
                                  "verification": "Gere uma tabela ou documento com pelo menos 5 fontes categorizadas e avaliadas.",
                                  "estimatedTime": "25 minutos",
                                  "materials": [
                                    "Planilha Google Sheets ou Markdown",
                                    "Modelos de tabela prontos"
                                  ],
                                  "tips": "Priorize fontes com licenças CC0 ou MIT para uso educacional.",
                                  "learningObjective": "Sintetizar fontes externas em uma lista acionável para projetos reais.",
                                  "commonMistakes": [
                                    "Listar fontes irrelevantes sem ligação ao contexto",
                                    "Ignorar atualizações ou depreciações de APIs"
                                  ]
                                }
                              ],
                              "practicalExample": "Para um projeto de previsão de vendas de varejo, identifique: dataset de vendas no Kaggle (ex: Rossmann Store Sales), UCI para séries temporais, API do Alpha Vantage para dados de ações e Google Trends API para tendências de busca.",
                              "finalVerifications": [
                                "Pode listar pelo menos 5 fontes externas com exemplos específicos (3 datasets + 2 APIs).",
                                "Demonstra compreensão de categorias: repositórios, APIs, serviços de terceiros.",
                                "Testou pelo menos uma API e baixou um dataset.",
                                "Cria uma tabela comparativa de fontes para um tema dado.",
                                "Identifica licenças e restrições em todas as fontes listadas.",
                                "Explica prós/contras de pelo menos 3 fontes."
                              ],
                              "assessmentCriteria": [
                                "Precisão na categorização e exemplos (APIs vs. datasets).",
                                "Profundidade de detalhes (metadados, links, testes).",
                                "Relevância das fontes ao contexto de ciência de dados.",
                                "Inclusão de verificações éticas e práticas (licenças, rate limits).",
                                "Clareza na documentação (tabelas, relatórios acionáveis).",
                                "Criatividade em conexões com casos reais."
                              ],
                              "crossCurricularConnections": [
                                "Programação: Integração via Python (pandas, requests) ou R.",
                                "Estatística: Avaliação de qualidade de dados (limpeza inicial).",
                                "Ética Digital: Privacidade e uso responsável de dados públicos.",
                                "Pesquisa: Técnicas de busca avançada em repositórios acadêmicos."
                              ],
                              "realWorldApplication": "Data scientists usam isso diariamente para prototipar modelos em competições Kaggle ou dashboards empresariais, acelerando a coleta de dados para análises de mercado, saúde pública ou clima."
                            },
                            "estimatedTime": "1.5 horas",
                            "difficulty": "beginner",
                            "status": "not_started",
                            "prerequisites": []
                          },
                          {
                            "id": "10.1.4.1.1.3",
                            "name": "Diferenciar fontes estruturadas e não estruturadas",
                            "description": "Explicar a diferença entre dados estruturados (ex.: SQL), semi-estruturados (ex.: JSON, XML) e não estruturados (ex.: texto, imagens), com exemplos de fontes para cada tipo.",
                            "atomicExpansion": {
                              "steps": [
                                {
                                  "stepNumber": 1,
                                  "title": "Compreender dados estruturados",
                                  "subSteps": [
                                    "Defina dados estruturados como aqueles organizados em formato tabular com linhas e colunas fixas.",
                                    "Estude exemplos clássicos: bancos de dados relacionais como SQL (tabelas com schema rígido).",
                                    "Analise formatos como CSV e Excel, que seguem estruturas pré-definidas.",
                                    "Identifique características: fácil consulta via SQL, esquemas fixos.",
                                    "Compare com armazenamento em bancos como MySQL ou PostgreSQL."
                                  ],
                                  "verification": "Crie uma tabela simples representando dados estruturados e explique seu schema.",
                                  "estimatedTime": "20 minutos",
                                  "materials": [
                                    "Documentação SQL básica",
                                    "Exemplo de tabela CSV",
                                    "Ferramenta como Google Sheets"
                                  ],
                                  "tips": [
                                    "Sempre pense em 'colunas fixas' como chave para estruturados."
                                  ],
                                  "learningObjective": "Identificar e exemplificar dados estruturados com precisão.",
                                  "commonMistakes": [
                                    "Confundir com semi-estruturados que têm tags flexíveis.",
                                    "Ignorar a rigidez do schema."
                                  ]
                                },
                                {
                                  "stepNumber": 2,
                                  "title": "Explorar dados semi-estruturados",
                                  "subSteps": [
                                    "Defina como dados com estrutura parcial, usando tags ou marcadores hierárquicos.",
                                    "Estude formatos: JSON (chave-valor aninhado), XML (tags hierárquicas).",
                                    "Analise exemplos: APIs REST que retornam JSON, feeds RSS em XML.",
                                    "Identifique características: esquema flexível, auto-descritivo, mas navegável.",
                                    "Pratique parsing simples de um JSON usando ferramenta online."
                                  ],
                                  "verification": "Parse um arquivo JSON simples e descreva sua hierarquia.",
                                  "estimatedTime": "25 minutos",
                                  "materials": [
                                    "Editor JSON online (jsonformatter.org)",
                                    "Exemplo de API JSON pública",
                                    "Documentação XML básica"
                                  ],
                                  "tips": [
                                    "Procure por 'tags' ou 'chaves' que indicam semi-estrutura."
                                  ],
                                  "learningObjective": "Reconhecer e diferenciar semi-estruturados de estruturados e não estruturados.",
                                  "commonMistakes": [
                                    "Tratar como não estruturado por falta de rigidez total.",
                                    "Confundir com texto puro."
                                  ]
                                },
                                {
                                  "stepNumber": 3,
                                  "title": "Analisar dados não estruturados",
                                  "subSteps": [
                                    "Defina como dados sem formato pré-definido ou schema rígido.",
                                    "Estude exemplos: textos livres (e-mails, documentos Word), imagens, áudio, vídeos.",
                                    "Analise fontes: redes sociais (posts), logs de servidores, fotos de câmeras.",
                                    "Identifique desafios: requer processamento como NLP para texto ou CV para imagens.",
                                    "Liste ferramentas iniciais: OCR para imagens, TF-IDF para texto."
                                  ],
                                  "verification": "Classifique 5 fontes comuns (ex.: foto, e-mail) como não estruturadas e justifique.",
                                  "estimatedTime": "20 minutos",
                                  "materials": [
                                    "Imagens e textos livres",
                                    "Exemplos de logs de servidor",
                                    "Ferramentas como Notepad++"
                                  ],
                                  "tips": [
                                    "Pergunte: 'Precisa de IA para extrair estrutura?' Se sim, é não estruturado."
                                  ],
                                  "learningObjective": "Exemplificar fontes não estruturadas e seus desafios.",
                                  "commonMistakes": [
                                    "Classificar semi-estruturados como não estruturados.",
                                    "Subestimar volume de dados não estruturados (90% dos dados modernos)."
                                  ]
                                },
                                {
                                  "stepNumber": 4,
                                  "title": "Diferenciar e classificar fontes de dados",
                                  "subSteps": [
                                    "Crie uma tabela comparativa: colunas para estruturado, semi e não estruturado com exemplos.",
                                    "Classifique fontes reais: banco SQL (estruturado), log JSON (semi), imagem (não).",
                                    "Discuta implicações: ferramentas (SQL vs NoSQL vs ML).",
                                    "Pratique com dataset misto: identifique tipos em um conjunto de dados.",
                                    "Explique escolhas de processamento baseadas no tipo."
                                  ],
                                  "verification": "Classifique corretamente 10 fontes variadas em uma tabela.",
                                  "estimatedTime": "30 minutos",
                                  "materials": [
                                    "Planilha comparativa vazia",
                                    "Datasets mistos (Kaggle intro)",
                                    "Diagramas de tipos de dados"
                                  ],
                                  "tips": [
                                    "Use fluxograma: tem schema fixo? Estruturado. Tem tags? Semi. Nada? Não estruturado."
                                  ],
                                  "learningObjective": "Diferenciar tipos com exemplos precisos e fontes reais.",
                                  "commonMistakes": [
                                    "Ignorar semi-estruturados como categoria distinta.",
                                    "Exemplos errados, como chamar HTML de não estruturado."
                                  ]
                                }
                              ],
                              "practicalExample": "Em uma empresa de e-commerce, classifique: vendas em banco SQL (estruturado), comentários de clientes em texto (não estruturado), pedidos em JSON de API (semi-estruturado). Escolha ferramentas: SQL para relatórios, NLP para sentimentos, MongoDB para JSON.",
                              "finalVerifications": [
                                "Explique diferenças sem consultar notas.",
                                "Classifique 5 fontes novas corretamente.",
                                "Crie tabela comparativa com exemplos originais.",
                                "Discuta prós/contras de cada tipo.",
                                "Identifique ferramenta ideal para cada tipo."
                              ],
                              "assessmentCriteria": [
                                "Precisão nas definições (100% correto).",
                                "Exemplos relevantes e variados (mín. 3 por tipo).",
                                "Diferenciação clara entre os 3 tipos.",
                                "Uso correto de termos técnicos (schema, tags, NLP).",
                                "Capacidade de classificar fontes reais.",
                                "Compreensão de implicações práticas."
                              ],
                              "crossCurricularConnections": [
                                "Banco de Dados: esquemas SQL vs NoSQL.",
                                "Programação: parsing JSON/XML em Python.",
                                "Inteligência Artificial: processamento de não estruturados com ML.",
                                "Estatística: limpeza de dados por tipo."
                              ],
                              "realWorldApplication": "Cientistas de dados em empresas como Google usam essa diferenciação para escolher pipelines: SQL para analítica estruturada, Hadoop para não estruturados em big data, e Elasticsearch para semi-estruturados em buscas."
                            },
                            "estimatedTime": "2 horas",
                            "difficulty": "intermediate",
                            "status": "not_started",
                            "prerequisites": []
                          }
                        ]
                      },
                      {
                        "id": "10.1.4.1.2",
                        "name": "Técnicas de Coleta de Dados",
                        "description": "Métodos e processos para aquisição de dados brutos, abrangendo desde extrações manuais até automatizadas.",
                        "specificSkills": [
                          {
                            "id": "10.1.4.1.2.1",
                            "name": "Utilizar APIs para coleta de dados",
                            "description": "Descrever o processo de autenticação e requisições HTTP via APIs (ex.: RESTful), com exemplos como Twitter API ou OpenWeatherMap.",
                            "atomicExpansion": {
                              "steps": [
                                {
                                  "stepNumber": 1,
                                  "title": "Entender conceitos básicos de APIs e preparar o ambiente",
                                  "subSteps": [
                                    "Estude o que é uma API RESTful e métodos HTTP (GET, POST, etc.).",
                                    "Instale bibliotecas necessárias no Python (ex: requests via pip).",
                                    "Escolha uma API pública simples como OpenWeatherMap e leia sua documentação.",
                                    "Crie um script Python básico para testes.",
                                    "Configure variáveis de ambiente para chaves de API."
                                  ],
                                  "verification": "Ambiente configurado com script rodando um print('Ambiente pronto') sem erros.",
                                  "estimatedTime": "45 minutos",
                                  "materials": [
                                    "Python instalado",
                                    "Editor de código (VS Code ou Jupyter)",
                                    "Documentação da OpenWeatherMap API"
                                  ],
                                  "tips": [
                                    "Comece com APIs gratuitas sem autenticação complexa.",
                                    "Use Jupyter Notebook para testes interativos."
                                  ],
                                  "learningObjective": "Compreender fundamentos de APIs e preparar ferramentas para uso.",
                                  "commonMistakes": [
                                    "Ignorar rate limits da API.",
                                    "Não ler a documentação antes de começar."
                                  ]
                                },
                                {
                                  "stepNumber": 2,
                                  "title": "Obter credenciais e configurar autenticação",
                                  "subSteps": [
                                    "Registre-se na API escolhida (ex: OpenWeatherMap) e obtenha API key.",
                                    "Implemente autenticação via headers ou query params no código Python.",
                                    "Teste autenticação com uma requisição simples de validação.",
                                    "Armazene a chave de forma segura (variáveis de ambiente).",
                                    "Trate erros de autenticação comuns (401 Unauthorized)."
                                  ],
                                  "verification": "Requisição de teste retorna status 200 e dados válidos sem erros de autenticação.",
                                  "estimatedTime": "30 minutos",
                                  "materials": [
                                    "Conta gratuita na OpenWeatherMap",
                                    "Biblioteca requests",
                                    "Documentação de autenticação da API"
                                  ],
                                  "tips": [
                                    "Nunca hardcode chaves no código.",
                                    "Use dotenv para gerenciar variáveis."
                                  ],
                                  "learningObjective": "Dominar processo de autenticação segura em APIs.",
                                  "commonMistakes": [
                                    "Expor chaves de API publicamente.",
                                    "Usar método errado de autenticação (OAuth vs API Key)."
                                  ]
                                },
                                {
                                  "stepNumber": 3,
                                  "title": "Realizar requisições HTTP e capturar dados",
                                  "subSteps": [
                                    "Construa URL endpoint com parâmetros necessários (ex: cidade para previsão do tempo).",
                                    "Use requests.get() com headers de autenticação e params.",
                                    "Adicione tratamento de exceções para erros HTTP (404, 429).",
                                    "Salve resposta JSON em uma variável e imprima para inspeção.",
                                    "Implemente paginação ou múltiplas requisições se aplicável."
                                  ],
                                  "verification": "Dados JSON capturados e impressos corretamente de pelo menos 3 requisições diferentes.",
                                  "estimatedTime": "60 minutos",
                                  "materials": [
                                    "Código Python do step anterior",
                                    "Postman ou Insomnia para testar endpoints manualmente"
                                  ],
                                  "tips": [
                                    "Sempre verifique status_code antes de parsear JSON.",
                                    "Use time.sleep() para respeitar rate limits."
                                  ],
                                  "learningObjective": "Executar requisições HTTP eficazes e lidar com respostas.",
                                  "commonMistakes": [
                                    "Esquecer params obrigatórios na URL.",
                                    "Não tratar respostas vazias ou erros de rede."
                                  ]
                                },
                                {
                                  "stepNumber": 4,
                                  "title": "Processar dados coletados e validar resultados",
                                  "subSteps": [
                                    "Parse o JSON com json.loads() ou response.json().",
                                    "Extraia campos relevantes (ex: temperatura, umidade).",
                                    "Converta dados para DataFrame Pandas para análise básica.",
                                    "Salve dados em CSV ou visualize com matplotlib.",
                                    "Teste integridade dos dados com verificações simples (não nulos)."
                                  ],
                                  "verification": "Dados processados salvos em arquivo CSV com pelo menos 10 registros válidos.",
                                  "estimatedTime": "45 minutos",
                                  "materials": [
                                    "Bibliotecas pandas e matplotlib",
                                    "Dados da requisição anterior"
                                  ],
                                  "tips": [
                                    "Use try-except para parsing JSON falho.",
                                    "Valide tipos de dados após parsing."
                                  ],
                                  "learningObjective": "Transformar dados brutos de API em formato utilizável para análise.",
                                  "commonMistakes": [
                                    "Assumir estrutura fixa do JSON.",
                                    "Ignorar campos aninhados."
                                  ]
                                }
                              ],
                              "practicalExample": "Usando OpenWeatherMap API: Registre-se, obtenha API key, faça GET para 'api.openweathermap.org/data/2.5/weather?q=London&appid={key}', extraia temperatura e umidade, salve em CSV com Pandas.",
                              "finalVerifications": [
                                "Script completo roda sem erros e coleta dados reais.",
                                "Dados processados mostram estatísticas básicas (média de temperatura).",
                                "Autenticação funciona em múltiplas execuções.",
                                "Tratamento de erros cobre cenários comuns (sem internet, API down).",
                                "Código é legível com comentários.",
                                "Dados salvos em formato reutilizável."
                              ],
                              "assessmentCriteria": [
                                "Precisão na implementação de autenticação (sem vazamento de keys).",
                                "Robustez no tratamento de erros HTTP e JSON.",
                                "Eficiência na coleta (respeito a limits, timeouts).",
                                "Qualidade do processamento de dados (extração correta, validação).",
                                "Documentação e comentários no código.",
                                "Criatividade na extensão (ex: múltiplas cidades)."
                              ],
                              "crossCurricularConnections": [
                                "Matemática: Cálculo de estatísticas descritivas nos dados coletados.",
                                "Programação: Estruturas de dados e manipulação de JSON.",
                                "Estatística: Análise exploratória de dados meteorológicos.",
                                "Geografia: Integração com mapas para visualização espacial."
                              ],
                              "realWorldApplication": "Coleta automatizada de dados climáticos para previsão de safras agrícolas, análise de tendências de tráfego via APIs de mapas (Google Maps), ou monitoramento de redes sociais para análise de sentimentos em tempo real."
                            },
                            "estimatedTime": "2 horas",
                            "difficulty": "intermediate",
                            "status": "not_started",
                            "prerequisites": [
                              "10.1.4.1.1.1"
                            ]
                          },
                          {
                            "id": "10.1.4.1.2.2",
                            "name": "Aplicar web scraping",
                            "description": "Explicar técnicas de extração de dados de websites usando bibliotecas como BeautifulSoup e Selenium, considerando robots.txt e limitações éticas.",
                            "atomicExpansion": {
                              "steps": [
                                {
                                  "stepNumber": 1,
                                  "title": "Planejamento e Considerações Éticas",
                                  "subSteps": [
                                    "Analise o site alvo: identifique estrutura HTML e conteúdo a extrair.",
                                    "Verifique robots.txt do site para respeitar regras de crawling.",
                                    "Pesquise termos de uso e leis de proteção de dados (LGPD/GDPR).",
                                    "Defina escopo: quais dados, frequência e volume de requisições.",
                                    "Planeje rate limiting para evitar sobrecarga no servidor."
                                  ],
                                  "verification": "Documento de planejamento criado com análise de robots.txt e escopo definido.",
                                  "estimatedTime": "45 minutos",
                                  "materials": [
                                    "Acesso à internet",
                                    "Navegador web",
                                    "Editor de texto"
                                  ],
                                  "tips": "Use ferramentas como robots.txt checker online para validar permissões.",
                                  "learningObjective": "Compreender implicações éticas e legais do web scraping.",
                                  "commonMistakes": [
                                    "Ignorar robots.txt",
                                    "Fazer scraping em massa sem rate limiting",
                                    "Não citar fonte dos dados"
                                  ]
                                },
                                {
                                  "stepNumber": 2,
                                  "title": "Configuração do Ambiente de Desenvolvimento",
                                  "subSteps": [
                                    "Instale Python e crie ambiente virtual com venv.",
                                    "Instale bibliotecas: pip install requests beautifulsoup4 selenium pandas.",
                                    "Configure driver do Selenium (ChromeDriver ou similar).",
                                    "Teste importações básicas em um script simples.",
                                    "Crie estrutura de projeto com pastas para scripts e dados."
                                  ],
                                  "verification": "Script de teste roda sem erros de importação ou configuração.",
                                  "estimatedTime": "30 minutos",
                                  "materials": [
                                    "Python 3.8+",
                                    "pip",
                                    "ChromeDriver",
                                    "VS Code ou PyCharm"
                                  ],
                                  "tips": "Sempre use ambientes virtuais para isolar dependências.",
                                  "learningObjective": "Configurar ferramentas essenciais para web scraping.",
                                  "commonMistakes": [
                                    "Versões incompatíveis de driver e navegador",
                                    "Esquecer de ativar venv",
                                    "Instalar pacotes globalmente"
                                  ]
                                },
                                {
                                  "stepNumber": 3,
                                  "title": "Extração de Dados com BeautifulSoup",
                                  "subSteps": [
                                    "Faça requisição HTTP com requests.get() e verifique status code.",
                                    "Parse o HTML com BeautifulSoup(..., 'html.parser').",
                                    "Use seletores CSS ou find_all para extrair elementos específicos.",
                                    "Armazene dados em lista de dicionários.",
                                    "Adicione headers User-Agent para simular navegador."
                                  ],
                                  "verification": "Dados extraídos impressos no console correspondem ao site.",
                                  "estimatedTime": "1 hora",
                                  "materials": [
                                    "Site de teste como quotes.toscrape.com",
                                    "Bibliotecas requests e bs4"
                                  ],
                                  "tips": "Inspecione elementos com DevTools do navegador para selectors precisos.",
                                  "learningObjective": "Extrair dados de páginas estáticas de forma eficiente.",
                                  "commonMistakes": [
                                    "Parsing sem checar status 200",
                                    "Selectors frágeis que quebram com mudanças no site",
                                    "Não tratar encoding UTF-8"
                                  ]
                                },
                                {
                                  "stepNumber": 4,
                                  "title": "Manipulação de Conteúdo Dinâmico com Selenium",
                                  "subSteps": [
                                    "Inicie driver do Selenium com opções headless se necessário.",
                                    "Navegue até a página e aguarde carregamento com WebDriverWait.",
                                    "Interaja com elementos: click, scroll ou preencha forms.",
                                    "Extraia dados do DOM renderizado com find_element.",
                                    "Feche o driver após extração."
                                  ],
                                  "verification": "Dados dinâmicos (ex: após JS load) são capturados corretamente.",
                                  "estimatedTime": "1 hora e 15 minutos",
                                  "materials": [
                                    "Selenium instalado",
                                    "Driver compatível",
                                    "Site com JS como dynamic.toscrape.com"
                                  ],
                                  "tips": "Use explicit waits para evitar falhas em elementos assíncronos.",
                                  "learningObjective": "Lidar com sites que carregam conteúdo via JavaScript.",
                                  "commonMistakes": [
                                    "Timeouts curtos",
                                    "Rodar sem headless em produção",
                                    "Não gerenciar múltiplas abas"
                                  ]
                                },
                                {
                                  "stepNumber": 5,
                                  "title": "Processamento, Armazenamento e Validação",
                                  "subSteps": [
                                    "Limpe e normalize dados com pandas ou string methods.",
                                    "Salve em CSV/JSON com pandas.to_csv() ou json.dump().",
                                    "Implemente logging e tratamento de exceções.",
                                    "Teste script com diferentes cenários (erros 404, bloqueios).",
                                    "Documente o código com comentários e README."
                                  ],
                                  "verification": "Arquivo de dados gerado com integridade (sem NaNs, duplicatas).",
                                  "estimatedTime": "45 minutos",
                                  "materials": [
                                    "Pandas instalado",
                                    "Editor de código"
                                  ],
                                  "tips": "Valide dados com asserts ou funções de checagem.",
                                  "learningObjective": "Transformar dados brutos em formato utilizável e robusto.",
                                  "commonMistakes": [
                                    "Não tratar exceções de rede",
                                    "Salvar dados sujos",
                                    "Falta de logging"
                                  ]
                                }
                              ],
                              "practicalExample": "Desenvolva um script para extrair títulos, autores e preços de livros do site de teste http://books.toscrape.com, salvando em CSV. Inclua checagem de robots.txt e rate limiting de 2s entre páginas.",
                              "finalVerifications": [
                                "Script roda end-to-end sem erros e respeita robots.txt.",
                                "Dados extraídos são precisos e completos (comparar manualmente).",
                                "Arquivo CSV/JSON gerado com estrutura limpa e sem duplicatas.",
                                "Tratamento de erros implementado (ex: retry em falhas).",
                                "Tempo de execução razoável (<5min para 20 páginas).",
                                "Código documentado e modular."
                              ],
                              "assessmentCriteria": [
                                "Precisão dos dados extraídos (>95% match com site).",
                                "Eficiência: uso mínimo de recursos e requisições.",
                                "Robustez: lida com variações no site e erros de rede.",
                                "Conformidade ética: rate limiting e checagem de permissões.",
                                "Qualidade do código: legível, comentado e versionável.",
                                "Escalabilidade: pronto para múltiplas páginas."
                              ],
                              "crossCurricularConnections": [
                                "Ética em Computação e Direito Digital (LGPD).",
                                "Programação em Python e Estruturas de Dados.",
                                "Análise de Dados e Manipulação com Pandas.",
                                "Redes e Protocolos Web (HTTP, HTML/CSS/JS).",
                                "Matemática Computacional: Processamento de Grandes Volumes de Dados."
                              ],
                              "realWorldApplication": "Monitorar preços em e-commerces para alertas de promoções, coletar dados públicos para análises de mercado em empresas de consultoria, ou apoiar jornalismo de dados raspando estatísticas governamentais."
                            },
                            "estimatedTime": "3 horas",
                            "difficulty": "intermediate",
                            "status": "not_started",
                            "prerequisites": [
                              "10.1.4.1.1.2"
                            ]
                          },
                          {
                            "id": "10.1.4.1.2.3",
                            "name": "Realizar consultas em bancos de dados",
                            "description": "Executar queries SQL básicas para extrair dados de bancos relacionais como MySQL ou PostgreSQL, focando em SELECT com filtros.",
                            "atomicExpansion": {
                              "steps": [
                                {
                                  "stepNumber": 1,
                                  "title": "Preparar o ambiente de banco de dados",
                                  "subSteps": [
                                    "Instalar um SGBD relacional como MySQL ou PostgreSQL.",
                                    "Criar um banco de dados de teste com pelo menos uma tabela de exemplo (ex: tabela 'vendas' com colunas id, produto, preco, data).",
                                    "Configurar um cliente SQL como MySQL Workbench, pgAdmin ou linha de comando.",
                                    "Estabelecer conexão com o banco de dados usando credenciais.",
                                    "Executar um comando simples como SHOW DATABASES ou \\l para listar bancos."
                                  ],
                                  "verification": "Conexão bem-sucedida e visualização da lista de bancos de dados sem erros.",
                                  "estimatedTime": "20 minutos",
                                  "materials": [
                                    "Instalador MySQL/PostgreSQL",
                                    "Cliente SQL (Workbench/pgAdmin)",
                                    "Script SQL de criação de tabela de exemplo"
                                  ],
                                  "tips": "Use senhas fortes e documente credenciais em um arquivo seguro.",
                                  "learningObjective": "Configurar e conectar a um banco relacional funcional.",
                                  "commonMistakes": [
                                    "Porta errada (padrão 3306 MySQL, 5432 PostgreSQL)",
                                    "Esquecer de iniciar o serviço do SGBD",
                                    "Credenciais incorretas"
                                  ]
                                },
                                {
                                  "stepNumber": 2,
                                  "title": "Explorar a estrutura do banco de dados",
                                  "subSteps": [
                                    "Listar todas as tabelas com SHOW TABLES ou \\dt.",
                                    "Descrever a estrutura de uma tabela específica com DESCRIBE tabela ou \\d tabela.",
                                    "Executar SELECT * FROM tabela LIMIT 5 para visualizar dados amostra.",
                                    "Identificar chaves primárias, estrangeiras e tipos de dados das colunas.",
                                    "Anotar relacionamentos entre tabelas se houver múltiplas."
                                  ],
                                  "verification": "Lista de tabelas e estrutura detalhada de pelo menos uma tabela exibida corretamente.",
                                  "estimatedTime": "15 minutos",
                                  "materials": [
                                    "Cliente SQL conectado",
                                    "Banco de dados com dados de exemplo"
                                  ],
                                  "tips": "Sempre use LIMIT em SELECT * para evitar sobrecarga em tabelas grandes.",
                                  "learningObjective": "Compreender o esquema do banco para planejar queries.",
                                  "commonMistakes": [
                                    "Ignorar tipos de dados (ex: string vs int em filtros)",
                                    "Não notar constraints como NOT NULL"
                                  ]
                                },
                                {
                                  "stepNumber": 3,
                                  "title": "Construir consultas SELECT básicas",
                                  "subSteps": [
                                    "Escrever SELECT * FROM tabela para retornar todos os registros.",
                                    "Selecionar colunas específicas: SELECT coluna1, coluna2 FROM tabela.",
                                    "Usar aliases: SELECT coluna AS alias FROM tabela.",
                                    "Executar a query e verificar saída.",
                                    "Salvar a query em um arquivo .sql para reutilização."
                                  ],
                                  "verification": "Query retorna dados corretos sem erros de sintaxe.",
                                  "estimatedTime": "25 minutos",
                                  "materials": [
                                    "Cliente SQL",
                                    "Tabela com dados"
                                  ],
                                  "tips": "Teste queries em partes pequenas antes de complexificar.",
                                  "learningObjective": "Extrair subconjuntos de dados usando SELECT.",
                                  "commonMistakes": [
                                    "Vírgulas faltando entre colunas",
                                    "Nome de tabela/coluna errado (case-sensitive em PostgreSQL)"
                                  ]
                                },
                                {
                                  "stepNumber": 4,
                                  "title": "Aplicar filtros com cláusulas WHERE, ORDER BY e LIMIT",
                                  "subSteps": [
                                    "Adicionar WHERE: SELECT * FROM tabela WHERE preco > 100.",
                                    "Combinar condições: WHERE preco > 100 AND data > '2023-01-01'.",
                                    "Ordenar: ORDER BY preco DESC.",
                                    "Limitar resultados: LIMIT 10.",
                                    "Testar variações com diferentes operadores (=, >, LIKE, IN)."
                                  ],
                                  "verification": "Resultados filtrados correspondem aos critérios esperados.",
                                  "estimatedTime": "30 minutos",
                                  "materials": [
                                    "Cliente SQL",
                                    "Queries básicas salvas"
                                  ],
                                  "tips": "Use aspas simples para strings e datas; evite aspas duplas.",
                                  "learningObjective": "Refinar extrações de dados com filtros precisos.",
                                  "commonMistakes": [
                                    "Operadores lógicos errados (AND vs OR)",
                                    "Falta de aspas em strings",
                                    "ORDER BY coluna inexistente"
                                  ]
                                },
                                {
                                  "stepNumber": 5,
                                  "title": "Executar, depurar e validar queries",
                                  "subSteps": [
                                    "Executar query completa e comparar com dados conhecidos.",
                                    "Identificar erros comuns (sintaxe, lógica) e corrigi-los.",
                                    "Usar EXPLAIN para analisar performance básica.",
                                    "Exportar resultados para CSV.",
                                    "Documentar a query com comentários (-- comentário)."
                                  ],
                                  "verification": "Query executa sem erros, resultados validados e documentados.",
                                  "estimatedTime": "20 minutos",
                                  "materials": [
                                    "Cliente SQL",
                                    "Todas queries anteriores"
                                  ],
                                  "tips": "Sempre valide com COUNT(*) antes de SELECT grande.",
                                  "learningObjective": "Garantir queries funcionais e otimizadas.",
                                  "commonMistakes": [
                                    "Não escapar caracteres especiais",
                                    "Ignorar índices em WHERE",
                                    "Queries sem LIMIT em produção"
                                  ]
                                }
                              ],
                              "practicalExample": "Em um banco de vendas, execute: SELECT produto, preco FROM vendas WHERE preco > 50 AND data >= '2024-01-01' ORDER BY preco DESC LIMIT 5; para listar os 5 produtos mais caros vendidos em 2024 acima de R$50.",
                              "finalVerifications": [
                                "Executa SELECT * FROM tabela corretamente.",
                                "Aplica WHERE com múltiplas condições sem erros.",
                                "Usa ORDER BY e LIMIT para refinar resultados.",
                                "Explora esquema com DESCRIBE/SHOW.",
                                "Depura erros de sintaxe comuns.",
                                "Exporta resultados para análise externa."
                              ],
                              "assessmentCriteria": [
                                "Sintaxe SQL 100% correta e compatível com MySQL/PostgreSQL.",
                                "Resultados exatos conforme critérios de filtro.",
                                "Eficiência: uso de LIMIT e índices implícitos.",
                                "Documentação clara com comentários.",
                                "Tratamento de edge cases (dados nulos, vazios).",
                                "Tempo de execução razoável (<5s para tabelas médias)."
                              ],
                              "crossCurricularConnections": [
                                "Matemática: Estatística descritiva em resultados agregados.",
                                "Programação: Integração SQL com Python (pandas.read_sql).",
                                "Ciência de Dados: Pré-processamento de dados coletados.",
                                "Estatística: Filtros probabilísticos e amostragem."
                              ],
                              "realWorldApplication": "Em empresas de e-commerce, analistas usam queries SQL para extrair vendas por região e período, gerando relatórios para decisões de estoque e marketing."
                            },
                            "estimatedTime": "2.5 horas",
                            "difficulty": "intermediate",
                            "status": "not_started",
                            "prerequisites": [
                              "10.1.4.1.1.1"
                            ]
                          },
                          {
                            "id": "10.1.4.1.2.4",
                            "name": "Coletar dados de sensores e IoT",
                            "description": "Descrever aquisição de dados em tempo real de dispositivos IoT, como sensores via MQTT ou streams de dados.",
                            "atomicExpansion": {
                              "steps": [
                                {
                                  "stepNumber": 1,
                                  "title": "Preparar Hardware e Ambiente IoT",
                                  "subSteps": [
                                    "Selecionar sensor adequado (ex: DHT22 para temperatura e umidade)",
                                    "Verificar conectividade Wi-Fi no dispositivo (ex: ESP32)",
                                    "Instalar e configurar broker MQTT local (Mosquitto) ou cloud (HiveMQ)",
                                    "Criar tópicos MQTT (ex: /sensor/dados/temp)",
                                    "Testar conectividade básica entre dispositivo e broker"
                                  ],
                                  "verification": "Broker MQTT rodando e acessível via porta 1883; ping ou telnet confirma conexão.",
                                  "estimatedTime": "25 minutos",
                                  "materials": [
                                    "Dispositivo IoT (ESP32 ou Raspberry Pi)",
                                    "Sensor (DHT22)",
                                    "Computador com Arduino IDE ou Thonny",
                                    "Mosquitto broker instalado"
                                  ],
                                  "tips": [
                                    "Use broker local para evitar custos iniciais",
                                    "Documente credenciais de usuário/senha do broker"
                                  ],
                                  "learningObjective": "Identificar e configurar componentes essenciais para rede IoT.",
                                  "commonMistakes": [
                                    "Firewall bloqueando porta MQTT (1883)",
                                    "Esquecer de habilitar Wi-Fi no dispositivo"
                                  ]
                                },
                                {
                                  "stepNumber": 2,
                                  "title": "Configurar Publicador de Dados no Sensor",
                                  "subSteps": [
                                    "Instalar biblioteca MQTT no firmware (PubSubClient para ESP32/Arduino)",
                                    "Implementar leitura periódica do sensor (a cada 10 segundos)",
                                    "Estabelecer conexão segura com broker (QoS 1)",
                                    "Formatar e publicar payload JSON no tópico definido",
                                    "Adicionar loop de reconexão em caso de falha"
                                  ],
                                  "verification": "Mensagens publicadas visíveis no cliente MQTT Explorer.",
                                  "estimatedTime": "40 minutos",
                                  "materials": [
                                    "Arduino IDE com ESP32 board support",
                                    "Biblioteca PubSubClient",
                                    "Cabos jumper e protoboard"
                                  ],
                                  "tips": [
                                    "Use JSON para payload: {'temp': 25.5, 'umidade': 60}",
                                    "Defina intervalo de publicação para evitar spam"
                                  ],
                                  "learningObjective": "Implementar envio de dados em tempo real via protocolo MQTT.",
                                  "commonMistakes": [
                                    "Payload mal formatado causando perda de dados",
                                    "Não tratar desconexões Wi-Fi"
                                  ]
                                },
                                {
                                  "stepNumber": 3,
                                  "title": "Desenvolver Cliente Subscriptor para Coleta",
                                  "subSteps": [
                                    "Instalar biblioteca paho-mqtt em Python (pip install paho-mqtt)",
                                    "Criar script para subscrever ao tópico MQTT",
                                    "Parsear mensagens recebidas e armazenar em lista ou CSV",
                                    "Implementar callback on_message para processamento assíncrono",
                                    "Adicionar persistência em arquivo ou SQLite"
                                  ],
                                  "verification": "Dados coletados aparecem em console ou arquivo CSV em tempo real.",
                                  "estimatedTime": "35 minutos",
                                  "materials": [
                                    "Python 3.8+",
                                    "Biblioteca paho-mqtt",
                                    "Editor de código (VS Code)"
                                  ],
                                  "tips": [
                                    "Use threading para não bloquear o loop principal",
                                    "Log erros de conexão para debug"
                                  ],
                                  "learningObjective": "Receber e processar streams de dados IoT de forma eficiente.",
                                  "commonMistakes": [
                                    "Não definir client ID único",
                                    "Ignorar retained messages"
                                  ]
                                },
                                {
                                  "stepNumber": 4,
                                  "title": "Testar, Monitorar e Visualizar Coleta em Tempo Real",
                                  "subSteps": [
                                    "Executar simultaneamente publicador e subscriptor",
                                    "Medir latência entre publicação e recebimento",
                                    "Criar dashboard simples com Matplotlib ou Plotly para visualização",
                                    "Simular falhas (desconectar Wi-Fi) e verificar recuperação",
                                    "Analisar dados coletados para anomalias"
                                  ],
                                  "verification": "Gráfico atualizando em tempo real com dados precisos e baixa latência (<5s).",
                                  "estimatedTime": "30 minutos",
                                  "materials": [
                                    "Biblioteca matplotlib ou plotly",
                                    "MQTT Explorer para monitoramento"
                                  ],
                                  "tips": [
                                    "Grave timestamps para análise temporal",
                                    "Use wildcards em tópicos para múltiplos sensores"
                                  ],
                                  "learningObjective": "Validar pipeline completo de coleta IoT e identificar gargalos.",
                                  "commonMistakes": [
                                    "Sobrecarga de rede com alta frequência de publicação",
                                    "Não validar precisão dos dados do sensor"
                                  ]
                                }
                              ],
                              "practicalExample": "Configure um ESP32 com sensor DHT22 para publicar temperatura e umidade a cada 10 segundos via MQTT em tópico '/fazenda/sensor1'. Um script Python subscreve, armazena em CSV e plota gráfico em tempo real com Matplotlib, simulando monitoramento de estufa.",
                              "finalVerifications": [
                                "Dados fluindo continuamente por 10+ minutos sem perda",
                                "Latência média < 3 segundos entre envio e recebimento",
                                "Reconexão automática após interrupção de rede",
                                "Dados armazenados corretamente em formato estruturado (CSV/JSON)",
                                "Visualização em dashboard atualizando em tempo real",
                                "Precisão dos dados confirmada por medição manual"
                              ],
                              "assessmentCriteria": [
                                "Configuração correta de broker e tópicos MQTT (QoS e segurança)",
                                "Tratamento robusto de erros e reconexões",
                                "Eficiência no parsing e armazenamento de streams",
                                "Escalabilidade para múltiplos sensores/tópicos",
                                "Baixa latência e alta taxa de entrega de mensagens",
                                "Código limpo com logging e comentários"
                              ],
                              "crossCurricularConnections": [
                                "Programação: Python e Embedded C para IoT",
                                "Redes: Protocolos MQTT, TCP/IP e QoS",
                                "Física: Princípios de sensores e calibração",
                                "Matemática Computacional: Séries temporais e estatísticas descritivas",
                                "Ciência de Dados: Pipelines de ingestão de dados"
                              ],
                              "realWorldApplication": "Em agricultura de precisão, sensores IoT em campos coletam dados de solo, clima e umidade via MQTT para sistemas centrais que otimizam irrigação e alertam sobre pragas, reduzindo desperdício de água em 30%."
                            },
                            "estimatedTime": "2 horas",
                            "difficulty": "advanced",
                            "status": "not_started",
                            "prerequisites": [
                              "10.1.4.1.1.3"
                            ]
                          }
                        ]
                      },
                      {
                        "id": "10.1.4.1.3",
                        "name": "Processos e Considerações na Coleta",
                        "description": "Procedimentos para garantir a qualidade inicial, ética e legalidade na aquisição de dados brutos.",
                        "specificSkills": [
                          {
                            "id": "10.1.4.1.3.1",
                            "name": "Avaliar qualidade inicial dos dados coletados",
                            "description": "Verificar completude, precisão e relevância dos dados brutos imediatamente após a coleta, usando métricas básicas como taxa de valores ausentes.",
                            "atomicExpansion": {
                              "steps": [
                                {
                                  "stepNumber": 1,
                                  "title": "Carregar e Inspecionar Inicial os Dados Brutos",
                                  "subSteps": [
                                    "Importe as bibliotecas necessárias (ex: pandas, numpy) em um ambiente Python como Jupyter Notebook.",
                                    "Carregue o dataset usando pd.read_csv() ou equivalente, especificando o caminho do arquivo.",
                                    "Execute dataset.shape para verificar dimensões (linhas e colunas).",
                                    "Use dataset.info() para overview de tipos de dados, memória e valores não-nulos.",
                                    "Visualize as primeiras e últimas linhas com dataset.head(10) e dataset.tail(10)."
                                  ],
                                  "verification": "Confirme que o dataset foi carregado corretamente sem erros de parsing e que shape e info mostram estrutura esperada.",
                                  "estimatedTime": "15 minutos",
                                  "materials": [
                                    "Python com pandas e numpy instalados",
                                    "Jupyter Notebook ou IDE similar",
                                    "Arquivo de dados brutos (CSV/Excel)"
                                  ],
                                  "tips": "Sempre defina encoding='utf-8' no read_csv para evitar problemas com caracteres especiais.",
                                  "learningObjective": "Entender a estrutura básica do dataset para identificar problemas iniciais.",
                                  "commonMistakes": [
                                    "Esquecer de importar bibliotecas",
                                    "Caminho incorreto do arquivo levando a erro FileNotFound",
                                    "Ignorar warnings de dtype inference"
                                  ]
                                },
                                {
                                  "stepNumber": 2,
                                  "title": "Avaliar Completude dos Dados (Valores Ausentes)",
                                  "subSteps": [
                                    "Calcule valores ausentes por coluna: dataset.isnull().sum().",
                                    "Compute a taxa percentual de missing values: (dataset.isnull().sum() / len(dataset)) * 100.",
                                    "Identifique colunas com alta taxa de missing (>10-20%) usando sort_values().",
                                    "Visualize missing data com heatmap (seaborn.heatmap(dataset.isnull())) ou msno.matrix().",
                                    "Documente colunas problemáticas em um dicionário ou DataFrame separado."
                                  ],
                                  "verification": "Gere um relatório mostrando % de missing por coluna, com todas as taxas calculadas corretamente.",
                                  "estimatedTime": "20 minutos",
                                  "materials": [
                                    "Biblioteca seaborn ou missingno para visualizações",
                                    "Dataset carregado do step anterior"
                                  ],
                                  "tips": "Priorize colunas com >50% missing para possível remoção imediata.",
                                  "learningObjective": "Quantificar e visualizar lacunas nos dados para priorizar tratamentos.",
                                  "commonMistakes": [
                                    "Confundir NaN com zeros ou strings vazias",
                                    "Não calcular percentuais, só contagens absolutas",
                                    "Ignorar missing em colunas categóricas"
                                  ]
                                },
                                {
                                  "stepNumber": 3,
                                  "title": "Verificar Precisão dos Dados (Tipos, Duplicatas e Ranges)",
                                  "subSteps": [
                                    "Inspecione tipos de dados: dataset.dtypes e converta se necessário (pd.to_numeric()).",
                                    "Detecte duplicatas: dataset.duplicated().sum() e dataset.drop_duplicates(keep=False).",
                                    "Verifique ranges e estatísticas descritivas: dataset.describe() para numéricos e value_counts() para categóricos.",
                                    "Identifique outliers básicos comparando quartis (Q1, Q3) e IQR.",
                                    "Corrija tipos evidentes errados, como datas como strings."
                                  ],
                                  "verification": "Relatório confirmando tipos corretos, número de duplicatas removidas e ranges dentro de expectativas domínio-específicas.",
                                  "estimatedTime": "25 minutos",
                                  "materials": [
                                    "Pandas para describe() e duplicated()",
                                    "Conhecimento do domínio dos dados (ex: ranges esperados)"
                                  ],
                                  "tips": "Use assert statements para validar ranges, ex: assert dataset['idade'].max() < 120.",
                                  "learningObjective": "Garantir que dados sejam precisos e confiáveis para análises subsequentes.",
                                  "commonMistakes": [
                                    "Não detectar duplicatas exatas vs. fuzzy",
                                    "Assumir tipos sem verificar (ex: float como int)",
                                    "Ignorar outliers como erros reais"
                                  ]
                                },
                                {
                                  "stepNumber": 4,
                                  "title": "Avaliar Relevância e Gerar Relatório Inicial de Qualidade",
                                  "subSteps": [
                                    "Liste colunas e avalie relevância para o objetivo da análise (ex: remover IDs irrelevantes).",
                                    "Compile métricas em um DataFrame de qualidade: colunas, %missing, duplicatas, etc.",
                                    "Gere visualizações resumidas (barplot de %missing).",
                                    "Escreva conclusões: dados prontos? Ações necessárias (imputação, drop)?",
                                    "Salve relatório como CSV ou PDF para compartilhamento."
                                  ],
                                  "verification": "Relatório final salvo com todas métricas, recomendações e score geral de qualidade (ex: 85% bom).",
                                  "estimatedTime": "20 minutos",
                                  "materials": [
                                    "Matplotlib/Seaborn para plots",
                                    "Pandas para exportar relatório"
                                  ],
                                  "tips": "Defina score de qualidade como média ponderada: (1 - avg_missing) * 100.",
                                  "learningObjective": "Sintetizar achados para decisões informadas sobre próximos passos.",
                                  "commonMistakes": [
                                    "Manter colunas irrelevantes por 'cautela'",
                                    "Relatório sem visualizações ou quantificações",
                                    "Não contextualizar com objetivo da análise"
                                  ]
                                }
                              ],
                              "practicalExample": "Em um dataset de vendas de e-commerce (vendas.csv com colunas: data, produto, quantidade, preco), calcule 15% missing em 'preco', detecte 5 duplicatas de transações e remova coluna 'id_interno' irrelevante, resultando em relatório recomendando imputação mediana para preco.",
                              "finalVerifications": [
                                "Taxa média de valores ausentes calculada e <20% por coluna crítica.",
                                "Duplicatas identificadas e tratadas.",
                                "Tipos de dados validados e corrigidos onde aplicável.",
                                "Relatório de qualidade gerado e salvo.",
                                "Recomendações claras para tratamento de issues.",
                                "Visualizações de missing data e descritivas incluídas."
                              ],
                              "assessmentCriteria": [
                                "Precisão nas métricas de completude (erro <1% nos cálculos).",
                                "Identificação correta de pelo menos 80% dos issues reais no dataset.",
                                "Relatório completo com todas as seções e visualizações.",
                                "Código limpo, comentado e reproduzível.",
                                "Interpretação contextualizada dos resultados.",
                                "Tempo respeitado sem sacrificar qualidade."
                              ],
                              "crossCurricularConnections": [
                                "Estatística Descritiva: Uso de métricas como mean, std, quartis.",
                                "Programação: Manipulação de dataframes em Python/Pandas.",
                                "Ética em Dados: Garantir qualidade para evitar vieses em decisões.",
                                "Gestão de Projetos: Documentação inicial para equipes."
                              ],
                              "realWorldApplication": "Em empresas como bancos ou saúde, avaliações iniciais de dados de transações ou pacientes previnem erros caros em modelos de ML, economizando horas de depuração e garantindo compliance regulatório (ex: LGPD/GDPR)."
                            },
                            "estimatedTime": "1.5 horas",
                            "difficulty": "intermediate",
                            "status": "not_started",
                            "prerequisites": [
                              "10.1.4.1.2.1"
                            ]
                          },
                          {
                            "id": "10.1.4.1.3.2",
                            "name": "Aplicar princípios éticos na coleta",
                            "description": "Discutir conformidade com LGPD/GDPR, consentimento para dados pessoais e evitar viés na seleção de fontes.",
                            "atomicExpansion": {
                              "steps": [
                                {
                                  "stepNumber": 1,
                                  "title": "Identificar princípios éticos e leis aplicáveis na coleta de dados",
                                  "subSteps": [
                                    "Pesquise as principais leis de proteção de dados, como LGPD no Brasil e GDPR na Europa.",
                                    "Liste princípios éticos fundamentais: privacidade, transparência, consentimento e não-maleficência.",
                                    "Analise o contexto do projeto de coleta para mapear riscos éticos específicos.",
                                    "Crie um checklist inicial de conformidade ética.",
                                    "Consulte exemplos de violações éticas em casos reais de coleta de dados."
                                  ],
                                  "verification": "Checklist preenchido com leis e princípios identificados corretamente, sem erros factuais.",
                                  "estimatedTime": "45 minutos",
                                  "materials": [
                                    "Documentos oficiais LGPD/GDPR (sites governamentais)",
                                    "Artigos sobre ética em ciência de dados",
                                    "Checklist template em Google Docs"
                                  ],
                                  "tips": "Comece pelo escopo do seu projeto para focar apenas nas leis relevantes ao seu contexto geográfico.",
                                  "learningObjective": "Compreender o framework legal e ético aplicável à coleta de dados.",
                                  "commonMistakes": "Ignorar diferenças entre LGPD e GDPR; assumir que leis locais não se aplicam a dados internacionais."
                                },
                                {
                                  "stepNumber": 2,
                                  "title": "Implementar processos de consentimento para dados pessoais",
                                  "subSteps": [
                                    "Desenvolva um formulário de consentimento claro, explicando propósito, uso e direitos do titular.",
                                    "Garanta que o consentimento seja livre, informado e revogável a qualquer momento.",
                                    "Integre opções de anonimização ou pseudonimização de dados sensíveis.",
                                    "Teste o formulário com um pequeno grupo para verificar clareza.",
                                    "Registre todos os consentimentos com data, hora e identificação."
                                  ],
                                  "verification": "Formulário de consentimento criado e testado, com todos os elementos obrigatórios presentes.",
                                  "estimatedTime": "1 hora",
                                  "materials": [
                                    "Modelos de formulários de consentimento (ex: templates do GDPR.eu)",
                                    "Ferramentas como Google Forms ou Typeform",
                                    "Guia de melhores práticas de consentimento"
                                  ],
                                  "tips": "Use linguagem simples e evite jargões; inclua um link para política de privacidade.",
                                  "learningObjective": "Dominar a obtenção e documentação de consentimento ético e legal.",
                                  "commonMistakes": "Consentimento implícito ou genérico; não fornecer meios fáceis de revogação."
                                },
                                {
                                  "stepNumber": 3,
                                  "title": "Evitar viés na seleção e coleta de fontes de dados",
                                  "subSteps": [
                                    "Avalie fontes potenciais quanto a representatividade demográfica e diversidade.",
                                    "Aplique técnicas de amostragem estratificada para equilibrar grupos sub-representados.",
                                    "Documente critérios de seleção de fontes para transparência.",
                                    "Use ferramentas de auditoria de viés para inspecionar dados iniciais.",
                                    "Planeje coletas complementares se viés for detectado."
                                  ],
                                  "verification": "Relatório de análise de viés gerado, mostrando medidas tomadas para mitigação.",
                                  "estimatedTime": "50 minutos",
                                  "materials": [
                                    "Ferramentas como IBM AI Fairness 360 ou Aequitas",
                                    "Datasets de exemplo para prática",
                                    "Guia de viés em dados (ex: Google PAIR)"
                                  ],
                                  "tips": "Pergunte: 'Quem está excluído das minhas fontes?' e ajuste accordingly.",
                                  "learningObjective": "Identificar e mitigar viés para garantir equidade na coleta.",
                                  "commonMistakes": "Selecionar fontes convenientes sem checagem de viés; ignorar viés cultural em dados globais."
                                },
                                {
                                  "stepNumber": 4,
                                  "title": "Documentar e auditar o processo ético de coleta",
                                  "subSteps": [
                                    "Crie um relatório ético detalhando todas as decisões tomadas nos steps anteriores.",
                                    "Estabeleça um protocolo de auditoria interna ou externa.",
                                    "Implemente logs automáticos de acesso e modificação de dados.",
                                    "Prepare para revisões éticas, incluindo IRB se aplicável.",
                                    "Revise o processo com pares para feedback."
                                  ],
                                  "verification": "Relatório ético completo e auditado, pronto para revisão.",
                                  "estimatedTime": "40 minutos",
                                  "materials": [
                                    "Templates de relatório ético (ex: de universidades)",
                                    "Ferramentas de versionamento como Git para logs",
                                    "Checklist de auditoria ética"
                                  ],
                                  "tips": "Mantenha registros imutáveis para facilitar auditorias futuras.",
                                  "learningObjective": "Garantir accountability através de documentação robusta.",
                                  "commonMistakes": "Documentação superficial; não planejar auditorias proativamente."
                                }
                              ],
                              "practicalExample": "Ao coletar dados de opiniões públicas sobre saúde via redes sociais para um projeto de análise de sentimentos, você identifica que a amostra inicial é enviesada para jovens urbanos. Aplica LGPD obtendo consentimento explícito via formulário no app, anonimiza IPs, usa amostragem estratificada para incluir idosos rurais e documenta tudo em um relatório ético submetido ao comitê de ética da instituição.",
                              "finalVerifications": [
                                "Todos os consentimentos foram obtidos e documentados conforme LGPD/GDPR.",
                                "Análise de viés confirma representatividade mínima de 80% em grupos demográficos chave.",
                                "Relatório ético completo com checklists preenchidos.",
                                "Logs de coleta auditados sem discrepâncias.",
                                "Formulários de consentimento aprovados por revisão paritária.",
                                "Nenhum dado pessoal coletado sem anonimização."
                              ],
                              "assessmentCriteria": [
                                "Precisão na identificação de leis e princípios éticos (90%+ acurácia).",
                                "Qualidade e completude do formulário de consentimento (todos elementos presentes).",
                                "Eficácia na mitigação de viés (medidas quantificáveis aplicadas).",
                                "Completude da documentação e relatório ético.",
                                "Criatividade e praticidade nas dicas e substeps.",
                                "Integração coerente de todos os aspectos éticos no processo."
                              ],
                              "crossCurricularConnections": [
                                "Direito: Estudo de legislações de privacidade e direitos digitais.",
                                "Filosofia: Debates éticos sobre utilitarismo vs. deontologia em dados.",
                                "Sociologia: Análise de viés social e desigualdades em datasets.",
                                "Tecnologia da Informação: Ferramentas de criptografia e blockchain para privacidade."
                              ],
                              "realWorldApplication": "Em empresas como Google ou hospitais, aplica-se para evitar multas milionárias por violações de privacidade (ex: Cambridge Analytica), garantir confiança pública em pesquisas médicas e promover IA ética em recrutamento automatizado."
                            },
                            "estimatedTime": "2 horas",
                            "difficulty": "intermediate",
                            "status": "not_started",
                            "prerequisites": []
                          },
                          {
                            "id": "10.1.4.1.3.3",
                            "name": "Documentar o processo de coleta",
                            "description": "Criar relatórios e metadados sobre origem, método e timestamp da coleta para reprodutibilidade.",
                            "atomicExpansion": {
                              "steps": [
                                {
                                  "stepNumber": 1,
                                  "title": "Identificar elementos essenciais para documentação",
                                  "subSteps": [
                                    "Liste a origem dos dados (fonte, URL, API ou banco de dados).",
                                    "Defina o método de coleta (scraping, download, query SQL, sensor).",
                                    "Registre timestamps de início e fim da coleta.",
                                    "Inclua metadados como licenças, tamanho do dataset e filtros aplicados.",
                                    "Crie um template inicial com seções padronizadas."
                                  ],
                                  "verification": "Verifique se o template lista todos os 5 elementos principais sem lacunas.",
                                  "estimatedTime": "20 minutos",
                                  "materials": "Notebook ou editor de texto (Google Docs, Markdown), template de relatório em PDF ou MD.",
                                  "tips": "Use um checklist para não esquecer itens críticos como licenças.",
                                  "learningObjective": "Compreender os componentes chave para reprodutibilidade da coleta.",
                                  "commonMistakes": "Ignorar licenças de uso ou supor que timestamps são automáticos."
                                },
                                {
                                  "stepNumber": 2,
                                  "title": "Registrar metadados durante a coleta de dados",
                                  "subSteps": [
                                    "Inicie um log em tempo real com data/hora exata de cada ação.",
                                    "Anote parâmetros usados (ex: queries, limites de páginas).",
                                    "Capture screenshots ou hashes de arquivos baixados.",
                                    "Registre ferramentas e versões (ex: Python 3.9, pandas 1.5).",
                                    "Salve logs em formato JSON ou CSV para fácil parsing."
                                  ],
                                  "verification": "Confira se o log contém entradas para cada sub-passo com timestamps precisos.",
                                  "estimatedTime": "30 minutos",
                                  "materials": "Ferramenta de coleta (Python, curl), editor de log (VS Code), relógio com timestamp.",
                                  "tips": "Automatize timestamps com funções como datetime.now() em scripts.",
                                  "learningObjective": "Praticar registro em tempo real para precisão e rastreabilidade.",
                                  "commonMistakes": "Anotar metadados após a coleta, perdendo detalhes exatos."
                                },
                                {
                                  "stepNumber": 3,
                                  "title": "Estruturar o relatório de coleta",
                                  "subSteps": [
                                    "Organize seções: Introdução, Origem, Método, Metadados, Artefatos.",
                                    "Inclua código-fonte usado na coleta como apêndice.",
                                    "Adicione diagrama de fluxo da coleta (usando Draw.io ou Mermaid).",
                                    "Gere um resumo executivo com chave-valor de metadados.",
                                    "Exporte em formato legível (PDF + JSON raw)."
                                  ],
                                  "verification": "O relatório deve ter todas as seções preenchidas e ser legível por terceiros.",
                                  "estimatedTime": "40 minutos",
                                  "materials": "Editor Markdown (Obsidian, Typora), ferramentas de diagrama (Draw.io), exportador PDF.",
                                  "tips": "Use templates GitHub como README para padronização.",
                                  "learningObjective": "Desenvolver habilidade em criar relatórios profissionais e estruturados.",
                                  "commonMistakes": "Relatório muito técnico sem resumo acessível."
                                },
                                {
                                  "stepNumber": 4,
                                  "title": "Validar e versionar a documentação",
                                  "subSteps": [
                                    "Revise por completude usando checklist de reprodutibilidade.",
                                    "Teste reproduzindo a coleta com base no relatório.",
                                    "Adicione versão (v1.0) e changelog.",
                                    "Compartilhe em repositório (GitHub) com licença aberta.",
                                    "Colete feedback simulado de pares."
                                  ],
                                  "verification": "Coleta deve ser reproduzível em 100% seguindo o relatório.",
                                  "estimatedTime": "25 minutos",
                                  "materials": "Git/GitHub, checklist de verificação, ambiente de teste idêntico.",
                                  "tips": "Use git commit com mensagens descritivas para histórico.",
                                  "learningObjective": "Garantir qualidade e versionamento para colaboração.",
                                  "commonMistakes": "Pular teste de reprodução, assumindo perfeição."
                                }
                              ],
                              "practicalExample": "Ao coletar dados de preços de ações da API do Yahoo Finance: Registre URL='https://query1.finance.yahoo.com/v8/finance/chart/AAPL', método='GET com headers', timestamp='2024-01-15 14:30 UTC', filtros='período=1y, interval=1d', licença='Yahoo Finance Terms', salvando em 'coleta_acoes_v1.json' com log completo.",
                              "finalVerifications": [
                                "Relatório inclui origem, método e timestamps precisos.",
                                "Metadados permitem reprodução exata da coleta.",
                                "Formato é legível e inclui artefatos como código e diagramas.",
                                "Versão e changelog estão presentes.",
                                "Checklist de reprodutibilidade assinalado como completo.",
                                "Teste de reprodução bem-sucedido."
                              ],
                              "assessmentCriteria": [
                                "Completude: Todos elementos essenciais documentados (90%+).",
                                "Precisão: Timestamps e parâmetros exatos sem ambiguidades.",
                                "Estrutura: Relatório organizado com seções claras e resumo.",
                                "Reprodutibilidade: Coleta replicável por outra pessoa.",
                                "Profissionalismo: Uso de ferramentas padrão e formatação limpa.",
                                "Inovação: Inclusão de diagramas ou automação extra."
                              ],
                              "crossCurricularConnections": [
                                "Ética e Direito: Licenças e privacidade de dados (Direito Digital).",
                                "Programação: Automatização de logs com Python (Lógica de Programação).",
                                "Gestão de Projetos: Versionamento com Git (Metodologias Ágeis).",
                                "Comunicação: Redação técnica de relatórios (Língua Portuguesa)."
                              ],
                              "realWorldApplication": "Em equipes de Data Science, como no Kaggle ou empresas de ML, documentação garante reprodutibilidade em pipelines de produção, evitando erros caros em auditorias ou colaborações."
                            },
                            "estimatedTime": "1 hora",
                            "difficulty": "beginner",
                            "status": "not_started",
                            "prerequisites": [
                              "10.1.4.1.2.2"
                            ]
                          }
                        ]
                      }
                    ]
                  },
                  {
                    "id": "10.1.4.2",
                    "name": "Integração e Armazenamento de Dados",
                    "description": "Técnicas para combinar dados de múltiplas fontes e armazená-los de forma eficiente.",
                    "individualConcepts": [
                      {
                        "id": "10.1.4.2.1",
                        "name": "Integração de Dados de Múltiplas Fontes",
                        "description": "Processo de combinar dados provenientes de diferentes origens, resolvendo incompatibilidades de formato, esquema e qualidade para formar um conjunto unificado.",
                        "specificSkills": [
                          {
                            "id": "10.1.4.2.1.1",
                            "name": "Identificar e Extrair Dados de Fontes Heterogêneas",
                            "description": "Reconhecer tipos de fontes de dados (bancos relacionais, arquivos CSV, APIs, NoSQL) e aplicar métodos de extração como consultas SQL, parsing de JSON/XML ou uso de bibliotecas como Pandas em Python.",
                            "atomicExpansion": {
                              "steps": [
                                {
                                  "stepNumber": 1,
                                  "title": "Reconhecer e Classificar Tipos de Fontes de Dados Heterogêneas",
                                  "subSteps": [
                                    "Estude as características de fontes relacionais (ex: SQL Server, MySQL com esquemas fixos e consultas estruturadas).",
                                    "Identifique fontes de arquivos planos como CSV e TSV, focando em delimitadores e encoding.",
                                    "Analise fontes semi-estruturadas como JSON e XML, notando hierarquias e parsing necessário.",
                                    "Explore fontes dinâmicas como APIs REST e bases NoSQL (ex: MongoDB com documentos flexíveis).",
                                    "Crie uma tabela comparativa resumindo prós, contras e métodos de extração para cada tipo."
                                  ],
                                  "verification": "Classifique corretamente 5 exemplos de fontes fornecidos em uma tabela comparativa sem erros.",
                                  "estimatedTime": "1.5 horas",
                                  "materials": [
                                    "Documentação oficial de SQL, Pandas e requests (Python)",
                                    "Amostras de arquivos CSV/JSON/XML",
                                    "Notebook Jupyter"
                                  ],
                                  "tips": "Use diagramas visuais para mapear diferenças entre fontes estruturadas e não-estruturadas.",
                                  "learningObjective": "Diferenciar tipos de fontes de dados e associar métodos de extração apropriados.",
                                  "commonMistakes": [
                                    "Confundir fontes relacionais com NoSQL",
                                    "Ignorar problemas de encoding em CSV",
                                    "Subestimar autenticação em APIs"
                                  ]
                                },
                                {
                                  "stepNumber": 2,
                                  "title": "Extrair Dados de Bancos Relacionais Usando SQL",
                                  "subSteps": [
                                    "Conecte-se a um banco de dados relacional usando bibliotecas como sqlite3 ou SQLAlchemy em Python.",
                                    "Escreva consultas SELECT básicas para extrair colunas específicas com WHERE e JOINs.",
                                    "Filtre e agregue dados usando GROUP BY, HAVING e funções como COUNT, SUM.",
                                    "Exporte resultados para DataFrames Pandas ou CSV para integração.",
                                    "Teste a consulta em um banco de amostra com dados reais simulados."
                                  ],
                                  "verification": "Execute uma consulta SQL que extraia e agregue dados de pelo menos 3 tabelas, salvando em Pandas sem erros.",
                                  "estimatedTime": "2 horas",
                                  "materials": [
                                    "SQLite ou PostgreSQL local",
                                    "Biblioteca SQLAlchemy e Pandas",
                                    "Dataset de amostra como 'Chinook' database"
                                  ],
                                  "tips": "Sempre use LIMIT em consultas iniciais para evitar sobrecarga de dados.",
                                  "learningObjective": "Aplicar consultas SQL para extração eficiente de dados relacionais.",
                                  "commonMistakes": [
                                    "Esquecer JOINs corretos levando a cartesian products",
                                    "Não tratar NULLs adequadamente",
                                    "Ignorar índices para performance"
                                  ]
                                },
                                {
                                  "stepNumber": 3,
                                  "title": "Extrair Dados de Arquivos CSV, JSON e XML com Pandas e Parsing",
                                  "subSteps": [
                                    "Carregue arquivos CSV usando pd.read_csv(), lidando com delimitadores, headers e tipos de dados.",
                                    "Parse JSON com pd.read_json() ou json.loads(), navegando em estruturas aninhadas.",
                                    "Extraia dados XML usando xml.etree.ElementTree ou pandas.read_xml(), focando em tags e atributos.",
                                    "Limpe dados iniciais: remova duplicatas, trate missing values com fillna() ou dropna().",
                                    "Combine extrações de múltiplos arquivos em um único DataFrame usando concat()."
                                  ],
                                  "verification": "Crie um script que leia e integre dados de um CSV, JSON e XML, produzindo um DataFrame limpo com shape correto.",
                                  "estimatedTime": "2.5 horas",
                                  "materials": [
                                    "Arquivos de amostra: sales.csv, users.json, products.xml",
                                    "Pandas, json e xml libraries em Python",
                                    "Jupyter Notebook"
                                  ],
                                  "tips": "Defina dtypes explicitamente em read_csv para otimizar memória e evitar erros de tipo.",
                                  "learningObjective": "Manipular e extrair dados de fontes de arquivos heterogêneas com precisão.",
                                  "commonMistakes": [
                                    "Não especificar encoding='utf-8'",
                                    "Falhar em flatten nested JSON",
                                    "Ignorar namespaces em XML"
                                  ]
                                },
                                {
                                  "stepNumber": 4,
                                  "title": "Extrair Dados de APIs e NoSQL e Integrar Tudo",
                                  "subSteps": [
                                    "Faça requests para APIs REST usando requests.get(), lidando com autenticação (API keys, OAuth).",
                                    "Parse respostas JSON de APIs e armazene em Pandas DataFrames.",
                                    "Conecte a NoSQL como MongoDB com pymongo, query documents com find() e projeções.",
                                    "Integre todos os dados extraídos (SQL + arquivos + API/NoSQL) em um DataFrame unificado.",
                                    "Valide a integração checando shapes, tipos e ausência de duplicatas."
                                  ],
                                  "verification": "Desenvolva um pipeline que extraia de 4 fontes diferentes e produza um dataset integrado validado.",
                                  "estimatedTime": "3 horas",
                                  "materials": [
                                    "API gratuita como JSONPlaceholder ou OpenWeather",
                                    "MongoDB local/Atlas",
                                    "Bibliotecas: requests, pymongo, Pandas"
                                  ],
                                  "tips": "Use try-except para rate limiting e erros de API; paginate com parâmetros offset/limit.",
                                  "learningObjective": "Integrar extrações de fontes dinâmicas e heterogêneas em um fluxo coeso.",
                                  "commonMistakes": [
                                    "Não gerenciar rate limits em APIs",
                                    "Queries ineficientes em NoSQL sem índices",
                                    "Perda de dados na integração por schemas incompatíveis"
                                  ]
                                }
                              ],
                              "practicalExample": "Em um projeto de análise de e-commerce, extraia vendas de um banco MySQL (SQL), produtos de um CSV, reviews de uma API REST (JSON) e logs de usuário de MongoDB, integrando tudo em um Pandas DataFrame para análise de churn.",
                              "finalVerifications": [
                                "Classifique corretamente 10 fontes de dados heterogêneas em tipos apropriados.",
                                "Execute extrações independentes de SQL, CSV/JSON/XML e API sem erros de sintaxe ou runtime.",
                                "Integre dados de 4 fontes em um DataFrame unificado com validação de integridade.",
                                "Otimize uma consulta lenta identificando e corrigindo gargalos (ex: adicionar índices).",
                                "Documente o pipeline de extração em um notebook reproduzível.",
                                "Resolva um erro comum como encoding ou autenticação em menos de 10 minutos."
                              ],
                              "assessmentCriteria": [
                                "Precisão na classificação de fontes (100% correto).",
                                "Eficiência das extrações (tempo de execução < 30s para datasets médios).",
                                "Qualidade da integração (sem perda de dados, tipos consistentes).",
                                "Robustez do código (trata erros, logging básico).",
                                "Documentação clara com comentários e README.",
                                "Criatividade na aplicação a um caso real."
                              ],
                              "crossCurricularConnections": [
                                "Programação em Python (bibliotecas como Pandas, requests).",
                                "Banco de Dados (SQL e NoSQL conceitos).",
                                "Estatística (limpeza inicial de dados para análise).",
                                "Redes e Web (protocolos HTTP, JSON/XML).",
                                "Ética em Dados (privacidade em extrações de APIs)."
                              ],
                              "realWorldApplication": "Analistas de dados em empresas como Google ou Amazon usam isso diariamente para ETL (Extract, Transform, Load) em pipelines de big data, combinando dados de databases internos, logs de apps, APIs de terceiros e arquivos exportados para dashboards e ML models."
                            },
                            "estimatedTime": "2 horas",
                            "difficulty": "beginner",
                            "status": "not_started",
                            "prerequisites": []
                          },
                          {
                            "id": "10.1.4.2.1.2",
                            "name": "Realizar Junções e Merges de Datasets",
                            "description": "Aplicar operações de join (inner, outer, left, right) e merge para combinar tabelas baseadas em chaves comuns, utilizando ferramentas como Pandas merge() ou SQL JOIN.",
                            "atomicExpansion": {
                              "steps": [
                                {
                                  "stepNumber": 1,
                                  "title": "Compreender os Conceitos de Junções e Merges",
                                  "subSteps": [
                                    "Estude os tipos de junções: inner (apenas registros comuns), left (todos da esquerda + comuns), right (todos da direita + comuns) e outer (todos os registros)",
                                    "Identifique chaves comuns (chaves primárias e estrangeiras) em datasets",
                                    "Analise exemplos visuais de Venn diagrams para cada tipo de join",
                                    "Diferencie merge() do Pandas (flexível, por índice ou colunas) de JOIN do SQL (declarativo)",
                                    "Liste cenários onde cada tipo é útil (ex: inner para relatórios completos, left para dados opcionais)"
                                  ],
                                  "verification": "Crie um diagrama manual de um inner join com dois datasets de 3 linhas cada e explique o resultado esperado",
                                  "estimatedTime": "45 minutos",
                                  "materials": [
                                    "Documentação Pandas merge(): https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.merge.html",
                                    "Tutoriais SQL JOIN: W3Schools ou similar",
                                    "Papel e caneta para diagramas"
                                  ],
                                  "tips": "Use diagramas de Venn para visualizar overlaps; memorize: inner=interseção, outer=união",
                                  "learningObjective": "Dominar os tipos de junções e suas diferenças para escolher a correta no contexto",
                                  "commonMistakes": [
                                    "Confundir left com right join",
                                    "Ignorar chaves não únicas levando a duplicatas",
                                    "Esquecer de tratar NaNs em outer joins"
                                  ]
                                },
                                {
                                  "stepNumber": 2,
                                  "title": "Preparar Datasets de Exemplo em Pandas",
                                  "subSteps": [
                                    "Crie dois DataFrames: 'vendas' (id_produto, quantidade, data) e 'produtos' (id_produto, nome, preco)",
                                    "Popule com 5-10 linhas cada, garantindo chaves comuns e exclusivas",
                                    "Inspecione com .head(), .info() e .describe() para verificar tipos e valores",
                                    "Salve em CSV para reutilização e pratique .read_csv()",
                                    "Adicione uma coluna extra em um dataset para testar tratamentos de colunas conflitantes"
                                  ],
                                  "verification": "Execute df1.merge(df2, on='id_produto', how='inner').shape e confirme o número de linhas esperado",
                                  "estimatedTime": "30 minutos",
                                  "materials": [
                                    "Jupyter Notebook ou Google Colab",
                                    "Biblioteca Pandas: import pandas as pd"
                                  ],
                                  "tips": "Use pd.set_option('display.max_columns', None) para visualizar tudo; comece com dados pequenos",
                                  "learningObjective": "Preparar datasets limpos e realistas para experimentação segura de merges",
                                  "commonMistakes": [
                                    "Chaves com tipos diferentes (int vs str)",
                                    "Datasets vazios ou sem chaves comuns",
                                    "Não resetar índices após manipulações"
                                  ]
                                },
                                {
                                  "stepNumber": 3,
                                  "title": "Implementar Merges em Pandas com Diferentes Tipos de Join",
                                  "subSteps": [
                                    "Aplique inner join: df.merge(other, on='chave', how='inner') e compare com expectativa",
                                    "Teste left join: how='left' e verifique registros exclusivos da esquerda sem NaNs indesejados",
                                    "Execute right e outer joins, analisando o resultado com .isnull().sum()",
                                    "Use suffixes=('_esq', '_dir') para colunas duplicadas e múltiplas chaves: on=['chave1', 'chave2']",
                                    "Salve resultados em novos DataFrames e visualize com .to_csv() ou gráficos simples (ex: seaborn)"
                                  ],
                                  "verification": "Para cada join, crie uma tabela de comparação manual vs resultado e confirme 100% match",
                                  "estimatedTime": "60 minutos",
                                  "materials": [
                                    "Datasets preparados no step 2",
                                    "Pandas e Matplotlib/Seaborn para visualização"
                                  ],
                                  "tips": "Sempre use validate='one_to_one' ou 'many_to_one' para detectar duplicatas em chaves",
                                  "learningObjective": "Executar merges fluentemente em Pandas, interpretando resultados corretamente",
                                  "commonMistakes": [
                                    "Esquecer 'how='inner'' (padrão é inner, mas confirme)",
                                    "Não tratar multi-index em merges complexos",
                                    "Ignorar ordem das colunas no output"
                                  ]
                                },
                                {
                                  "stepNumber": 4,
                                  "title": "Aplicar JOINs em SQL e Comparar com Pandas",
                                  "subSteps": [
                                    "Carregue datasets em SQLite: pd.to_sql() e conecte com sqlite3",
                                    "Escreva queries: SELECT * FROM vendas INNER JOIN produtos ON vendas.id_produto = produtos.id_produto",
                                    "Teste LEFT, RIGHT, FULL OUTER JOIN (use UNION para FULL em SQLite)",
                                    "Compare DataFrames Pandas vs pd.read_sql_query() para igualdade com .equals()",
                                    "Otimize queries com WHERE e otimize merges Pandas com index=True se aplicável"
                                  ],
                                  "verification": "Execute pd.read_sql_query(full_join_query).equals(pandas_outer_merge) e obtenha True",
                                  "estimatedTime": "45 minutos",
                                  "materials": [
                                    "SQLite via sqlite3 ou DuckDB",
                                    "Exemplos de queries SQL"
                                  ],
                                  "tips": "SQL é case-insensitive em JOINs; use aliases (AS) para clareza em colunas duplicadas",
                                  "learningObjective": "Comparar e escolher entre Pandas/SQL baseado em escala e performance",
                                  "commonMistakes": [
                                    "Sintaxe errada em FULL OUTER (não nativo em todos DBs)",
                                    "Não alinhar ordem de colunas na comparação",
                                    "Queries ineficientes sem índices"
                                  ]
                                },
                                {
                                  "stepNumber": 5,
                                  "title": "Validar, Debugar e Aplicar em Cenário Real",
                                  "subSteps": [
                                    "Identifique erros comuns: use .merge(..., indicator=True) para ver '_merge' coluna",
                                    "Debugue com amostras pequenas e expanda gradualmente",
                                    "Aplique em dataset real (ex: Kaggle Titanic + passengers)",
                                    "Meça performance com %timeit para merges grandes",
                                    "Documente o processo em um relatório Jupyter com conclusões"
                                  ],
                                  "verification": "Crie um merge complexo com >1000 linhas sem erros e exporte resultado validado",
                                  "estimatedTime": "30 minutos",
                                  "materials": [
                                    "Datasets reais do Kaggle",
                                    "Jupyter para relatório"
                                  ],
                                  "tips": "Sempre valide com known truths (registros conhecidos); profile memory com .memory_usage()",
                                  "learningObjective": "Garantir robustez em merges reais, lidando com edge cases",
                                  "commonMistakes": [
                                    "Não checar duplicatas pós-merge",
                                    "Sobrecarga de memória em datasets grandes",
                                    "Ignorar timezone em datas durante merge"
                                  ]
                                }
                              ],
                              "practicalExample": "Combine dataset 'vendas.csv' (id_produto, qtde, valor) com 'produtos.csv' (id_produto, nome, categoria) usando left join para listar todas vendas, mesmo produtos sem estoque, resultando em relatório de vendas por categoria com NaNs preenchidos como 'Descontinuado'. Em Pandas: vendas.merge(produtos, on='id_produto', how='left').fillna({'categoria': 'Descontinuado'}).groupby('categoria').sum()",
                              "finalVerifications": [
                                "Pode executar todos os 4 tipos de join em Pandas sem erros de sintaxe",
                                "Resultados de Pandas e SQL coincidem em pelo menos 3 testes (usando .equals())",
                                "Identifica e corrige duplicatas em chaves comuns",
                                "Trata NaNs e colunas conflitantes corretamente",
                                "Aplica merge em dataset real com >500 linhas e valida manualmente 10% dos registros",
                                "Documenta código com comentários explicando escolhas de 'how'"
                              ],
                              "assessmentCriteria": [
                                "Precisão: 100% match entre resultados esperados e obtidos (80% nota)",
                                "Eficiência: Tempo de execução <5s para datasets médios (15% nota)",
                                "Robustez: Código lida com edge cases (NaNs, duplicatas) sem crashes (10% nota)",
                                "Clareza: Código comentado e visualizações incluídas (10% nota)",
                                "Criatividade: Aplicação em cenário não-trivial com groupby ou fillna (15% nota)",
                                "Compreensão: Explicação oral ou escrita dos tipos de join correta (10% nota)"
                              ],
                              "crossCurricularConnections": [
                                "Programação: Manipulação de estruturas de dados (dicts, lists para prep)",
                                "Banco de Dados: Normalização e queries relacionais",
                                "Estatística: Preparação de dados para análises agregadas (groupby)",
                                "Matemática: Teoria de conjuntos (união, interseção)",
                                "Negócios: ETL em pipelines de BI (Power BI, Tableau)"
                              ],
                              "realWorldApplication": "Em e-commerce, merge de tabelas de pedidos e clientes para análise de churn; em saúde, juntar registros de pacientes e tratamentos para estudos epidemiológicos; em finanças, combinar transações e cotações de moedas para relatórios consolidados, economizando horas manuais e reduzindo erros humanos."
                            },
                            "estimatedTime": "3 horas",
                            "difficulty": "intermediate",
                            "status": "not_started",
                            "prerequisites": []
                          },
                          {
                            "id": "10.1.4.2.1.3",
                            "name": "Resolver Inconsistências de Esquema e Formato",
                            "description": "Padronizar esquemas de dados, tratar diferenças em nomes de colunas, tipos de dados e valores nulos durante a integração, com técnicas como renaming, type casting e mapeamento de entidades.",
                            "atomicExpansion": {
                              "steps": [
                                {
                                  "stepNumber": 1,
                                  "title": "Identificar Inconsistências de Esquema e Formato",
                                  "subSteps": [
                                    "Carregue os datasets de múltiplas fontes usando pandas ou similar.",
                                    "Compare esquemas: liste colunas, tipos de dados e estatísticas básicas (df.info(), df.describe()).",
                                    "Identifique discrepâncias: nomes de colunas diferentes (e.g., 'idade' vs 'age'), tipos inconsistentes (string vs int), valores nulos irregulares.",
                                    "Crie um relatório de inconsistências em um DataFrame comparativo.",
                                    "Visualize diferenças com gráficos de distribuição para valores nulos."
                                  ],
                                  "verification": "Relatório de inconsistências gerado e salvo como CSV, confirmando todas as discrepâncias listadas.",
                                  "estimatedTime": "45 minutos",
                                  "materials": [
                                    "Python com pandas, numpy",
                                    "Datasets de exemplo (CSV1: vendas_lojaA.csv, CSV2: vendas_lojaB.csv)",
                                    "Jupyter Notebook"
                                  ],
                                  "tips": "Use df.columns.tolist() para listas rápidas e pd.concat com axis=1 para comparação visual.",
                                  "learningObjective": "Aprender a diagnosticar problemas de esquema em datasets heterogêneos.",
                                  "commonMistakes": [
                                    "Ignorar valores nulos como inconsistência",
                                    "Não comparar estatísticas descritivas",
                                    "Assumir tipos de dados sem verificação"
                                  ]
                                },
                                {
                                  "stepNumber": 2,
                                  "title": "Padronizar Nomes de Colunas (Renaming)",
                                  "subSteps": [
                                    "Defina um esquema padrão baseado no domínio (e.g., 'customer_id', 'product_name', 'quantity').",
                                    "Crie um dicionário de mapeamento para renomear colunas em cada dataset (df.rename(columns=mapping)).",
                                    "Aplique renaming em todos os datasets e verifique com df.columns.",
                                    "Trate colunas ausentes adicionando-as com valores padrão (df['nova_coluna'] = None).",
                                    "Salve versões renomeadas temporariamente para inspeção."
                                  ],
                                  "verification": "Todas as colunas renomeadas conforme esquema padrão, verificado por df.columns.equals(esquema_padrao).",
                                  "estimatedTime": "30 minutos",
                                  "materials": [
                                    "Pandas",
                                    "Dicionário de mapeamento em JSON",
                                    "Datasets renomeados anteriores"
                                  ],
                                  "tips": "Use expressões regulares (df.rename(columns=lambda x: x.lower().replace(' ', '_'))) para padronizações automáticas.",
                                  "learningObjective": "Dominar técnicas de renaming para unificar esquemas.",
                                  "commonMistakes": [
                                    "Erros de digitação no mapping",
                                    "Não tratar colunas faltantes",
                                    "Sobrescrever dados originais sem backup"
                                  ]
                                },
                                {
                                  "stepNumber": 3,
                                  "title": "Normalizar Tipos de Dados e Tratar Valores Nulos",
                                  "subSteps": [
                                    "Defina tipos padrão (e.g., 'customer_id': 'int64', 'price': 'float64').",
                                    "Aplique type casting (pd.to_numeric, pd.to_datetime) com erros='coerce' para nulos.",
                                    "Preencha nulos estrategicamente: média para numéricos (df.fillna(df.mean())), 'unknown' para categóricos.",
                                    "Valide tipos pós-casting com df.dtypes e df.isnull().sum().",
                                    "Repita para todos os datasets e concatene para teste unificado."
                                  ],
                                  "verification": "df.dtypes corresponde ao esquema padrão e soma de nulos < 5% por coluna.",
                                  "estimatedTime": "40 minutos",
                                  "materials": [
                                    "Pandas",
                                    "Esquema de tipos em dict",
                                    "Datasets renomeados"
                                  ],
                                  "tips": "Use df.astype() para casting em batch e df.interpolate() para nulos em séries temporais.",
                                  "learningObjective": "Aplicar type casting e imputação de nulos de forma robusta.",
                                  "commonMistakes": [
                                    "Forçar casting sem coerce levando a erros",
                                    "Imputar nulos com zeros sem contexto",
                                    "Ignorar perda de precisão em floats"
                                  ]
                                },
                                {
                                  "stepNumber": 4,
                                  "title": "Mapear Entidades e Validar Integração",
                                  "subSteps": [
                                    "Crie chaves de junção (e.g., 'customer_id') e mapeie entidades duplicadas com fuzzy matching (fuzzywuzzy library).",
                                    "Una datasets com pd.merge ou pd.concat, lidando com duplicatas (drop_duplicates).",
                                    "Execute validações: checks de integridade referencial e consistência de valores.",
                                    "Gere relatório final de métricas (shape, dtypes, nulls).",
                                    "Salve dataset integrado final."
                                  ],
                                  "verification": "Dataset integrado sem erros de chave, métricas finais atendem thresholds (e.g., nulls=0%).",
                                  "estimatedTime": "35 minutos",
                                  "materials": [
                                    "Pandas, fuzzywuzzy",
                                    "Datasets normalizados",
                                    "Script de validação"
                                  ],
                                  "tips": "Teste merges com small samples primeiro para eficiência.",
                                  "learningObjective": "Integrar dados mapeados com validação completa.",
                                  "commonMistakes": [
                                    "Merges errados por chaves inconsistentes",
                                    "Não remover duplicatas pós-merge",
                                    "Pular validações finais"
                                  ]
                                }
                              ],
                              "practicalExample": "Integre dois CSVs de vendas: lojaA tem 'ID_Cliente', 'Preco' (string com '$'), 10% nulos em 'Quantidade'; lojaB tem 'customerID' (int), 'price' (float), sem nulos. Após steps: nomes unificados para 'customer_id', 'price' (float), nulos imputados com mediana, merge por customer_id resultando em dataset limpo de 1000+ registros.",
                              "finalVerifications": [
                                "Esquemas idênticos em todos datasets (df.columns.equals()).",
                                "Tipos de dados padronizados (df.dtypes == schema_dtypes).",
                                "Valores nulos tratados (<2% restante).",
                                "Sem duplicatas pós-integração (df.duplicated().sum() == 0).",
                                "Integridade de chaves mantida (no orphans em merges).",
                                "Relatório de métricas salvo e auditável."
                              ],
                              "assessmentCriteria": [
                                "Precisão no identification de 100% das inconsistências.",
                                "Eficiência no renaming e casting sem perda de dados.",
                                "Qualidade da imputação de nulos contextualizada.",
                                "Robustez do mapeamento de entidades (fuzzy score >90%).",
                                "Validações finais completas e documentadas.",
                                "Tempo total dentro de 2.5 horas."
                              ],
                              "crossCurricularConnections": [
                                "Programação: Manipulação de dados em Python/R.",
                                "Estatística: Análise descritiva e imputação.",
                                "Banco de Dados: Conceitos de schema e ETL.",
                                "Matemática: Mapeamento de funções e normalização."
                              ],
                              "realWorldApplication": "Em pipelines ETL de empresas como bancos ou e-commerces, onde dados de filiais/CRM/APIs são integrados para relatórios unificados, evitando erros em dashboards de BI como Tableau ou Power BI."
                            },
                            "estimatedTime": "2 horas",
                            "difficulty": "intermediate",
                            "status": "not_started",
                            "prerequisites": []
                          }
                        ]
                      },
                      {
                        "id": "10.1.4.2.2",
                        "name": "Armazenamento Eficiente de Dados Integrados",
                        "description": "Seleção e implementação de estruturas de armazenamento otimizadas para grandes volumes de dados, garantindo acessibilidade, escalabilidade e performance.",
                        "specificSkills": [
                          {
                            "id": "10.1.4.2.2.1",
                            "name": "Escolher Sistemas de Armazenamento Adequados",
                            "description": "Comparar bancos relacionais (SQL), NoSQL (MongoDB, Cassandra), data warehouses (Snowflake, BigQuery) e data lakes (S3), com base em critérios como volume, variedade e velocidade de acesso.",
                            "atomicExpansion": {
                              "steps": [
                                {
                                  "stepNumber": 1,
                                  "title": "Entender os Fundamentos dos Tipos de Sistemas de Armazenamento",
                                  "subSteps": [
                                    "Estude bancos relacionais SQL (ex: PostgreSQL, MySQL): esquema fixo, ACID, consultas estruturadas.",
                                    "Explore NoSQL: MongoDB (documentos), Cassandra (colunas amplas) - esquema flexível, escalabilidade horizontal.",
                                    "Aprenda data warehouses: Snowflake, BigQuery - otimizados para OLAP, consultas analíticas em grandes volumes.",
                                    "Compreenda data lakes: S3 - armazenamento de objetos brutos, suporte a dados não estruturados.",
                                    "Identifique diferenças chave em estrutura de dados e casos de uso iniciais."
                                  ],
                                  "verification": "Crie um mapa mental ou tabela resumindo características únicas de cada tipo.",
                                  "estimatedTime": "2 horas",
                                  "materials": [
                                    "Documentação oficial: PostgreSQL, MongoDB, Cassandra, Snowflake, BigQuery, AWS S3",
                                    "Vídeos introdutórios no YouTube (Coursera/Khan Academy)"
                                  ],
                                  "tips": "Use diagramas visuais para comparar estruturas de dados; foque em exemplos simples primeiro.",
                                  "learningObjective": "Identificar e descrever as características principais de cada sistema de armazenamento.",
                                  "commonMistakes": [
                                    "Confundir NoSQL com 'sem SQL' (é No RELACIONAL SQL)",
                                    "Ignorar suporte a transações em NoSQL modernos",
                                    "Achar que data lakes são só para 'bagunça' de dados"
                                  ]
                                },
                                {
                                  "stepNumber": 2,
                                  "title": "Dominar os Critérios de Comparação: Volume, Variedade e Velocidade",
                                  "subSteps": [
                                    "Defina Volume: escalabilidade para petabytes; compare limites de SQL vs. NoSQL/data lakes.",
                                    "Analise Variedade: tipos de dados (estruturados, semi, não estruturados) suportados por cada.",
                                    "Estude Velocidade: latência de leitura/escrita, throughput; OLTP vs. OLAP.",
                                    "Crie uma matriz de critérios com escalas qualitativas (baixa/média/alta) para cada sistema.",
                                    "Inclua outros fatores como custo e consistência CAP theorem."
                                  ],
                                  "verification": "Preencha uma tabela comparativa com notas para cada critério em pelo menos 3 sistemas.",
                                  "estimatedTime": "1.5 horas",
                                  "materials": [
                                    "Artigos sobre 3 V's do Big Data",
                                    "Planilhas Google Sheets para matriz",
                                    "Infográficos de comparações (Databricks ou Towards Data Science)"
                                  ],
                                  "tips": "Atribua pesos aos critérios baseado em cenários reais para prática.",
                                  "learningObjective": "Aplicar os critérios 3 V's para avaliar sistemas de armazenamento de forma quantitativa.",
                                  "commonMistakes": [
                                    "Focar só em volume ignorando variedade",
                                    "Confundir velocidade de escrita com leitura",
                                    "Não considerar custos de escalabilidade"
                                  ]
                                },
                                {
                                  "stepNumber": 3,
                                  "title": "Realizar Comparação Detalhada entre os Sistemas",
                                  "subSteps": [
                                    "Compare SQL vs. NoSQL: use casos como transações bancárias (SQL) vs. logs de IoT (Cassandra).",
                                    "Avalie data warehouses vs. lakes: analytics em Snowflake vs. raw data em S3.",
                                    "Preencha matriz completa: ex. MongoDB alta variedade/velocidade média; BigQuery alto volume analítico.",
                                    "Simule trade-offs: ex. SQL bom em consistência mas ruim em escala massiva.",
                                    "Documente prós/contras em uma tabela com exemplos numéricos (ex. queries por segundo)."
                                  ],
                                  "verification": "Gere uma tabela Markdown ou Excel com comparação completa de todos os sistemas.",
                                  "estimatedTime": "2.5 horas",
                                  "materials": [
                                    "Ferramentas: Draw.io para diagramas",
                                    "Benchmarks: DB-Engines ranking, papers acadêmicos",
                                    "Datasets de teste: Kaggle para simulações"
                                  ],
                                  "tips": "Use benchmarks reais (ex. TPC benchmarks) para dados concretos.",
                                  "learningObjective": "Comparar sistemas usando critérios específicos e identificar trade-offs.",
                                  "commonMistakes": [
                                    "Generalizar 'NoSQL é sempre melhor para big data'",
                                    "Ignorar híbridos como SQL em data lakes",
                                    "Não quantificar velocidades"
                                  ]
                                },
                                {
                                  "stepNumber": 4,
                                  "title": "Praticar Seleção de Sistemas para Cenários Reais",
                                  "subSteps": [
                                    "Analise cenários: e-commerce (SQL + NoSQL), streaming (Cassandra), BI (BigQuery), ML raw (S3).",
                                    "Para cada, aplique 3 V's e justifique escolha com matriz.",
                                    "Crie fluxograma de decisão: perguntas como 'Alta variedade? -> NoSQL/Data Lake'.",
                                    "Teste com mini-projeto: mock dataset e escolha sistema.",
                                    "Revise decisões com pares ou auto-avaliação."
                                  ],
                                  "verification": "Desenvolva 3 relatórios de recomendação para cenários diferentes.",
                                  "estimatedTime": "2 horas",
                                  "materials": [
                                    "Cenários fictícios/real: Netflix case studies",
                                    "Ferramentas: Lucidchart para fluxogramas",
                                    "Datasets sample: public AWS/GCP"
                                  ],
                                  "tips": "Comece com cenários familiares para ganhar confiança.",
                                  "learningObjective": "Selecionar o sistema ótimo baseado em análise de requisitos.",
                                  "commonMistakes": [
                                    "Escolher baseado em hype (ex. sempre MongoDB)",
                                    "Não considerar migração/custo total",
                                    "Ignorar requisitos não-3V como segurança"
                                  ]
                                }
                              ],
                              "practicalExample": "Para um app de e-commerce com 1M usuários/dia: alto volume (BigQuery para analytics), variedade média (MongoDB para perfis flexíveis), velocidade alta em leituras (S3 para logs). Justificativa: SQL para transações, NoSQL para catálogos dinâmicos.",
                              "finalVerifications": [
                                "Lista corretamente características de SQL, NoSQL (MongoDB/Cassandra), warehouses (Snowflake/BigQuery) e lakes (S3).",
                                "Preenche matriz 3 V's com acurácia >80%.",
                                "Recomenda sistema correto para 3 cenários variados.",
                                "Identifica trade-offs em pelo menos 2 sistemas.",
                                "Cria fluxograma de decisão funcional.",
                                "Explica CAP theorem aplicado a escolhas."
                              ],
                              "assessmentCriteria": [
                                "Precisão na descrição de sistemas (30%)",
                                "Aplicação correta dos 3 V's (25%)",
                                "Qualidade da comparação/tabulares (20%)",
                                "Relevância das recomendações (15%)",
                                "Criatividade em exemplos reais (10%)"
                              ],
                              "crossCurricularConnections": [
                                "Programação: Integração via APIs (Python com PyMongo, SQLAlchemy).",
                                "Estatística: Análise de performance com queries OLAP.",
                                "Negócios: Avaliação de ROI em storage costs.",
                                "Ética: Privacidade de dados em lakes vs. warehouses."
                              ],
                              "realWorldApplication": "Empresas como Netflix usam Cassandra para alto volume/velocidade em recomendações; Google BigQuery para analytics em petabytes de dados de busca, permitindo decisões rápidas em marketing."
                            },
                            "estimatedTime": "2 horas",
                            "difficulty": "beginner",
                            "status": "not_started",
                            "prerequisites": []
                          },
                          {
                            "id": "10.1.4.2.2.2",
                            "name": "Implementar Processos ETL/ELT",
                            "description": "Construir pipelines ETL (Extract, Transform, Load) ou ELT para carregar dados integrados em repositórios centrais, utilizando ferramentas como Apache Airflow, Talend ou scripts Python com SQLAlchemy.",
                            "atomicExpansion": {
                              "steps": [
                                {
                                  "stepNumber": 1,
                                  "title": "Planejar e Implementar a Fase de Extração (Extract)",
                                  "subSteps": [
                                    "Identifique as fontes de dados (ex: CSV, API, banco de dados SQL) e defina esquemas de entrada.",
                                    "Instale bibliotecas necessárias como pandas, requests e SQLAlchemy via pip.",
                                    "Escreva funções para conectar e extrair dados de múltiplas fontes, lidando com autenticação e paginação.",
                                    "Implemente tratamento inicial de erros como timeouts e falhas de conexão.",
                                    "Salve dados extraídos em formato temporário (ex: Parquet) para otimização."
                                  ],
                                  "verification": "Execute o script de extração e confirme que dados de amostra são lidos corretamente sem erros, verificando logs e tamanho dos arquivos temporários.",
                                  "estimatedTime": "2 horas",
                                  "materials": [
                                    "Python 3.8+, pandas, requests, SQLAlchemy, Jupyter Notebook"
                                  ],
                                  "tips": "Use context managers (with statement) para conexões seguras e teste extrações pequenas primeiro.",
                                  "learningObjective": "Dominar a extração de dados heterogêneos de fontes variadas com resiliência a falhas.",
                                  "commonMistakes": [
                                    "Ignorar paginação em APIs levando a dados incompletos",
                                    "Não tratar exceções causando crashes",
                                    "Extrair volumes grandes sem amostragem inicial"
                                  ]
                                },
                                {
                                  "stepNumber": 2,
                                  "title": "Desenvolver a Fase de Transformação (Transform)",
                                  "subSteps": [
                                    "Carregue dados extraídos e realize profiling com pandas (describe(), info()).",
                                    "Aplique limpezas: remoção de nulos, deduplicação, padronização de formatos (datas, strings).",
                                    "Implemente transformações de negócios: agregações, joins entre datasets, cálculos derivados (ex: KPIs).",
                                    "Valide qualidade de dados com asserts ou bibliotecas como Great Expectations.",
                                    "Otimize performance com vectorização pandas ou Dask para grandes volumes."
                                  ],
                                  "verification": "Gere relatório de profiling pré/pós-transformação e confirme métricas de qualidade (ex: 95% completude).",
                                  "estimatedTime": "3 horas",
                                  "materials": [
                                    "pandas, numpy, Great Expectations (opcional)"
                                  ],
                                  "tips": "Use funções modulares para transformações reutilizáveis e versionamento com Git.",
                                  "learningObjective": "Aplicar transformações escaláveis e validadas para preparar dados integrados.",
                                  "commonMistakes": [
                                    "Joins ineficientes causando explosão cartesiana",
                                    "Perda de dados por filtros agressivos sem backup",
                                    "Ignorar fusos horários em datas"
                                  ]
                                },
                                {
                                  "stepNumber": 3,
                                  "title": "Executar a Fase de Carregamento (Load) em Repositório Central",
                                  "subSteps": [
                                    "Configure conexão ao data warehouse (ex: PostgreSQL via SQLAlchemy).",
                                    "Defina estratégia de load: append, upsert ou truncate-insert baseado no schema.",
                                    "Implemente batch loading com chunking para eficiência em grandes datasets.",
                                    "Crie índices e partições no destino para queries otimizadas.",
                                    "Registre metadados de load (timestamps, row counts) em tabela de audit."
                                  ],
                                  "verification": "Query o data warehouse e confirme contagens de linhas, integridade via checksums e ausência de duplicatas.",
                                  "estimatedTime": "2 horas",
                                  "materials": [
                                    "SQLAlchemy, PostgreSQL ou SQLite para teste, psycopg2"
                                  ],
                                  "tips": "Use UPSERT (ON CONFLICT) para idempotência e monitore uso de memória durante loads.",
                                  "learningObjective": "Carregar dados transformados de forma eficiente e idempotente em repositórios centrais.",
                                  "commonMistakes": [
                                    "Loads sem transações levando a estados inconsistentes",
                                    "Índices ausentes degradando performance futura",
                                    "Sobrecarga de memória por loads não chunked"
                                  ]
                                },
                                {
                                  "stepNumber": 4,
                                  "title": "Orquestrar e Monitorar o Pipeline ETL Completo",
                                  "subSteps": [
                                    "Instale e configure Apache Airflow (Docker ou pip).",
                                    "Defina DAGs com tasks sequenciais para Extract > Transform > Load.",
                                    "Adicione retries, SLAs e notificações por email/Slack em falhas.",
                                    "Implemente sensores para dependências externas e logging customizado.",
                                    "Teste end-to-end e monitore execução via Airflow UI."
                                  ],
                                  "verification": "Execute DAG completo, verifique status SUCCESS no UI e valide dados finais no warehouse.",
                                  "estimatedTime": "3 horas",
                                  "materials": [
                                    "Apache Airflow, Docker (opcional), PostgreSQL"
                                  ],
                                  "tips": "Comece com DAGs simples, use XComs para passar metadados entre tasks e teste em ambiente local.",
                                  "learningObjective": "Orquestrar pipelines ETL automatizados com monitoramento robusto.",
                                  "commonMistakes": [
                                    "DAGs cíclicos por dependências mal definidas",
                                    "Falta de retries causando falhas desnecessárias",
                                    "Ignorar custos de compute em execuções frequentes"
                                  ]
                                }
                              ],
                              "practicalExample": "Construa um pipeline ETL para integrar dados de vendas de um CSV local (transações diárias) e uma API de clientes (REST), transformando para calcular receita por região e carregando em PostgreSQL. Use Airflow para agendar diário às 6h, com alertas por email se falhar.",
                              "finalVerifications": [
                                "Pipeline roda end-to-end sem erros e dados são corretamente integrados no warehouse.",
                                "Transformações produzem KPIs precisos validados contra amostras manuais.",
                                "Airflow DAG executa em <10min com retries automáticos em falhas simuladas.",
                                "Logs e metadados auditáveis confirmam idempotência (re-run não duplica dados).",
                                "Queries no warehouse respondem em <2s para relatórios agregados.",
                                "Qualidade de dados atende >98% em checks automatizados."
                              ],
                              "assessmentCriteria": [
                                "Correção: Dados extraídos, transformados e carregados sem perdas ou corrupções.",
                                "Eficiência: Uso otimizado de memória/CPU, com chunking e indexing.",
                                "Robustez: Tratamento de erros, retries e idempotência implementados.",
                                "Escalabilidade: Pipeline orquestrado suporta volumes 10x maiores.",
                                "Documentação: Código comentado, README com setup e troubleshooting.",
                                "Inovação: Inclusão de monitoramento e validações customizadas."
                              ],
                              "crossCurricularConnections": [
                                "Programação Python: Manipulação avançada de dados com pandas/SQLAlchemy.",
                                "Banco de Dados: Modelagem, queries SQL e otimização de warehouses.",
                                "Engenharia de Software: Pipelines modulares, versionamento Git e CI/CD basics.",
                                "Estatística: Profiling e validação de qualidade de dados.",
                                "DevOps: Orquestração com Airflow e containerização Docker."
                              ],
                              "realWorldApplication": "Em empresas como e-commerces (ex: Mercado Livre), bancos de dados centrais alimentam BI tools como Tableau para dashboards de vendas; em healthtechs, integra EHRs para analytics preditivos, reduzindo tempo de ingestão de dias para horas."
                            },
                            "estimatedTime": "4 horas",
                            "difficulty": "intermediate",
                            "status": "not_started",
                            "prerequisites": []
                          },
                          {
                            "id": "10.1.4.2.2.3",
                            "name": "Aplicar Otimizações de Armazenamento",
                            "description": "Utilizar técnicas como compressão de dados, particionamento de tabelas, indexação e sharding para reduzir custos e melhorar consultas em sistemas de armazenamento distribuídos.",
                            "atomicExpansion": {
                              "steps": [
                                {
                                  "stepNumber": 1,
                                  "title": "Avaliar Necessidades Atuais de Armazenamento",
                                  "subSteps": [
                                    "Analise o tamanho atual dos dados e padrões de acesso usando ferramentas de monitoramento.",
                                    "Identifique gargalos como consultas lentas ou custos elevados de armazenamento.",
                                    "Colete métricas de uso: volume de dados, frequência de leitura/escrita e distribuição geográfica.",
                                    "Defina metas de otimização, como reduzir tamanho em 30% ou acelerar queries em 50%.",
                                    "Documente um relatório inicial com visualizações de dados."
                                  ],
                                  "verification": "Relatório gerado com métricas baseline e metas claras, validado por consulta a logs de performance.",
                                  "estimatedTime": "4 horas",
                                  "materials": "Ferramentas de monitoramento (ex: Prometheus, CloudWatch), datasets de exemplo, planilha ou Jupyter Notebook.",
                                  "tips": "Priorize dados com alta cardinalidade para indexação futura.",
                                  "learningObjective": "Compreender o estado atual do sistema para priorizar otimizações.",
                                  "commonMistakes": "Ignorar padrões sazonais de acesso ou superestimar benefícios sem baseline."
                                },
                                {
                                  "stepNumber": 2,
                                  "title": "Implementar Compressão de Dados",
                                  "subSteps": [
                                    "Escolha algoritmos de compressão adequados (ex: Snappy para velocidade, GZIP para ratio alto).",
                                    "Aplique compressão em colunas ou arquivos existentes via comandos SQL ou APIs de storage.",
                                    "Teste a compressão em um subset de dados para medir ratio e impacto em performance de query.",
                                    "Atualize configurações de pipeline de ingestão para compressão automática.",
                                    "Monitore overhead de CPU durante compressão/descompressão."
                                  ],
                                  "verification": "Dados comprimidos acessíveis com queries executando em <110% do tempo original e redução de tamanho verificada.",
                                  "estimatedTime": "3 horas",
                                  "materials": "Banco de dados distribuído (ex: BigQuery, Cassandra), bibliotecas como Snappy ou Zstandard.",
                                  "tips": "Use compressão columnar para dados analíticos onde queries leem poucas colunas.",
                                  "learningObjective": "Dominar compressão para reduzir custos sem sacrificar performance.",
                                  "commonMistakes": "Aplicar compressão em dados já comprimidos ou ignorar latência de descompressão."
                                },
                                {
                                  "stepNumber": 3,
                                  "title": "Aplicar Particionamento e Indexação",
                                  "subSteps": [
                                    "Defina chaves de particionamento baseadas em colunas de alta filtros (ex: data, região).",
                                    "Crie partições em tabelas usando DDL SQL ou configurações de storage distribuído.",
                                    "Construa índices em colunas frequentemente filtradas ou joined.",
                                    "Otimize índices compostos para queries comuns identificadas na análise inicial.",
                                    "Reorganize dados existentes para novas partições sem downtime."
                                  ],
                                  "verification": "Queries particionadas executam mais rápido, confirmado por EXPLAIN PLAN mostrando pruning de partições.",
                                  "estimatedTime": "5 horas",
                                  "materials": "SQL engines (ex: Spark SQL, Hive), ferramentas de query planner.",
                                  "tips": "Evite sobre-particionamento que fragmenta dados demais.",
                                  "learningObjective": "Melhorar eficiência de queries via estruturas de dados otimizadas.",
                                  "commonMistakes": "Particionar por colunas de baixa seletividade ou criar índices desnecessários que aumentam write latency."
                                },
                                {
                                  "stepNumber": 4,
                                  "title": "Implementar Sharding e Validar Otimizações",
                                  "subSteps": [
                                    "Escolha estratégia de sharding (ex: hash-based ou range-based por chave de shard).",
                                    "Configure clusters com sharding em storage distribuído (ex: MongoDB, Cassandra).",
                                    "Migre dados para shards balanceados, monitorando distribuição.",
                                    "Execute benchmarks comparativos pré e pós-otimizações.",
                                    "Ajuste configurações baseado em resultados e documente playbook."
                                  ],
                                  "verification": "Distribuição de dados balanceada (<20% variação entre shards) e redução global de latência/custo.",
                                  "estimatedTime": "6 horas",
                                  "materials": "Sistemas shardáveis (ex: Cassandra, Vitess), ferramentas de benchmarking (ex: YCSB).",
                                  "tips": "Comece com poucos shards e escale gradualmente.",
                                  "learningObjective": "Escalar horizontalmente para grandes volumes de dados.",
                                  "commonMistakes": "Hotspots em shards devido a chaves mal escolhidas ou migração sem backoff."
                                }
                              ],
                              "practicalExample": "Em um data warehouse com 10TB de logs de e-commerce no BigQuery, aplique compressão Snappy (reduz 40%), particione por data/usuário, indexe por produto_id e shard por região, cortando custos mensais de $5000 para $2500 e acelerando queries de vendas em 60%.",
                              "finalVerifications": [
                                "Tamanho de armazenamento reduzido em pelo menos 25%.",
                                "Tempo médio de query caiu 40% em benchmarks.",
                                "Custos de storage computados < meta inicial.",
                                "Distribuição de dados balanceada em shards.",
                                "Nenhuma regressão em throughput de writes.",
                                "Relatório de performance documentado e versionado."
                              ],
                              "assessmentCriteria": [
                                "Precisão na escolha de técnicas baseadas em análise inicial (80%+ alinhamento).",
                                "Implementação sem erros funcionais ou downtime.",
                                "Melhoria mensurável em métricas chave (redução comprovada).",
                                "Documentação clara de passos e lições aprendidas.",
                                "Capacidade de explicar trade-offs (ex: compressão vs velocidade).",
                                "Aplicação correta em ambiente simulado realista."
                              ],
                              "crossCurricularConnections": [
                                "Matemática: Estatística para análise de distribuições de dados e seleção de chaves.",
                                "Programação: Scripts em Python/Scala para automação de pipelines.",
                                "Redes: Entendimento de latência em sistemas distribuídos.",
                                "Economia: Modelagem de custos de cloud storage."
                              ],
                              "realWorldApplication": "Empresas como Netflix usam sharding e compressão em Cassandra para armazenar bilhões de eventos de streaming, reduzindo custos em escala e permitindo queries analíticas em tempo real para recomendações personalizadas."
                            },
                            "estimatedTime": "3 horas",
                            "difficulty": "advanced",
                            "status": "not_started",
                            "prerequisites": []
                          },
                          {
                            "id": "10.1.4.2.2.4",
                            "name": "Avaliar Escalabilidade e Performance",
                            "description": "Medir métricas de armazenamento (tempo de query, uso de espaço) e ajustar configurações para cenários de big data, considerando ferramentas como Apache Spark para processamento distribuído.",
                            "atomicExpansion": {
                              "steps": [
                                {
                                  "stepNumber": 1,
                                  "title": "Medir Métricas Básicas de Performance",
                                  "subSteps": [
                                    "Selecionar e preparar um dataset de teste representativo (ex: 100GB de dados tabulares).",
                                    "Configurar ferramentas de monitoramento como pg_stat_statements no PostgreSQL ou EXPLAIN ANALYZE.",
                                    "Executar queries de benchmark variadas (SELECT simples, JOINs complexos, agregações).",
                                    "Registrar tempo de query (latência média, throughput) e uso de espaço (tamanho em disco, memória RAM).",
                                    "Gerar relatórios comparativos pré e pós-otimização."
                                  ],
                                  "verification": "Relatório gerado com métricas numéricas (ex: tempo médio < 5s, espaço < 20% do esperado).",
                                  "estimatedTime": "1.5 horas",
                                  "materials": [
                                    "Banco de dados relacional (PostgreSQL/MySQL)",
                                    "Dataset TPC-H ou similar",
                                    "Ferramentas: psql, Jupyter Notebook"
                                  ],
                                  "tips": "Sempre execute queries múltiplas vezes e calcule médias para evitar variações de cache.",
                                  "learningObjective": "Dominar a medição precisa de tempo de query e uso de espaço em bancos de dados.",
                                  "commonMistakes": [
                                    "Ignorar variações de hardware",
                                    "Não normalizar datasets",
                                    "Medir apenas uma query única"
                                  ]
                                },
                                {
                                  "stepNumber": 2,
                                  "title": "Identificar Gargalos em Cenários de Big Data",
                                  "subSteps": [
                                    "Analisar planos de execução de queries com ferramentas como EXPLAIN ou query profiler.",
                                    "Simular carga de big data escalando dataset (ex: de 100GB para 1TB via duplicação).",
                                    "Monitorar métricas avançadas: I/O disk, CPU utilization, network latency.",
                                    "Mapear gargalos comuns: scans sequenciais, falta de índices, locks de concorrência.",
                                    "Documentar bottlenecks com gráficos (ex: flame graphs ou timelines)."
                                  ],
                                  "verification": "Mapa de gargalos documentado identificando pelo menos 3 problemas principais.",
                                  "estimatedTime": "2 horas",
                                  "materials": [
                                    "Ferramentas de profiling (pgBadger, FlameGraph)",
                                    "Datasets escaláveis (S3 ou local)",
                                    "Monitoramento: Prometheus/Grafana"
                                  ],
                                  "tips": "Use workloads realistas com múltiplos usuários simultâneos via pgbench.",
                                  "learningObjective": "Capacitar análise diagnóstica de performance em volumes massivos de dados.",
                                  "commonMistakes": [
                                    "Focar apenas em tempo total sem decompor por operação",
                                    "Testar em hardware insuficiente",
                                    "Ignorar concorrência"
                                  ]
                                },
                                {
                                  "stepNumber": 3,
                                  "title": "Ajustar Configurações para Escalabilidade",
                                  "subSteps": [
                                    "Implementar otimizações básicas: criar índices compostos, particionamento por range/hash.",
                                    "Ajustar parâmetros do banco: work_mem, shared_buffers, effective_cache_size.",
                                    "Aplicar técnicas de compressão (ex: columnar storage com Parquet).",
                                    "Testar sharding ou replicação para distribuição horizontal.",
                                    "Re-medir métricas e iterar ajustes até atingir thresholds (ex: query < 2s em 1TB)."
                                  ],
                                  "verification": "Métricas pós-ajuste mostram melhoria de pelo menos 50% em tempo/espaço.",
                                  "estimatedTime": "2.5 horas",
                                  "materials": [
                                    "Documentação de tuning (PostgreSQL docs)",
                                    "Ferramentas: pg_config, ALTER TABLE",
                                    "Datasets particionados"
                                  ],
                                  "tips": "Comece com otimizações low-hanging fruit como VACUUM ANALYZE antes de configs avançadas.",
                                  "learningObjective": "Aprender a otimizar configurações para cenários de alta escalabilidade.",
                                  "commonMistakes": [
                                    "Over-indexing levando a overhead em inserts",
                                    "Ignorar custos de manutenção de índices",
                                    "Ajustes sem baseline prévio"
                                  ]
                                },
                                {
                                  "stepNumber": 4,
                                  "title": "Implementar Processamento Distribuído com Apache Spark",
                                  "subSteps": [
                                    "Instalar e configurar cluster Spark local ou em nuvem (Databricks/EMR).",
                                    "Migrar dados para formato distribuído (Parquet em HDFS/S3).",
                                    "Escrever jobs Spark SQL/DataFrame para queries equivalentes às do banco centralizado.",
                                    "Otimizar Spark: particionamento dinâmico, caching, broadcast joins.",
                                    "Comparar performance: Spark vs monolítico em dataset >1TB."
                                  ],
                                  "verification": "Job Spark executado com tempo de query < monolítico e relatório comparativo.",
                                  "estimatedTime": "3 horas",
                                  "materials": [
                                    "Apache Spark 3.x",
                                    "Docker para cluster local",
                                    "Datasets em S3",
                                    "PySpark/Jupyter"
                                  ],
                                  "tips": "Use spark.sql.shuffle.partitions baseado no tamanho do cluster para evitar OOM.",
                                  "learningObjective": "Integrar Spark para escalar processamento além de limites de single-node.",
                                  "commonMistakes": [
                                    "Não persistir DataFrames",
                                    "Usar collect() em datasets grandes",
                                    "Ignorar skew de dados"
                                  ]
                                }
                              ],
                              "practicalExample": "Em um data warehouse de e-commerce com 5TB de logs de vendas, medir query de top produtos (inicial: 45s, 2TB espaço). Após índices e Spark: 3s, 800GB. Ajustes: particionamento por data + Spark SQL com caching.",
                              "finalVerifications": [
                                "Gera relatórios precisos de métricas de performance pré/pós-otimização.",
                                "Identifica e resolve gargalos em datasets >500GB.",
                                "Configura banco para suportar 10x crescimento sem degradação >20%.",
                                "Executa jobs Spark distribuídos com otimização automática.",
                                "Compara cenários monolíticos vs distribuídos quantitativamente.",
                                "Documenta trade-offs (custo vs performance)."
                              ],
                              "assessmentCriteria": [
                                "Precisão nas medições (±5% de variação).",
                                "Eficiência de otimizações (melhoria >40% em métricas chave).",
                                "Correta identificação de 80% dos gargalos simulados.",
                                "Código Spark idempotente e escalável.",
                                "Relatórios claros com visualizações.",
                                "Consideração de custos operacionais (ex: cluster size)."
                              ],
                              "crossCurricularConnections": [
                                "Programação (Python/SQL para automação de benchmarks).",
                                "Estatística (análise de distribuições de latência).",
                                "Engenharia de Software (design de sistemas distribuídos).",
                                "Matemática Computacional (algoritmos de otimização).",
                                "Gestão de Projetos (trade-offs de escalabilidade vs custo)."
                              ],
                              "realWorldApplication": "Em empresas como Uber ou Amazon, avaliando performance de data lakes para petabytes de dados em tempo real, otimizando queries de ML para milhões de usuários diários e reduzindo custos de infraestrutura em 60%."
                            },
                            "estimatedTime": "2 horas",
                            "difficulty": "intermediate",
                            "status": "not_started",
                            "prerequisites": []
                          }
                        ]
                      }
                    ]
                  },
                  {
                    "id": "10.1.4.3",
                    "name": "Análise Exploratória de Dados",
                    "description": "Métodos para resumir e entender características principais dos dados.",
                    "individualConcepts": [
                      {
                        "id": "10.1.4.3.1",
                        "name": "Estatísticas Descritivas",
                        "description": "Cálculo de medidas resumidas como tendência central, dispersão e forma da distribuição para entender características univariadas dos dados.",
                        "specificSkills": [
                          {
                            "id": "10.1.4.3.1.1",
                            "name": "Calcular medidas de tendência central",
                            "description": "Computar média aritmética, mediana e moda em conjuntos de dados numéricos para identificar valores centrais representativos.",
                            "atomicExpansion": {
                              "steps": [
                                {
                                  "stepNumber": 1,
                                  "title": "Calcular a Média Aritmética",
                                  "subSteps": [
                                    "Colete o conjunto de dados numéricos.",
                                    "Some todos os valores do conjunto.",
                                    "Conte o número total de valores (n).",
                                    "Divida a soma por n para obter a média.",
                                    "Arredonde o resultado se necessário para precisão decimal desejada."
                                  ],
                                  "verification": "Verifique somando os valores e dividindo manualmente; compare com calculadora.",
                                  "estimatedTime": "15 minutos",
                                  "materials": "Papel, lápis, calculadora ou planilha Excel/Google Sheets",
                                  "tips": "Use uma planilha para somas automáticas em grandes datasets.",
                                  "learningObjective": "Compreender e aplicar a fórmula da média aritmética em conjuntos numéricos.",
                                  "commonMistakes": "Esquecer de dividir pela contagem correta de elementos ou incluir valores não numéricos."
                                },
                                {
                                  "stepNumber": 2,
                                  "title": "Determinar a Mediana",
                                  "subSteps": [
                                    "Ordene o conjunto de dados em ordem crescente.",
                                    "Identifique se o número de elementos (n) é ímpar ou par.",
                                    "Se ímpar, a mediana é o valor central (posição (n+1)/2).",
                                    "Se par, a mediana é a média dos dois valores centrais.",
                                    "Confirme a ordenação e calcule conforme o caso."
                                  ],
                                  "verification": "Reordene os dados e verifique se o valor central corresponde ao esperado.",
                                  "estimatedTime": "20 minutos",
                                  "materials": "Papel, lápis ou software de ordenação como Python (sorted()) ou Excel (SORT function)",
                                  "tips": "Sempre ordene primeiro para evitar erros de posicionamento.",
                                  "learningObjective": "Dominar o cálculo da mediana para conjuntos ordenados, considerando paridade.",
                                  "commonMistakes": "Não ordenar corretamente ou calcular errado para conjuntos pares."
                                },
                                {
                                  "stepNumber": 3,
                                  "title": "Identificar a Moda",
                                  "subSteps": [
                                    "Liste todos os valores únicos no conjunto de dados.",
                                    "Conte a frequência de ocorrência de cada valor.",
                                    "Identifique o(s) valor(es) com a maior frequência.",
                                    "Declare unimodal (uma moda), bimodal (duas) ou amodal (nenhuma).",
                                    "Registre a moda com sua frequência para clareza."
                                  ],
                                  "verification": "Conte manualmente as frequências e compare com tabela de frequência.",
                                  "estimatedTime": "15 minutos",
                                  "materials": "Papel, lápis ou ferramentas como Excel (COUNTIF) ou Python (collections.Counter)",
                                  "tips": "Use tabelas de frequência para datasets maiores.",
                                  "learningObjective": "Aplicar contagem de frequências para encontrar a(s) moda(s).",
                                  "commonMistakes": "Ignorar múltiplas modas ou confundir com valor mais frequente em dados desordenados."
                                },
                                {
                                  "stepNumber": 4,
                                  "title": "Aplicar e Comparar Medidas em um Conjunto de Dados Completo",
                                  "subSteps": [
                                    "Selecione um conjunto de dados real (ex: notas de 10 alunos).",
                                    "Calcule média, mediana e moda sequencialmente.",
                                    "Registre os resultados em uma tabela comparativa.",
                                    "Interprete brevemente: qual medida é mais representativa e por quê?",
                                    "Teste com um dataset alterado para observar mudanças."
                                  ],
                                  "verification": "Compare resultados com ferramenta computacional (ex: Python pandas.describe()).",
                                  "estimatedTime": "25 minutos",
                                  "materials": "Conjunto de dados exemplo, calculadora, Python/Excel ou Jupyter Notebook",
                                  "tips": "Escolha datasets com outliers para ver diferenças entre medidas.",
                                  "learningObjective": "Integrar cálculos de tendência central e interpretar contextualmente.",
                                  "commonMistakes": "Não considerar o impacto de outliers na escolha da medida apropriada."
                                }
                              ],
                              "practicalExample": "Dataset de notas de alunos: [65, 78, 92, 78, 85, 78, 99, 70, 88, 92]. Média: 82.5; Mediana: 81.5 (média de 78 e 85); Moda: 78 (frequência 3).",
                              "finalVerifications": [
                                "Cálculo correto da média em pelo menos 3 datasets diferentes.",
                                "Mediana precisa para conjuntos ímpares e pares.",
                                "Identificação correta de moda, incluindo casos multimodais.",
                                "Tabela comparativa completa com interpretações.",
                                "Validação cruzada com ferramenta computacional sem discrepâncias.",
                                "Explicação verbal das diferenças entre as medidas."
                              ],
                              "assessmentCriteria": [
                                "Precisão matemática nos cálculos (100% correto).",
                                "Ordenação correta e tratamento de paridade na mediana.",
                                "Detecção precisa de modas múltiplas.",
                                "Uso adequado de ferramentas e verificações.",
                                "Interpretação contextual das medidas.",
                                "Clareza na documentação de passos e resultados."
                              ],
                              "crossCurricularConnections": [
                                "Ciência de Dados: Uso em análise exploratória com Python/Pandas.",
                                "Estatística: Base para dispersão e distribuições.",
                                "Programação: Implementação em loops e funções.",
                                "Economia: Análise de médias salariais ou vendas."
                              ],
                              "realWorldApplication": "Em ciência de dados, calcular tendência central para resumir dados de vendas em e-commerce (média de ticket), salários em RH (mediana para salários medianos resistentes a outliers) ou popularidade de produtos (moda em preferências de clientes)."
                            },
                            "estimatedTime": "2 horas",
                            "difficulty": "beginner",
                            "status": "not_started",
                            "prerequisites": []
                          },
                          {
                            "id": "10.1.4.3.1.2",
                            "name": "Calcular medidas de dispersão",
                            "description": "Determinar variância, desvio padrão, amplitude interquartil e coeficiente de variação para quantificar a variabilidade dos dados.",
                            "atomicExpansion": {
                              "steps": [
                                {
                                  "stepNumber": 1,
                                  "title": "Preparar dados e calcular medidas de tendência central",
                                  "subSteps": [
                                    "Coletar e organizar o conjunto de dados em uma lista ordenada.",
                                    "Calcular a média aritmética somando todos os valores e dividindo pelo número de observações.",
                                    "Determinar a mediana: encontrar o valor central para conjuntos ímpares ou a média dos dois centrais para pares.",
                                    "Calcular os quartis Q1 (25%) e Q3 (75%) dividindo os dados ordenados em quatro partes iguais.",
                                    "Verificar cálculos com uma ferramenta como Excel ou Python para validação inicial."
                                  ],
                                  "verification": "Conferir se média, mediana, Q1 e Q3 batem com cálculos manuais e computacionais em pelo menos dois conjuntos de dados de teste.",
                                  "estimatedTime": "45 minutos",
                                  "materials": [
                                    "Conjunto de dados de exemplo (ex: notas de 10 alunos)",
                                    "Calculadora",
                                    "Planilha Excel ou Jupyter Notebook com NumPy"
                                  ],
                                  "tips": "Ordene os dados sempre antes de calcular quartis para evitar erros de posicionamento.",
                                  "learningObjective": "Dominar o cálculo de medidas de posição necessárias para dispersão.",
                                  "commonMistakes": [
                                    "Esquecer de ordenar dados para mediana/quartis",
                                    "Confundir média com mediana em conjuntos assimétricos"
                                  ]
                                },
                                {
                                  "stepNumber": 2,
                                  "title": "Calcular amplitude e amplitude interquartil (IQR)",
                                  "subSteps": [
                                    "Calcular a amplitude: subtrair o menor valor do maior no conjunto ordenado.",
                                    "Calcular IQR: subtrair Q1 de Q3.",
                                    "Interpretar: amplitude sensível a outliers, IQR mais robusta.",
                                    "Aplicar em dados reais: use um conjunto com outlier para comparar.",
                                    "Registrar valores e anotar impactos de outliers."
                                  ],
                                  "verification": "Reproduzir cálculos em um novo conjunto de dados e confirmar IQR > 0 para variabilidade.",
                                  "estimatedTime": "30 minutos",
                                  "materials": [
                                    "Dados ordenados do Step 1",
                                    "Papel e lápis para anotações",
                                    "Software de planilhas"
                                  ],
                                  "tips": "Use IQR para conjuntos com outliers; amplitude só para dados sem extremos.",
                                  "learningObjective": "Entender e computar medidas de dispersão resistentes a outliers.",
                                  "commonMistakes": [
                                    "Incluir outlier na amplitude sem notar sensibilidade",
                                    "Invertar Q3 - Q1"
                                  ]
                                },
                                {
                                  "stepNumber": 3,
                                  "title": "Calcular variância e desvio padrão",
                                  "subSteps": [
                                    "Calcular variância populacional: média dos quadrados das desvios em relação à média.",
                                    "Calcular variância amostral: dividir por (n-1) em vez de n.",
                                    "Derivar desvio padrão: raiz quadrada da variância.",
                                    "Comparar pop vs amostra em um dataset pequeno.",
                                    "Usar fórmula: σ² = Σ(xi - μ)² / N."
                                  ],
                                  "verification": "Verificar se desvio padrão é unidade da média e variância é quadrado dela.",
                                  "estimatedTime": "60 minutos",
                                  "materials": [
                                    "Calculadora científica",
                                    "Python com NumPy (np.var, np.std)",
                                    "Tabela de dados"
                                  ],
                                  "tips": "Use amostral para inferência; prefira computacional para grandes n.",
                                  "learningObjective": "Computar variância e desvio padrão com fórmulas precisas.",
                                  "commonMistakes": [
                                    "Usar N em vez de n-1 para amostra",
                                    "Esquecer raiz quadrada no desvio"
                                  ]
                                },
                                {
                                  "stepNumber": 4,
                                  "title": "Calcular coeficiente de variação e interpretar todas as medidas",
                                  "subSteps": [
                                    "Calcular CV: (desvio padrão / média) * 100%.",
                                    "Comparar todas as medidas: amplitude, IQR, DP, CV em múltiplos datasets.",
                                    "Interpretar: CV < 10% baixa variabilidade; > 30% alta.",
                                    "Criar tabela comparativa.",
                                    "Discutir quando usar cada uma (ex: CV para comparar escalas diferentes)."
                                  ],
                                  "verification": "Aplicar CV a dois conjuntos com médias diferentes e confirmar qual tem mais variabilidade relativa.",
                                  "estimatedTime": "45 minutos",
                                  "materials": [
                                    "Resultados dos steps anteriores",
                                    "Gráficos boxplot no Excel/Python"
                                  ],
                                  "tips": "CV é adimensional, ideal para comparações entre variáveis.",
                                  "learningObjective": "Integrar e interpretar medidas de dispersão holisticamente.",
                                  "commonMistakes": [
                                    "Dividir por mediana em vez de média no CV",
                                    "Ignorar zero na média"
                                  ]
                                }
                              ],
                              "practicalExample": "Dados de alturas de 10 alunos: [160, 165, 170, 162, 180, 155, 168, 172, 175, 169]. Calcule: amplitude=25cm, IQR=10cm, variância amostral≈57.6 cm², DP≈7.6cm, CV≈4.6% (baixa variabilidade).",
                              "finalVerifications": [
                                "Calcular corretamente todas as medidas para um dataset inédito.",
                                "Identificar outlier impactando amplitude mas não IQR.",
                                "Explicar diferença entre variância pop/amostra.",
                                "Comparar dispersão relativa via CV entre dois grupos.",
                                "Gerar boxplot mostrando IQR e outliers.",
                                "Verificar fórmulas em código Python."
                              ],
                              "assessmentCriteria": [
                                "Precisão nos cálculos (±0.1 para DP/CV).",
                                "Correta distinção pop/amostra.",
                                "Interpretação contextual de cada medida.",
                                "Uso adequado de ferramentas computacionais.",
                                "Identificação de erros comuns e correções.",
                                "Tabela comparativa clara e completa."
                              ],
                              "crossCurricularConnections": [
                                "Programação: Implementar em Python (NumPy/Pandas) para Ciência de Dados.",
                                "Estatística Biológica: Analisar variabilidade em medidas corporais ou experimentos.",
                                "Economia: Dispersão em retornos financeiros ou salários.",
                                "Física: Variabilidade em medições experimentais."
                              ],
                              "realWorldApplication": "Em análise de dados de vendas, calcular DP das vendas diárias para prever estoque; em saúde, CV de pressões arteriais para monitorar pacientes; em qualidade industrial, IQR para detectar defeitos em produção."
                            },
                            "estimatedTime": "2 horas",
                            "difficulty": "beginner",
                            "status": "not_started",
                            "prerequisites": []
                          },
                          {
                            "id": "10.1.4.3.1.3",
                            "name": "Analisar assimetria e curtose",
                            "description": "Avaliar coeficientes de assimetria (skewness) e curtose para descrever a forma da distribuição dos dados.",
                            "atomicExpansion": {
                              "steps": [
                                {
                                  "stepNumber": 1,
                                  "title": "Compreender os Conceitos Fundamentais de Assimetria e Curtose",
                                  "subSteps": [
                                    "Defina assimetria (skewness) como a medida da falta de simetria na distribuição de dados.",
                                    "Classifique skewness: positiva (cauda à direita), negativa (cauda à esquerda) e zero (simétrica).",
                                    "Defina curtose (kurtosis) como a medida da espessura das caudas e pico central em relação à normal.",
                                    "Classifique kurtosis: leptocúrtica (>3, caudas pesadas), mesocúrtica (≈3, normal) e platicúrtica (<3, caudas leves).",
                                    "Relacione conceitos com visualizações como histogramas e boxplots."
                                  ],
                                  "verification": "Explique em suas palavras os conceitos e forneça um exemplo verbal de cada tipo de skewness e kurtosis.",
                                  "estimatedTime": "20 minutos",
                                  "materials": [
                                    "Documentação de estatística descritiva (Khan Academy ou Wikipedia)",
                                    "Vídeos tutoriais sobre distribuições (YouTube: StatQuest)"
                                  ],
                                  "tips": "Use analogias cotidianas: skewness positiva como distribuição de renda (poucos ricos), kurtosis alta como eventos raros (terremotos).",
                                  "learningObjective": "Dominar as definições e classificações de assimetria e curtose para descrever formas de distribuições.",
                                  "commonMistakes": [
                                    "Confundir skewness (assimetria) com kurtosis (caudas)",
                                    "Ignorar que kurtosis é comparada à distribuição normal (valor 3 para mesocúrtica)"
                                  ]
                                },
                                {
                                  "stepNumber": 2,
                                  "title": "Preparar e Explorar um Dataset para Análise",
                                  "subSteps": [
                                    "Selecione ou crie um dataset numérico (ex: 50-100 observações).",
                                    "Carregue os dados em Python usando pandas: pd.read_csv('dataset.csv').",
                                    "Calcule estatísticas básicas: média, mediana, desvio padrão com df.describe().",
                                    "Crie visualizações iniciais: histograma e boxplot com matplotlib ou seaborn.",
                                    "Identifique indícios visuais de assimetria ou caudas pesadas."
                                  ],
                                  "verification": "Gere gráficos e estatísticas básicas que mostrem potenciais assimetrias no dataset.",
                                  "estimatedTime": "25 minutos",
                                  "materials": [
                                    "Python com pandas, numpy, matplotlib/seaborn",
                                    "Jupyter Notebook",
                                    "Dataset exemplo: 'salarios.csv' ou 'alturas.csv'"
                                  ],
                                  "tips": "Sempre verifique outliers primeiro, pois afetam skewness e kurtosis.",
                                  "learningObjective": "Preparar dados para cálculo preciso de medidas de forma da distribuição.",
                                  "commonMistakes": [
                                    "Não tratar missing values ou outliers antes da análise",
                                    "Usar dataset muito pequeno (<30 observações), levando a instabilidade"
                                  ]
                                },
                                {
                                  "stepNumber": 3,
                                  "title": "Calcular os Coeficientes de Assimetria e Curtose",
                                  "subSteps": [
                                    "Importe bibliotecas: from scipy.stats import skew, kurtosis.",
                                    "Calcule skewness: skew_value = skew(dataset, bias=False).",
                                    "Calcule kurtosis: kurt_value = kurtosis(dataset, fisher=True).",
                                    "Compare com cálculo manual para subconjunto pequeno usando fórmulas: skewness = E[(X-μ)^3]/σ^3.",
                                    "Registre valores e compare com limiares: |skew| < 0.5 (simétrica), kurtosis ≈ 0 (mesocúrtica no Fisher)."
                                  ],
                                  "verification": "Execute código e obtenha valores numéricos corretos para skewness e kurtosis em um dataset teste.",
                                  "estimatedTime": "30 minutos",
                                  "materials": [
                                    "Python com scipy.stats",
                                    "Jupyter Notebook",
                                    "Dataset preparado no step anterior"
                                  ],
                                  "tips": "Use bias=False para amostras; Fisher=True subtrai 3 para kurtosis centrada em 0.",
                                  "learningObjective": "Aplicar fórmulas e funções computacionais para quantificar assimetria e curtose.",
                                  "commonMistakes": [
                                    "Usar kurtosis excess=False (Pearson=3 para normal) sem ajustar para Fisher",
                                    "Calcular em dados não numéricos ou com NaNs"
                                  ]
                                },
                                {
                                  "stepNumber": 4,
                                  "title": "Interpretar e Descrever a Forma da Distribuição",
                                  "subSteps": [
                                    "Interprete skewness: >0.5 (positiva), <-0.5 (negativa), próximo de 0 (simétrica).",
                                    "Interprete kurtosis: >0 (leptocúrtica), <0 (platicúrtica), ≈0 (mesocúrtica).",
                                    "Descreva a distribuição combinando ambos: ex. 'assimétrica à direita com caudas pesadas'.",
                                    "Valide com Q-Q plot: from scipy.stats import probplot; probplot(data, plot=plt).",
                                    "Escreva um relatório curto resumindo a forma da distribuição."
                                  ],
                                  "verification": "Forneça uma descrição escrita correta da forma da distribuição baseada nos valores calculados.",
                                  "estimatedTime": "25 minutos",
                                  "materials": [
                                    "Python com scipy.stats e matplotlib",
                                    "Relatório template em Markdown"
                                  ],
                                  "tips": "Combine com outros descritivos (mediana vs média) para robustez na interpretação.",
                                  "learningObjective": "Traduzir valores numéricos em descrições qualitativas acionáveis da distribuição.",
                                  "commonMistakes": [
                                    "Interpretar valores absolutos sem contexto (ex. ignorar tamanho da amostra)",
                                    "Não validar visualmente cálculos numéricos"
                                  ]
                                }
                              ],
                              "practicalExample": "Carregue o dataset de salários anuais de uma empresa (salarios.csv com 100 entradas). Calcule skewness ≈ 1.2 (positiva, poucos altos salários) e kurtosis ≈ 0.8 (leptocúrtica, extremos salariais raros mas impactantes). Descreva: 'Distribuição assimétrica à direita com caudas pesadas, indicando desigualdade salarial'. Valide com histograma mostrando cauda longa à direita.",
                              "finalVerifications": [
                                "Calcular corretamente skewness e kurtosis para dataset fornecido usando Python.",
                                "Interpretar valores: classificar corretamente como positiva/negativa e leptocúrtica/etc.",
                                "Gerar visualizações (histograma, Q-Q plot) que confirmem as medidas.",
                                "Escrever descrição qualitativa precisa da forma da distribuição.",
                                "Comparar resultados manuais vs computacionais com erro <1%.",
                                "Identificar impactos em análises subsequentes (ex. não normalidade)."
                              ],
                              "assessmentCriteria": [
                                "Precisão nos cálculos (erro <0.01 nos coeficientes).",
                                "Correção na interpretação qualitativa (alinhada com regras padrão).",
                                "Qualidade das visualizações (claras, rotuladas, relevantes).",
                                "Completude do relatório (inclui valores, interpretação e validação).",
                                "Uso adequado de bibliotecas e tratamento de dados.",
                                "Criatividade em conexões com contexto real do dataset."
                              ],
                              "crossCurricularConnections": [
                                "Programação em Python/R: implementação de funções stats.",
                                "Estatística Inferencial: pré-requisito para testes de normalidade (Shapiro-Wilk).",
                                "Economia/Finanças: análise de distribuições de renda ou retornos de investimentos.",
                                "Biologia/Ciências: modelagem de medidas biológicas (ex. tamanhos de espécies).",
                                "Machine Learning: detecção de feature engineering para não-normalidade."
                              ],
                              "realWorldApplication": "Em análise de risco financeiro, skewness positiva em retornos de ações indica maior probabilidade de perdas grandes (cauda esquerda); em controle de qualidade industrial, kurtosis alta sinaliza defeitos raros mas catastróficos, guiando decisões de processo."
                            },
                            "estimatedTime": "1 hora",
                            "difficulty": "intermediate",
                            "status": "not_started",
                            "prerequisites": []
                          }
                        ]
                      },
                      {
                        "id": "10.1.4.3.2",
                        "name": "Visualização de Dados",
                        "description": "Uso de gráficos e plots para representar visualmente as características principais dos dados, facilitando a identificação de padrões.",
                        "specificSkills": [
                          {
                            "id": "10.1.4.3.2.1",
                            "name": "Criar histogramas e boxplots",
                            "description": "Construir histogramas para distribuições univariadas e boxplots para detectar outliers e resumir quartis.",
                            "atomicExpansion": {
                              "steps": [
                                {
                                  "stepNumber": 1,
                                  "title": "Preparar o ambiente e os dados",
                                  "subSteps": [
                                    "Instalar bibliotecas necessárias: pandas, matplotlib e seaborn via pip.",
                                    "Carregar um dataset univariado adequado (ex: alturas de alunos).",
                                    "Explorar os dados com .describe() e .info() para entender estrutura.",
                                    "Limpar dados: remover NaNs e outliers iniciais se necessário.",
                                    "Verificar distribuição básica com .hist() rápido do pandas."
                                  ],
                                  "verification": "Dataset carregado sem erros, .describe() mostra estatísticas corretas e sem NaNs.",
                                  "estimatedTime": "15 minutos",
                                  "materials": "Python/Jupyter Notebook, dataset CSV (ex: heights.csv), bibliotecas pandas, matplotlib, seaborn.",
                                  "tips": "Use datasets prontos do seaborn como 'tips' para testes rápidos.",
                                  "learningObjective": "Configurar ambiente de análise de dados para visualizações.",
                                  "commonMistakes": "Esquecer de importar bibliotecas ou ignorar NaNs nos dados."
                                },
                                {
                                  "stepNumber": 2,
                                  "title": "Construir histogramas para distribuições univariadas",
                                  "subSteps": [
                                    "Importar matplotlib.pyplot e seaborn.",
                                    "Criar histograma básico com plt.hist(data['coluna'], bins=30).",
                                    "Personalizar: adicionar título, labels de eixos e densidade com kde=True no seaborn.",
                                    "Ajustar bins automaticamente com 'auto' ou Sturges' rule.",
                                    "Salvar o plot como PNG para revisão."
                                  ],
                                  "verification": "Histograma gerado mostra distribuição unimodal/bimodal corretamente, com eixos rotulados.",
                                  "estimatedTime": "20 minutos",
                                  "materials": "Jupyter Notebook, dataset preparado.",
                                  "tips": "Use seaborn.histplot para visualizações mais elegantes e informativas.",
                                  "learningObjective": "Visualizar e interpretar distribuições de frequência de variáveis contínuas.",
                                  "commonMistakes": "Escolher poucos bins (perde detalhes) ou muitos bins (ruído excessivo)."
                                },
                                {
                                  "stepNumber": 3,
                                  "title": "Construir boxplots para resumo de quartis e detecção de outliers",
                                  "subSteps": [
                                    "Criar boxplot básico com plt.boxplot(data['coluna']) ou sns.boxplot.",
                                    "Comparar múltiplas categorias com sns.boxplot(x='categoria', y='valor').",
                                    "Identificar outliers visuais (pontos além dos whiskers).",
                                    "Adicionar estatísticas: mediana, Q1, Q3 anotadas no plot.",
                                    "Combinar com histograma em subplots para comparação."
                                  ],
                                  "verification": "Boxplot exibe quartis corretos, mediana central e outliers marcados.",
                                  "estimatedTime": "25 minutos",
                                  "materials": "Mesmo notebook e dataset, adicionar seaborn.",
                                  "tips": "Whiskers padrão vão até 1.5*IQR; ajuste para detecção personalizada.",
                                  "learningObjective": "Resumir dados com boxplots e detectar anomalias.",
                                  "commonMistakes": "Confundir mediana com média ou ignorar escala log para dados skewed."
                                },
                                {
                                  "stepNumber": 4,
                                  "title": "Interpretar visualizações e gerar relatório",
                                  "subSteps": [
                                    "Analisar histograma: skewness, moda, unimodalidade.",
                                    "Analisar boxplot: assimetria via quartis, outliers por categoria.",
                                    "Comparar ambos: validar outliers no histograma.",
                                    "Documentar insights em Markdown no notebook.",
                                    "Exportar plots e relatório como PDF/HTML."
                                  ],
                                  "verification": "Relatório escrito identifica pelo menos 3 insights (ex: 5% outliers em grupo X).",
                                  "estimatedTime": "20 minutos",
                                  "materials": "Notebook com plots gerados.",
                                  "tips": "Use plt.suptitle() para títulos comparativos em múltiplos plots.",
                                  "learningObjective": "Extrair insights acionáveis de histogramas e boxplots.",
                                  "commonMistakes": "Interpretar outliers como erros sem contexto ou ignorar skewness."
                                }
                              ],
                              "practicalExample": "Usando dataset 'tips' do seaborn: crie histograma da coluna 'total_bill' para ver distribuição de contas; boxplot de 'total_bill' por 'day' para detectar dias com outliers altos (ex: sábados com contas acima de $50).",
                              "finalVerifications": [
                                "Histogramas mostram distribuição correta com bins adequados e KDE suave.",
                                "Boxplots exibem quartis precisos (Q1, mediana, Q3) e outliers destacados.",
                                "Combinação de plots em subplots permite comparação visual clara.",
                                "Insights documentados incluem skewness, outliers e implicações.",
                                "Todos plots salvos com labels, títulos e legenda.",
                                "Código executável sem erros em ambiente limpo."
                              ],
                              "assessmentCriteria": [
                                "Precisão na geração de histogramas (bins ótimos, normalização).",
                                "Correção em boxplots (quartis calculados via IQR, outliers detectados).",
                                "Qualidade visual: labels claros, cores acessíveis, layout profissional.",
                                "Profundidade de interpretação: identificação de padrões e anomalias.",
                                "Eficiência do código: uso de funções seaborn/matplotlib otimizadas.",
                                "Relatório completo com pelo menos 3 insights acionáveis."
                              ],
                              "crossCurricularConnections": [
                                "Estatística: conceitos de quartis, IQR e testes de normalidade.",
                                "Programação: manipulação de dataframes com pandas e plotting APIs.",
                                "Ciência de Dados: EDA como pré-processamento para modelagem.",
                                "Matemática: distribuições probabilísticas e medidas de tendência central."
                              ],
                              "realWorldApplication": "Em análise de dados de e-commerce, histogramas revelam distribuição de tempos de entrega (identificar delays comuns), enquanto boxplots por região detectam outliers de atrasos, permitindo otimizar logística e reduzir custos."
                            },
                            "estimatedTime": "3 horas",
                            "difficulty": "beginner",
                            "status": "not_started",
                            "prerequisites": []
                          },
                          {
                            "id": "10.1.4.3.2.2",
                            "name": "Gerar scatter plots e heatmaps",
                            "description": "Produzir gráficos de dispersão para relações bivariadas e mapas de calor para matrizes de correlação.",
                            "atomicExpansion": {
                              "steps": [
                                {
                                  "stepNumber": 1,
                                  "title": "Preparar o ambiente e os dados",
                                  "subSteps": [
                                    "Instale e importe bibliotecas necessárias: pandas, matplotlib.pyplot e seaborn.",
                                    "Carregue um dataset bivariado ou multivariado, como o dataset Iris do seaborn.",
                                    "Explore os dados com .info(), .describe() e .corr() para identificar variáveis adequadas.",
                                    "Trate valores ausentes ou outliers se necessário usando .dropna() ou .fillna().",
                                    "Selecione colunas específicas para scatter (ex: sepal_length vs sepal_width) e compute matriz de correlação."
                                  ],
                                  "verification": "Confirme que os dados estão carregados sem erros e a matriz de correlação é exibida corretamente via print(df.corr()).",
                                  "estimatedTime": "20 minutos",
                                  "materials": [
                                    "Python com pandas, matplotlib, seaborn instalados",
                                    "Dataset Iris: import seaborn as sns; iris = sns.load_dataset('iris')"
                                  ],
                                  "tips": "Sempre verifique o shape do dataset com .shape para entender dimensões antes de plotar.",
                                  "learningObjective": "Configurar ambiente de visualização e preparar dados limpos para análise bivariada e correlações.",
                                  "commonMistakes": [
                                    "Esquecer de importar bibliotecas",
                                    "Não tratar NaNs levando a erros no plot",
                                    "Usar dataset inadequado sem variáveis numéricas"
                                  ]
                                },
                                {
                                  "stepNumber": 2,
                                  "title": "Gerar scatter plot para relações bivariadas",
                                  "subSteps": [
                                    "Use sns.scatterplot(data=iris, x='sepal_length', y='sepal_width', hue='species') para plot básico.",
                                    "Adicione título com plt.title('Scatter Plot: Sepal Length vs Width') e labels com plt.xlabel(), plt.ylabel().",
                                    "Personalize com alpha=0.7 para transparência e size='petal_length' para bolhas variáveis.",
                                    "Exiba o gráfico com plt.show() e ajuste layout com plt.tight_layout().",
                                    "Salve o gráfico como PNG com plt.savefig('scatter_iris.png', dpi=300)."
                                  ],
                                  "verification": "O scatter plot é exibido com pontos dispersos coloridos por categoria, eixos rotulados e salvo sem erros.",
                                  "estimatedTime": "25 minutos",
                                  "materials": [
                                    "Código Python/Jupyter Notebook",
                                    "Dataset Iris preparado"
                                  ],
                                  "tips": "Use hue para categorizar e revelar padrões ocultos em relações bivariadas.",
                                  "learningObjective": "Criar e customizar scatter plots para visualizar correlações e clusters em dados bivariados.",
                                  "commonMistakes": [
                                    "Confundir x e y invertendo variáveis",
                                    "Esquecer plt.show() resultando em gráfico não visível",
                                    "Overplotting sem alpha levando a pontos sobrepostos"
                                  ]
                                },
                                {
                                  "stepNumber": 3,
                                  "title": "Gerar heatmap para matriz de correlação",
                                  "subSteps": [
                                    "Compute a matriz de correlação numérica: corr = iris.select_dtypes(include=['float64', 'int64']).corr().",
                                    "Crie o heatmap com sns.heatmap(corr, annot=True, cmap='coolwarm', center=0).",
                                    "Adicione título plt.title('Matriz de Correlação - Iris Dataset') e ajuste figura com plt.figure(figsize=(8,6)).",
                                    "Personalize com fmt='.2f' para formatação de números e linewidths=0.5 para grade.",
                                    "Exiba e salve com plt.show() e plt.savefig('heatmap_corr.png')."
                                  ],
                                  "verification": "Heatmap mostra valores de correlação coloridos, com anotações numéricas e simetria perfeita.",
                                  "estimatedTime": "20 minutos",
                                  "materials": [
                                    "Código Python",
                                    "Matriz de correlação computada"
                                  ],
                                  "tips": "Escolha cmap='coolwarm' para destacar correlações positivas (vermelho) e negativas (azul).",
                                  "learningObjective": "Visualizar forças e direções de correlações multivariadas usando heatmaps.",
                                  "commonMistakes": [
                                    "Incluir variáveis categóricas na corr() causando erros",
                                    "Não usar annot=True perdendo valores numéricos",
                                    "Ignorar center=0 distorcendo zero"
                                  ]
                                },
                                {
                                  "stepNumber": 4,
                                  "title": "Interpretar, customizar e integrar visualizações",
                                  "subSteps": [
                                    "Analise o scatter: identifique clusters, outliers e tendências lineares.",
                                    "No heatmap, destaque as maiores correlações (ex: >0.7 ou <-0.7).",
                                    "Crie subplots combinando scatter e heatmap com plt.subplot(1,2,1) e plt.subplot(1,2,2).",
                                    "Adicione insights em texto ou legenda, como 'Alta correlação entre petal_length e petal_width'.",
                                    "Teste com novo dataset (ex: tips do seaborn) e documente observações."
                                  ],
                                  "verification": "Relatório escrito resume padrões encontrados em ambos os gráficos com evidências visuais.",
                                  "estimatedTime": "15 minutos",
                                  "materials": [
                                    "Gráficos gerados",
                                    "Notebook para anotações"
                                  ],
                                  "tips": "Use subplots para comparações lado a lado economizando espaço.",
                                  "learningObjective": "Integrar e interpretar visualizações para insights acionáveis em análise exploratória.",
                                  "commonMistakes": [
                                    "Interpretar correlação como causalidade",
                                    "Não escalar dados levando a distorções",
                                    "Sobrecarregar gráfico com muitas customizações"
                                  ]
                                }
                              ],
                              "practicalExample": "Usando o dataset Iris: Crie um scatter plot de sepal_length vs sepal_width colorido por species, revelando clusters separados. Em seguida, gere um heatmap da matriz de correlação das 4 features numéricas, destacando forte correlação positiva (0.96) entre petal_length e petal_width.",
                              "finalVerifications": [
                                "Scatter plot exibe relação bivariada clara com distinção de categorias via cor.",
                                "Heatmap mostra matriz simétrica com anotações precisas e gradiente de cores intuitivo.",
                                "Ambos gráficos são salvos em alta resolução e interpretados corretamente.",
                                "Nenhum erro de dados ou plotting ocorreu durante a execução.",
                                "Insights escritos conectam visualizações a padrões nos dados."
                              ],
                              "assessmentCriteria": [
                                "Correta preparação de dados (sem NaNs, variáveis numéricas selecionadas).",
                                "Scatter plot com labels, título, hue e alpha adequados.",
                                "Heatmap com annot, cmap apropriado e center=0.",
                                "Customizações melhoram clareza sem sobrecarga visual.",
                                "Interpretação demonstra compreensão de correlações e clusters.",
                                "Código é limpo, comentado e reproduzível."
                              ],
                              "crossCurricularConnections": [
                                "Estatística: Compreensão de coeficiente de correlação de Pearson.",
                                "Programação: Manipulação de dados com pandas e plotting com seaborn/matplotlib.",
                                "Biologia: Análise de datasets reais como Iris para classificação de espécies.",
                                "Negócios: Visualização exploratória para detecção de padrões em vendas ou clientes."
                              ],
                              "realWorldApplication": "Em ciência de dados, scatter plots identificam outliers em dados de sensores IoT para manutenção preditiva, enquanto heatmaps de correlação guiam feature selection em modelos de machine learning para previsão de churn de clientes em e-commerce."
                            },
                            "estimatedTime": "2 horas",
                            "difficulty": "intermediate",
                            "status": "not_started",
                            "prerequisites": []
                          },
                          {
                            "id": "10.1.4.3.2.3",
                            "name": "Explorar dados categóricos com countplots",
                            "description": "Utilizar gráficos de barras e countplots para visualizar frequências e proporções em variáveis categóricas.",
                            "atomicExpansion": {
                              "steps": [
                                {
                                  "stepNumber": 1,
                                  "title": "Configurar ambiente e carregar dados categóricos",
                                  "subSteps": [
                                    "Instalar e importar bibliotecas necessárias: pandas, seaborn e matplotlib.",
                                    "Carregar um dataset com variáveis categóricas, como o Titanic do seaborn.",
                                    "Inspecionar o dataset com .head() e .info() para identificar colunas categóricas (ex: 'Sex', 'Pclass').",
                                    "Verificar valores únicos em colunas categóricas com .unique() ou .value_counts().",
                                    "Preparar o ambiente no Jupyter Notebook para visualizações."
                                  ],
                                  "verification": "Dataset carregado com sucesso, colunas categóricas identificadas e .value_counts() executado sem erros.",
                                  "estimatedTime": "15 minutos",
                                  "materials": [
                                    "Python 3.x",
                                    "Jupyter Notebook",
                                    "Bibliotecas: pandas, seaborn, matplotlib",
                                    "Dataset Titanic (sns.load_dataset('titanic'))"
                                  ],
                                  "tips": [
                                    "Use load_dataset do seaborn para datasets prontos.",
                                    "Sempre verifique tipos de dados com .dtypes."
                                  ],
                                  "learningObjective": "Configurar ambiente Python pronto para análise de dados categóricos.",
                                  "commonMistakes": [
                                    "Esquecer de importar seaborn como sns.",
                                    "Carregar dataset numérico sem categorias.",
                                    "Não recarregar kernel após instalações."
                                  ]
                                },
                                {
                                  "stepNumber": 2,
                                  "title": "Explorar frequências básicas de variáveis categóricas",
                                  "subSteps": [
                                    "Selecionar uma coluna categórica (ex: 'Sex').",
                                    "Executar df['Sex'].value_counts() para ver contagens absolutas.",
                                    "Calcular proporções com df['Sex'].value_counts(normalize=True).",
                                    "Identificar categorias dominantes e desbalanceadas.",
                                    "Documentar observações iniciais em uma célula de markdown."
                                  ],
                                  "verification": "Tabela de value_counts() gerada mostrando frequências e proporções corretas.",
                                  "estimatedTime": "10 minutos",
                                  "materials": [
                                    "Dataset carregado do Step 1",
                                    "Jupyter Notebook"
                                  ],
                                  "tips": [
                                    "Normalize=True para ver proporções em %.",
                                    "Ordene com sort_index() para melhor visualização."
                                  ],
                                  "learningObjective": "Compreender distribuições de frequências em dados categóricos.",
                                  "commonMistakes": [
                                    "Confundir contagens absolutas com proporções.",
                                    "Ignorar desbalanceamento de classes.",
                                    "Usar colunas numéricas como categóricas."
                                  ]
                                },
                                {
                                  "stepNumber": 3,
                                  "title": "Criar countplots básicos com Seaborn",
                                  "subSteps": [
                                    "Executar sns.countplot(data=df, x='Sex') para gráfico simples.",
                                    "Adicionar plt.title('Distribuição por Sexo') e plt.show().",
                                    "Criar countplot para outra variável (ex: 'Pclass').",
                                    "Comparar múltiplos countplots lado a lado com plt.subplot().",
                                    "Salvar gráfico com plt.savefig('countplot_sex.png')."
                                  ],
                                  "verification": "Gráficos de barras gerados corretamente, com eixos rotulados e visíveis.",
                                  "estimatedTime": "20 minutos",
                                  "materials": [
                                    "Seaborn importado",
                                    "Dataset do Step 1"
                                  ],
                                  "tips": [
                                    "Use figsize=(10,6) para tamanhos adequados.",
                                    "Rotacione rótulos com plt.xticks(rotation=45)."
                                  ],
                                  "learningObjective": "Gerar visualizações de countplots para frequências categóricas.",
                                  "commonMistakes": [
                                    "Esquecer plt.show() ou %matplotlib inline.",
                                    "Passar y em vez de x para countplot.",
                                    "Gráficos muito pequenos ou ilegíveis."
                                  ]
                                },
                                {
                                  "stepNumber": 4,
                                  "title": "Customizar e interpretar countplots avançados",
                                  "subSteps": [
                                    "Adicionar hue='Survived' em sns.countplot(data=df, x='Sex', hue='Survived').",
                                    "Customizar com palette='Set2', order=df['Pclass'].value_counts().index.",
                                    "Analisar proporções visuais: identificar padrões como maior sobrevivência em certas categorias.",
                                    "Adicionar anotações de percentuais nas barras usando loops.",
                                    "Documentar insights: 'Mulheres têm maior proporção de sobreviventes'."
                                  ],
                                  "verification": "Countplot customizado gerado com hue, legenda e interpretações documentadas.",
                                  "estimatedTime": "25 minutos",
                                  "materials": [
                                    "Código dos steps anteriores",
                                    "Documentação Seaborn"
                                  ],
                                  "tips": [
                                    "Use dodge=True para barras lado a lado no hue.",
                                    "Consulte docs.seaborn.pydata.org para parâmetros."
                                  ],
                                  "learningObjective": "Customizar countplots e extrair insights de frequências/proporções.",
                                  "commonMistakes": [
                                    "Sobrecarregar com muitos hues.",
                                    "Ignorar ordem das categorias.",
                                    "Má interpretação de barras empilhadas vs. agrupadas."
                                  ]
                                }
                              ],
                              "practicalExample": "No dataset Titanic, crie um countplot com x='Pclass' e hue='Sex' para visualizar a distribuição de passageiros por classe e gênero, revelando que a 1ª classe tem mais mulheres, e calcule proporções de sobreviventes por grupo.",
                              "finalVerifications": [
                                "Countplot básico gerado sem erros para pelo menos duas variáveis categóricas.",
                                "Frequências absolutas e proporções calculadas e comparadas com o gráfico.",
                                "Customizações aplicadas (hue, palette, títulos) em um countplot avançado.",
                                "Insights documentados baseados na visualização (ex: desbalanceamento).",
                                "Gráficos salvos e exportados corretamente.",
                                "Código reproduzível em novo notebook."
                              ],
                              "assessmentCriteria": [
                                "Precisão na geração de countplots (sem erros de dados ou sintaxe).",
                                "Qualidade visual: rótulos claros, cores adequadas, legibilidade.",
                                "Correta interpretação de frequências e proporções.",
                                "Uso avançado de parâmetros como hue e order.",
                                "Documentação de insights acionáveis.",
                                "Eficiência temporal e código limpo."
                              ],
                              "crossCurricularConnections": [
                                "Estatística Descritiva: cálculo de frequências e proporções.",
                                "Programação em Python: manipulação de dataframes com Pandas.",
                                "Visualização de Dados: princípios de design gráfico.",
                                "Ciência de Dados: análise exploratória (EDA).",
                                "Matemática Computacional: gráficos para insights quantitativos."
                              ],
                              "realWorldApplication": "Em marketing, usar countplots para analisar preferências de clientes por categoria demográfica (idade, gênero) em campanhas, identificando segmentos dominantes para targeting personalizado e otimização de estratégias."
                            },
                            "estimatedTime": "2 horas",
                            "difficulty": "beginner",
                            "status": "not_started",
                            "prerequisites": []
                          }
                        ]
                      },
                      {
                        "id": "10.1.4.3.3",
                        "name": "Análise de Correlações e Padrões",
                        "description": "Identificação de relações entre variáveis e detecção de anomalias através de métricas de associação e inspeção visual.",
                        "specificSkills": [
                          {
                            "id": "10.1.4.3.3.1",
                            "name": "Calcular matriz de correlação",
                            "description": "Computar coeficientes de Pearson, Spearman e Kendall para medir associações lineares e não-lineares entre variáveis.",
                            "atomicExpansion": {
                              "steps": [
                                {
                                  "stepNumber": 1,
                                  "title": "Preparar o ambiente de programação e carregar o dataset",
                                  "subSteps": [
                                    "Instale as bibliotecas necessárias: pandas, numpy, scipy e seaborn via pip.",
                                    "Importe as bibliotecas no Jupyter Notebook ou script Python.",
                                    "Carregue um dataset relevante, como o Iris do seaborn, com variáveis numéricas.",
                                    "Selecione apenas colunas numéricas e verifique valores ausentes com df.isnull().sum().",
                                    "Trate valores ausentes preenchendo com a média ou removendo linhas se necessário."
                                  ],
                                  "verification": "Dataset carregado corretamente sem NaNs e pronto para análise (use df.head() e df.describe()).",
                                  "estimatedTime": "15-20 minutos",
                                  "materials": [
                                    "Python 3.x",
                                    "Jupyter Notebook",
                                    "Bibliotecas: pandas, numpy, scipy, seaborn"
                                  ],
                                  "tips": "Sempre use datasets com pelo menos 3-5 variáveis numéricas para matrizes interessantes.",
                                  "learningObjective": "Configurar ambiente computacional e preparar dados limpos para cálculos de correlação.",
                                  "commonMistakes": [
                                    "Esquecer de importar bibliotecas",
                                    "Ignorar valores ausentes",
                                    "Usar dados categóricos sem codificação"
                                  ]
                                },
                                {
                                  "stepNumber": 2,
                                  "title": "Calcular a matriz de correlação de Pearson",
                                  "subSteps": [
                                    "Use df.corr(method='pearson') para gerar a matriz completa.",
                                    "Armazene o resultado em uma variável como pearson_corr = df.corr(method='pearson').",
                                    "Arredonde os valores para 3 casas decimais com .round(3).",
                                    "Identifique os pares de variáveis com correlação mais forte (|r| > 0.7).",
                                    "Salve a matriz em um arquivo CSV para referência."
                                  ],
                                  "verification": "Matriz Pearson gerada com valores entre -1 e 1, diagonal igual a 1.0 (use print(pearson_corr)).",
                                  "estimatedTime": "10-15 minutos",
                                  "materials": [
                                    "Código Python preparado no Step 1"
                                  ],
                                  "tips": "Pearson mede associações lineares; valores próximos a ±1 indicam forte relação linear.",
                                  "learningObjective": "Implementar e interpretar coeficientes de correlação linear de Pearson.",
                                  "commonMistakes": [
                                    "Confundir com outros métodos",
                                    "Não verificar simetria da matriz",
                                    "Interpretar causalidade como correlação"
                                  ]
                                },
                                {
                                  "stepNumber": 3,
                                  "title": "Calcular matrizes de correlação de Spearman e Kendall",
                                  "subSteps": [
                                    "Calcule Spearman com df.corr(method='spearman') e armazene em spearman_corr.",
                                    "Calcule Kendall com df.corr(method='kendall') e armazene em kendall_corr.",
                                    "Compare as três matrizes destacando diferenças em pares específicos.",
                                    "Arredonde e salve cada matriz em CSVs separados.",
                                    "Note quando Spearman/Kendall diferem de Pearson (indica não-linearidade)."
                                  ],
                                  "verification": "Matrizes Spearman e Kendall geradas corretamente, com valores consistentes (print das matrizes).",
                                  "estimatedTime": "15-20 minutos",
                                  "materials": [
                                    "Código Python do Step 2"
                                  ],
                                  "tips": "Spearman é rank-based (não-linear), Kendall é para ordinal; use para dados não-normais.",
                                  "learningObjective": "Diferenciar e computar medidas de correlação não-paramétricas.",
                                  "commonMistakes": [
                                    "Usar Pearson em dados não-lineares",
                                    "Esquecer de especificar method",
                                    "Ignorar diferenças entre métodos"
                                  ]
                                },
                                {
                                  "stepNumber": 4,
                                  "title": "Visualizar e interpretar a matriz de correlação",
                                  "subSteps": [
                                    "Use sns.heatmap(pearson_corr, annot=True, cmap='coolwarm') para visualizar Pearson.",
                                    "Crie heatmaps semelhantes para Spearman e Kendall.",
                                    "Interprete: destaque correlações fortes (>0.7 ou <-0.7) e padrões.",
                                    "Gere um relatório resumindo insights (ex: 'Sepal length e petal length fortemente correlacionados').",
                                    "Salve os plots como PNG para documentação."
                                  ],
                                  "verification": "Heatmaps gerados mostrando valores corretos e interpretações documentadas.",
                                  "estimatedTime": "20-25 minutos",
                                  "materials": [
                                    "Biblioteca seaborn e matplotlib"
                                  ],
                                  "tips": "Use cmap='coolwarm' para vermelho (positiva) e azul (negativa); annot=True para valores.",
                                  "learningObjective": "Visualizar matrizes e extrair insights acionáveis de correlações.",
                                  "commonMistakes": [
                                    "Má escala no heatmap",
                                    "Interpretar magnitude sem contexto",
                                    "Não salvar visualizações"
                                  ]
                                }
                              ],
                              "practicalExample": "No dataset Iris (seaborn.load_dataset('iris')), calcule correlações entre sepal_length, sepal_width, petal_length e petal_width. Espere Pearson ~0.87 entre petal_length e petal_width (forte linear), mas verifique Spearman para confirmar não-linearidades em espécies.",
                              "finalVerifications": [
                                "Três matrizes (Pearson, Spearman, Kendall) geradas e salvas corretamente.",
                                "Heatmaps visualizados com anotações precisas.",
                                "Relatório de interpretação identifica pelo menos 3 pares de correlações fortes.",
                                "Nenhum erro de NaN ou dados inválidos nas matrizes.",
                                "Comparação entre métodos destaca diferenças (ex: não-linearidades).",
                                "Arquivos CSV e PNG exportados."
                              ],
                              "assessmentCriteria": [
                                "Precisão nos cálculos (valores coincidem com funções built-in).",
                                "Qualidade da preparação de dados (sem NaNs, variáveis numéricas).",
                                "Correta distinção entre Pearson (linear), Spearman/Kendall (não-linear).",
                                "Visualizações claras e interpretáveis.",
                                "Insights derivados corretamente (evitando causalidade falsa).",
                                "Eficiência do código (uso de métodos pandas eficientes)."
                              ],
                              "crossCurricularConnections": [
                                "Estatística: Testes de hipótese para significância de correlações.",
                                "Programação: Manipulação de dados com pandas e visualização com seaborn.",
                                "Machine Learning: Seleção de features baseada em correlações baixas.",
                                "Economia/Finanças: Análise de risco em portfólios de investimentos."
                              ],
                              "realWorldApplication": "Em finanças, calcular matriz de correlação entre retornos de ações para diversificar portfólios (evitar ativos altamente correlacionados); em saúde, identificar associações entre variáveis biomédicas para pesquisas epidemiológicas."
                            },
                            "estimatedTime": "2 horas",
                            "difficulty": "intermediate",
                            "status": "not_started",
                            "prerequisites": []
                          },
                          {
                            "id": "10.1.4.3.3.2",
                            "name": "Detectar outliers e anomalias",
                            "description": "Aplicar métodos como Z-score, IQR e visualizações para identificar e analisar valores atípicos nos dados.",
                            "atomicExpansion": {
                              "steps": [
                                {
                                  "stepNumber": 1,
                                  "title": "Compreender Conceitos de Outliers e Preparar Dados",
                                  "subSteps": [
                                    "Defina outlier como um valor significativamente diferente do resto dos dados.",
                                    "Discuta impactos de outliers: distorcem médias, afetam modelos preditivos.",
                                    "Carregue um dataset de exemplo (ex: salários ou temperaturas) usando pandas.",
                                    "Realize limpeza básica: remova valores nulos e codifique variáveis categóricas.",
                                    "Selecione colunas numéricas relevantes para análise."
                                  ],
                                  "verification": "Dataset carregado e limpo, com resumo estatístico (describe()) exibido sem erros.",
                                  "estimatedTime": "20 minutos",
                                  "materials": [
                                    "Python com pandas e numpy instalados",
                                    "Dataset CSV de exemplo (ex: salaries.csv)"
                                  ],
                                  "tips": "Sempre verifique o shape e info() do dataset antes de prosseguir.",
                                  "learningObjective": "Entender o conceito de outliers e preparar dados para detecção.",
                                  "commonMistakes": [
                                    "Ignorar valores nulos levando a cálculos errados",
                                    "Não selecionar colunas apropriadas"
                                  ]
                                },
                                {
                                  "stepNumber": 2,
                                  "title": "Aplicar Método Z-Score para Detecção",
                                  "subSteps": [
                                    "Calcule Z-score: (x - mean) / std para cada valor na coluna.",
                                    "Defina threshold comum: |Z-score| > 3 como outlier.",
                                    "Identifique índices dos outliers usando np.abs(z_scores) > 3.",
                                    "Crie uma coluna booleana 'is_outlier_z' no DataFrame.",
                                    "Conte e liste os outliers detectados."
                                  ],
                                  "verification": "Coluna 'is_outlier_z' criada corretamente, com pelo menos 1 outlier identificado em dataset de teste.",
                                  "estimatedTime": "25 minutos",
                                  "materials": [
                                    "Bibliotecas: pandas, numpy, scipy.stats"
                                  ],
                                  "tips": "Use zscore de scipy.stats para precisão; normalize apenas colunas numéricas.",
                                  "learningObjective": "Calcular e interpretar Z-scores para identificar outliers estatísticos.",
                                  "commonMistakes": [
                                    "Usar threshold inadequado (ex: 2 em vez de 3)",
                                    "Aplicar em dados não normais sem transformação"
                                  ]
                                },
                                {
                                  "stepNumber": 3,
                                  "title": "Aplicar Método IQR para Detecção Robusta",
                                  "subSteps": [
                                    "Calcule Q1 (25%), Q3 (75%) e IQR = Q3 - Q1 para a coluna.",
                                    "Defina limites: lower = Q1 - 1.5*IQR, upper = Q3 + 1.5*IQR.",
                                    "Identifique outliers: valores < lower ou > upper.",
                                    "Crie coluna 'is_outlier_iqr' e visualize com boxplot.",
                                    "Compare número de outliers com Z-score."
                                  ],
                                  "verification": "Boxplot gerado mostrando outliers, coluna 'is_outlier_iqr' com detecções corretas.",
                                  "estimatedTime": "25 minutos",
                                  "materials": [
                                    "pandas, matplotlib ou seaborn para boxplot"
                                  ],
                                  "tips": "IQR é mais robusto a distribuições assimétricas que Z-score.",
                                  "learningObjective": "Usar método não-paramétrico IQR para detecção de outliers.",
                                  "commonMistakes": [
                                    "Calcular IQR errado (ex: inverter Q1/Q3)",
                                    "Ignorar múltiplos boxplots para comparação"
                                  ]
                                },
                                {
                                  "stepNumber": 4,
                                  "title": "Visualizar e Analisar Outliers Combinados",
                                  "subSteps": [
                                    "Crie scatter plots coloridos por método (Z-score e IQR).",
                                    "Gere histogramas com linhas de limite IQR/Z-score.",
                                    "Analise outliers: verifique contexto (ex: erros de digitação vs. reais).",
                                    "Decida ações: remover, investigar ou manter com flag.",
                                    "Gere relatório resumindo achados (tabela de outliers)."
                                  ],
                                  "verification": "Gráficos gerados e relatório com pelo menos 3 insights sobre outliers.",
                                  "estimatedTime": "30 minutos",
                                  "materials": [
                                    "matplotlib, seaborn para visualizações avançadas"
                                  ],
                                  "tips": "Use pairplots para múltiplas variáveis; combine métodos para robustez.",
                                  "learningObjective": "Integrar métodos e visualizações para análise completa de anomalias.",
                                  "commonMistakes": [
                                    "Visualizações sem labels/legends",
                                    "Remover outliers sem investigação"
                                  ]
                                }
                              ],
                              "practicalExample": "Em um dataset de transações bancárias, detecte outliers em 'valor_transacao' usando Z-score (flaga transferências >3 desvios) e IQR (boxplot revela fraudes acima de limite superior), visualizando scatter plot de valor vs. hora para confirmar padrões anômalos.",
                              "finalVerifications": [
                                "Todos os métodos (Z-score, IQR) aplicados corretamente em dataset.",
                                "Visualizações geradas e interpretadas com outliers destacados.",
                                "Relatório lista pelo menos 5 outliers com justificativa.",
                                "Comparação entre métodos mostra concordância >70%.",
                                "Ações propostas para cada outlier (remover/investigar).",
                                "Código executável sem erros em ambiente Jupyter."
                              ],
                              "assessmentCriteria": [
                                "Precisão na identificação: >90% de outliers reais detectados.",
                                "Justificativa clara para thresholds e decisões.",
                                "Qualidade das visualizações: legíveis, informativas.",
                                "Integração de múltiplos métodos com análise comparativa.",
                                "Eficiência do código: limpo, comentado e reutilizável.",
                                "Profundidade da interpretação: discute impactos nos dados."
                              ],
                              "crossCurricularConnections": [
                                "Estatística: Distribuições normais e quartis.",
                                "Programação: Manipulação de dados com Python/pandas.",
                                "Visualização de Dados: Gráficos exploratórios com matplotlib/seaborn.",
                                "Ciência de Dados: Pré-processamento para ML.",
                                "Matemática: Medidas de tendência central e dispersão."
                              ],
                              "realWorldApplication": "Na detecção de fraudes financeiras, manufatura (defeitos em sensores) ou saúde (anomalias em leituras vitais), onde outliers indicam eventos raros como erros de sensor ou comportamentos fraudulentos, melhorando modelos preditivos e decisões baseadas em dados."
                            },
                            "estimatedTime": "3 horas",
                            "difficulty": "intermediate",
                            "status": "not_started",
                            "prerequisites": []
                          },
                          {
                            "id": "10.1.4.3.3.3",
                            "name": "Identificar padrões multivariados",
                            "description": "Explorar relações entre múltiplas variáveis usando pairplots e análise de componentes principais básica.",
                            "atomicExpansion": {
                              "steps": [
                                {
                                  "stepNumber": 1,
                                  "title": "Preparar o dataset para análise multivariada",
                                  "subSteps": [
                                    "Carregue um dataset com múltiplas variáveis numéricas usando pandas (ex: Iris dataset).",
                                    "Realize limpeza básica: remova valores ausentes e verifique tipos de dados.",
                                    "Selecione apenas colunas numéricas relevantes para análise.",
                                    "Normalize ou padronize as variáveis se necessário para PCA.",
                                    "Divida os dados em features (X) e labels (y) se aplicável."
                                  ],
                                  "verification": "Execute df.info() e df.describe() para confirmar que os dados estão limpos e prontos, sem NaNs.",
                                  "estimatedTime": "20 minutos",
                                  "materials": "Python com pandas, seaborn, matplotlib; dataset Iris do sklearn.",
                                  "tips": "Sempre visualize os primeiros registros com df.head() para sanity check.",
                                  "learningObjective": "Entender a importância da preparação de dados para análises multivariadas.",
                                  "commonMistakes": "Ignorar escalas diferentes entre variáveis, levando a distorções em visualizações."
                                },
                                {
                                  "stepNumber": 2,
                                  "title": "Gerar pairplots para visualização de relações pareadas",
                                  "subSteps": [
                                    "Importe seaborn e configure o estilo.",
                                    "Crie um pairplot básico com sns.pairplot(df).",
                                    "Adicione hue para colorir por uma variável categórica (ex: espécie).",
                                    "Salve a figura como PNG para inspeção.",
                                    "Ajuste tamanho e layout para melhor legibilidade."
                                  ],
                                  "verification": "O pairplot deve mostrar matriz de scatterplots com histogramas na diagonal, sem erros de plotting.",
                                  "estimatedTime": "15 minutos",
                                  "materials": "Bibliotecas: seaborn, matplotlib; Jupyter Notebook.",
                                  "tips": "Use kind='kde' para densidade em vez de histograma se dados forem contínuos.",
                                  "learningObjective": "Visualizar relações bivariadas em múltiplas dimensões simultaneamente.",
                                  "commonMistakes": "Esquecer de especificar hue, perdendo insights de agrupamentos."
                                },
                                {
                                  "stepNumber": 3,
                                  "title": "Interpretar padrões nos pairplots",
                                  "subSteps": [
                                    "Identifique correlações lineares fortes (padrões lineares em scatterplots).",
                                    "Observe clusters ou separações por grupos (hue).",
                                    "Note distribuições marginais nos histogramas/diagonais.",
                                    "Anote padrões não-lineares ou outliers visíveis.",
                                    "Registre observações em um markdown ou dict para relatório."
                                  ],
                                  "verification": "Crie uma lista de 5+ observações chave baseadas no pairplot.",
                                  "estimatedTime": "25 minutos",
                                  "materials": "Pairplot gerado; papel ou notebook para anotações.",
                                  "tips": "Procure por 'ilhas' de pontos para detectar multicolinearidade.",
                                  "learningObjective": "Desenvolver habilidades de interpretação visual de dados multivariados.",
                                  "commonMistakes": "Focar apenas em pares adjacentes, ignorando a matriz completa."
                                },
                                {
                                  "stepNumber": 4,
                                  "title": "Aplicar Análise de Componentes Principais (PCA) básica",
                                  "subSteps": [
                                    "Importe sklearn.decomposition.PCA.",
                                    "Crie instância PCA(n_components=2) e ajuste com X escalado.",
                                    "Transforme os dados para o novo espaço.",
                                    "Crie scatterplot dos componentes principais com hue.",
                                    "Calcule variância explicada com pca.explained_variance_ratio_."
                                  ],
                                  "verification": "Scatterplot de PC1 vs PC2 mostra clusters claros e print da variância explicada >70%.",
                                  "estimatedTime": "20 minutos",
                                  "materials": "sklearn; StandardScaler; dados preparados.",
                                  "tips": "Sempre escale dados antes de PCA com StandardScaler.",
                                  "learningObjective": "Reduzir dimensionalidade preservando variância para padrões multivariados.",
                                  "commonMistakes": "Aplicar PCA sem escalar, dominado por variáveis de maior escala."
                                },
                                {
                                  "stepNumber": 5,
                                  "title": "Interpretar resultados da PCA e sintetizar padrões",
                                  "subSteps": [
                                    "Analise o scatterplot de componentes: identifique clusters e separações.",
                                    "Interprete loadings (pca.components_) para entender contribuições de variáveis.",
                                    "Compare com pairplot: valide padrões observados.",
                                    "Calcule score de silhouette para qualidade de clusters opcionais.",
                                    "Gere relatório final resumindo padrões multivariados identificados."
                                  ],
                                  "verification": "Relatório escrito com 3+ padrões descritos e justificados por evidências visuais.",
                                  "estimatedTime": "30 minutos",
                                  "materials": "Resultados de PCA e pairplot; ferramenta de escrita.",
                                  "tips": "Visualize loadings em barplot para clareza.",
                                  "learningObjective": "Integrar PCA e pairplots para insights holísticos multivariados.",
                                  "commonMistakes": "Interpretar componentes sem considerar loadings originais."
                                }
                              ],
                              "practicalExample": "Usando o dataset Iris (sepal_length, sepal_width, petal_length, petal_width, species), gere pairplot para ver separações claras entre espécies nos pares petal_length vs petal_width. Aplique PCA para projetar em 2D, confirmando ~95% variância explicada e clusters distintos.",
                              "finalVerifications": [
                                "Pairplot gerado corretamente com múltiplas relações visíveis.",
                                "PCA aplicada com variância explicada calculada e plotada.",
                                "Relatório de interpretação identifica pelo menos 3 padrões multivariados.",
                                "Código executável sem erros e reproduzível.",
                                "Insights conectam pairplot e PCA logicamente.",
                                "Outliers ou anomalias notados se presentes."
                              ],
                              "assessmentCriteria": [
                                "Precisão na preparação e visualização de dados (30%).",
                                "Qualidade da interpretação de padrões (25%).",
                                "Correta aplicação e interpretação de PCA (20%).",
                                "Clareza do relatório e exemplos práticos (15%).",
                                "Eficiência do código e uso de boas práticas (10%)."
                              ],
                              "crossCurricularConnections": [
                                "Estatística: Correlação e variância.",
                                "Programação: Manipulação de dados com Python/pandas.",
                                "Visualização: Gráficos com seaborn/matplotlib.",
                                "Matemática: Álgebra linear básica (vetores, matrizes)."
                              ],
                              "realWorldApplication": "Em biologia, identificar subespécies em dados genéticos; em marketing, segmentar clientes por padrões de comportamento multivariados para campanhas direcionadas."
                            },
                            "estimatedTime": "2 horas",
                            "difficulty": "advanced",
                            "status": "not_started",
                            "prerequisites": []
                          }
                        ]
                      }
                    ]
                  },
                  {
                    "id": "10.1.4.4",
                    "name": "Visualização de Dados",
                    "description": "Ferramentas e técnicas para representar dados graficamente e identificar padrões.",
                    "individualConcepts": [
                      {
                        "id": "10.1.4.4.1",
                        "name": "Princípios Fundamentais da Visualização de Dados",
                        "description": "Conceitos básicos que orientam a criação de representações gráficas claras, precisas e eficazes para comunicar insights dos dados.",
                        "specificSkills": [
                          {
                            "id": "10.1.4.4.1.1",
                            "name": "Identificar a importância da visualização na análise exploratória",
                            "description": "Compreender como a visualização auxilia na detecção inicial de padrões, anomalias e relações nos dados durante a etapa de análise exploratória da Ciência de Dados.",
                            "atomicExpansion": {
                              "steps": [
                                {
                                  "stepNumber": 1,
                                  "title": "Compreender os Conceitos Básicos de Análise Exploratória de Dados (EDA)",
                                  "subSteps": [
                                    "Defina EDA como a fase inicial de investigação de datasets para resumir características principais.",
                                    "Explique que EDA usa estatísticas descritivas e visualizações para insights iniciais.",
                                    "Diferencie EDA de modelagem preditiva, focando na descoberta sem hipóteses prévias.",
                                    "Liste objetivos da EDA: detectar padrões, anomalias e relações.",
                                    "Discuta limitações de análises puramente numéricas sem visualizações."
                                  ],
                                  "verification": "Resuma em 3 frases o que é EDA e seu papel inicial na Ciência de Dados.",
                                  "estimatedTime": "20 minutos",
                                  "materials": [
                                    "Notebook ou papel para anotações",
                                    "Artigo introdutório sobre EDA (ex: Towards Data Science)"
                                  ],
                                  "tips": "Use analogias cotidianas, como 'explorar uma nova cidade com mapa vs. só estatísticas'.",
                                  "learningObjective": "Entender EDA como etapa fundamental e introduzir visualização como ferramenta chave.",
                                  "commonMistakes": [
                                    "Confundir EDA com limpeza de dados",
                                    "Ignorar o foco exploratório sem hipóteses"
                                  ]
                                },
                                {
                                  "stepNumber": 2,
                                  "title": "Identificar Benefícios da Visualização na Detecção de Padrões",
                                  "subSteps": [
                                    "Aprenda que gráficos revelam tendências não óbvias em tabelas numéricas.",
                                    "Estude histogramas para distribuição de dados e detecção de multimodalidade.",
                                    "Analise boxplots para identificar padrões de dispersão e valores centrais.",
                                    "Compare visualizações com resumos estatísticos para destacar superioridade visual.",
                                    "Pratique descrevendo padrões em um histograma simples fornecido."
                                  ],
                                  "verification": "Descreva um padrão detectado em um histograma exemplo e por quê visualização ajudou.",
                                  "estimatedTime": "25 minutos",
                                  "materials": [
                                    "Ferramenta de visualização como Google Sheets ou Python Jupyter com Matplotlib",
                                    "Dataset exemplo: Iris dataset"
                                  ],
                                  "tips": "Foque em 'olhar' para o gráfico como um detetive procurando pistas.",
                                  "learningObjective": "Reconhecer como visualizações tornam padrões intuitivos e rápidos de identificar.",
                                  "commonMistakes": [
                                    "Interpretar apenas médias sem contexto visual",
                                    "Sobrecarregar gráficos com dados excessivos"
                                  ]
                                },
                                {
                                  "stepNumber": 3,
                                  "title": "Explorar Detecção de Anomalias e Relações via Visualização",
                                  "subSteps": [
                                    "Defina anomalias como outliers que desviam do padrão esperado.",
                                    "Use scatter plots para visualizar anomalias em relações bivariadas.",
                                    "Estude heatmaps para correlações e relações multivariadas.",
                                    "Identifique relações causais potenciais vs. correlações espúrias via gráficos.",
                                    "Crie um scatter plot simples e anote anomalias e relações observadas."
                                  ],
                                  "verification": "Apresente um scatter plot com 2 anomalias marcadas e 1 relação descrita.",
                                  "estimatedTime": "30 minutos",
                                  "materials": [
                                    "Python com Seaborn/Matplotlib",
                                    "Dataset exemplo: Boston Housing"
                                  ],
                                  "tips": "Zoom em regiões suspeitas para confirmar anomalias.",
                                  "learningObjective": "Dominar uso de visualizações para spotting de anomalias e relações ocultas.",
                                  "commonMistakes": [
                                    "Assumir causalidade de correlação visual",
                                    "Ignorar escala nos eixos"
                                  ]
                                },
                                {
                                  "stepNumber": 4,
                                  "title": "Sintetizar a Importância Geral da Visualização na EDA",
                                  "subSteps": [
                                    "Compile benefícios: acelera insights, intuitivo, revela o 'não dito' por números.",
                                    "Compare workflows com e sem visualização (tempo e qualidade).",
                                    "Discuta ferramentas comuns: Matplotlib, Seaborn, Tableau.",
                                    "Reflita sobre casos onde visualização evitou erros em análises.",
                                    "Escreva um parágrafo justificando visualização como essencial na EDA."
                                  ],
                                  "verification": "Escreva uma justificativa de 100 palavras sobre a importância da visualização na EDA.",
                                  "estimatedTime": "15 minutos",
                                  "materials": [
                                    "Documentos de anotações dos steps anteriores"
                                  ],
                                  "tips": "Pense em 'visualização como lente de aumento para dados'.",
                                  "learningObjective": "Articular convincentemente o valor crítico da visualização na análise exploratória.",
                                  "commonMistakes": [
                                    "Subestimar impacto em decisões reais",
                                    "Focar só em estética, não em insights"
                                  ]
                                }
                              ],
                              "practicalExample": "No dataset Titanic, um histograma de idade revela bimodalidade (crianças e adultos), um boxplot de fare por classe detecta anomalias de passagens caras, e scatter plot de idade vs. sobrevivência mostra relações iniciais por gênero e classe, guiando hipóteses sem análises numéricas complexas.",
                              "finalVerifications": [
                                "Explica 3 benefícios principais da visualização na EDA.",
                                "Identifica padrões, anomalias e relações em um gráfico fornecido.",
                                "Compara EDA com vs. sem visualização em termos de eficiência.",
                                "Cita exemplos reais de insights visuais em datasets conhecidos.",
                                "Justifica por quê visualização precede modelagem em fluxos de dados.",
                                "Lista 2 ferramentas de visualização e seus usos na EDA."
                              ],
                              "assessmentCriteria": [
                                "Clareza na explicação de conceitos EDA e visualização (20%)",
                                "Precisão na identificação de padrões/anomalias/relações (25%)",
                                "Uso de exemplos concretos e relevantes (20%)",
                                "Profundidade nos benefícios e limitações (20%)",
                                "Capacidade de síntese em justificativa escrita (15%)"
                              ],
                              "crossCurricularConnections": [
                                "Estatística: Integra descritivas com gráficos para inferência visual.",
                                "Programação: Uso de Python/R para gerar visualizações interativas.",
                                "Design Gráfico: Princípios de percepção visual (cor, layout).",
                                "Negócios: Aplicação em dashboards para tomada de decisão.",
                                "Psicologia Cognitiva: Como humanos processam informações visuais melhor."
                              ],
                              "realWorldApplication": "Em detecção de fraudes bancárias, visualizações de transações por localização/tempo revelam anomalias como gastos atípicos, permitindo ação rápida antes de perdas financeiras, comum em empresas como bancos e e-commerces."
                            },
                            "estimatedTime": "1 hora",
                            "difficulty": "beginner",
                            "status": "not_started",
                            "prerequisites": []
                          },
                          {
                            "id": "10.1.4.4.1.2",
                            "name": "Aplicar princípios de boas práticas em visualizações",
                            "description": "Utilizar regras como simplicidade, escolha correta de escalas, evitar distorções e garantir acessibilidade para criar gráficos informativos e não enganosos.",
                            "atomicExpansion": {
                              "steps": [
                                {
                                  "stepNumber": 1,
                                  "title": "Compreender os Princípios Fundamentais de Boas Práticas em Visualizações",
                                  "subSteps": [
                                    "Estude a regra da simplicidade: remover elementos desnecessários como grades excessivas ou legendas redundantes.",
                                    "Analise escolha correta de escalas: usar escalas lineares ou logarítmicas apropriadas ao dados.",
                                    "Identifique como evitar distorções: iniciar eixo Y em zero quando possível e evitar truncamentos enganosos.",
                                    "Revise acessibilidade: usar cores contrastantes, alt-text e tamanhos de fonte legíveis.",
                                    "Liste os 4 princípios principais em um resumo pessoal."
                                  ],
                                  "verification": "Criar um mindmap ou tabela resumindo os 4 princípios com exemplos breves.",
                                  "estimatedTime": "45 minutos",
                                  "materials": "Documentação de Tufte ou 'The Visual Display of Quantitative Information' (PDF online), caderno ou ferramenta de notas como Notion.",
                                  "tips": "Leia exemplos reais de gráficos ruins vs. bons em sites como FlowingData.",
                                  "learningObjective": "Memorizar e explicar os princípios chave de simplicidade, escalas, distorções e acessibilidade.",
                                  "commonMistakes": "Confundir acessibilidade com estética; ignorar contexto dos dados ao escolher escalas."
                                },
                                {
                                  "stepNumber": 2,
                                  "title": "Analisar Visualizações Existentes para Identificar Violações",
                                  "subSteps": [
                                    "Selecione 3 gráficos ruins de fontes como jornais ou relatórios online.",
                                    "Aplique checklist: verifique simplicidade (elementos extras?), escalas (distorções?), acessibilidade (cores para daltônicos?).",
                                    "Refaça um gráfico ruim manualmente corrigindo os erros identificados.",
                                    "Compare antes/depois e anote melhorias.",
                                    "Discuta com um par ou em fórum online as correções feitas."
                                  ],
                                  "verification": "Produzir relatório com screenshots 'antes/depois' e anotações de 3 análises.",
                                  "estimatedTime": "1 hora",
                                  "materials": "Imagens de gráficos ruins (buscar 'bad data visualizations'), editor de imagens como Canva ou PowerPoint.",
                                  "tips": "Use a ferramenta ColorBrewer para testar paletas de cores acessíveis.",
                                  "learningObjective": "Desenvolver olho crítico para detectar violações de boas práticas.",
                                  "commonMistakes": "Focar só em estética ignorando distorções numéricas; não testar acessibilidade em simuladores de daltonismo."
                                },
                                {
                                  "stepNumber": 3,
                                  "title": "Criar uma Visualização Simples Aplicando os Princípios",
                                  "subSteps": [
                                    "Carregue um dataset simples (ex: vendas mensais).",
                                    "Escolha tipo de gráfico apropriado (bar/line) com escalas corretas.",
                                    "Aplique simplicidade: remova grids, use labels claros.",
                                    "Evite distorções: eixo Y de 0, proporções honestas.",
                                    "Garanta acessibilidade: paleta de cores segura, fontes >12pt."
                                  ],
                                  "verification": "Gerar o gráfico e autoavaliar com checklist dos princípios.",
                                  "estimatedTime": "1 hora 30 minutos",
                                  "materials": "Python com Matplotlib/Seaborn, dataset CSV de exemplo (ex: do Kaggle 'sales data').",
                                  "tips": "Comece com seaborn para temas prontos acessíveis como 'whitegrid=False'.",
                                  "learningObjective": "Construir uma visualização do zero respeitando todos os princípios.",
                                  "commonMistakes": "Sobrecarregar com múltiplos gráficos; usar 3D desnecessariamente."
                                },
                                {
                                  "stepNumber": 4,
                                  "title": "Refinar e Validar a Visualização para Acessibilidade e Impacto",
                                  "subSteps": [
                                    "Teste acessibilidade: use WAVE tool ou Lighthouse para alt-text e contraste.",
                                    "Colete feedback: mostre a 2-3 pessoas e pergunte sobre clareza.",
                                    "Refine baseado em feedback: ajuste escalas ou simplicidade.",
                                    "Adicione título descritivo e fonte dos dados.",
                                    "Exporte em formatos acessíveis (PNG + SVG)."
                                  ],
                                  "verification": "Relatório final com versão refinada, feedback recebido e métricas de acessibilidade (ex: score WCAG).",
                                  "estimatedTime": "45 minutos",
                                  "materials": "Ferramentas de acessibilidade (Google Lighthouse, Color Oracle), Google Forms para feedback.",
                                  "tips": "Sempre inclua 'O que o gráfico comunica em 5 segundos?' no feedback.",
                                  "learningObjective": "Iterar visualizações para máxima clareza e inclusão.",
                                  "commonMistakes": "Ignorar feedback subjetivo; esquecer alt-text em exports digitais."
                                }
                              ],
                              "practicalExample": "Visualize vendas mensais de uma loja (Jan:100, Fev:120, Mar:90). Crie gráfico de barras com eixo Y de 0-150, cores azul/verde acessíveis, sem grid, título 'Vendas Mensais 2023', evitando distorção que inflaria Fev.",
                              "finalVerifications": [
                                "Gráfico transmite mensagem clara em <5 segundos sem leitura de números.",
                                "Eixos começam em zero ou justificativa explícita para não.",
                                "Contraste de cores passa AA WCAG (teste com tool).",
                                "Removeu pelo menos 50% dos elementos desnecessários iniciais.",
                                "Funciona em leitores de tela (alt-text descritivo).",
                                "Nenhum feedback indica distorção ou confusão."
                              ],
                              "assessmentCriteria": [
                                "Simplicidade: ausência de clutter (pontos 1-5).",
                                "Precisão de escalas: sem distorções mensuráveis (pontos 1-5).",
                                "Acessibilidade: conformidade WCAG básica (sim/não).",
                                "Clareza da mensagem principal (efetiva/inefetiva).",
                                "Uso apropriado de tipo de gráfico (correto/incorreto).",
                                "Impacto visual honesto e informativo (alta/média/baixa)."
                              ],
                              "crossCurricularConnections": [
                                "Estatística: interpretação correta de dados subjacentes.",
                                "Design Gráfico: princípios de UI/UX como hierarquia visual.",
                                "Comunicação: retórica visual para persuasão ética.",
                                "Ética: evitar manipulação de dados em jornalismo.",
                                "Programação: bibliotecas de viz como D3.js ou ggplot."
                              ],
                              "realWorldApplication": "Em relatórios empresariais para dashboards no Tableau/PowerBI, jornalismo de dados no The New York Times Graphics, ou relatórios científicos para comunicar achados sem bias, garantindo decisões baseadas em fatos precisos."
                            },
                            "estimatedTime": "1.5 horas",
                            "difficulty": "beginner",
                            "status": "not_started",
                            "prerequisites": []
                          },
                          {
                            "id": "10.1.4.4.1.3",
                            "name": "Diferenciar dados, informação e conhecimento via visualização",
                            "description": "Explicar como gráficos transformam dados brutos em informação visual e, subsequentemente, em conhecimento acionável, alinhado às definições da Ciência de Dados.",
                            "atomicExpansion": {
                              "steps": [
                                {
                                  "stepNumber": 1,
                                  "title": "Entender as Definições Fundamentais de Dados, Informação e Conhecimento",
                                  "subSteps": [
                                    "Ler definições padrão: Dados são fatos brutos sem contexto (ex: números isolados).",
                                    "Identificar informação como dados processados com contexto (ex: médias ou totais).",
                                    "Definir conhecimento como informação interpretada para decisões acionáveis.",
                                    "Comparar exemplos: 100, 200, 150 (dados) vs. gráfico de tendência crescente (conhecimento).",
                                    "Anotar diferenças em um diagrama mental."
                                  ],
                                  "verification": "Criar um fluxograma simples mostrando a progressão dados → informação → conhecimento e autoavaliar se está claro.",
                                  "estimatedTime": "20 minutos",
                                  "materials": [
                                    "Papel e caneta ou ferramenta digital como Draw.io",
                                    "Artigo introdutório sobre pirâmide DIKW"
                                  ],
                                  "tips": "Use analogias cotidianas, como ingredientes (dados), receita (informação) e prato cozido (conhecimento).",
                                  "learningObjective": "Dominar as distinções conceituais entre dados, informação e conhecimento na Ciência de Dados.",
                                  "commonMistakes": [
                                    "Confundir informação com dados organizados sem contexto",
                                    "Ignorar o aspecto acionável do conhecimento"
                                  ]
                                },
                                {
                                  "stepNumber": 2,
                                  "title": "Explorar Dados Brutos e Sua Transformação Inicial em Informação via Visualização",
                                  "subSteps": [
                                    "Coletar um conjunto de dados brutos simples (ex: temperaturas diárias: 20, 22, 19, 25).",
                                    "Criar uma tabela organizada adicionando contexto (datas, médias).",
                                    "Gerar um gráfico de linha básico usando Excel ou Google Sheets.",
                                    "Observar como o gráfico revela padrões (ex: tendência de aquecimento) que a tabela não mostra tão claramente.",
                                    "Documentar as diferenças visuais entre tabela e gráfico."
                                  ],
                                  "verification": "Comparar a tabela e o gráfico lado a lado e listar 3 insights visuais exclusivos do gráfico.",
                                  "estimatedTime": "30 minutos",
                                  "materials": [
                                    "Planilha Excel/Google Sheets",
                                    "Conjunto de dados de exemplo (temperaturas ou vendas)"
                                  ],
                                  "tips": "Escolha escalas adequadas no gráfico para evitar distorções visuais.",
                                  "learningObjective": "Compreender como a visualização inicial converte dados brutos em informação perceptível.",
                                  "commonMistakes": [
                                    "Usar gráficos inadequados (ex: pizza para tendências temporais)",
                                    "Sobrecarregar o gráfico com muitos dados"
                                  ]
                                },
                                {
                                  "stepNumber": 3,
                                  "title": "Analisar a Evolução de Informação Visual para Conhecimento Acionável",
                                  "subSteps": [
                                    "Interpretar o gráfico do Step 2: Identificar padrões, anomalias e implicações.",
                                    "Adicionar camadas à visualização (ex: linha de tendência, anotações).",
                                    "Formular perguntas acionáveis: 'O que isso implica para decisões futuras?'",
                                    "Simular cenários: Como esse conhecimento mudaria uma ação (ex: preparar para calor).",
                                    "Escrever um parágrafo resumindo o conhecimento extraído."
                                  ],
                                  "verification": "Responder a 3 perguntas: O que os dados dizem? O que a visualização informa? Que conhecimento gera?",
                                  "estimatedTime": "25 minutos",
                                  "materials": [
                                    "Gráfico do Step 2",
                                    "Ferramenta de edição de gráficos"
                                  ],
                                  "tips": "Pergunte 'Por quê?' repetidamente para elevar informação a conhecimento.",
                                  "learningObjective": "Diferenciar informação visual de conhecimento através de interpretação crítica.",
                                  "commonMistakes": [
                                    "Parar na descrição visual sem implicações acionáveis",
                                    "Ignorar contexto externo nos dados"
                                  ]
                                },
                                {
                                  "stepNumber": 4,
                                  "title": "Praticar Criação de Visualização Completa e Autoavaliação",
                                  "subSteps": [
                                    "Escolher um novo dataset (ex: vendas mensais de produtos).",
                                    "Criar visualização passo a passo: dados → tabela → gráfico → insights.",
                                    "Mapear explicitamente: dados brutos → informação visual → conhecimento.",
                                    "Compartilhar com um par para feedback inicial.",
                                    "Refinar com base no feedback."
                                  ],
                                  "verification": "Produzir um relatório de 1 página com visualização e explicação da cadeia DIK.",
                                  "estimatedTime": "40 minutos",
                                  "materials": [
                                    "Novo dataset (ex: Kaggle simples)",
                                    "Ferramenta de visualização (Tableau Public gratuito ou Python Matplotlib)"
                                  ],
                                  "tips": "Mantenha simplicidade: um gráfico bom é melhor que muitos ruins.",
                                  "learningObjective": "Aplicar o ciclo completo de diferenciação em um exemplo prático.",
                                  "commonMistakes": [
                                    "Escolher datasets complexos demais para iniciantes",
                                    "Pular a validação com outros"
                                  ]
                                }
                              ],
                              "practicalExample": "Dados brutos: Vendas diárias de uma loja (100, 150, 90, 200 unidades). Tabela com médias revela pico nos fins de semana (informação). Gráfico de barras destaca padrão sazonal, levando ao conhecimento acionável: 'Aumentar estoque aos sábados para maximizar lucros'.",
                              "finalVerifications": [
                                "Define corretamente dados como brutos, informação como contextualizada e conhecimento como acionável.",
                                "Explica com exemplo como um gráfico transforma números em insights visuais.",
                                "Identifica pelo menos 2 padrões em uma visualização fornecida.",
                                "Distingue informação estática de conhecimento interpretativo.",
                                "Cria uma visualização simples que demonstra a progressão DIK.",
                                "Responde perguntas sobre implicações reais de uma visualização."
                              ],
                              "assessmentCriteria": [
                                "Precisão conceitual nas definições (90% alinhado à pirâmide DIKW).",
                                "Clareza na explicação da transformação via visualização.",
                                "Qualidade e adequação da visualização criada (legibilidade, escolha de gráfico).",
                                "Profundidade dos insights acionáveis derivados.",
                                "Capacidade de evitar erros comuns como distorção visual.",
                                "Integração de contexto da Ciência de Dados."
                              ],
                              "crossCurricularConnections": [
                                "Estatística: Uso de medidas descritivas para informação.",
                                "Programação: Códigos para gerar visualizações (Python/R).",
                                "Comunicação: Apresentação de insights visuais.",
                                "Negócios: Tomada de decisões baseadas em dados.",
                                "Psicologia Cognitiva: Percepção humana de padrões visuais."
                              ],
                              "realWorldApplication": "Em análise de saúde pública, dados brutos de casos de doença viram gráficos de epidemias (informação), gerando conhecimento para políticas de lockdown e vacinação, salvando vidas em cenários como a COVID-19."
                            },
                            "estimatedTime": "1 hora",
                            "difficulty": "intermediate",
                            "status": "not_started",
                            "prerequisites": []
                          }
                        ]
                      },
                      {
                        "id": "10.1.4.4.2",
                        "name": "Tipos de Gráficos e Representações Gráficas",
                        "description": "Principais tipos de gráficos e suas aplicações específicas para representar diferentes naturezas de dados e revelar padrões.",
                        "specificSkills": [
                          {
                            "id": "10.1.4.4.2.1",
                            "name": "Criar e interpretar gráficos de barras e histogramas",
                            "description": "Construir gráficos de barras para comparações categóricas e histogramas para distribuições de frequência, identificando assimetrias e multimodalidades.",
                            "atomicExpansion": {
                              "steps": [
                                {
                                  "stepNumber": 1,
                                  "title": "Entender os conceitos fundamentais de gráficos de barras e histogramas",
                                  "subSteps": [
                                    "Defina gráfico de barras: usado para comparações categóricas discretas.",
                                    "Defina histograma: usado para distribuições de frequência contínua ou discreta em intervalos.",
                                    "Identifique diferenças chave: barras separadas vs. barras adjacentes sem gaps.",
                                    "Aprenda sobre eixos: x para categorias/frequências, y para valores.",
                                    "Estude exemplos simples de assimetria (skew) e multimodalidade (picos múltiplos)."
                                  ],
                                  "verification": "Explique verbalmente ou por escrito as diferenças entre gráfico de barras e histograma, com um desenho rápido de cada.",
                                  "estimatedTime": "20 minutos",
                                  "materials": [
                                    "Folha de papel e caneta",
                                    "Vídeo tutorial sobre tipos de gráficos (ex: Khan Academy)"
                                  ],
                                  "tips": "Use analogias: barras como 'comparar maçãs com laranjas', histograma como 'distribuição de alturas'.",
                                  "learningObjective": "Diferenciar precisamente os propósitos e estruturas de gráficos de barras e histogramas.",
                                  "commonMistakes": [
                                    "Confundir categorias discretas com dados contínuos",
                                    "Ignorar gaps entre barras no histograma"
                                  ]
                                },
                                {
                                  "stepNumber": 2,
                                  "title": "Coletar e preparar dados adequados",
                                  "subSteps": [
                                    "Colete dados categóricos para barras (ex: vendas por produto).",
                                    "Colete dados numéricos para histograma (ex: idades agrupadas em bins).",
                                    "Limpe dados: remova valores ausentes ou outliers.",
                                    "Defina bins para histograma (ex: 5-10 intervalos).",
                                    "Organize em tabela: colunas para categoria/valor e frequência."
                                  ],
                                  "verification": "Crie uma tabela de dados preparada com pelo menos 10 entradas para cada tipo de gráfico.",
                                  "estimatedTime": "25 minutos",
                                  "materials": [
                                    "Planilha Excel ou Google Sheets",
                                    "Conjunto de dados de exemplo (ex: CSV de vendas)"
                                  ],
                                  "tips": "Comece com dados pequenos (10-20 itens) para prática rápida.",
                                  "learningObjective": "Preparar dados limpos e estruturados otimizados para cada tipo de visualização.",
                                  "commonMistakes": [
                                    "Usar dados contínuos em barras sem bins",
                                    "Escolher bins muito largos ou estreitos no histograma"
                                  ]
                                },
                                {
                                  "stepNumber": 3,
                                  "title": "Construir gráficos de barras e histogramas usando ferramentas computacionais",
                                  "subSteps": [
                                    "Abra ferramenta (ex: Python com Matplotlib ou Excel).",
                                    "Para barras: plote categorias no eixo x, valores no y.",
                                    "Para histograma: defina bins e plote frequências.",
                                    "Adicione títulos, labels e legendas.",
                                    "Ajuste escalas e cores para clareza."
                                  ],
                                  "verification": "Gere e salve dois gráficos corretos a partir dos dados preparados.",
                                  "estimatedTime": "40 minutos",
                                  "materials": [
                                    "Python (Jupyter Notebook com pandas/matplotlib)",
                                    "Ou Excel/Google Sheets"
                                  ],
                                  "tips": "Use plt.bar() para barras e plt.hist() para histogramas em Python; teste com dados de amostra primeiro.",
                                  "learningObjective": "Criar visualizações precisas e profissionais usando software.",
                                  "commonMistakes": [
                                    "Escala y errada (log vs linear)",
                                    "Falta de labels legíveis"
                                  ]
                                },
                                {
                                  "stepNumber": 4,
                                  "title": "Interpretar gráficos identificando padrões como assimetria e multimodalidade",
                                  "subSteps": [
                                    "Analise barras: compare alturas para insights categóricos.",
                                    "Para histograma: observe forma (simétrica, assimétrica esquerda/direita).",
                                    "Identifique multimodalidade: múltiplos picos indicando subgrupos.",
                                    "Calcule medidas básicas: moda, mediana visual.",
                                    "Descreva insights: 'Distribuição assimétrica à direita sugere outliers altos'."
                                  ],
                                  "verification": "Escreva um parágrafo de interpretação para cada gráfico criado, destacando padrões.",
                                  "estimatedTime": "30 minutos",
                                  "materials": [
                                    "Gráficos gerados no step anterior",
                                    "Folha para anotações"
                                  ],
                                  "tips": "Pergunte: 'O que isso diz sobre os dados?' Foque em storytelling.",
                                  "learningObjective": "Extrair e comunicar insights acionáveis de visualizações.",
                                  "commonMistakes": [
                                    "Ignorar o contexto dos dados",
                                    "Confundir correlação com causalidade"
                                  ]
                                }
                              ],
                              "practicalExample": "Dados de vendas: Produto A (50 unidades), B (30), C (70) → gráfico de barras mostrando C como líder. Dados de notas de 50 alunos (de 0-100) → histograma com bins de 10, revelando assimetria à direita (muitos alunos abaixo de 60) e bimodalidade (picos em 40-50 e 80-90).",
                              "finalVerifications": [
                                "Construa corretamente um gráfico de barras e um histograma de dados fornecidos.",
                                "Diferencie visualmente barras vs. histograma sem hesitação.",
                                "Identifique e explique assimetria em um histograma dado.",
                                "Descreva multimodalidade em um exemplo com múltiplos picos.",
                                "Interprete insights de um gráfico real-world fornecido.",
                                "Ajuste bins em histograma para melhor representação."
                              ],
                              "assessmentCriteria": [
                                "Precisão na construção: eixos, labels e escalas corretos (30%)",
                                "Qualidade da preparação de dados: limpeza e adequação (20%)",
                                "Profundidade da interpretação: identificação de padrões como skew/multimodal (25%)",
                                "Clareza visual: títulos, cores e legibilidade (15%)",
                                "Comunicação de insights: explicação acionável e concisa (10%)"
                              ],
                              "crossCurricularConnections": [
                                "Estatística: cálculo de medidas de tendência central a partir de histogramas.",
                                "Ciência de Dados: integração com pandas para análise pré-visualização.",
                                "Biologia: visualização de distribuições de traços genéticos.",
                                "Economia: comparações categóricas de vendas ou PIB por setor."
                              ],
                              "realWorldApplication": "Em marketing, use barras para comparar vendas por região; em saúde, histogramas para distribuições de BMI paciente, detectando assimetrias para intervenções precoces."
                            },
                            "estimatedTime": "2 horas",
                            "difficulty": "beginner",
                            "status": "not_started",
                            "prerequisites": []
                          },
                          {
                            "id": "10.1.4.4.2.2",
                            "name": "Utilizar gráficos de linha e scatter plots",
                            "description": "Aplicar gráficos de linha para séries temporais e tendências, e scatter plots para correlações bivariadas, calculando e visualizando coeficientes de correlação.",
                            "atomicExpansion": {
                              "steps": [
                                {
                                  "stepNumber": 1,
                                  "title": "Compreender conceitos fundamentais de gráficos de linha e scatter plots",
                                  "subSteps": [
                                    "Estude a definição de gráfico de linha: ideal para séries temporais e tendências contínuas ao longo do tempo.",
                                    "Analise exemplos de séries temporais, como evolução de temperaturas diárias ou vendas mensais.",
                                    "Entenda o scatter plot: usado para correlações bivariadas, mostrando relação entre duas variáveis numéricas.",
                                    "Revise exemplos de scatter plots, como altura vs. peso ou temperatura vs. consumo de energia.",
                                    "Compare os dois: linha para sequências ordenadas no tempo; scatter para dispersão sem ordem temporal."
                                  ],
                                  "verification": "Resuma em 3 frases as diferenças e usos de cada gráfico, confirmando compreensão conceitual.",
                                  "estimatedTime": "20 minutos",
                                  "materials": [
                                    "Documentação Matplotlib/Seaborn",
                                    "Vídeos tutoriais sobre visualização de dados"
                                  ],
                                  "tips": "Use diagramas mentais para associar gráficos a cenários reais.",
                                  "learningObjective": "Diferenciar aplicações de gráficos de linha e scatter plots em contextos de dados.",
                                  "commonMistakes": [
                                    "Confundir scatter com linha em dados não temporais",
                                    "Ignorar a ordenação temporal em linhas"
                                  ]
                                },
                                {
                                  "stepNumber": 2,
                                  "title": "Preparar e explorar dados para visualização",
                                  "subSteps": [
                                    "Instale e importe bibliotecas: pandas, matplotlib.pyplot e seaborn.",
                                    "Carregue um dataset com séries temporais (ex: datas e valores) e bivariados (ex: duas colunas numéricas).",
                                    "Limpe dados: verifique NaNs, converta datas para datetime e resample se necessário.",
                                    "Explore estatísticas básicas: df.describe(), df.plot() para pré-visualização.",
                                    "Segmente dados: crie subconjuntos para linha (por tempo) e scatter (pares de variáveis)."
                                  ],
                                  "verification": "Execute df.info() e df.head() sem erros, confirmando dados prontos para plotagem.",
                                  "estimatedTime": "25 minutos",
                                  "materials": [
                                    "Jupyter Notebook",
                                    "Dataset exemplo (ex: vendas.csv com colunas data, valor, temp)"
                                  ],
                                  "tips": "Sempre defina index como datetime para séries temporais com pd.to_datetime().",
                                  "learningObjective": "Preparar datasets limpos e estruturados para cada tipo de gráfico.",
                                  "commonMistakes": [
                                    "Esquecer de converter tipos de dados",
                                    "Não tratar valores ausentes"
                                  ]
                                },
                                {
                                  "stepNumber": 3,
                                  "title": "Criar e customizar gráficos de linha para tendências",
                                  "subSteps": [
                                    "Crie gráfico de linha básico: plt.plot(df['data'], df['valor']) ou sns.lineplot().",
                                    "Adicione customizações: títulos, labels nos eixos, grid e legendas com plt.title(), plt.xlabel().",
                                    "Melhore visual: múltiplas linhas para comparações, cores e markers.",
                                    "Salve o gráfico: plt.savefig('linha.png') e exiba com plt.show().",
                                    "Interprete: identifique tendências crescentes/decrescentes e picos."
                                  ],
                                  "verification": "Gere um gráfico de linha legível que mostre claramente uma tendência temporal.",
                                  "estimatedTime": "30 minutos",
                                  "materials": [
                                    "Código Python pronto",
                                    "Exemplos de datasets temporais"
                                  ],
                                  "tips": "Use fig, ax = plt.subplots() para controle fino de layout.",
                                  "learningObjective": "Construir gráficos de linha profissionais para análise de tendências.",
                                  "commonMistakes": [
                                    "Eixos invertidos (tempo no y)",
                                    "Escala inadequada distorcendo tendências"
                                  ]
                                },
                                {
                                  "stepNumber": 4,
                                  "title": "Criar scatter plots e calcular/visualizar coeficientes de correlação",
                                  "subSteps": [
                                    "Crie scatter básico: plt.scatter(x, y) ou sns.scatterplot(data=df, x='var1', y='var2').",
                                    "Adicione linha de regressão: sns.regplot() para tendência visual.",
                                    "Calcule coeficiente de correlação: df['var1'].corr(df['var2']) ou sns.heatmap(corr_matrix).",
                                    "Customizar: inclua r-value no título, ajuste transparência (alpha) para densidade.",
                                    "Interprete: valores próximos de 1/-1 indicam forte correlação positiva/negativa; 0 indica ausência."
                                  ],
                                  "verification": "Produza scatter plot com valor de correlação anotado e interpretação escrita.",
                                  "estimatedTime": "35 minutos",
                                  "materials": [
                                    "Biblioteca scipy.stats para pearsonr",
                                    "Dataset bivariado exemplo"
                                  ],
                                  "tips": "Use corr() em DataFrame para matriz completa de correlações.",
                                  "learningObjective": "Visualizar e quantificar correlações bivariadas com precisão.",
                                  "commonMistakes": [
                                    "Interpretar correlação como causalidade",
                                    "Usar scatter em dados categóricos"
                                  ]
                                }
                              ],
                              "practicalExample": "Carregue um dataset de vendas de sorvete com colunas 'temperatura' e 'vendas'. Crie um gráfico de linha para vendas ao longo dos meses (tendência sazonal) e um scatter plot temperatura vs. vendas, calculando r=0.85 para mostrar forte correlação positiva.",
                              "finalVerifications": [
                                "Gráfico de linha exibe eixo temporal ordenado e tendência clara.",
                                "Scatter plot mostra dispersão de pontos com linha de regressão opcional.",
                                "Coeficiente de correlação calculado corretamente e visualizado (ex: anotado no plot).",
                                "Todos os gráficos têm títulos, labels e legendas adequados.",
                                "Interpretação escrita identifica tendências e força da correlação.",
                                "Códigos executam sem erros em Jupyter Notebook."
                              ],
                              "assessmentCriteria": [
                                "Precisão conceitual: uso correto de cada gráfico (20%)",
                                "Qualidade de preparação de dados: limpeza e exploração (20%)",
                                "Execução técnica: plots customizados e correlação calculada (30%)",
                                "Interpretação analítica: tendências e correlações explicadas (20%)",
                                "Apresentação visual: legibilidade e profissionalismo (10%)"
                              ],
                              "crossCurricularConnections": [
                                "Estatística: cálculo e interpretação de coeficientes de correlação.",
                                "Programação: manipulação de dados com Python/pandas.",
                                "Ciências: análise de dados ambientais (ex: clima vs. fenômenos).",
                                "Economia: tendências de mercado e correlações econômicas.",
                                "Física: gráficos de movimento (tempo vs. posição como linha)."
                              ],
                              "realWorldApplication": "Em ciência de dados, analistas usam gráficos de linha para prever vendas futuras baseadas em tendências históricas (ex: e-commerce) e scatter plots para identificar correlações em marketing (ex: gasto em ads vs. conversões), auxiliando decisões empresariais como alocação de estoque ou campanhas."
                            },
                            "estimatedTime": "2 horas",
                            "difficulty": "beginner",
                            "status": "not_started",
                            "prerequisites": []
                          },
                          {
                            "id": "10.1.4.4.2.3",
                            "name": "Empregar boxplots e violin plots para resumos estatísticos",
                            "description": "Gerar boxplots para quartis, mediana e outliers, e violin plots para densidade probabilística, facilitando comparações entre grupos.",
                            "atomicExpansion": {
                              "steps": [
                                {
                                  "stepNumber": 1,
                                  "title": "Compreender os Conceitos Fundamentais de Boxplots e Violin Plots",
                                  "subSteps": [
                                    "Estude a estrutura de um boxplot: mediana (linha central), caixa (Q1 a Q3), whiskers (1.5 * IQR), e outliers (pontos além dos whiskers).",
                                    "Analise o violin plot: combina boxplot com estimativa de densidade kernel (KDE), mostrando a distribuição probabilística assimétrica.",
                                    "Compare os dois: boxplot foca em resumo estatístico (quartis/outliers), violin em forma da distribuição.",
                                    "Revise exemplos visuais de datasets com distribuições normais, assimétricas e multimodais.",
                                    "Identifique quando usar cada: boxplot para comparações rápidas de grupos, violin para densidade detalhada."
                                  ],
                                  "verification": "Explique verbalmente ou por escrito a diferença entre mediana, Q1/Q3 e KDE, com um desenho simples de cada gráfico.",
                                  "estimatedTime": "20-30 minutos",
                                  "materials": [
                                    "Documentação oficial do Seaborn/Matplotlib",
                                    "Imagens de exemplo de boxplots e violin plots online"
                                  ],
                                  "tips": "Use animações interativas (ex: no ObservableHQ) para visualizar como os plots se formam.",
                                  "learningObjective": "Dominar os componentes estatísticos e visuais de boxplots e violin plots para interpretação correta.",
                                  "commonMistakes": [
                                    "Confundir whiskers com intervalos de confiança",
                                    "Ignorar assimetria revelada pelo violin plot"
                                  ]
                                },
                                {
                                  "stepNumber": 2,
                                  "title": "Preparar o Ambiente e o Dataset para Análise",
                                  "subSteps": [
                                    "Instale/inicie Jupyter Notebook com bibliotecas: pandas, matplotlib, seaborn.",
                                    "Carregue um dataset exemplo (ex: tips do Seaborn ou dados de salários por gênero/idade).",
                                    "Explore os dados: df.describe(), df.groupby('grupo').describe(), cheque missing values.",
                                    "Selecione variáveis numéricas para plotar (ex: total_bill por day) e crie grupos categóricos.",
                                    "Normalize ou filtre outliers iniciais se necessário para foco na visualização."
                                  ],
                                  "verification": "Execute df.head() e df.groupby('grupo')['variavel'].describe() sem erros, salvando o dataset preparado.",
                                  "estimatedTime": "15-25 minutos",
                                  "materials": [
                                    "Jupyter Notebook",
                                    "pip install pandas seaborn matplotlib",
                                    "Dataset 'tips' via seaborn.load_dataset('tips')"
                                  ],
                                  "tips": "Sempre use plt.style.use('seaborn-v0_8') para plots mais limpos.",
                                  "learningObjective": "Preparar dados limpos e estruturados prontos para geração de plots estatísticos.",
                                  "commonMistakes": [
                                    "Esquecer de importar bibliotecas",
                                    "Usar colunas erradas sem groupby"
                                  ]
                                },
                                {
                                  "stepNumber": 3,
                                  "title": "Gerar e Customizar Boxplots para Resumos Estatísticos",
                                  "subSteps": [
                                    "Crie um boxplot básico: sns.boxplot(x='grupo', y='variavel', data=df).",
                                    "Adicione customizações: sns.boxplot(..., hue='subgrupo', palette='Set2', showfliers=True).",
                                    "Interprete: identifique mediana, quartis, outliers visuais entre grupos.",
                                    "Salve o plot: plt.savefig('boxplot.png') e exiba com plt.show().",
                                    "Teste com múltiplos grupos para comparações lado a lado."
                                  ],
                                  "verification": "Gere um boxplot com pelo menos 3 grupos, interpretando 2 outliers e comparando medianas.",
                                  "estimatedTime": "25-35 minutos",
                                  "materials": [
                                    "Código Jupyter pronto",
                                    "Dataset preparado do step 2"
                                  ],
                                  "tips": "Use order=['grupo1', 'grupo2'] para ordenar eixos categóricos logicamente.",
                                  "learningObjective": "Criar boxplots personalizados que destaquem quartis, medianas e outliers para comparações grupais.",
                                  "commonMistakes": [
                                    "Não ativar showfliers=False quando outliers poluem",
                                    "Escala y inadequada distorcendo comparações"
                                  ]
                                },
                                {
                                  "stepNumber": 4,
                                  "title": "Gerar e Customizar Violin Plots para Densidade Probabilística",
                                  "subSteps": [
                                    "Crie um violin plot básico: sns.violinplot(x='grupo', y='variavel', data=df).",
                                    "Customizações: sns.violinplot(..., hue='subgrupo', split=True, inner='box') para overlay com boxplot.",
                                    "Analise a forma: identifique picos de densidade, assimetria e multimodais.",
                                    "Compare com boxplot no mesmo dataset: sns.boxplot() seguido de sns.violinplot().",
                                    "Salve e anote o plot com plt.title('Comparação Box vs Violin')."
                                  ],
                                  "verification": "Produza um violin plot split com inner box, descrevendo uma assimetria observada.",
                                  "estimatedTime": "25-35 minutos",
                                  "materials": [
                                    "Código Jupyter",
                                    "Dataset do step 2"
                                  ],
                                  "tips": "split=True é ótimo para comparações densidade lado a lado em poucos grupos.",
                                  "learningObjective": "Utilizar violin plots para visualizar distribuições completas e facilitar comparações densidade-grupais.",
                                  "commonMistakes": [
                                    "Interpretar largura como frequência absoluta sem normalização",
                                    "Muitos hue causando sobreposição"
                                  ]
                                },
                                {
                                  "stepNumber": 5,
                                  "title": "Interpretar, Comparar e Comunicar Insights dos Plots",
                                  "subSteps": [
                                    "Compare boxplot vs violin: o que o violin revela que o box não (ex: bimodalidade)?",
                                    "Extraia estatísticas: calcule IQR manualmente e valide com plot.",
                                    "Crie um relatório: descreva diferenças grupais, outliers e implicações.",
                                    "Teste sensibilidade: altere dados e re-plot para ver mudanças.",
                                    "Compartilhe: exporte como imagem ou HTML interativo (plotly opcional)."
                                  ],
                                  "verification": "Escreva um parágrafo resumindo 3 insights de um dataset comparado via box e violin.",
                                  "estimatedTime": "20-30 minutos",
                                  "materials": [
                                    "Plots gerados",
                                    "Notebook com markdown para relatório"
                                  ],
                                  "tips": "Sempre pergunte: 'Qual história os dados contam?' para guiar interpretação.",
                                  "learningObjective": "Integrar boxplots e violin plots em análises comparativas acionáveis.",
                                  "commonMistakes": [
                                    "Sobre-generalizar outliers como erros",
                                    "Ignorar contexto de amostra pequena"
                                  ]
                                }
                              ],
                              "practicalExample": "Usando o dataset 'tips' do Seaborn, gere boxplot e violin plot de 'total_bill' por 'day' (seg/ter/qui/sex), identificando que sextas têm maior variância e outliers altos, com violin revelando pico bimodal em sábados.",
                              "finalVerifications": [
                                "Gere boxplot e violin plot idênticos sem erros de código.",
                                "Explique corretamente quartis, mediana e KDE em um plot específico.",
                                "Identifique e interprete pelo menos 2 outliers em um boxplot.",
                                "Descreva uma assimetria ou multimodalidade via violin plot.",
                                "Compare insights exclusivos de cada plot em um relatório curto.",
                                "Customiza plots com hue, split e palette adequadamente."
                              ],
                              "assessmentCriteria": [
                                "Precisão técnica: plots gerados corretamente com todos elementos visíveis.",
                                "Interpretação estatística: identificação correta de quartis, outliers e densidade.",
                                "Comparação grupal: análise clara de diferenças entre categorias.",
                                "Customização: uso efetivo de parâmetros como hue, inner e palette.",
                                "Comunicação: relatório conciso com insights acionáveis.",
                                "Eficiência: código limpo, sem warnings ou erros desnecessários."
                              ],
                              "crossCurricularConnections": [
                                "Estatística: cálculo manual de quartis e IQR.",
                                "Programação: manipulação de dataframes em Python/pandas.",
                                "Biologia: visualização de medidas experimentais por tratamento.",
                                "Economia: análise de distribuições de renda/salários por demografia.",
                                "Design de Dados: princípios de visualização (Tufte: dados-ink ratio)."
                              ],
                              "realWorldApplication": "Em análise de dados de e-commerce, use boxplots para detectar outliers em tempos de entrega por região e violin plots para entender padrões de distribuição de preços, auxiliando decisões de estoque e precificação."
                            },
                            "estimatedTime": "1.5 horas",
                            "difficulty": "intermediate",
                            "status": "not_started",
                            "prerequisites": []
                          },
                          {
                            "id": "10.1.4.4.2.4",
                            "name": "Explorar gráficos de calor e mapas de dispersão",
                            "description": "Criar heatmaps para matrizes de correlação e pairplots para relações multivariadas, auxiliando na detecção de padrões em conjuntos de dados de alta dimensionalidade.",
                            "atomicExpansion": {
                              "steps": [
                                {
                                  "stepNumber": 1,
                                  "title": "Preparar o Conjunto de Dados e Calcular a Matriz de Correlação",
                                  "subSteps": [
                                    "Carregue um dataset multivariado usando pandas (ex: dataset Iris ou um dataset customizado com pelo menos 5 variáveis numéricas).",
                                    "Selecione apenas colunas numéricas e verifique valores ausentes com df.isnull().sum().",
                                    "Calcule a matriz de correlação usando df.corr() para obter coeficientes de Pearson.",
                                    "Inspecione a matriz com df.corr().round(2) para uma visão inicial das correlações.",
                                    "Salve a matriz em uma variável para uso posterior."
                                  ],
                                  "verification": "Execute df.corr() e confirme que a matriz é simétrica com valores entre -1 e 1, sem NaNs.",
                                  "estimatedTime": "20 minutos",
                                  "materials": [
                                    "Python com pandas e numpy instalados",
                                    "Dataset de exemplo (ex: from sklearn.datasets import load_iris)"
                                  ],
                                  "tips": "Use datasets built-in como Iris para testes rápidos; sempre rode df.describe() antes para entender os dados.",
                                  "learningObjective": "Compreender como preparar dados numéricos e gerar matrizes de correlação para visualização.",
                                  "commonMistakes": [
                                    "Incluir variáveis categóricas na correlação (causa erros)",
                                    "Ignorar valores ausentes (leva a NaNs na matriz)"
                                  ]
                                },
                                {
                                  "stepNumber": 2,
                                  "title": "Criar e Customizar Heatmap da Matriz de Correlação",
                                  "subSteps": [
                                    "Importe seaborn: import seaborn as sns e configure plt.style.use('seaborn-v0_8-whitegrid').",
                                    "Crie o heatmap com sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', center=0, square=True, linewidths=0.5).",
                                    "Adicione rótulos e título: plt.title('Matriz de Correlação - Heatmap').",
                                    "Ajuste o layout com plt.tight_layout() e exiba com plt.show().",
                                    "Experimente máscaras para triângulo superior: mask = np.triu(np.ones_like(corr_matrix, dtype=bool))."
                                  ],
                                  "verification": "O heatmap deve mostrar cores variando de azul (correlação negativa) a vermelho (positiva), com anotações numéricas legíveis.",
                                  "estimatedTime": "25 minutos",
                                  "materials": [
                                    "Bibliotecas: seaborn, matplotlib.pyplot as plt, numpy as np"
                                  ],
                                  "tips": "Use cmap='RdBu_r' para melhor distinção; annot=True facilita leitura rápida.",
                                  "learningObjective": "Dominar a criação de heatmaps informativos para identificar forças e direções de correlações.",
                                  "commonMistakes": [
                                    "Não centralizar em 0 (distorce percepções)",
                                    "Heatmap muito pequeno (use figsize=(10,8))"
                                  ]
                                },
                                {
                                  "stepNumber": 3,
                                  "title": "Gerar Pairplot para Exploração Multivariada",
                                  "subSteps": [
                                    "Prepare o dataset com colunas numéricas selecionadas (ex: iris.data com feature_names).",
                                    "Crie o pairplot com sns.pairplot(df[num_cols], diag_kind='kde', plot_kws={'alpha':0.6}).",
                                    "Adicione hue para categorização se aplicável: sns.pairplot(df, hue='species').",
                                    "Personalize: markers=['o', 's'], palette='husl' para distinção visual.",
                                    "Exiba e salve a figura: plt.suptitle('Pairplot Multivariado'); plt.show()."
                                  ],
                                  "verification": "O pairplot deve mostrar matriz de scatterplots com histogramas ou KDE na diagonal, revelando padrões lineares e clusters.",
                                  "estimatedTime": "30 minutos",
                                  "materials": [
                                    "Seaborn e matplotlib",
                                    "Dataset com variáveis categóricas opcionais para hue"
                                  ],
                                  "tips": "Limite a 4-6 variáveis para evitar sobrecarga visual; use kind='reg' para linhas de regressão.",
                                  "learningObjective": "Explorar relações pareadas em múltiplas dimensões via pairplots para detecção de padrões não lineares.",
                                  "commonMistakes": [
                                    "Pairplot em datasets muito grandes (use amostragem)",
                                    "Esquecer alpha para sobreposições opacas"
                                  ]
                                },
                                {
                                  "stepNumber": 4,
                                  "title": "Interpretar Visualizações e Detectar Padrões",
                                  "subSteps": [
                                    "Analise o heatmap: identifique pares com |corr| > 0.7 (alta correlação) e padrões de clusters.",
                                    "No pairplot, observe distribuições marginais, scatterplots e densidades condicionais.",
                                    "Documente insights: ex: 'Sepal length e petal length fortemente correlacionados (r=0.87)'.",
                                    "Compare heatmap vs pairplot: heatmap resume magnitudes, pairplot mostra não-linearidades.",
                                    "Gere relatório com plt.savefig('analise.png') e anotações textuais."
                                  ],
                                  "verification": "Escreva 3-5 insights específicos baseados nas visualizações, como correlações fortes ou outliers visíveis.",
                                  "estimatedTime": "25 minutos",
                                  "materials": [
                                    "Código das steps anteriores",
                                    "Notebook Jupyter para anotações"
                                  ],
                                  "tips": "Procure por multicolinearidade (muitos vermelhos no heatmap); use zoom em pairplot para detalhes.",
                                  "learningObjective": "Extrair insights acionáveis de heatmaps e pairplots para detecção de padrões em dados de alta dimensionalidade.",
                                  "commonMistakes": [
                                    "Confundir correlação com causalidade",
                                    "Ignorar outliers que distorcem padrões"
                                  ]
                                }
                              ],
                              "practicalExample": "Usando o dataset Iris: carregue dados, calcule corr() em sepal_length, sepal_width, petal_length, petal_width. Crie heatmap revelando forte correlação positiva entre petal_length e petal_width (r≈0.96). Pairplot mostra clusters por species, auxiliando na detecção de separabilidade em alta dimensionalidade.",
                              "finalVerifications": [
                                "Heatmap gerado com anotações e cores intuitivas, sem erros de shape.",
                                "Pairplot exibe scatterplots pareados e distribuições marginais claras.",
                                "Identificados pelo menos 3 padrões/correlações no dataset de exemplo.",
                                "Código reproduzível gera as mesmas visualizações em nova execução.",
                                "Insights documentados conectam visualizações a detecção de padrões multivariados.",
                                "Figuras salvas em alta resolução para relatórios."
                              ],
                              "assessmentCriteria": [
                                "Precisão na preparação da matriz de correlação (simetria e ausência de NaNs).",
                                "Qualidade visual do heatmap (legibilidade, paleta apropriada, máscaras opcionais).",
                                "Completude do pairplot (customizações como hue, alpha e diag_kind).",
                                "Profundidade da interpretação (quantificação de padrões e limitações).",
                                "Eficiência do código (modularidade e boas práticas como tight_layout).",
                                "Criatividade em aplicações (ex: detecção de multicolinearidade para ML)."
                              ],
                              "crossCurricularConnections": [
                                "Estatística: Interpretação de coeficientes de correlação de Pearson.",
                                "Machine Learning: Detecção de multicolinearidade pré-modelagem.",
                                "Análise de Dados: EDA (Exploratory Data Analysis) em pipelines de Ciência de Dados.",
                                "Matemática: Álgebra linear em matrizes de covariância.",
                                "Programação: Manipulação de dados com pandas e visualização com seaborn."
                              ],
                              "realWorldApplication": "Em finanças, heatmaps de correlação de ativos detectam riscos de portfólio; em saúde, pairplots de biomarcadores revelam padrões multivariados para diagnósticos precoces em genômica."
                            },
                            "estimatedTime": "2 horas",
                            "difficulty": "intermediate",
                            "status": "not_started",
                            "prerequisites": []
                          }
                        ]
                      },
                      {
                        "id": "10.1.4.4.3",
                        "name": "Ferramentas e Técnicas para Visualização",
                        "description": "Bibliotecas e métodos práticos em Python para implementar visualizações, com foco em integração com o fluxo da Ciência de Dados.",
                        "specificSkills": [
                          {
                            "id": "10.1.4.4.3.1",
                            "name": "Implementar visualizações básicas com Matplotlib",
                            "description": "Configurar plots simples, personalizar eixos, legendas e salvar figuras usando a biblioteca Matplotlib, conforme exemplos em livros de Ciência de Dados com Python.",
                            "atomicExpansion": {
                              "steps": [
                                {
                                  "stepNumber": 1,
                                  "title": "Configurar o ambiente para Matplotlib",
                                  "subSteps": [
                                    "Instale a biblioteca Matplotlib usando pip: pip install matplotlib",
                                    "Importe a biblioteca no seu script Python: import matplotlib.pyplot as plt",
                                    "Verifique a instalação criando um plot vazio e exibindo-o com plt.show()",
                                    "Configure o backend se necessário (ex: %matplotlib inline no Jupyter)",
                                    "Teste com dados simples para garantir que não há erros de importação"
                                  ],
                                  "verification": "Execute um comando de plot vazio e confirme que uma janela ou imagem aparece sem erros",
                                  "estimatedTime": "15 minutos",
                                  "materials": [
                                    "Python 3.x instalado",
                                    "pip ou conda",
                                    "Editor de código como VS Code ou Jupyter Notebook"
                                  ],
                                  "tips": "Use um ambiente virtual (venv) para evitar conflitos de dependências. No Jupyter, reinicie o kernel após instalação.",
                                  "learningObjective": "Configurar corretamente o Matplotlib para uso imediato em projetos de visualização",
                                  "commonMistakes": [
                                    "Esquecer de instalar no ambiente correto",
                                    "Não usar plt.show() ou inline no Jupyter",
                                    "Conflitos com versões antigas do Python"
                                  ]
                                },
                                {
                                  "stepNumber": 2,
                                  "title": "Criar plots simples de linha",
                                  "subSteps": [
                                    "Prepare dados simples: x = [1,2,3,4]; y = [1,4,2,3]",
                                    "Use plt.plot(x, y) para gerar o gráfico de linha",
                                    "Adicione plt.show() para exibir o plot",
                                    "Experimente com diferentes tipos de dados (ex: listas ou arrays numpy)",
                                    "Crie múltiplas linhas no mesmo plot com plt.plot(x, y1) e plt.plot(x, y2)"
                                  ],
                                  "verification": "O gráfico de linha é exibido corretamente com eixos proporcionais e dados plotados",
                                  "estimatedTime": "20 minutos",
                                  "materials": [
                                    "Script Python ou Jupyter Notebook",
                                    "Biblioteca NumPy (opcional para arrays)"
                                  ],
                                  "tips": "Sempre defina x e y com o mesmo comprimento para evitar erros de indexação. Use markers='o' para pontos visíveis.",
                                  "learningObjective": "Gerar visualizações básicas de dados sequenciais usando plot de linha",
                                  "commonMistakes": [
                                    "Comprimentos diferentes em x e y",
                                    "Esquecer plt.show()",
                                    "Não limpar figura anterior com plt.clf()"
                                  ]
                                },
                                {
                                  "stepNumber": 3,
                                  "title": "Personalizar eixos, legendas e títulos",
                                  "subSteps": [
                                    "Adicione título: plt.title('Meu Primeiro Gráfico')",
                                    "Label eixos: plt.xlabel('Eixo X'); plt.ylabel('Eixo Y')",
                                    "Crie legenda: plt.plot(x, y, label='Linha 1'); plt.legend()",
                                    "Ajuste limites: plt.xlim(0, 5); plt.ylim(0, 5)",
                                    "Melhore aparência: plt.grid(True) para grade e plt.tight_layout() para espaçamento"
                                  ],
                                  "verification": "O plot exibe título, labels nos eixos, legenda e grade visíveis e bem posicionados",
                                  "estimatedTime": "25 minutos",
                                  "materials": [
                                    "Script Python com dados de exemplo"
                                  ],
                                  "tips": "Use strings com LaTeX para títulos matemáticos: r'$y = x^2$'. Posicione legenda com loc='upper right'.",
                                  "learningObjective": "Customizar elementos visuais para tornar gráficos informativos e profissionais",
                                  "commonMistakes": [
                                    "Labels sobrepostos (use tight_layout)",
                                    "Legenda sem label nas linhas",
                                    "Escala inadequada nos eixos"
                                  ]
                                },
                                {
                                  "stepNumber": 4,
                                  "title": "Salvar e exportar figuras",
                                  "subSteps": [
                                    "Use plt.savefig('meu_grafico.png', dpi=300, bbox_inches='tight') antes de plt.show()",
                                    "Experimente formatos: .png, .pdf, .svg",
                                    "Ajuste resolução e tamanho: figsize=(10,6) em plt.figure()",
                                    "Salve múltiplos plots em loop, limpando com plt.clf() entre eles",
                                    "Verifique o arquivo salvo abrindo em um visualizador de imagens"
                                  ],
                                  "verification": "Arquivos são salvos corretamente com qualidade alta e sem cortes",
                                  "estimatedTime": "15 minutos",
                                  "materials": [
                                    "Diretório de trabalho acessível para salvar arquivos"
                                  ],
                                  "tips": "Sempre use bbox_inches='tight' para evitar cortes em labels. Para relatórios, prefira PDF para vetores escaláveis.",
                                  "learningObjective": "Exportar visualizações para uso em relatórios e apresentações",
                                  "commonMistakes": [
                                    "Salvar após show() (pode salvar tela em branco)",
                                    "DPI baixo resultando em baixa qualidade",
                                    "Caminho de arquivo inválido"
                                  ]
                                }
                              ],
                              "practicalExample": "Carregue dados de vendas diárias (dias = [1,2,3,4,5]; vendas = [100,150,120,180,200]). Crie um line plot com título 'Vendas Semanais', labels 'Dias' e 'Vendas (R$)', legenda 'Vendas Diárias', grade ativada e salve como 'vendas.png'.",
                              "finalVerifications": [
                                "Matplotlib importado e ambiente configurado sem erros",
                                "Plot de linha simples criado e exibido corretamente",
                                "Elementos de personalização (título, labels, legenda) funcionam",
                                "Figura salva em formato PNG/PDF com alta qualidade",
                                "Múltiplos plots gerenciados sem sobreposições",
                                "Ajustes de eixos e grid aplicados com sucesso"
                              ],
                              "assessmentCriteria": [
                                "Código executa sem erros de sintaxe ou importação",
                                "Gráficos exibem dados precisos e proporcionais",
                                "Personalizações melhoram legibilidade (labels claros, legenda posicionada)",
                                "Arquivos salvos são legíveis e profissionais",
                                "Tempo de execução eficiente (sem loops desnecessários)",
                                "Uso correto de boas práticas (tight_layout, grid)"
                              ],
                              "crossCurricularConnections": [
                                "Estatística: Interpretação de tendências e distribuições em gráficos",
                                "Programação: Manipulação de bibliotecas e dados em Python",
                                "Matemática: Representação gráfica de funções e relações",
                                "Comunicação: Criação de visuais para relatórios e apresentações"
                              ],
                              "realWorldApplication": "Em análise de dados para empresas, criando dashboards de vendas ou KPIs; em pesquisa científica para visualizar resultados experimentais; em relatórios acadêmicos para ilustrar modelos matemáticos."
                            },
                            "estimatedTime": "2 horas",
                            "difficulty": "beginner",
                            "status": "not_started",
                            "prerequisites": []
                          },
                          {
                            "id": "10.1.4.4.3.2",
                            "name": "Criar visualizações avançadas com Seaborn",
                            "description": "Utilizar Seaborn para gráficos estatísticos automatizados, como jointplots e pairgrids, integrando com Pandas para análise exploratória eficiente.",
                            "atomicExpansion": {
                              "steps": [
                                {
                                  "stepNumber": 1,
                                  "title": "Configurar ambiente e preparar dados com Pandas para Seaborn",
                                  "subSteps": [
                                    "Instalar e importar bibliotecas necessárias: pandas, seaborn e matplotlib",
                                    "Carregar um dataset exemplo usando seaborn.load_dataset('tips')",
                                    "Explorar estrutura dos dados com df.head(), df.info() e df.describe()",
                                    "Verificar e tratar valores ausentes ou tipos de dados inadequados",
                                    "Preparar colunas numéricas para plotting selecionando variáveis relevantes"
                                  ],
                                  "verification": "Dataset carregado corretamente, exibido df.head() e df.info() sem erros, dados prontos para visualização",
                                  "estimatedTime": "15 minutos",
                                  "materials": [
                                    "Jupyter Notebook ou ambiente Python",
                                    "Bibliotecas: pandas, seaborn, matplotlib",
                                    "Dataset 'tips' via seaborn.load_dataset"
                                  ],
                                  "tips": "Sempre use seaborn.load_dataset() para datasets prontos e evite downloads manuais iniciais",
                                  "learningObjective": "Configurar um ambiente funcional para integrações Pandas-Seaborn em EDA",
                                  "commonMistakes": [
                                    "Esquecer 'import seaborn as sns'",
                                    "Ignorar verificação de dados categóricos vs numéricos",
                                    "Não tratar NaNs antes de plotar"
                                  ]
                                },
                                {
                                  "stepNumber": 2,
                                  "title": "Criar jointplots para análise bivariada automatizada",
                                  "subSteps": [
                                    "Executar sns.jointplot(x='total_bill', y='tip', data=df) para plot básico",
                                    "Experimentar kind='scatter' com hue='smoker' para categorização",
                                    "Testar kind='hex' e kind='kde' para densidades",
                                    "Adicionar marginal_kws para customizar margens",
                                    "Interpretar correlações visuais e salvar com plt.savefig()"
                                  ],
                                  "verification": "Jointplot gerado com múltiplos kinds, exibindo distribuições conjuntas e salvo como imagem",
                                  "estimatedTime": "20 minutos",
                                  "materials": [
                                    "Dataset preparado do Step 1",
                                    "Documentação Seaborn jointplot"
                                  ],
                                  "tips": "Use hue para variáveis categóricas para insights rápidos em subgrupos",
                                  "learningObjective": "Dominar jointplots para explorar relações bivariadas com distribuições marginais",
                                  "commonMistakes": [
                                    "Não especificar data=df corretamente",
                                    "Confundir x e y em variáveis não numéricas",
                                    "Ignorar interpretação da nuvem de pontos vs densidade"
                                  ]
                                },
                                {
                                  "stepNumber": 3,
                                  "title": "Construir pairgrids para visualizações multivariadas",
                                  "subSteps": [
                                    "Criar g = sns.PairGrid(df, vars=['total_bill', 'tip', 'size'])",
                                    "Mapear scatterplots com g.map(sns.scatterplot, alpha=0.6)",
                                    "Customizar diagonal com g.map_diag(sns.histplot)",
                                    "Adicionar regressões lineares com g.map_offdiag(sns.regplot)",
                                    "Finalizar com g.tight_layout() e exibir g.fig"
                                  ],
                                  "verification": "PairGrid completo exibido com matriz de scatters, histogramas e regressões sem erros",
                                  "estimatedTime": "25 minutos",
                                  "materials": [
                                    "Dataset do Step 1",
                                    "Documentação PairGrid Seaborn"
                                  ],
                                  "tips": "Limite vars a 4-6 colunas para evitar sobrecarga visual",
                                  "learningObjective": "Utilizar PairGrid para matrizes de plots automatizadas em EDA multivariada",
                                  "commonMistakes": [
                                    "Esquecer g.map() após inicialização",
                                    "Misturar tipos de dados incompatíveis em vars",
                                    "Não ajustar alpha para sobreposições"
                                  ]
                                },
                                {
                                  "stepNumber": 4,
                                  "title": "Customizar e integrar visualizações em análise exploratória",
                                  "subSteps": [
                                    "Aplicar paletas de cores com sns.set_palette('husl') e palette em plots",
                                    "Combinar jointplot e pairgrid em subplots com plt.subplot",
                                    "Extrair insights como correlações fortes (ex: total_bill vs tip)",
                                    "Exportar figuras para PDF/PNG e adicionar títulos/anotações",
                                    "Documentar workflow completo em notebook com markdown"
                                  ],
                                  "verification": "Visualizações customizadas integradas, insights anotados e exportadas corretamente",
                                  "estimatedTime": "20 minutos",
                                  "materials": [
                                    "Figuras dos steps anteriores",
                                    "Ferramentas de export: plt.savefig, fig.savefig"
                                  ],
                                  "tips": "Use sns.set_style('whitegrid') para legibilidade profissional",
                                  "learningObjective": "Personalizar Seaborn para relatórios acionáveis e workflows de EDA eficientes",
                                  "commonMistakes": [
                                    "Sobrecarregar plots com muitas customizações",
                                    "Não anotar insights derivados",
                                    "Esquecer de resetar estilos globais com sns.reset_defaults()"
                                  ]
                                },
                                {
                                  "stepNumber": 5,
                                  "title": "Avaliar e refinar visualizações para eficiência analítica",
                                  "subSteps": [
                                    "Comparar múltiplas versões de plots e selecionar melhores",
                                    "Testar com dataset diferente (ex: 'iris') para generalização",
                                    "Calcular correlações numéricas com df.corr() e sobrepor em plots",
                                    "Otimizar performance para datasets grandes com sample",
                                    "Preparar dashboard simples concatenando figs"
                                  ],
                                  "verification": "Análise refinada com insights validados por métricas numéricas e generalizada",
                                  "estimatedTime": "15 minutos",
                                  "materials": [
                                    "Datasets adicionais: 'iris'",
                                    "pandas corr()"
                                  ],
                                  "tips": "Sempre valide visualizações com estatísticas descritivas",
                                  "learningObjective": "Refinar Seaborn para análises exploratórias robustas e escaláveis",
                                  "commonMistakes": [
                                    "Generalizar de um dataset sem testes",
                                    "Ignorar discrepâncias visuais vs numéricas",
                                    "Não considerar performance em big data"
                                  ]
                                }
                              ],
                              "practicalExample": "No dataset 'tips' do Seaborn, crie um jointplot entre 'total_bill' e 'tip' com hue='time' para ver padrões de gorjetas em almoço/janta, e um PairGrid com variáveis numéricas para identificar correlações como tamanho da mesa vs conta total, gerando insights para otimização de serviços em restaurantes.",
                              "finalVerifications": [
                                "Jointplot bivariado com margens KDE/hex exibido corretamente",
                                "PairGrid multivariado com diagonal histplot e off-diag regplot funcional",
                                "Customizações de cor/hue aplicadas sem erros",
                                "Insights exploratórios documentados (ex: correlação >0.7 identificada)",
                                "Figuras exportadas e workflow reproduzível em notebook novo"
                              ],
                              "assessmentCriteria": [
                                "Precisão na configuração Pandas-Seaborn (sem erros de import/dados)",
                                "Diversidade de kinds/plots utilizados (jointplot, pairgrid variantes)",
                                "Qualidade interpretativa dos insights gerados",
                                "Eficiência customização (legibilidade, paletas profissionais)",
                                "Escalabilidade demonstrada em múltiplos datasets"
                              ],
                              "crossCurricularConnections": [
                                "Estatística Descritiva (distribuições, correlações)",
                                "Programação Python (manipulação de dataframes)",
                                "Design Gráfico (princípios de visualização clara)",
                                "Análise de Dados (EDA workflows)",
                                "Matemática Computacional (automatização estatística)"
                              ],
                              "realWorldApplication": "Em equipes de Data Science de e-commerce, usar Seaborn pairgrids para EDA rápida em vendas, identificando padrões como correlação entre preço e cliques, otimizando recomendações de produtos e aumentando conversões em 15-20%."
                            },
                            "estimatedTime": "2.5 horas",
                            "difficulty": "intermediate",
                            "status": "not_started",
                            "prerequisites": []
                          },
                          {
                            "id": "10.1.4.4.3.3",
                            "name": "Identificar padrões e anomalias através de técnicas visuais",
                            "description": "Aplicar zoom, facet grids e interações para detectar clusters, outliers e tendências ocultas, preparando dados para etapas subsequentes como modelagem.",
                            "atomicExpansion": {
                              "steps": [
                                {
                                  "stepNumber": 1,
                                  "title": "Preparar o dataset e criar visualização inicial interativa",
                                  "subSteps": [
                                    "Carregue um dataset relevante usando pandas (ex: dataset de vendas ou Iris).",
                                    "Selecione variáveis adequadas para scatter plot ou heatmap.",
                                    "Crie uma visualização interativa inicial com Plotly Express (scatter ou line plot).",
                                    "Ative tooltips e legendas para inspeção básica.",
                                    "Salve a visualização em HTML para interatividade offline."
                                  ],
                                  "verification": "Visualização interativa renderizada corretamente com tooltips funcionando ao passar o mouse.",
                                  "estimatedTime": "20 minutos",
                                  "materials": "Python com pandas e plotly; dataset CSV (ex: Iris ou sales_data.csv)",
                                  "tips": "Use px.scatter(df, x='feature1', y='feature2', color='category') para começar rápido.",
                                  "learningObjective": "Configurar uma base visual interativa para exploração de dados.",
                                  "commonMistakes": "Ignorar tipos de dados (ex: strings em eixos numéricos) ou não ativar interatividade."
                                },
                                {
                                  "stepNumber": 2,
                                  "title": "Aplicar zoom e pan para exploração detalhada",
                                  "subSteps": [
                                    "Use as ferramentas de zoom (caixa ou roda do mouse) na visualização Plotly.",
                                    "Pan para mover pela visualização e focar em regiões densas.",
                                    "Identifique clusters iniciais (grupos apertados) e outliers (pontos isolados).",
                                    "Registre screenshots ou anotações de áreas suspeitas.",
                                    "Ajuste escalas de eixos se necessário para melhor visibilidade."
                                  ],
                                  "verification": "Capturas de tela mostrando zoom em clusters/outliers com anotações.",
                                  "estimatedTime": "15 minutos",
                                  "materials": "Visualização HTML interativa do Step 1; software de anotação (ex: Snipping Tool)",
                                  "tips": "Mantenha zoom progressivo: comece amplo, refine gradualmente para evitar perda de contexto.",
                                  "learningObjective": "Dominar navegação interativa para revelar padrões locais.",
                                  "commonMistakes": "Zoom excessivo sem contexto global ou ignorar tooltips durante zoom."
                                },
                                {
                                  "stepNumber": 3,
                                  "title": "Implementar facet grids para decomposição multidimensional",
                                  "subSteps": [
                                    "Adicione facet_col ou facet_row no Plotly para dividir por uma variável categórica (ex: região ou classe).",
                                    "Crie múltiplos painéis e aplique zoom/pan sincronizado entre eles.",
                                    "Compare padrões entre facetas para tendências ocultas (ex: outliers consistentes em uma faceta).",
                                    "Ajuste layouts para uniformidade e adicione títulos descritivos.",
                                    "Salve versão atualizada da visualização."
                                  ],
                                  "verification": "Facet grid renderizado com padrões comparáveis entre painéis e sincronia de zoom.",
                                  "estimatedTime": "25 minutos",
                                  "materials": "Código Plotly atualizado; dataset com variável categórica",
                                  "tips": "Use facet_col='category' para divisões horizontais intuitivas em telas widescreen.",
                                  "learningObjective": "Usar facetas para isolar dimensões e detectar anomalias condicionais.",
                                  "commonMistakes": "Escolha errada de variável de faceta (use categóricas com 3-10 níveis) ou falta de sincronia."
                                },
                                {
                                  "stepNumber": 4,
                                  "title": "Detectar, anotar e preparar padrões/anomalias para modelagem",
                                  "subSteps": [
                                    "Liste clusters, outliers e tendências observados em um relatório Markdown.",
                                    "Anotar visualizações com shapes/linhas no Plotly para destacar achados.",
                                    "Filtre dataset baseado em achados (ex: df_outliers = df.query('condicao')).",
                                    "Exporte subconjuntos para CSV e planeje próximos passos (ex: modelagem).",
                                    "Valide achados com estatísticas descritivas básicas (boxplots agregados)."
                                  ],
                                  "verification": "Relatório com lista de padrões/anomalias, visualizações anotadas e datasets filtrados exportados.",
                                  "estimatedTime": "20 minutos",
                                  "materials": "Visualizações anteriores; pandas para filtros; Markdown editor",
                                  "tips": "Priorize anomalias impactantes (ex: >3 desvios padrão) para eficiência.",
                                  "learningObjective": "Sintetizar insights visuais em ações preparatórias para análise avançada.",
                                  "commonMistakes": "Subjetividade sem validação estatística ou não exportar dados processados."
                                }
                              ],
                              "practicalExample": "Em um dataset de vendas de e-commerce (colunas: vendas, região, data), use scatter plot de vendas vs. data faceteado por região. Zoom revela outliers de picos fraudulentos em uma região específica, permitindo filtro para investigação de fraudes.",
                              "finalVerifications": [
                                "Visualizações interativas salvas com zoom/facetas funcionais.",
                                "Relatório lista pelo menos 3 padrões/anomalias com evidências visuais.",
                                "Datasets filtrados exportados corretamente.",
                                "Validação estatística confirma achados visuais (ex: z-scores).",
                                "Preparação documentada para modelagem subsequente.",
                                "Screenshots anotados de clusters/outliers."
                              ],
                              "assessmentCriteria": [
                                "Precisão na detecção: 80% dos outliers confirmados estatisticamente.",
                                "Uso completo de ferramentas: zoom, pan e facetas aplicados corretamente.",
                                "Profundidade de substeps: Todos subpassos executados com detalhes.",
                                "Qualidade do relatório: Claro, conciso e acionável.",
                                "Eficiência temporal: Concluído dentro de 80 minutos totais.",
                                "Criatividade em anotações: Insights únicos além do óbvio."
                              ],
                              "crossCurricularConnections": [
                                "Estatística: Cálculo de desvios padrão para validar outliers visuais.",
                                "Programação: Manipulação de dados com Python/pandas e bibliotecas de plot.",
                                "Matemática: Análise de clusters via distância euclidiana implícita.",
                                "Negócios: Aplicação em detecção de fraudes ou otimização de vendas."
                              ],
                              "realWorldApplication": "Em bancos, detectar transações fraudulentas via zoom em gráficos de valores vs. tempo faceteados por localização; em saúde, identificar anomalias em dados de pacientes para alertas precoces de epidemias."
                            },
                            "estimatedTime": "2 horas",
                            "difficulty": "intermediate",
                            "status": "not_started",
                            "prerequisites": []
                          }
                        ]
                      }
                    ]
                  },
                  {
                    "id": "10.1.4.5",
                    "name": "Limpeza de Dados",
                    "description": "Procedimentos para tratar dados ausentes, inconsistentes e outliers.",
                    "individualConcepts": [
                      {
                        "id": "10.1.4.5.1",
                        "name": "Tratamento de Dados Ausentes",
                        "description": "Procedimentos para identificar e lidar com valores faltantes nos dados, considerando tipos como MCAR, MAR e MNAR, e aplicando técnicas de imputação adequadas.",
                        "specificSkills": [
                          {
                            "id": "10.1.4.5.1.1",
                            "name": "Identificar Tipos de Dados Ausentes",
                            "description": "Classificar dados ausentes em Missing Completely At Random (MCAR), Missing At Random (MAR) e Missing Not At Random (MNAR), utilizando testes estatísticos como Little's MCAR test.",
                            "atomicExpansion": {
                              "steps": [
                                {
                                  "stepNumber": 1,
                                  "title": "Compreender os Conceitos Teóricos dos Tipos de Dados Ausentes",
                                  "subSteps": [
                                    "Estude as definições: MCAR (ausência completamente aleatória, independente de valores observados ou não observados)",
                                    "Aprenda MAR (ausência aleatória, depende de valores observados mas não dos ausentes)",
                                    "Entenda MNAR (ausência não aleatória, depende dos valores ausentes em si)",
                                    "Compare com diagramas causais (DAGs) para visualizar dependências",
                                    "Revise implicações para análise de dados: viés em MCAR vs. MNAR"
                                  ],
                                  "verification": "Explique em suas próprias palavras as diferenças entre MCAR, MAR e MNAR com um exemplo simples cada.",
                                  "estimatedTime": "30 minutos",
                                  "materials": [
                                    "Artigo ou vídeo sobre missing data mechanisms (ex: 'Missing Data Mechanisms' de Stef van Buuren)",
                                    "Notebook para anotações"
                                  ],
                                  "tips": "Use mnemônicos: MCAR='completamente aleatório como lançar uma moeda', MAR='aleatório condicional aos observados', MNAR='não aleatório, escondido'.",
                                  "learningObjective": "Diferenciar conceitualmente os três tipos de mecanismos de dados ausentes.",
                                  "commonMistakes": [
                                    "Confundir MAR com MNAR (lembre: MAR depende só de observados)",
                                    "Ignorar que MCAR é ideal mas raro na prática"
                                  ]
                                },
                                {
                                  "stepNumber": 2,
                                  "title": "Preparar um Dataset para Análise de Dados Ausentes",
                                  "subSteps": [
                                    "Carregue um dataset público (ex: Titanic do seaborn) com dados ausentes reais ou introduza missing values artificialmente",
                                    "Explore visualmente os missing data usando heatmaps (biblioteca missingno)",
                                    "Calcule estatísticas descritivas: % de missing por variável e padrões (ex: missing em Age correlacionado com Pclass?)",
                                    "Documente hipóteses iniciais sobre o mecanismo (MCAR? MAR por variáveis observadas?)",
                                    "Divida o dataset em treino/teste se necessário para validação"
                                  ],
                                  "verification": "Gere um relatório visual mostrando padrões de missing data e % por feature.",
                                  "estimatedTime": "45 minutos",
                                  "materials": [
                                    "Python com pandas, seaborn, missingno: `pip install missingno`",
                                    "Dataset Titanic: `import seaborn as sns; df = sns.load_dataset('titanic')`"
                                  ],
                                  "tips": "Sempre visualize primeiro: `msno.matrix(df)` revela clusters de missing.",
                                  "learningObjective": "Identificar e documentar padrões de missing data em um dataset real.",
                                  "commonMistakes": [
                                    "Não checar múltiplas features (missing pode ser MAR em uma e MNAR em outra)",
                                    "Assumir MCAR sem teste"
                                  ]
                                },
                                {
                                  "stepNumber": 3,
                                  "title": "Executar o Teste de Little's MCAR",
                                  "subSteps": [
                                    "Instale e importe a biblioteca necessária (ex: `pip install statsmodels`; use `from statsmodels.stats.contingency_tables import mcnemar` ou pacote pymcartest)",
                                    "Prepare o dataset para o teste: selecione variáveis categóricas ou discretize contínuas",
                                    "Execute Little's MCAR test: `little_test(df)` e capture p-value",
                                    "Repita para subconjuntos de variáveis se o dataset for grande",
                                    "Registre o p-value e estatística qui-quadrado"
                                  ],
                                  "verification": "Obtenha um p-value do teste e interprete se rejeita H0 (MCAR).",
                                  "estimatedTime": "40 minutos",
                                  "materials": [
                                    "Python/Jupyter: `pip install pymcar` ou implementação custom via scipy.stats.chi2_contingency`",
                                    "Código exemplo: https://github.com/... (link para repo com Little's test)"
                                  ],
                                  "tips": "Teste é sensível a amostras pequenas; use n>100 por feature. Para contínuas, binne primeiro.",
                                  "learningObjective": "Aplicar o teste estatístico Little's MCAR em dados reais.",
                                  "commonMistakes": [
                                    "Aplicar teste em variáveis com >50% missing (poder baixo)",
                                    "Interpretar p>0.05 como 'provado MCAR' (só falha em rejeitar)"
                                  ]
                                },
                                {
                                  "stepNumber": 4,
                                  "title": "Interpretar Resultados e Classificar o Mecanismo de Missing Data",
                                  "subSteps": [
                                    "Interprete p-value: p>0.05 sugere MCAR; p<0.05 indica não-MCAR (então MAR ou MNAR)",
                                    "Para não-MCAR, teste MAR: modele logit de missingness vs. variáveis observadas (auxiliary variables)",
                                    "Avalie MNAR: compare distribuições observadas vs. imputadas sensíveis",
                                    "Classifique formalmente: MCAR/MAR/MNAR com justificativa e confiança",
                                    "Documente em relatório com visualizações e recomendações de imputação"
                                  ],
                                  "verification": "Classifique o dataset como MCAR/MAR/MNAR com evidências do teste e análise auxiliar.",
                                  "estimatedTime": "35 minutos",
                                  "materials": [
                                    "Python com statsmodels para logit: `import statsmodels.api as sm`",
                                    "Relatório template em Markdown"
                                  ],
                                  "tips": "Para MAR, use `missing_indicator = df.isnull().any(axis=1)` e regresse em preditores.",
                                  "learningObjective": "Classificar mecanismos de missing data baseado em testes e análises complementares.",
                                  "commonMistakes": [
                                    "Parar em Little's test sem diferenciar MAR de MNAR",
                                    "Ignorar tamanho do efeito além de p-value"
                                  ]
                                }
                              ],
                              "practicalExample": "No dataset Titanic, Age tem ~20% missing. Execute Little's test nas variáveis categóricas (Survived, Pclass). Se p<0.05 e missingness em Age correlaciona com Pclass (observada), classifique como MAR. Implique: imputar Age usando Pclass melhora modelo de sobrevivência.",
                              "finalVerifications": [
                                "Explique corretamente MCAR, MAR e MNAR com exemplos distintos",
                                "Execute Little's MCAR test em um dataset e interprete p-value",
                                "Identifique padrões visuais de missing data via heatmap",
                                "Proponha teste auxiliar para diferenciar MAR de MNAR",
                                "Recomende estratégia de imputação baseada na classificação",
                                "Gere relatório com código reproduzível"
                              ],
                              "assessmentCriteria": [
                                "Precisão conceitual: 100% correto nos 3 mecanismos",
                                "Execução técnica: Teste roda sem erros e p-value correto",
                                "Interpretação: Justificativa lógica para classificação",
                                "Profundidade: Inclui testes auxiliares para MAR/MNAR",
                                "Aplicação: Liga classificação a impacto em downstream tasks",
                                "Clareza: Relatório visual e escrito compreensível"
                              ],
                              "crossCurricularConnections": [
                                "Estatística: Testes qui-quadrado e modelagem logística",
                                "Machine Learning: Impacto de missing data em bias/variância de modelos",
                                "Programação: Manipulação de dataframes em Python/pandas",
                                "Visualização de Dados: Heatmaps e matrizes de missingness",
                                "Inferência Causal: DAGs para mecanismos de missing"
                              ],
                              "realWorldApplication": "Em dados médicos (ex: EHRs), classificar missing em biomarkers como MNAR permite imputação sensível (ex: MAR via idade/sexo), evitando viés em predições de risco de doença e conformidade com regulamentações como GDPR/HIPAA."
                            },
                            "estimatedTime": "1 hora",
                            "difficulty": "beginner",
                            "status": "not_started",
                            "prerequisites": []
                          },
                          {
                            "id": "10.1.4.5.1.2",
                            "name": "Aplicar Imputação Simples",
                            "description": "Implementar métodos de imputação como substituição por média, mediana ou moda para variáveis numéricas e categóricas, usando bibliotecas como Pandas em Python.",
                            "atomicExpansion": {
                              "steps": [
                                {
                                  "stepNumber": 1,
                                  "title": "Preparar o Ambiente e Carregar o Dataset",
                                  "subSteps": [
                                    "Instale as bibliotecas necessárias: Pandas e NumPy via pip.",
                                    "Importe as bibliotecas no Jupyter Notebook ou script Python.",
                                    "Carregue um dataset de exemplo com valores ausentes (ex: CSV com colunas numéricas e categóricas).",
                                    "Examine a estrutura inicial do dataset com .info() e .head().",
                                    "Identifique colunas com dados ausentes usando .isnull().sum()."
                                  ],
                                  "verification": "Dataset carregado corretamente e relatório de valores ausentes exibido sem erros.",
                                  "estimatedTime": "15 minutos",
                                  "materials": "Jupyter Notebook, dataset CSV de exemplo (ex: Titanic ou housing com NaNs introduzidos), Python 3+ com Pandas/NumPy.",
                                  "tips": "Use datasets públicos do Kaggle para prática realista.",
                                  "learningObjective": "Configurar ambiente e inspecionar dados iniciais para identificar problemas de missing values.",
                                  "commonMistakes": "Esquecer de importar bibliotecas ou carregar caminho incorreto do arquivo."
                                },
                                {
                                  "stepNumber": 2,
                                  "title": "Identificar e Planejar Imputação para Variáveis Numéricas",
                                  "subSteps": [
                                    "Selecione colunas numéricas com .select_dtypes(include=['number']).",
                                    "Calcule estatísticas descritivas (.describe()) para média e mediana das colunas selecionadas.",
                                    "Decida o método: média para distribuições simétricas, mediana para assimétricas ou outliers.",
                                    "Crie uma cópia do dataset original para evitar perda de dados.",
                                    "Aplique imputação: df[coluna] = df[coluna].fillna(df[coluna].mean()) ou .median()."
                                  ],
                                  "verification": "Valores ausentes em colunas numéricas reduzidos a zero via .isnull().sum().",
                                  "estimatedTime": "20 minutos",
                                  "materials": "Dataset preparado do Step 1, Pandas DataFrame.",
                                  "tips": "Visualize distribuições com histograms (matplotlib/seaborn) antes de escolher método.",
                                  "learningObjective": "Escolher e aplicar imputação adequada para dados numéricos preservando distribuição.",
                                  "commonMistakes": "Usar média em dados com outliers extremos, distorcendo os dados."
                                },
                                {
                                  "stepNumber": 3,
                                  "title": "Aplicar Imputação para Variáveis Categóricas",
                                  "subSteps": [
                                    "Selecione colunas categóricas com .select_dtypes(include=['object']).",
                                    "Calcule a moda para cada coluna: df[coluna].mode()[0].",
                                    "Aplique imputação: df[coluna] = df[coluna].fillna(df[coluna].mode()[0]).",
                                    "Verifique balanceamento de categorias pós-imputação com .value_counts().",
                                    "Registre mudanças em um log ou nova coluna para rastreabilidade."
                                  ],
                                  "verification": "Nenhum NaN em colunas categóricas e moda aplicada corretamente.",
                                  "estimatedTime": "15 minutos",
                                  "materials": "DataFrame atualizado do Step 2.",
                                  "tips": "Para múltiplas modas, use .mode()[0] para a mais frequente.",
                                  "learningObjective": "Implementar imputação por moda em dados categóricos mantendo representatividade.",
                                  "commonMistakes": "Ignorar colunas categóricas ou preencher com valor arbitrário em vez de moda."
                                },
                                {
                                  "stepNumber": 4,
                                  "title": "Verificar, Avaliar e Salvar o Dataset Imputado",
                                  "subSteps": [
                                    "Execute .isnull().sum().all() para confirmar zero missing values.",
                                    "Compare estatísticas pré e pós-imputação (.describe() para numéricas).",
                                    "Visualize mudanças com boxplots ou bar charts.",
                                    "Salve o dataset limpo como novo CSV: df.to_csv('dataset_limpo.csv').",
                                    "Documente escolhas de imputação em comentários ou relatório."
                                  ],
                                  "verification": "Dataset final sem NaNs, estatísticas plausíveis e salvo corretamente.",
                                  "estimatedTime": "10 minutos",
                                  "materials": "DataFrame final, ferramentas de visualização (opcional: matplotlib).",
                                  "tips": "Sempre compare antes/depois para validar impacto mínimo.",
                                  "learningObjective": "Garantir qualidade da imputação e preparar dados para próximas etapas de análise.",
                                  "commonMistakes": "Não verificar pós-imputação, levando a vieses não detectados."
                                }
                              ],
                              "practicalExample": "Em um dataset de vendas de imóveis (ex: housing.csv com 'preco' numérico ausente e 'bairro' categórico ausente): calcule média de 'preco' (R$500k), mediana de 'area' (100m²); moda de 'bairro' ('Centro'); aplique fillna e verifique: sem NaNs, distribuições estáveis.",
                              "finalVerifications": [
                                "Dataset final possui zero valores NaN em todas as colunas.",
                                "Estatísticas descritivas (média, mediana) alteradas minimamente (<5%).",
                                "Distribuições visuais (histograms/boxplots) semelhantes pré/pós-imputação.",
                                "Código executa sem erros e é reproduzível.",
                                "Log de mudanças documentado.",
                                "Dataset salvo e carregável corretamente."
                              ],
                              "assessmentCriteria": [
                                "Escolha correta de método (média/mediana/moda) baseada em análise de dados.",
                                "Código limpo, comentado e eficiente (O(n) tempo).",
                                "Verificação completa pré/pós-imputação.",
                                "Tratamento adequado de numéricas vs. categóricas.",
                                "Documentação de decisões e impactos.",
                                "Ausência de introdução de vieses evidentes."
                              ],
                              "crossCurricularConnections": [
                                "Estatística: Cálculo de medidas de tendência central (média, mediana, moda).",
                                "Programação: Manipulação de DataFrames em Python/Pandas.",
                                "Matemática Computacional: Pré-processamento para modelagem.",
                                "Análise de Dados: Impacto de missing data em machine learning."
                              ],
                              "realWorldApplication": "Em empresas de e-commerce, imputar preços ausentes por média para análise de tendências; em saúde, moda para categorias de sintomas em registros eletrônicos, preparando dados para previsões de vendas ou diagnósticos sem perda de amostras."
                            },
                            "estimatedTime": "2 horas",
                            "difficulty": "beginner",
                            "status": "not_started",
                            "prerequisites": []
                          },
                          {
                            "id": "10.1.4.5.1.3",
                            "name": "Usar Imputação Avançada",
                            "description": "Aplicar técnicas como KNN Imputer ou Iterative Imputer para preencher valores ausentes baseados em similaridades entre observações.",
                            "atomicExpansion": {
                              "steps": [
                                {
                                  "stepNumber": 1,
                                  "title": "Compreender os princípios da imputação avançada",
                                  "subSteps": [
                                    "Estude o KNN Imputer: baseia-se em k-vizinhos mais próximos para estimar valores ausentes usando similaridade euclidiana.",
                                    "Analise o Iterative Imputer: modela cada feature com missing values como função das outras features, iterando até convergência.",
                                    "Compare com imputação simples (média/mediana): destaque vantagens em preservar relações entre variáveis.",
                                    "Identifique cenários ideais: datasets com padrões locais de missing data e alta dimensionalidade.",
                                    "Revise métricas de avaliação como MAE ou distribuição estatística pós-imputação."
                                  ],
                                  "verification": "Resuma em um parágrafo as diferenças entre KNN e Iterative Imputer, com exemplos hipotéticos.",
                                  "estimatedTime": "30 minutos",
                                  "materials": [
                                    "Documentação scikit-learn (KNNImputer, IterativeImputer)",
                                    "Notebook Jupyter para anotações"
                                  ],
                                  "tips": "Use diagramas para visualizar similaridades entre observações; foque em quando evitar (ex.: missing >50%).",
                                  "learningObjective": "Diferenciar e justificar o uso de imputadores avançados versus simples.",
                                  "commonMistakes": [
                                    "Confundir distância euclidiana com manhattan",
                                    "Ignorar escalonamento prévio dos dados"
                                  ]
                                },
                                {
                                  "stepNumber": 2,
                                  "title": "Preparar o ambiente e o dataset",
                                  "subSteps": [
                                    "Instale bibliotecas: pip install scikit-learn pandas numpy matplotlib.",
                                    "Carregue um dataset com missing values (ex.: Titanic ou housing dataset).",
                                    "Explore dados: df.info(), df.isnull().sum(), visualizações com heatmap de missing data.",
                                    "Escalone features numéricas usando StandardScaler para melhor performance.",
                                    "Separe features com missing values e backup do dataset original."
                                  ],
                                  "verification": "Confirme que o dataset está carregado, escalonado e relatório de missing values gerado.",
                                  "estimatedTime": "20 minutos",
                                  "materials": [
                                    "Python 3.8+, Jupyter Notebook",
                                    "Datasets de exemplo (sklearn.datasets ou Kaggle)"
                                  ],
                                  "tips": "Sempre backup o original com df_original = df.copy(); use seed para reprodutibilidade.",
                                  "learningObjective": "Preparar dados adequadamente para evitar vieses na imputação.",
                                  "commonMistakes": [
                                    "Esquecer escalonamento, causando dominância de features com maior escala",
                                    "Não tratar categóricas separadamente"
                                  ]
                                },
                                {
                                  "stepNumber": 3,
                                  "title": "Implementar e aplicar KNN Imputer",
                                  "subSteps": [
                                    "Importe: from sklearn.impute import KNNImputer.",
                                    "Crie instância: imputer = KNNImputer(n_neighbors=5, weights='uniform').",
                                    "Ajuste e transforme: df_imputed = pd.DataFrame(imputer.fit_transform(df_scaled), columns=df.columns).",
                                    "Visualize resultados: compare df_original.isnull().sum() vs df_imputed.",
                                    "Avalie: plote distribuições antes/depois com histograms."
                                  ],
                                  "verification": "Execute código e confirme zero missing values; distribuições semelhantes às originais.",
                                  "estimatedTime": "25 minutos",
                                  "materials": [
                                    "Código Python em Jupyter",
                                    "scikit-learn >=1.0"
                                  ],
                                  "tips": "Teste n_neighbors=3 a 10; use 'distance' weights para vizinhos mais próximos terem mais peso.",
                                  "learningObjective": "Executar imputação KNN com parâmetros otimizados.",
                                  "commonMistakes": [
                                    "Não converter de volta para DataFrame com índices",
                                    "Aplicar em todo dataset sem split train/test"
                                  ]
                                },
                                {
                                  "stepNumber": 4,
                                  "title": "Implementar Iterative Imputer e comparar resultados",
                                  "subSteps": [
                                    "Importe: from sklearn.experimental import enable_iterative_imputer; from sklearn.impute import IterativeImputer.",
                                    "Crie instância: imputer_iter = IterativeImputer(random_state=42, max_iter=10).",
                                    "Ajuste e transforme no dataset escalonado.",
                                    "Compare com KNN: calcule MAE entre imputações ou correlações pós-imputação.",
                                    "Documente escolha final baseada em performance (ex.: menor distorção estatística)."
                                  ],
                                  "verification": "Gere tabela comparativa de métricas (missing count, mean/std por feature).",
                                  "estimatedTime": "30 minutos",
                                  "materials": [
                                    "scikit-learn experimental",
                                    "Métricas: from sklearn.metrics import mean_absolute_error"
                                  ],
                                  "tips": "Limite max_iter para evitar overfitting; use sample_posterior=False para determinístico.",
                                  "learningObjective": "Comparar múltiplos imputadores e selecionar o melhor.",
                                  "commonMistakes": [
                                    "Não habilitar experimental module",
                                    "Ignorar warnings de convergência"
                                  ]
                                },
                                {
                                  "stepNumber": 5,
                                  "title": "Validar, integrar e documentar",
                                  "subSteps": [
                                    "Valide com cross-validation: simule missing data e compare RMSE de modelo downstream.",
                                    "Integre em pipeline: from sklearn.pipeline import Pipeline; inclua imputer no fluxo.",
                                    "Gere relatório: estatísticas descritivas pré/pós-imputação.",
                                    "Teste sensibilidade: varie proporção de missing data artificial.",
                                    "Salve modelo: joblib.dump(imputer, 'imputer.pkl')."
                                  ],
                                  "verification": "Pipeline roda sem erros e relatório salvo como Markdown/PDF.",
                                  "estimatedTime": "25 minutos",
                                  "materials": [
                                    "joblib",
                                    "Pipeline scikit-learn"
                                  ],
                                  "tips": "Sempre valide com hold-out set; documente parâmetros usados para reprodutibilidade.",
                                  "learningObjective": "Incorporar imputação em workflows profissionais de ML.",
                                  "commonMistakes": [
                                    "Validar só em treino",
                                    "Não salvar estado do imputer"
                                  ]
                                }
                              ],
                              "practicalExample": "Em um dataset de imóveis (ex.: Boston Housing), onde 15% dos valores de 'número de quartos' estão ausentes, aplique KNNImputer com k=5 para preencher baseando-se em casas similares por renda e crime rate, resultando em estimativas mais precisas que a mediana simples.",
                              "finalVerifications": [
                                "Dataset final possui zero valores missing confirmados por df.isnull().sum().sum() == 0.",
                                "Distribuições estatísticas (mean, std, quartis) das features imputadas são consistentes com originais.",
                                "Comparação MAE entre KNN e Iterative < threshold definido (ex.: 0.1).",
                                "Pipeline completo treina modelo downstream (ex.: regressão) com score > baseline sem imputação.",
                                "Relatório documentado com código, plots e métricas salvas.",
                                "Teste de reprodutibilidade: re-execute com seed e resultados idênticos."
                              ],
                              "assessmentCriteria": [
                                "Implementação correta dos dois imputadores sem erros de sintaxe ou runtime.",
                                "Escolha e justificativa de parâmetros baseada em exploração de dados.",
                                "Avaliação quantitativa robusta com múltiplas métricas e visualizações.",
                                "Integração em pipeline demonstrando escalabilidade.",
                                "Documentação clara, incluindo limitações e quando não usar.",
                                "Eficiência computacional: tempo de execução razoável para dataset médio (<5min)."
                              ],
                              "crossCurricularConnections": [
                                "Estatística: Medidas de distância e similaridade (euclidiana, regressão múltipla).",
                                "Programação: Manipulação de dados com Pandas/NumPy e pipelines Scikit-learn.",
                                "Machine Learning: Pré-processamento em fluxos end-to-end.",
                                "Análise de Dados: EDA para decidir estratégias de imputação.",
                                "Matemática Computacional: Otimização iterativa e vizinhança em espaços vetoriais."
                              ],
                              "realWorldApplication": "Em bancos para imputar dados de crédito ausentes em solicitações de empréstimo, permitindo aprovações mais precisas baseadas em perfis similares de clientes, reduzindo perdas por decisões incompletas."
                            },
                            "estimatedTime": "2 horas",
                            "difficulty": "intermediate",
                            "status": "not_started",
                            "prerequisites": []
                          }
                        ]
                      },
                      {
                        "id": "10.1.4.5.2",
                        "name": "Tratamento de Dados Inconsistentes",
                        "description": "Técnicas para detectar e corrigir inconsistências como duplicatas, erros de digitação, formatos variados e valores inválidos em colunas.",
                        "specificSkills": [
                          {
                            "id": "10.1.4.5.2.1",
                            "name": "Detectar e Remover Duplicatas",
                            "description": "Identificar linhas duplicadas usando funções como duplicated() no Pandas e removê-las com drop_duplicates(), considerando índices e subconjuntos de colunas.",
                            "atomicExpansion": {
                              "steps": [
                                {
                                  "stepNumber": 1,
                                  "title": "Preparar o Ambiente e Carregar o Dataset",
                                  "subSteps": [
                                    "Instale o Pandas via pip se não estiver instalado: pip install pandas",
                                    "Importe a biblioteca: import pandas as pd",
                                    "Carregue um dataset de exemplo com duplicatas conhecidas usando pd.read_csv('dataset.csv')",
                                    "Inspecione o dataset inicial com df.shape, df.head() e df.info() para entender a estrutura",
                                    "Crie uma cópia do dataset original: df_original = df.copy()"
                                  ],
                                  "verification": "Dataset carregado corretamente, shape e head exibidos sem erros",
                                  "estimatedTime": "15 minutos",
                                  "materials": [
                                    "Jupyter Notebook ou Google Colab",
                                    "Dataset CSV com duplicatas (ex: sales_data.csv com ~100 linhas e 10% duplicatas)"
                                  ],
                                  "tips": [
                                    "Use encoding='utf-8' em read_csv para evitar erros de caracteres especiais",
                                    "Sempre trabalhe em cópias para preservar dados originais"
                                  ],
                                  "learningObjective": "Configurar ambiente Pandas e carregar dados para análise de duplicatas",
                                  "commonMistakes": [
                                    "Esquecer de importar pandas",
                                    "Caminho incorreto do arquivo CSV",
                                    "Não criar cópia do dataset original"
                                  ]
                                },
                                {
                                  "stepNumber": 2,
                                  "title": "Detectar Linhas Duplicadas",
                                  "subSteps": [
                                    "Conte duplicatas totais: duplicatas_totais = df.duplicated().sum(); print(duplicatas_totais)",
                                    "Gere máscara booleana: mask = df.duplicated(); print(mask.sum())",
                                    "Visualize linhas duplicadas: print(df[mask].head())",
                                    "Detecte duplicatas em subconjuntos de colunas: mask_subset = df.duplicated(subset=['nome', 'email']); print(mask_subset.sum())",
                                    "Identifique duplicatas considerando apenas valores exatos, ignorando NaN por padrão"
                                  ],
                                  "verification": "Número de duplicatas totais e por subset reportado e linhas duplicadas visualizadas",
                                  "estimatedTime": "20 minutos",
                                  "materials": [
                                    "Dataset carregado do Step 1",
                                    "Documentação Pandas: duplicated()"
                                  ],
                                  "tips": [
                                    "Use duplicated(keep=False) para marcar todas as ocorrências de duplicatas",
                                    "Combine com loc[] para filtrar e inspecionar"
                                  ],
                                  "learningObjective": "Aplicar duplicated() para identificar duplicatas globais e em subconjuntos",
                                  "commonMistakes": [
                                    "Confundir duplicated() com drop_duplicates()",
                                    "Ignorar subset para duplicatas parciais",
                                    "Não usar keep=False para ver todas as duplicatas"
                                  ]
                                },
                                {
                                  "stepNumber": 3,
                                  "title": "Remover Duplicatas do Dataset",
                                  "subSteps": [
                                    "Remova duplicatas básicas: df_clean = df.drop_duplicates()",
                                    "Remova com subconjunto: df_clean_subset = df.drop_duplicates(subset=['nome', 'email'])",
                                    "Especifique keep: df_clean_first = df.drop_duplicates(keep='first') ou keep='last'",
                                    "Inplace se desejado: df.drop_duplicates(inplace=True)",
                                    "Reset índice após remoção: df_clean.reset_index(drop=True, inplace=True)"
                                  ],
                                  "verification": "Novo DataFrame criado com shape reduzido e sem duplicatas via duplicated().sum() == 0",
                                  "estimatedTime": "25 minutos",
                                  "materials": [
                                    "Dataset com duplicatas do Step 2",
                                    "Documentação Pandas: drop_duplicates()"
                                  ],
                                  "tips": [
                                    "Escolha keep='first' para priorizar registros mais antigos",
                                    "Sempre reset_index após drop_duplicates para índices limpos"
                                  ],
                                  "learningObjective": "Executar drop_duplicates() com parâmetros para remoção precisa",
                                  "commonMistakes": [
                                    "Não especificar subset levando a remoções erradas",
                                    "Esquecer reset_index causando índices bagunçados",
                                    "Usar inplace=True sem backup"
                                  ]
                                },
                                {
                                  "stepNumber": 4,
                                  "title": "Validar Limpeza e Documentar Mudanças",
                                  "subSteps": [
                                    "Compare shapes: print('Antes:', df_original.shape, 'Depois:', df_clean.shape)",
                                    "Verifique duplicatas residuais: assert df_clean.duplicated().sum() == 0",
                                    "Salve dataset limpo: df_clean.to_csv('dataset_limpo.csv', index=False)",
                                    "Gere relatório: print(f'Removidas {df_original.shape[0] - df_clean.shape[0]} duplicatas')",
                                    "Teste com subset: confirme limpeza específica com duplicated(subset=['colunas'])"
                                  ],
                                  "verification": "Asserção passa, shapes comparados e arquivo salvo sem erros",
                                  "estimatedTime": "15 minutos",
                                  "materials": [
                                    "DataFrames original e limpo",
                                    "Diretório para salvar CSV"
                                  ],
                                  "tips": [
                                    "Use assert para automação de testes",
                                    "Documente parâmetros usados no relatório"
                                  ],
                                  "learningObjective": "Validar resultados da limpeza e preparar dados para uso downstream",
                                  "commonMistakes": [
                                    "Não comparar shapes antes/depois",
                                    "Salvar com index=True preservando índices ruins",
                                    "Esquecer verificação final de duplicatas"
                                  ]
                                }
                              ],
                              "practicalExample": "Em um dataset de vendas de e-commerce (sales_data.csv com colunas: id_venda, cliente_email, data_compra, valor), detecte duplicatas baseadas em 'cliente_email' e 'data_compra' (subset), visualize 5 duplicatas, remova mantendo 'first', reset_index e salve como sales_clean.csv. Resultado: de 1000 linhas para 950, sem duplicatas.",
                              "finalVerifications": [
                                "df_clean.duplicated().sum() retorna 0",
                                "Shape reduzido reflete número exato de duplicatas removidas",
                                "Duplicatas em subset específico também == 0",
                                "Índices são sequenciais de 0 a n-1 após reset_index",
                                "Dataset salvo carrega corretamente sem duplicatas",
                                "Relatório documenta mudanças (ex: 'Removidas 50 duplicatas')"
                              ],
                              "assessmentCriteria": [
                                "Uso correto de duplicated() com e sem subset",
                                "Aplicação precisa de drop_duplicates() incluindo keep e subset",
                                "Verificação antes/depois com comparações de shape e assert",
                                "Reset_index aplicado após remoção",
                                "Relatório gerado explicando ações e impactos",
                                "Tratamento de edge cases como todas linhas duplicadas"
                              ],
                              "crossCurricularConnections": [
                                "Estatística: Reduzir viés e melhorar representatividade de amostras",
                                "Programação: Similar a remoção de elementos duplicados em listas Python",
                                "Banco de Dados: Equivalente a SELECT DISTINCT ou DELETE com GROUP BY",
                                "Ética em Dados: Garantir integridade para decisões baseadas em dados",
                                "Matemática: Conceitos de conjuntos e unicidade em relações"
                              ],
                              "realWorldApplication": "Em pipelines de ETL para relatórios financeiros, remove duplicatas de transações bancárias por CPF e data para precisão em balanços; em Machine Learning, previne overfitting e viés em datasets de treinamento como imagens duplicadas em visão computacional."
                            },
                            "estimatedTime": "1 hora",
                            "difficulty": "beginner",
                            "status": "not_started",
                            "prerequisites": []
                          },
                          {
                            "id": "10.1.4.5.2.2",
                            "name": "Padronizar Formatos de Dados",
                            "description": "Corrigir inconsistências em strings (ex: maiúsculas/minúsculas, espaços extras) e datas usando str.lower(), str.strip() e pd.to_datetime().",
                            "atomicExpansion": {
                              "steps": [
                                {
                                  "stepNumber": 1,
                                  "title": "Preparar o Ambiente e Identificar Inconsistências em Strings e Datas",
                                  "subSteps": [
                                    "Importe as bibliotecas necessárias: pandas e datetime.",
                                    "Carregue o dataset de exemplo com colunas de texto e datas inconsistentes.",
                                    "Inspecione os dados usando df.head(), df.info() e df.describe() para identificar problemas como maiúsculas/minúsculas variadas, espaços extras e formatos de data mistos (ex: '2023-01-01', '01/01/2023').",
                                    "Crie uma cópia do dataset original para trabalhar sem perder os dados iniciais.",
                                    "Gere relatórios iniciais de amostras problemáticas com value_counts() ou unique()."
                                  ],
                                  "verification": "Confirme que o dataset foi carregado corretamente e inconsistências foram listadas em um relatório legível.",
                                  "estimatedTime": "20 minutos",
                                  "materials": [
                                    "Jupyter Notebook ou IDE Python",
                                    "Biblioteca pandas instalada",
                                    "Dataset CSV de exemplo com dados inconsistentes"
                                  ],
                                  "tips": "Sempre trabalhe em uma cópia do DataFrame para preservar os originais; use print() ou display() para visualizações rápidas.",
                                  "learningObjective": "Identificar e documentar tipos comuns de inconsistências em dados de texto e datas.",
                                  "commonMistakes": [
                                    "Não importar bibliotecas corretamente",
                                    "Modificar o dataset original sem backup",
                                    "Ignorar formatos de data não padronizados"
                                  ]
                                },
                                {
                                  "stepNumber": 2,
                                  "title": "Padronizar Formatos de Strings de Texto",
                                  "subSteps": [
                                    "Selecione colunas de texto com inconsistências (ex: nomes de produtos ou cidades).",
                                    "Aplique str.lower() para converter tudo para minúsculas.",
                                    "Aplique str.strip() para remover espaços extras no início e fim.",
                                    "Use str.replace() ou regex para limpezas adicionais se necessário (ex: remover acentos).",
                                    "Atribua as colunas transformadas de volta ao DataFrame e verifique com sample()."
                                  ],
                                  "verification": "Todas as strings nas colunas selecionadas devem estar em minúsculas sem espaços extras, confirmado via df[coluna].str.lower().str.strip().equals(df[coluna]).",
                                  "estimatedTime": "25 minutos",
                                  "materials": [
                                    "Pandas DataFrame carregado",
                                    "Conhecimento básico de métodos string do Pandas"
                                  ],
                                  "tips": "Encadeie métodos como df['coluna'] = df['coluna'].str.strip().str.lower() para eficiência; teste em uma amostra pequena primeiro.",
                                  "learningObjective": "Dominar a aplicação de métodos str.lower() e str.strip() para uniformizar strings.",
                                  "commonMistakes": [
                                    "Aplicar lower() em colunas numéricas",
                                    "Esquecer de lidar com NaN values",
                                    "Não verificar resultados após transformação"
                                  ]
                                },
                                {
                                  "stepNumber": 3,
                                  "title": "Padronizar Formatos de Datas",
                                  "subSteps": [
                                    "Identifique a coluna de datas com formatos mistos.",
                                    "Use pd.to_datetime() com infer_datetime_format=True ou format específico para converter strings em datetime.",
                                    "Lide com erros usando errors='coerce' para transformar inválidos em NaT.",
                                    "Padronize o formato de saída com dt.strftime('%Y-%m-%d') se necessário para strings uniformes.",
                                    "Preencha ou remova NaT se apropriado e verifique com df['data'].dtype."
                                  ],
                                  "verification": "A coluna de datas deve ter dtype 'datetime64[ns]', confirmado com df.info(), e todas as datas válidas formatadas corretamente.",
                                  "estimatedTime": "30 minutos",
                                  "materials": [
                                    "Pandas e biblioteca datetime",
                                    "Dataset com coluna de datas inconsistentes"
                                  ],
                                  "tips": "Especifique o formato exato em pd.to_datetime(format='%d/%m/%Y') para datasets grandes e evite inferência automática se possível.",
                                  "learningObjective": "Converter e padronizar colunas de datas usando pd.to_datetime() de forma robusta.",
                                  "commonMistakes": [
                                    "Não especificar formato levando a erros de parsing",
                                    "Ignorar valores NaT após conversão",
                                    "Misturar fusos horários sem padronizar"
                                  ]
                                },
                                {
                                  "stepNumber": 4,
                                  "title": "Verificar e Validar as Padronizações Aplicadas",
                                  "subSteps": [
                                    "Compare o dataset original e processado lado a lado.",
                                    "Execute testes unitários simples para cada coluna transformada (ex: assert all lowercase).",
                                    "Calcule métricas como percentual de dados limpos e perda de informação.",
                                    "Salve o dataset limpo em um novo arquivo CSV.",
                                    "Documente as mudanças em um relatório ou log."
                                  ],
                                  "verification": "Relatório final confirma 100% de padronização sem perda indesejada de dados.",
                                  "estimatedTime": "15 minutos",
                                  "materials": [
                                    "Datasets original e processado",
                                    "Funções de assert do Pandas"
                                  ],
                                  "tips": "Use pd.testing.assert_series_equal() para comparações precisas; sempre salve versões versionadas.",
                                  "learningObjective": "Validar transformações de dados para garantir integridade e qualidade.",
                                  "commonMistakes": [
                                    "Não testar edge cases como valores vazios",
                                    "Sobrescrever o original sem backup",
                                    "Ignorar discrepâncias sutis"
                                  ]
                                }
                              ],
                              "practicalExample": "Em um dataset de vendas de e-commerce com 1000 linhas: coluna 'produto' tem 'iPhone', 'iphone ', 'IPhoneX'; coluna 'data_venda' tem '2023-01-15', '15/01/2023'. Após padronização: 'produto' vira 'iphone' e 'iphonex' uniformes; 'data_venda' vira datetime padronizado em '2023-01-15'. Código: df['produto'] = df['produto'].str.strip().str.lower(); df['data_venda'] = pd.to_datetime(df['data_venda'], errors='coerce').",
                              "finalVerifications": [
                                "Todas strings em colunas texto estão em minúsculas sem espaços extras.",
                                "Coluna de datas é do tipo datetime64 sem NaT inesperados.",
                                "Número de linhas e valores não nulos preservados ou justificados.",
                                "Unique values reduzidos indicando padronização.",
                                "Comparação visual head() antes/depois sem anomalias.",
                                "Testes assert passam para todas transformações."
                              ],
                              "assessmentCriteria": [
                                "Precisão das transformações (sem erros de parsing >1%).",
                                "Eficiência do código (uso de vectorized operations).",
                                "Tratamento robusto de edge cases (NaN, inválidos).",
                                "Documentação clara de mudanças e verificações.",
                                "Qualidade final dos dados (uniformidade 100%).",
                                "Tempo de execução otimizado para datasets médios."
                              ],
                              "crossCurricularConnections": [
                                "Programação: Manipulação de strings e datas em Python.",
                                "Estatística: Preparação de dados para análises descritivas.",
                                "Banco de Dados: Normalização de campos para queries SQL.",
                                "Ética em Dados: Preservação de integridade durante limpeza."
                              ],
                              "realWorldApplication": "Em empresas de varejo, padronizar nomes de clientes e datas de transações permite análises precisas de churn e forecasting, evitando erros em relatórios que custam milhões, como no caso da Walmart otimizando supply chain com dados limpos."
                            },
                            "estimatedTime": "1.5 horas",
                            "difficulty": "beginner",
                            "status": "not_started",
                            "prerequisites": []
                          },
                          {
                            "id": "10.1.4.5.2.3",
                            "name": "Corrigir Valores Inválidos",
                            "description": "Validar e substituir valores fora de domínio esperado, como idades negativas ou categorias não listadas, usando máscaras condicionais e replace().",
                            "atomicExpansion": {
                              "steps": [
                                {
                                  "stepNumber": 1,
                                  "title": "Analisar o Dataset para Identificar Valores Potencialmente Inválidos",
                                  "subSteps": [
                                    "Carregue o dataset usando pandas.read_csv() ou similar.",
                                    "Execute df.describe() e df.info() para inspecionar estatísticas e tipos de dados.",
                                    "Visualize os dados com df.head() e df.tail() para detectar anomalias óbvias como valores negativos em campos positivos.",
                                    "Crie histogramas ou boxplots para colunas numéricas usando matplotlib ou seaborn.",
                                    "Liste valores únicos em colunas categóricas com df['coluna'].unique() para identificar categorias não esperadas."
                                  ],
                                  "verification": "Confirme que um relatório de anomalias foi gerado listando exemplos de valores inválidos potenciais.",
                                  "estimatedTime": "20 minutos",
                                  "materials": [
                                    "Pandas",
                                    "Matplotlib/Seaborn",
                                    "Notebook Jupyter"
                                  ],
                                  "tips": "Sempre comece com visualizações; elas revelam padrões que estatísticas resumidas perdem.",
                                  "learningObjective": "Desenvolver habilidade em inspeção inicial de dados para detecção de inconsistências.",
                                  "commonMistakes": [
                                    "Ignorar colunas categóricas",
                                    "Confiar apenas em describe() sem visualizações"
                                  ]
                                },
                                {
                                  "stepNumber": 2,
                                  "title": "Definir Critérios de Validade e Domínios Esperados",
                                  "subSteps": [
                                    "Liste os domínios esperados para cada coluna (ex: idade entre 0 e 120, categorias em uma lista pré-definida).",
                                    "Crie um dicionário de regras: {'idade': (0, 120), 'genero': ['M', 'F', 'Outro']}.",
                                    "Documente exceções ou valores NA aceitáveis.",
                                    "Valide regras com amostras manuais: df.sample(10).apply(regras).",
                                    "Atualize regras baseado em conhecimento de domínio do dataset."
                                  ],
                                  "verification": "Crie um DataFrame de teste com valores válidos/inválidos e confirme que as regras os classificam corretamente.",
                                  "estimatedTime": "15 minutos",
                                  "materials": [
                                    "Dicionário Python",
                                    "Pandas Series"
                                  ],
                                  "tips": "Consulte documentação de domínio ou stakeholders para regras precisas.",
                                  "learningObjective": "Estabelecer regras claras e reutilizáveis para validação de dados.",
                                  "commonMistakes": [
                                    "Regras muito rígidas ignorando variações reais",
                                    "Esquecer valores NA"
                                  ]
                                },
                                {
                                  "stepNumber": 3,
                                  "title": "Criar Máscaras Condicionais para Detectar Valores Inválidos",
                                  "subSteps": [
                                    "Para colunas numéricas: mask = (df['idade'] < 0) | (df['idade'] > 120).",
                                    "Para categóricas: mask = ~df['genero'].isin(['M', 'F', 'Outro']).",
                                    "Combine máscaras múltiplas: invalid_mask = mask_idade | mask_genero.",
                                    "Aplique máscara para visualizar inválidos: df[invalid_mask].",
                                    "Conte inválidos: invalid_mask.sum()."
                                  ],
                                  "verification": "A máscara deve selecionar exatamente os valores conhecidos como inválidos do passo 1.",
                                  "estimatedTime": "25 minutos",
                                  "materials": [
                                    "Pandas Boolean Indexing"
                                  ],
                                  "tips": "Use .loc para máscaras complexas evitando SettingWithCopyWarning.",
                                  "learningObjective": "Dominar indexação booleana para filtragem condicional eficiente.",
                                  "commonMistakes": [
                                    "Usar == em vez de .isin() para listas",
                                    "Não lidar com NaN em máscaras"
                                  ]
                                },
                                {
                                  "stepNumber": 4,
                                  "title": "Substituir Valores Inválidos Usando replace() ou Métodos Equivalentes",
                                  "subSteps": [
                                    "Para valores específicos: df.loc[invalid_mask, 'idade'] = df['idade'].median().",
                                    "Use replace() para categóricas: df['genero'] = df['genero'].replace('Invalido', 'Desconhecido').",
                                    "Para múltiplos: df = df.where(~invalid_mask, other=df.median()).",
                                    "Registre mudanças: crie coluna 'corrigido' com valores originais.",
                                    "Salve versão corrigida: df.to_csv('dataset_corrigido.csv')."
                                  ],
                                  "verification": "Reaplique máscaras do passo 3; deve retornar zero inválidos.",
                                  "estimatedTime": "30 minutos",
                                  "materials": [
                                    "Pandas .loc, .replace(), .where()"
                                  ],
                                  "tips": "Prefira imputação baseada em mediana/moda em vez de zeros para evitar viés.",
                                  "learningObjective": "Aplicar correções condicionais preservando integridade dos dados.",
                                  "commonMistakes": [
                                    "Sobrescrever sem backup",
                                    "Usar média em dados enviesados"
                                  ]
                                },
                                {
                                  "stepNumber": 5,
                                  "title": "Validar e Documentar as Correções Realizadas",
                                  "subSteps": [
                                    "Reexecute df.describe() e compare com original.",
                                    "Verifique distribuições com plots antes/depois.",
                                    "Gere relatório: número de correções por coluna.",
                                    "Teste com subset aleatório para consistência.",
                                    "Adicione comentários no código explicando escolhas de imputação."
                                  ],
                                  "verification": "Relatório confirma ausência de inválidos e mudanças lógicas.",
                                  "estimatedTime": "20 minutos",
                                  "materials": [
                                    "Pandas describe()",
                                    "Matplotlib para plots comparativos"
                                  ],
                                  "tips": "Sempre versionar datasets com git ou timestamps.",
                                  "learningObjective": "Garantir qualidade pós-correção através de validação sistemática.",
                                  "commonMistakes": [
                                    "Pular validação pós-correção",
                                    "Não documentar decisões"
                                  ]
                                }
                              ],
                              "practicalExample": "Em um dataset de pacientes hospitalares, corrija idades negativas (ex: -5) substituindo pela mediana das idades positivas, e categorias de 'genero' como 'X' ou '99' por 'Desconhecido'. Código: mask_idade = df['idade'] < 0; df.loc[mask_idade, 'idade'] = df['idade'][df['idade'] >= 0].median(); df['genero'] = df['genero'].replace({'X': 'Desconhecido', '99': 'Desconhecido'}).",
                              "finalVerifications": [
                                "Nenhum valor fora do domínio em colunas corrigidas (máscaras somam zero).",
                                "Estatísticas descritivas alteradas logicamente (ex: mínimo de idade >= 0).",
                                "Número de correções registrado e plausível (<10% do dataset).",
                                "Distribuição visual pós-correção sem outliers artificiais.",
                                "Código reproduzível gera mesmos resultados em rerun.",
                                "Backup original preservado."
                              ],
                              "assessmentCriteria": [
                                "Precisão: 100% dos inválidos detectados e corrigidos.",
                                "Eficiência: Código usa vetores pandas sem loops.",
                                "Robustez: Lida com NaN e edge cases.",
                                "Documentação: Regras e mudanças explicadas no código.",
                                "Escalabilidade: Funciona em datasets grandes (>10k linhas).",
                                "Impacto mínimo: Correções não distorcem estatísticas globais."
                              ],
                              "crossCurricularConnections": [
                                "Programação: Uso avançado de pandas e indexação booleana.",
                                "Estatística: Imputação e preservação de distribuições.",
                                "Ética em Dados: Decisões transparentes em limpeza para evitar viés.",
                                "Visualização de Dados: Plots para inspeção e validação."
                              ],
                              "realWorldApplication": "Em análise de dados de saúde pública, corrigir idades inválidas em registros de vacinação garante precisão em relatórios epidemiológicos; em e-commerce, tratar categorias de produto inconsistentes melhora recomendações de machine learning."
                            },
                            "estimatedTime": "2 horas",
                            "difficulty": "intermediate",
                            "status": "not_started",
                            "prerequisites": []
                          }
                        ]
                      },
                      {
                        "id": "10.1.4.5.3",
                        "name": "Detecção e Tratamento de Outliers",
                        "description": "Métodos estatísticos e visuais para identificar outliers e decidir sobre remoção, transformação ou imputação, preservando a integridade dos dados.",
                        "specificSkills": [
                          {
                            "id": "10.1.4.5.3.1",
                            "name": "Detectar Outliers com Estatísticas Descritivas",
                            "description": "Usar Z-score (>3 desvios padrão) e método IQR (Q1 - 1.5*IQR a Q3 + 1.5*IQR) para identificar outliers em distribuições univariadas.",
                            "atomicExpansion": {
                              "steps": [
                                {
                                  "stepNumber": 1,
                                  "title": "Revisar Estatísticas Descritivas Básicas",
                                  "subSteps": [
                                    "Calcule a média aritmética de um conjunto de dados univariado.",
                                    "Compute o desvio padrão populacional ou amostral.",
                                    "Determine os quartis Q1 (25%), Q2 (mediana, 50%) e Q3 (75%).",
                                    "Calcule o Intervalo Interquartil (IQR = Q3 - Q1).",
                                    "Defina os limites para outliers: Z-score |Z| > 3 e IQR (Q1 - 1.5*IQR, Q3 + 1.5*IQR)."
                                  ],
                                  "verification": "Explique verbalmente ou por escrito as fórmulas e thresholds com um exemplo numérico simples.",
                                  "estimatedTime": "25 minutos",
                                  "materials": [
                                    "Planilha (Excel/Google Sheets)",
                                    "Notebook para cálculos manuais",
                                    "Dataset de exemplo com 10-20 valores"
                                  ],
                                  "tips": "Use histogramas para visualizar a distribuição e ganhar intuição sobre centralidade e dispersão.",
                                  "learningObjective": "Compreender as métricas fundamentais necessárias para detecção de outliers.",
                                  "commonMistakes": [
                                    "Confundir desvio padrão com variância (dividir por N vs N-1)",
                                    "Ignorar se os dados são simétricos para escolha do método",
                                    "Calcular quartis incorretamente em datasets pequenos"
                                  ]
                                },
                                {
                                  "stepNumber": 2,
                                  "title": "Implementar Detecção de Outliers via Z-score",
                                  "subSteps": [
                                    "Padronize cada valor: Z = (x - média) / desvio_padrão.",
                                    "Identifique valores com |Z| > 3 como potenciais outliers.",
                                    "Registre os Z-scores de todos os pontos e destaque os outliers.",
                                    "Compare com visualização (scatter plot ou histograma).",
                                    "Discuta limitações: assume normalidade da distribuição."
                                  ],
                                  "verification": "Aplique a um dataset de 20 valores e liste corretamente os outliers com seus Z-scores.",
                                  "estimatedTime": "30 minutos",
                                  "materials": [
                                    "Python com NumPy/Pandas (ou planilha)",
                                    "Dataset univariado de exemplo (ex: alturas de pessoas)"
                                  ],
                                  "tips": "Sempre verifique se o desvio padrão é zero (dataset constante).",
                                  "learningObjective": "Aplicar o método Z-score para identificar desvios extremos em distribuições.",
                                  "commonMistakes": [
                                    "Usar desvio padrão amostral incorretamente",
                                    "Aplicar Z-score em dados não-numéricos ou multivariados",
                                    "Definir threshold como >2 em vez de >3"
                                  ]
                                },
                                {
                                  "stepNumber": 3,
                                  "title": "Implementar Detecção de Outliers via Método IQR",
                                  "subSteps": [
                                    "Ordene o dataset e calcule Q1, Q3 e IQR.",
                                    "Defina limites inferior (Q1 - 1.5*IQR) e superior (Q3 + 1.5*IQR).",
                                    "Identifique valores abaixo do limite inferior ou acima do superior como outliers.",
                                    "Crie um boxplot para validar visualmente.",
                                    "Compare resultados com Z-score no mesmo dataset."
                                  ],
                                  "verification": "Gere um boxplot e liste outliers coincidentes ou divergentes entre métodos.",
                                  "estimatedTime": "30 minutos",
                                  "materials": [
                                    "Python com Matplotlib/Seaborn ou Excel para boxplot",
                                    "Mesmo dataset do Step 2"
                                  ],
                                  "tips": "O método IQR é robusto a distribuições não-normais ou assimétricas.",
                                  "learningObjective": "Dominar o método não-paramétrico IQR para detecção de outliers.",
                                  "commonMistakes": [
                                    "Usar 1.0*IQR em vez de 1.5",
                                    "Não ordenar dados antes de quartis",
                                    "Ignorar outliers em datasets com poucos pontos"
                                  ]
                                },
                                {
                                  "stepNumber": 4,
                                  "title": "Comparar Métodos e Aplicar em Dataset Real",
                                  "subSteps": [
                                    "Aplique ambos métodos (Z-score e IQR) em um dataset real (ex: salários).",
                                    "Identifique outliers comuns e exclusivos de cada método.",
                                    "Decida qual método usar baseado na distribuição (normal vs skewed).",
                                    "Documente achados em relatório curto.",
                                    "Teste sensibilidade alterando o multiplicador (ex: 2.5*IQR)."
                                  ],
                                  "verification": "Produza tabela comparativa de outliers e justifique escolha do método.",
                                  "estimatedTime": "35 minutos",
                                  "materials": [
                                    "Dataset real (ex: Kaggle salaries.csv)",
                                    "Python Jupyter Notebook"
                                  ],
                                  "tips": "Sempre valide com domínio do negócio: outlier pode ser sinal importante.",
                                  "learningObjective": "Integrar ambos métodos para detecção robusta de outliers.",
                                  "commonMistakes": [
                                    "Tratar todos outliers como erros sem contexto",
                                    "Não considerar tamanho da amostra",
                                    "Sobrepor métodos sem comparação"
                                  ]
                                }
                              ],
                              "practicalExample": "Dataset de salários mensais: [2000, 2500, 3000, 3200, 3500, 4000, 15000]. Média=4717, DP≈4245. Z-scores: maioria ~ -0.6 a -0.3, 15000 tem Z≈2.4 (não >3). IQR: Q1=2500, Q3=3750, IQR=1250, limite sup=5625, inf=1250 → 15000 é outlier por IQR, mas não por Z (distribuição skewed).",
                              "finalVerifications": [
                                "Calcular corretamente Z-scores e limites IQR para dataset fornecido.",
                                "Identificar e listar outliers usando ambos métodos.",
                                "Gerar boxplot mostrando outliers.",
                                "Explicar quando preferir Z-score vs IQR.",
                                "Aplicar a um novo dataset sem erros.",
                                "Discutir impacto de remoção de outliers."
                              ],
                              "assessmentCriteria": [
                                "Precisão nos cálculos de média, DP, quartis e Z-scores (100% correto).",
                                "Correta identificação de outliers com justificativa por método.",
                                "Uso apropriado de visualizações (boxplot/histograma).",
                                "Análise comparativa clara entre métodos.",
                                "Relatório conciso com conclusões acionáveis.",
                                "Tratamento de edge cases (ex: dataset pequeno)."
                              ],
                              "crossCurricularConnections": [
                                "Programação: Implementação em Python (NumPy, Pandas, Matplotlib).",
                                "Visualização de Dados: Boxplots e histograms para validação.",
                                "Machine Learning: Pré-processamento em pipelines de dados.",
                                "Estatística Inferencial: Testes de normalidade (Shapiro-Wilk).",
                                "Análise de Dados: Exploração em ETL (Extract, Transform, Load)."
                              ],
                              "realWorldApplication": "Em bancos, detectar transações fraudulentas (valores atípicos em gastos); em saúde, identificar medidas vitais anormais para alertas médicos; em e-commerce, flutuações de vendas suspeitas para otimização de estoque."
                            },
                            "estimatedTime": "1.5 horas",
                            "difficulty": "intermediate",
                            "status": "not_started",
                            "prerequisites": []
                          },
                          {
                            "id": "10.1.4.5.3.2",
                            "name": "Visualizar Outliers",
                            "description": "Aplicar boxplots, scatter plots e histograms com Seaborn/Matplotlib para detectar outliers visualmente e confirmar detecções estatísticas.",
                            "atomicExpansion": {
                              "steps": [
                                {
                                  "stepNumber": 1,
                                  "title": "Preparar o Ambiente e Carregar Dados",
                                  "subSteps": [
                                    "Instale as bibliotecas necessárias: pandas, matplotlib e seaborn via pip.",
                                    "Importe as bibliotecas no Jupyter Notebook ou script Python.",
                                    "Carregue um dataset exemplo com colunas numéricas (ex: dataset de salários ou casas).",
                                    "Explore os dados básicos com df.describe() e df.info().",
                                    "Selecione colunas numéricas relevantes para análise de outliers."
                                  ],
                                  "verification": "Verifique se os dados foram carregados corretamente executando df.head() e df.describe(), confirmando ausência de erros e estatísticas básicas visíveis.",
                                  "estimatedTime": "20 minutos",
                                  "materials": [
                                    "Python 3.x",
                                    "Jupyter Notebook ou IDE como VS Code",
                                    "Dataset CSV exemplo (ex: Boston Housing ou salários fictícios)"
                                  ],
                                  "tips": [
                                    "Use um dataset pequeno inicialmente para testes rápidos.",
                                    "Sempre verifique o encoding do CSV ao carregar com pd.read_csv()."
                                  ],
                                  "learningObjective": "Configurar ambiente de análise de dados e preparar dataset para visualização de outliers.",
                                  "commonMistakes": [
                                    "Esquecer de importar bibliotecas levando a NameError.",
                                    "Carregar dataset com colunas não-numéricas sem filtrar.",
                                    "Ignorar valores missing antes da análise."
                                  ]
                                },
                                {
                                  "stepNumber": 2,
                                  "title": "Criar Boxplots para Detecção Inicial de Outliers",
                                  "subSteps": [
                                    "Selecione uma coluna numérica e crie um boxplot simples com sns.boxplot(data=df, x='coluna').",
                                    "Adicione títulos e labels: plt.title('Boxplot de Salários'), plt.xlabel('Salários').",
                                    "Crie boxplots múltiplos para várias colunas: sns.boxplot(data=df[['col1', 'col2']]).",
                                    "Identifique outliers visualmente como pontos além das whiskers (1.5 * IQR).",
                                    "Salve o gráfico: plt.savefig('boxplot_outliers.png')."
                                  ],
                                  "verification": "O boxplot exibe caixas com whiskers e pontos isolados fora delas; rode plt.show() para visualizar.",
                                  "estimatedTime": "25 minutos",
                                  "materials": [
                                    "Bibliotecas: seaborn, matplotlib, pandas",
                                    "Dataset preparado do Step 1"
                                  ],
                                  "tips": [
                                    "Use hue='categoria' para boxplots por grupos se aplicável.",
                                    "Ajuste figsize=(10,6) para melhor visualização."
                                  ],
                                  "learningObjective": "Usar boxplots para identificar outliers univariados de forma visual e intuitiva.",
                                  "commonMistakes": [
                                    "Confundir whiskers com outliers (whiskers vão até 1.5*IQR).",
                                    "Não rotacionar labels em boxplots múltiplos com plt.xticks(rotation=45).",
                                    "Escala inadequada ocultando outliers."
                                  ]
                                },
                                {
                                  "stepNumber": 3,
                                  "title": "Gerar Scatter Plots e Histograms para Confirmação Multivariada",
                                  "subSteps": [
                                    "Crie scatter plot bivariado: sns.scatterplot(data=df, x='col1', y='col2') para detectar outliers em relações.",
                                    "Adicione histogram univariate: sns.histplot(data=df, x='coluna', kde=True) com bins otimizados.",
                                    "Combine em subplots: fig, axes = plt.subplots(1,2); sns.boxplot(ax=axes[0]); sns.scatterplot(ax=axes[1]).",
                                    "Destaque outliers potenciais com sns.scatterplot e pontos coloridos baseados em thresholds.",
                                    "Analise padrões: caudas longas no histograma ou clusters isolados no scatter."
                                  ],
                                  "verification": "Gráficos mostram distribuições claras com outliers evidentes (ex: pontos distantes no scatter); verifique com plt.tight_layout() e show().",
                                  "estimatedTime": "30 minutos",
                                  "materials": [
                                    "Bibliotecas: seaborn, matplotlib",
                                    "Dataset do Step 1"
                                  ],
                                  "tips": [
                                    "Use kde=True no histplot para suavizar e ver densidade.",
                                    "Filtre dados grandes com sample(1000) para performance."
                                  ],
                                  "learningObjective": "Aplicar scatter plots e histograms para validar outliers em contextos univariados e bivariados.",
                                  "commonMistakes": [
                                    "Overplotting em datasets grandes sem alpha=0.5.",
                                    "Escolha errada de bins no histograma distorcendo a visualização.",
                                    "Ignorar correlações no scatter levando a falsos positivos."
                                  ]
                                },
                                {
                                  "stepNumber": 4,
                                  "title": "Confirmar Detecções Visuals com Estatísticas e Interpretar",
                                  "subSteps": [
                                    "Calcule IQR estatístico: Q1 = df['col'].quantile(0.25), Q3 = df['col'].quantile(0.75), lower = Q1 - 1.5*(Q3-Q1).",
                                    "Identifique outliers numéricos: outliers = df[(df['col'] < lower) | (df['col'] > upper)].",
                                    "Compare lista de outliers visuais vs. estatísticos e anote discrepâncias.",
                                    "Gere relatório: print(f'Outliers detectados: {len(outliers)}') e plote apenas outliers.",
                                    "Documente insights em um markdown cell ou arquivo."
                                  ],
                                  "verification": "Lista de outliers coincide com pontos visuais nos gráficos; estatísticas batem com detecções manuais.",
                                  "estimatedTime": "25 minutos",
                                  "materials": [
                                    "Pandas para cálculos",
                                    "Gráficos gerados nos steps anteriores"
                                  ],
                                  "tips": [
                                    "Use df.boxplot() nativo do pandas como alternativa rápida.",
                                    "Salve outliers em novo DF para tratamento futuro."
                                  ],
                                  "learningObjective": "Integrar visualizações com métodos estatísticos para confirmação robusta de outliers.",
                                  "commonMistakes": [
                                    "Usar threshold fixo em vez de IQR adaptativo.",
                                    "Não considerar contexto de domínio (ex: salário milionário válido).",
                                    "Confundir missing values com outliers."
                                  ]
                                }
                              ],
                              "practicalExample": "Em um dataset de salários de funcionários de uma empresa (colunas: salario, idade, anos_experiencia), use boxplot em 'salario' para detectar valores acima de R$500k (whiskers mostram limite), scatter de salario vs anos_experiencia revela 2 pontos isolados (possíveis fraudes), e histograma confirma cauda direita extrema. Confirme com IQR: 5 outliers identificados, priorizando remoção ou investigação.",
                              "finalVerifications": [
                                "Boxplots, scatter plots e histograms foram gerados sem erros e exibem outliers claros.",
                                "Outliers visuais coincidem com cálculos IQR em pelo menos 80%.",
                                "Gráficos possuem labels, títulos e são legíveis (sem sobreposição).",
                                "Relatório lista número e posições de outliers por coluna.",
                                "Código é reproduzível em novo ambiente.",
                                "Insights interpretados considerando contexto do dataset."
                              ],
                              "assessmentCriteria": [
                                "Precisão na geração de gráficos: todos os tipos solicitados corretamente implementados (boxplot, scatter, hist).",
                                "Detecção consistente: outliers identificados visual e estatisticamente alinhados.",
                                "Qualidade visual: eixos labelados, títulos descritivos, escalas adequadas.",
                                "Eficiência do código: uso otimizado de bibliotecas sem redundâncias.",
                                "Interpretação profunda: explicação de padrões e implicações dos outliers.",
                                "Documentação: comentários no código e relatório claro."
                              ],
                              "crossCurricularConnections": [
                                "Estatística: Conceitos de quartis, IQR e distribuições.",
                                "Programação: Manipulação de dados com Pandas e visualização com Matplotlib/Seaborn.",
                                "Análise de Dados: Integração de EDA (Exploratory Data Analysis) com limpeza.",
                                "Matemática Computacional: Visualização como complemento a métodos algorítmicos.",
                                "Negócios: Aplicação em detecção de anomalias em finanças ou RH."
                              ],
                              "realWorldApplication": "Em bancos, detectar outliers em transações via scatter plots de valor vs tempo para identificar fraudes; em saúde, boxplots de medidas laboratoriais flagram erros de entrada ou casos raros; em e-commerce, histograms de preços revelam listagens suspeitas para auditoria."
                            },
                            "estimatedTime": "1 hora",
                            "difficulty": "beginner",
                            "status": "not_started",
                            "prerequisites": []
                          },
                          {
                            "id": "10.1.4.5.3.3",
                            "name": "Tratar Outliers",
                            "description": "Implementar remoção, capping (winsorização), transformação logarítmica ou imputação por mediana para outliers detectados.",
                            "atomicExpansion": {
                              "steps": [
                                {
                                  "stepNumber": 1,
                                  "title": "Avaliar e Selecionar Método de Tratamento",
                                  "subSteps": [
                                    "Analise o impacto dos outliers na distribuição dos dados usando boxplots e histogramas pré e pós-tratamento.",
                                    "Considere o contexto do dataset: remoção para dados não críticos, capping para preservar tamanho da amostra.",
                                    "Calcule estatísticas descritivas (média, mediana, IQR) para decidir entre remoção, capping, log ou imputação.",
                                    "Documente a escolha do método com justificativa baseada em domínio (ex: remoção se outliers são erros de entrada).",
                                    "Crie uma máscara booleana para identificar outliers previamente detectados."
                                  ],
                                  "verification": "Método selecionado documentado em comentário no código e visualizações comparativas salvas.",
                                  "estimatedTime": "30 minutes",
                                  "materials": [
                                    "Python 3.x",
                                    "Pandas",
                                    "NumPy",
                                    "Matplotlib/Seaborn",
                                    "Jupyter Notebook"
                                  ],
                                  "tips": "Priorize métodos que preservam dados quando possível; teste múltiplos em subconjuntos pequenos.",
                                  "learningObjective": "Compreender critérios para escolher tratamento de outliers baseado em análise exploratória.",
                                  "commonMistakes": [
                                    "Ignorar contexto de domínio",
                                    "Aplicar método único sem avaliação",
                                    "Não visualizar antes/depois"
                                  ]
                                },
                                {
                                  "stepNumber": 2,
                                  "title": "Implementar Remoção de Outliers",
                                  "subSteps": [
                                    "Crie uma máscara booleana para rows com outliers (ex: df[abs(z_score) > 3]).",
                                    "Crie cópia do dataset original: df_clean = df[~mask].",
                                    "Verifique tamanho da amostra pós-remoção e compare estatísticas.",
                                    "Salve dataset limpo e plote boxplot comparativo.",
                                    "Registre percentual de dados removidos."
                                  ],
                                  "verification": "Dataset limpo tem menos de 5% de perda de dados e boxplot sem pontos extremos.",
                                  "estimatedTime": "20 minutes",
                                  "materials": [
                                    "Pandas",
                                    "NumPy",
                                    "Matplotlib"
                                  ],
                                  "tips": "Sempre trabalhe em cópias para evitar perda irreversível de dados.",
                                  "learningObjective": "Executar remoção segura de outliers preservando integridade do dataset.",
                                  "commonMistakes": [
                                    "Remover >10% sem justificativa",
                                    "Não criar backup",
                                    "Confundir máscara (~ para inverter)"
                                  ]
                                },
                                {
                                  "stepNumber": 3,
                                  "title": "Implementar Capping (Winsorização)",
                                  "subSteps": [
                                    "Use scipy.stats.mstats.winsorize ou calcule manualmente limites (percentis 1% e 99%).",
                                    "Aplique: df['coluna'] = np.clip(df['coluna'], lower_limit, upper_limit).",
                                    "Compare distribuições pré/pós com QQ-plot.",
                                    "Atualize dataset e salve versão winsorizada.",
                                    "Calcule skewness/kurtosis para validar normalização."
                                  ],
                                  "verification": "Valores extremos substituídos por limites; estatísticas mostram redução em skew.",
                                  "estimatedTime": "25 minutes",
                                  "materials": [
                                    "Pandas",
                                    "NumPy",
                                    "SciPy",
                                    "Seaborn"
                                  ],
                                  "tips": "Escolha limites assimétricos se distribuição skewed (ex: 5% low, 1% high).",
                                  "learningObjective": "Aplicar winsorização para limitar impacto de outliers sem perda de observações.",
                                  "commonMistakes": [
                                    "Clip incorreto (limites trocados)",
                                    "Não importar mstats",
                                    "Ignorar assimetria"
                                  ]
                                },
                                {
                                  "stepNumber": 4,
                                  "title": "Implementar Transformação Logarítmica",
                                  "subSteps": [
                                    "Verifique valores >0; use np.log1p para zeros.",
                                    "Aplique: df['coluna_log'] = np.log1p(df['coluna']).",
                                    "Plote histogramas pré/pós para checar normalização.",
                                    "Opcional: reverta com np.expm1 para validação.",
                                    "Atualize pipeline de features com nova coluna."
                                  ],
                                  "verification": "Distribuição pós-log mais próxima de normal (teste Shapiro-Wilk p>0.05).",
                                  "estimatedTime": "20 minutes",
                                  "materials": [
                                    "Pandas",
                                    "NumPy",
                                    "SciPy.stats"
                                  ],
                                  "tips": "Útil para dados right-skewed como salários ou preços.",
                                  "learningObjective": "Usar transformações não-lineares para mitigar skew causado por outliers.",
                                  "commonMistakes": [
                                    "Aplicar log em negativos/zeros sem ajuste",
                                    "Não testar reversibilidade",
                                    "Esquecer log1p"
                                  ]
                                },
                                {
                                  "stepNumber": 5,
                                  "title": "Implementar Imputação por Mediana",
                                  "subSteps": [
                                    "Calcule mediana da coluna sem outliers: median = df[~mask]['coluna'].median().",
                                    "Substitua: df.loc[mask, 'coluna'] = median.",
                                    "Valide com summary stats e boxplot.",
                                    "Compare variância pré/pós-imputação.",
                                    "Documente taxa de imputação."
                                  ],
                                  "verification": "Outliers substituídos por mediana; sem mudança drástica na mediana geral.",
                                  "estimatedTime": "15 minutes",
                                  "materials": [
                                    "Pandas",
                                    "NumPy"
                                  ],
                                  "tips": "Mediana robusta a outliers restantes; ideal para datasets pequenos.",
                                  "learningObjective": "Realizar imputação robusta preservando tamanho e centralidade dos dados.",
                                  "commonMistakes": [
                                    "Usar média em vez de mediana",
                                    "Imputar sem máscara precisa",
                                    "Não validar mudança"
                                  ]
                                }
                              ],
                              "practicalExample": "Em um dataset de salários de funcionários (salaries.csv), detectados outliers acima de R$50k (3 desvios). Aplique capping nos top 5%: limites = df['salary'].quantile([0.05, 0.95]); df['salary_capped'] = np.clip(df['salary'], limites[0], limites[1]). Resultado: skew reduzido de 2.5 para 0.8, melhor para modelo de regressão.",
                              "finalVerifications": [
                                "Estatísticas descritivas (média, mediana, std) estabilizadas pós-tratamento.",
                                "Boxplots e QQ-plots mostram distribuição sem extremos.",
                                "Percentual de outliers reduzido para <1%.",
                                "Teste de normalidade (Shapiro) melhora p-value.",
                                "Código reproduzível em notebook com seeds fixas.",
                                "Comparação pré/pós salva em relatórios visuais."
                              ],
                              "assessmentCriteria": [
                                "Código executa sem erros e trata todos métodos solicitados.",
                                "Escolha de método justificada com análise exploratória.",
                                "Visualizações comparativas claras e interpretáveis.",
                                "Preservação de pelo menos 95% dos dados quando possível.",
                                "Documentação completa com comentários e métricas.",
                                "Aplicação correta em dataset real/simulado."
                              ],
                              "crossCurricularConnections": [
                                "Estatística Descritiva (IQR, Z-score)",
                                "Programação em Python (Pandas/NumPy)",
                                "Visualização de Dados (Matplotlib/Seaborn)",
                                "Machine Learning (pré-processamento features)",
                                "Análise Exploratória de Dados (EDA)"
                              ],
                              "realWorldApplication": "Em análise de crédito bancário, tratar outliers em scores de risco via capping previne distorções em modelos de aprovação de empréstimos, aumentando precisão em 10-15% e reduzindo perdas financeiras."
                            },
                            "estimatedTime": "2 horas",
                            "difficulty": "intermediate",
                            "status": "not_started",
                            "prerequisites": []
                          },
                          {
                            "id": "10.1.4.5.3.4",
                            "name": "Avaliar Impacto de Outliers",
                            "description": "Comparar estatísticas descritivas e métricas de modelo antes e após tratamento para validar a eficácia da limpeza.",
                            "atomicExpansion": {
                              "steps": [
                                {
                                  "stepNumber": 1,
                                  "title": "Calcular Estatísticas Descritivas Antes do Tratamento",
                                  "subSteps": [
                                    "Carregue o dataset original utilizando Pandas.",
                                    "Identifique as colunas numéricas relevantes com dtypes ou info().",
                                    "Compute estatísticas como média, mediana, desvio padrão, quartis, min e max usando df.describe().",
                                    "Gere visualizações como boxplots e histogramas para evidenciar outliers.",
                                    "Salve as métricas pré-tratamento em um dicionário ou DataFrame para comparação futura."
                                  ],
                                  "verification": "Verifique se as métricas pré-tratamento estão salvas e as visualizações mostram outliers claramente.",
                                  "estimatedTime": "20 minutos",
                                  "materials": [
                                    "Python/Jupyter Notebook",
                                    "Bibliotecas: Pandas, Matplotlib, Seaborn"
                                  ],
                                  "tips": "Use df.describe() para um resumo rápido e salve em variável como pre_stats = df.describe().",
                                  "learningObjective": "Compreender o estado inicial dos dados e identificar influência de outliers nas estatísticas.",
                                  "commonMistakes": [
                                    "Ignorar variáveis categóricas",
                                    "Não salvar métricas para comparação",
                                    "Visualizações sem rótulos claros"
                                  ]
                                },
                                {
                                  "stepNumber": 2,
                                  "title": "Aplicar Tratamento de Outliers e Calcular Estatísticas Pós-Tratamento",
                                  "subSteps": [
                                    "Aplique o método de tratamento escolhido (remoção, imputação ou transformação) nas colunas afetadas.",
                                    "Recalcule as estatísticas descritivas no dataset tratado usando o mesmo método do passo anterior.",
                                    "Gere novas visualizações (boxplots e histogramas) para comparar visualmente.",
                                    "Salve as métricas pós-tratamento em um dicionário ou DataFrame similar ao pré-tratamento.",
                                    "Documente o método de tratamento utilizado para rastreabilidade."
                                  ],
                                  "verification": "Confirme que as métricas pós-tratamento estão salvas e visualizações mostram redução de outliers.",
                                  "estimatedTime": "25 minutos",
                                  "materials": [
                                    "Python/Jupyter Notebook",
                                    "Bibliotecas: Pandas, NumPy, Matplotlib, Seaborn"
                                  ],
                                  "tips": "Mantenha uma cópia do dataset original para evitar perda de dados: df_treated = df.copy().",
                                  "learningObjective": "Aplicar tratamento e observar mudanças imediatas nas estatísticas descritivas.",
                                  "commonMistakes": [
                                    "Alterar o dataset original sem backup",
                                    "Usar método inadequado sem justificativa",
                                    "Esquecer de recalcular todas as métricas"
                                  ]
                                },
                                {
                                  "stepNumber": 3,
                                  "title": "Comparar Estatísticas Descritivas Pré e Pós-Tratamento",
                                  "subSteps": [
                                    "Crie uma tabela de comparação lado a lado com métricas pré e pós (ex: pd.concat).",
                                    "Calcule diferenças percentuais: (pós - pré)/pré * 100 para média, mediana e desvio padrão.",
                                    "Analise mudanças qualitativas: redução na amplitude, convergência média-mediana, etc.",
                                    "Visualize comparações com gráficos de barras ou linhas para métricas chave.",
                                    "Identifique padrões como redução na variância ou estabilização de quartis."
                                  ],
                                  "verification": "Tabela e gráficos de comparação gerados, com diferenças quantificadas.",
                                  "estimatedTime": "20 minutos",
                                  "materials": [
                                    "Python/Jupyter Notebook",
                                    "Bibliotecas: Pandas, Matplotlib"
                                  ],
                                  "tips": "Use melt() do Pandas para facilitar gráficos de comparação.",
                                  "learningObjective": "Quantificar o impacto do tratamento nas estatísticas descritivas.",
                                  "commonMistakes": [
                                    "Comparar métricas erradas",
                                    "Ignorar diferenças insignificantes",
                                    "Gráficos sem escalas adequadas"
                                  ]
                                },
                                {
                                  "stepNumber": 4,
                                  "title": "Avaliar Impacto em Métricas de Modelo e Concluir Eficácia",
                                  "subSteps": [
                                    "Treine um modelo simples (ex: regressão linear) nos datasets pré e pós-tratamento.",
                                    "Calcule métricas de performance como RMSE, MAE ou R² para ambos.",
                                    "Compare as métricas de modelo e correlacione com mudanças descritivas.",
                                    "Interprete resultados: o tratamento melhorou a performance? Por quê?",
                                    "Gere um relatório resumido com conclusões sobre a eficácia da limpeza."
                                  ],
                                  "verification": "Relatório com comparações de modelo gerado e conclusões claras.",
                                  "estimatedTime": "30 minutos",
                                  "materials": [
                                    "Python/Jupyter Notebook",
                                    "Bibliotecas: Scikit-learn, Pandas"
                                  ],
                                  "tips": "Use train_test_split para validação e cross-validation para robustez.",
                                  "learningObjective": "Validar a eficácia da limpeza através de impacto em modelos preditivos.",
                                  "commonMistakes": [
                                    "Treinar modelo sem divisão treino/teste",
                                    "Atribuir causalidade sem análise",
                                    "Ignorar overfitting"
                                  ]
                                }
                              ],
                              "practicalExample": "Em um dataset de preços de casas (Boston Housing), outliers em 'preço' devido a mansões raras elevam a média para $25k. Após remoção via IQR, média cai para $22k, desvio padrão reduz 15%, e RMSE de regressão linear melhora de 5.2 para 4.1, validando eficácia.",
                              "finalVerifications": [
                                "Média e mediana convergiram (razão < 1.2).",
                                "Desvio padrão reduziu em pelo menos 10%.",
                                "Visualizações pós-tratamento sem outliers evidentes.",
                                "Métricas de modelo melhoraram (ex: RMSE menor).",
                                "Relatório documenta todas as mudanças quantitativamente.",
                                "Tratamento não removeu mais de 5% dos dados."
                              ],
                              "assessmentCriteria": [
                                "Precisão nas cálculos de métricas pré e pós (erro <1%).",
                                "Comparação quantitativa e qualitativa completa.",
                                "Interpretação correta do impacto (causalidade lógica).",
                                "Uso adequado de visualizações para suporte.",
                                "Conclusões alinhadas com evidências.",
                                "Eficiência no tempo e recursos."
                              ],
                              "crossCurricularConnections": [
                                "Estatística Descritiva (métricas centrais e dispersão)",
                                "Machine Learning (avaliação de modelos)",
                                "Programação em Python (manipulação de dados)",
                                "Visualização de Dados (gráficos comparativos)",
                                "Análise Crítica (interpretação de resultados)"
                              ],
                              "realWorldApplication": "Em análises financeiras de bancos, avaliar impacto de outliers em transações fraudulentas garante relatórios precisos, evitando decisões erradas como aprovar empréstimos baseados em médias distorcidas."
                            },
                            "estimatedTime": "1 hora",
                            "difficulty": "intermediate",
                            "status": "not_started",
                            "prerequisites": []
                          }
                        ]
                      }
                    ]
                  },
                  {
                    "id": "10.1.4.6",
                    "name": "Ajuste e Avaliação de Modelos",
                    "description": "Construção, treinamento e métricas para validar modelos preditivos, com exemplos e estudos de caso.",
                    "individualConcepts": [
                      {
                        "id": "10.1.4.6.1",
                        "name": "Construção e Treinamento de Modelos Preditivos",
                        "description": "Processo de seleção de algoritmos de aprendizado de máquina supervisionado e não-supervisionado, preparação dos dados sob a assunção de ambiente i.i.d. (independente e identicamente distribuído), divisão em conjuntos de treino e teste, e execução do treinamento inicial de modelos.",
                        "specificSkills": [
                          {
                            "id": "10.1.4.6.1.1",
                            "name": "Identificar tipos de aprendizado supervisionado e não-supervisionado",
                            "description": "Diferenciar tarefas de regressão e classificação (supervisionado) de clustering e redução de dimensionalidade (não-supervisionado), com exemplos como regressão linear e k-means.",
                            "atomicExpansion": {
                              "steps": [
                                {
                                  "stepNumber": 1,
                                  "title": "Entender os Conceitos Básicos de Aprendizado Supervisionado",
                                  "subSteps": [
                                    "Leia a definição: Aprendizado supervisionado usa dados rotulados (inputs e outputs conhecidos) para treinar modelos.",
                                    "Identifique subtarefas: Regressão (prever valores contínuos, ex: preço de casas) e Classificação (prever categorias, ex: spam ou não-spam).",
                                    "Estude exemplos clássicos: Regressão linear para previsão de vendas.",
                                    "Anote as características principais: Necessita de labels, foco em minimizar erro entre predição e real.",
                                    "Visualize diagramas de fluxo de dados supervisionados."
                                  ],
                                  "verification": "Crie um diagrama simples mostrando input rotulado -> modelo -> predição, e explique verbalmente.",
                                  "estimatedTime": "30 minutos",
                                  "materials": [
                                    "Notebook para anotações",
                                    "Artigo ou vídeo introdutório sobre ML supervisionado (ex: Khan Academy)"
                                  ],
                                  "tips": "Comece com exemplos cotidianos para fixar conceitos, como prever notas de alunos baseadas em horas de estudo.",
                                  "learningObjective": "Diferenciar supervisionado de outros tipos e listar suas subtarefas principais.",
                                  "commonMistakes": [
                                    "Confundir regressão com classificação; achar que todos os dados precisam de labels para qualquer ML."
                                  ]
                                },
                                {
                                  "stepNumber": 2,
                                  "title": "Explorar o Aprendizado Não-Supervisionado",
                                  "subSteps": [
                                    "Defina o conceito: Usa dados não rotulados para encontrar padrões intrínsecos.",
                                    "Identifique tarefas chave: Clustering (agrupar dados semelhantes, ex: k-means para clientes) e Redução de Dimensionalidade (ex: PCA para simplificar dados).",
                                    "Analise exemplos: K-means para segmentação de mercado sem labels prévios.",
                                    "Compare com supervisionado: Sem outputs conhecidos, foco em estrutura oculta.",
                                    "Pratique identificando quando usar: Dados sem labels disponíveis."
                                  ],
                                  "verification": "Liste 3 cenários onde não-supervisionado é ideal e justifique a ausência de labels.",
                                  "estimatedTime": "35 minutos",
                                  "materials": [
                                    "Acesso a tutoriais interativos (ex: Google Colab com k-means demo)",
                                    "Papel e caneta para esboços de clusters"
                                  ],
                                  "tips": "Pense em 'descoberta' vs 'ensino': Não-supervisionado descobre padrões sozinho.",
                                  "learningObjective": "Reconhecer tarefas não-supervisionadas e seus exemplos típicos.",
                                  "commonMistakes": [
                                    "Achar que clustering sempre precisa de número exato de grupos; confundir com classificação sem labels."
                                  ]
                                },
                                {
                                  "stepNumber": 3,
                                  "title": "Comparar e Contrastar os Dois Tipos de Aprendizado",
                                  "subSteps": [
                                    "Crie uma tabela comparativa: Colunas para Supervisionado vs Não-Supervisionado (dados, tarefas, exemplos, avaliação).",
                                    "Destaque diferenças: Labels presentes (sup) vs ausentes (não-sup); predição vs descoberta.",
                                    "Inclua exemplos cruzados: Regressão linear (sup) vs K-means (não-sup).",
                                    "Discuta avaliação: Métricas como MSE para regressão vs Silhouette para clustering.",
                                    "Resolva exercícios: Classifique 5 tarefas como sup ou não-sup."
                                  ],
                                  "verification": "Preencha uma tabela comparativa corretamente e explique 2 diferenças chave.",
                                  "estimatedTime": "40 minutos",
                                  "materials": [
                                    "Planilha Google Sheets ou Excel para tabela",
                                    "Lista de 10 tarefas de ML para classificar"
                                  ],
                                  "tips": "Use mnemônicos: 'Supervisado = professor com gabarito; Não-sup = aluno explorando sozinho'.",
                                  "learningObjective": "Diferenciar claramente os tipos com exemplos precisos.",
                                  "commonMistakes": [
                                    "Ignorar que semi-supervisionado existe, mas focar só nesses dois; misturar redução de dim. com clustering."
                                  ]
                                },
                                {
                                  "stepNumber": 4,
                                  "title": "Aplicar o Conhecimento em Exemplos Práticos",
                                  "subSteps": [
                                    "Escolha um dataset simples (ex: Iris para classificação sup vs clustering não-sup).",
                                    "Simule treinamento: Descreva passos para sup (treino com labels) e não-sup (agrupamento).",
                                    "Avalie resultados: Como medir sucesso em cada caso.",
                                    "Discuta trade-offs: Quando escolher um sobre o outro.",
                                    "Crie um fluxograma de decisão para selecionar tipo de aprendizado."
                                  ],
                                  "verification": "Desenvolva um fluxograma e aplique a 2 cenários reais.",
                                  "estimatedTime": "45 minutos",
                                  "materials": [
                                    "Python/Jupyter Notebook com scikit-learn",
                                    "Dataset Iris baixado"
                                  ],
                                  "tips": "Execute código simples para visualizar: fit modelo sup e kmeans no mesmo data.",
                                  "learningObjective": "Aplicar conceitos para decidir tipo de aprendizado em problemas reais.",
                                  "commonMistakes": [
                                    "Não considerar custo de labels; assumir que sup é sempre melhor."
                                  ]
                                }
                              ],
                              "practicalExample": "Em um dataset de flores Iris: Supervisionado (classificação com labels de espécies para prever nova flor); Não-supervisionado (k-means para agrupar flores por similaridades sem saber espécies prévias).",
                              "finalVerifications": [
                                "Explique verbalmente a diferença entre regressão/classificação e clustering/redução de dim.",
                                "Classifique corretamente 5 tarefas como sup ou não-sup.",
                                "Descreva exemplos: regressão linear e k-means.",
                                "Crie tabela comparativa sem erros.",
                                "Justifique escolha de tipo para um problema dado (ex: previsão de preços vs segmentação clientes).",
                                "Identifique métricas de avaliação para cada tipo."
                              ],
                              "assessmentCriteria": [
                                "Precisão nas definições de supervisionado e não-supervisionado (100% correto).",
                                "Correta distinção entre subtarefas (regressão/classificação vs clustering/redução).",
                                "Exemplos relevantes e precisos (pelo menos 2 por tipo).",
                                "Compreensão de dados rotulados vs não-rotulados.",
                                "Capacidade de aplicar em cenários novos.",
                                "Tabela ou fluxograma claro e lógico."
                              ],
                              "crossCurricularConnections": [
                                "Estatística: Análise de distribuições e métricas de erro (MSE, Silhouette).",
                                "Programação: Implementação em Python com scikit-learn.",
                                "Matemática Computacional: Otimização e vetores em regressão linear.",
                                "Negócios: Aplicações em marketing e finanças.",
                                "Visualização de Dados: Gráficos de clusters e predições."
                              ],
                              "realWorldApplication": "Em e-commerce: Supervisionado para prever churn de clientes (classificação); Não-supervisionado para segmentar usuários em grupos de comportamento (clustering) e recomendar produtos."
                            },
                            "estimatedTime": "1 hora",
                            "difficulty": "beginner",
                            "status": "not_started",
                            "prerequisites": []
                          },
                          {
                            "id": "10.1.4.6.1.2",
                            "name": "Preparar dados para treinamento",
                            "description": "Dividir dataset em treino (70-80%), validação e teste (20-30%), normalizar features e lidar com desbalanceamento, utilizando bibliotecas como scikit-learn em Python.",
                            "atomicExpansion": {
                              "steps": [
                                {
                                  "stepNumber": 1,
                                  "title": "Carregar e Explorar o Dataset",
                                  "subSteps": [
                                    "Importar bibliotecas necessárias: pandas, numpy e scikit-learn.",
                                    "Carregar o dataset de um arquivo CSV ou fonte similar.",
                                    "Inspecionar shape, tipos de dados, valores ausentes e estatísticas descritivas com describe().",
                                    "Visualizar distribuições de classes e features com histograma ou value_counts().",
                                    "Identificar features categóricas e numéricas para planejamento."
                                  ],
                                  "verification": "Relatório de exploração gerado mostrando shape, missing values = 0 ou tratados, e distribuições claras.",
                                  "estimatedTime": "20 minutos",
                                  "materials": [
                                    "Python com pandas, numpy, scikit-learn e matplotlib instalados",
                                    "Dataset exemplo (ex: breast_cancer.csv do sklearn)"
                                  ],
                                  "tips": "Use df.info() e df.describe() para visão rápida; salve gráficos para referência.",
                                  "learningObjective": "Compreender a estrutura, qualidade e características iniciais do dataset para decisões informadas.",
                                  "commonMistakes": [
                                    "Esquecer de tratar missing values antes da exploração",
                                    "Ignorar desbalanceamento inicial de classes",
                                    "Não separar features (X) e target (y) cedo"
                                  ]
                                },
                                {
                                  "stepNumber": 2,
                                  "title": "Dividir o Dataset em Conjuntos de Treino, Validação e Teste",
                                  "subSteps": [
                                    "Definir proporções: 70-80% treino, 10-15% validação, 10-15% teste.",
                                    "Usar train_test_split do sklearn para dividir X e y em treino+val e teste (test_size=0.2-0.3).",
                                    "Dividir novamente treino+val em treino e validação (test_size=0.15-0.2 do subconjunto).",
                                    "Aplicar stratify=y para manter proporções de classes.",
                                    "Verificar shapes e distribuições de classes em cada split."
                                  ],
                                  "verification": "Shapes confirmados (ex: treino 70%, val 15%, teste 15%) e distribuições estratificadas iguais ao original.",
                                  "estimatedTime": "15 minutos",
                                  "materials": [
                                    "Biblioteca scikit-learn (train_test_split)",
                                    "Código Jupyter Notebook para splits"
                                  ],
                                  "tips": "Use random_state=42 para reprodutibilidade em experimentos.",
                                  "learningObjective": "Garantir separação independente de dados para evitar vazamento e avaliação realista.",
                                  "commonMistakes": [
                                    "Não usar stratify em datasets desbalanceados",
                                    "Dividir incorretamente (ex: fit scaler no full dataset)",
                                    "Esquecer de dividir tanto X quanto y"
                                  ]
                                },
                                {
                                  "stepNumber": 3,
                                  "title": "Normalizar ou Escalar as Features",
                                  "subSteps": [
                                    "Escolher scaler apropriado: StandardScaler para distribuições gaussianas ou MinMaxScaler para [0,1].",
                                    "Fit o scaler APENAS nos dados de treino.",
                                    "Transformar treino, validação e teste com o scaler treinado.",
                                    "Verificar estatísticas pós-transformação (média ~0, std ~1 para StandardScaler).",
                                    "Visualizar distribuições antes/depois para confirmação."
                                  ],
                                  "verification": "Features normalizadas com média próxima de 0 e desvio padrão de 1 no treino; mesmas transformações aplicadas em val/teste.",
                                  "estimatedTime": "20 minutos",
                                  "materials": [
                                    "Scikit-learn: StandardScaler ou MinMaxScaler",
                                    "Gráficos com seaborn/matplotlib"
                                  ],
                                  "tips": "Nunca fit scaler em val/teste para simular dados reais não vistos.",
                                  "learningObjective": "Padronizar features para melhorar convergência e performance de modelos.",
                                  "commonMistakes": [
                                    "Fit scaler no dataset completo causando data leakage",
                                    "Aplicar scaler em features categóricas",
                                    "Ignorar outliers que afetam normalização"
                                  ]
                                },
                                {
                                  "stepNumber": 4,
                                  "title": "Lidar com Desbalanceamento de Classes",
                                  "subSteps": [
                                    "Verificar distribuição de classes em treino com value_counts().",
                                    "Escolher técnica: SMOTE para oversampling ou RandomUnderSampler para undersampling no treino.",
                                    "Aplicar resample APENAS no conjunto de treino (não em val/teste).",
                                    "Verificar nova distribuição balanceada no treino pós-resample.",
                                    "Ajustar pipeline para incluir resampling automaticamente."
                                  ],
                                  "verification": "Distribuição de classes no treino balanceada (ex: 50/50 ou próxima); val/teste inalterados.",
                                  "estimatedTime": "25 minutos",
                                  "materials": [
                                    "Imbalanced-learn: SMOTE, RandomUnderSampler",
                                    "Instalar via pip install imbalanced-learn"
                                  ],
                                  "tips": "Combine SMOTE com undersampling para datasets muito desbalanceados.",
                                  "learningObjective": "Mitigar viés em datasets desbalanceados para modelos mais justos e precisos.",
                                  "commonMistakes": [
                                    "Aplicar resampling em test set, inflando métricas",
                                    "Usar técnicas sem verificar impacto em recall/precision",
                                    "Ignorar custo computacional de SMOTE em grandes datasets"
                                  ]
                                },
                                {
                                  "stepNumber": 5,
                                  "title": "Verificar e Finalizar Preparação dos Dados",
                                  "subSteps": [
                                    "Checar shapes, tipos e ausência de NaNs em todos os splits.",
                                    "Verificar correlações e multicolinearidade entre features.",
                                    "Testar compatibilidade com um modelo simples (ex: fit LogisticRegression).",
                                    "Salvar splits preparados em arquivos pickle ou joblib.",
                                    "Documentar pipeline de preparação para reprodutibilidade."
                                  ],
                                  "verification": "Todos splits prontos para treinamento: sem erros em modelo teste, shapes consistentes, sem leakage detectado.",
                                  "estimatedTime": "15 minutos",
                                  "materials": [
                                    "Joblib ou pickle para salvar",
                                    "Scikit-learn pipeline para encapsular"
                                  ],
                                  "tips": "Crie um Pipeline com ColumnTransformer para automação.",
                                  "learningObjective": "Assegurar dados preparados de alta qualidade e prontos para treinamento de modelos.",
                                  "commonMistakes": [
                                    "Não salvar scaler/resampler para uso futuro",
                                    "Deixar features com escalas diferentes",
                                    "Vazamento sutil via agregações globais"
                                  ]
                                }
                              ],
                              "practicalExample": "No dataset Breast Cancer do sklearn: carregar dados, dividir 75%/12.5%/12.5% com stratify, aplicar StandardScaler (fit só treino), usar SMOTE no treino para balancear classes maligna/benigna (de 62%/38% para 50/50), verificar shapes (treino: 455x30 pós-SMOTE) e treinar um KNN sem erros.",
                              "finalVerifications": [
                                "Proporções de split corretas (70-80% treino, resto val/teste) com estratificação.",
                                "Features normalizadas (média=0, std=1 no treino).",
                                "Classes balanceadas apenas no treino; val/teste originais.",
                                "Sem valores missing ou infinitos em qualquer split.",
                                "Pipeline salva e reproduzível com random_state fixo.",
                                "Modelo teste treina sem erros e shapes consistentes."
                              ],
                              "assessmentCriteria": [
                                "Código executa sem erros e produz splits corretos.",
                                "Normalização aplicada corretamente sem data leakage.",
                                "Resampling balanceia treino efetivamente (métrica como balanced accuracy > baseline).",
                                "Documentação clara de escolhas e verificações.",
                                "Tempo total dentro de 1.5h; reprodutível por pares.",
                                "Uso adequado de bibliotecas (sklearn, imblearn)."
                              ],
                              "crossCurricularConnections": [
                                "Estatística: Distribuições, normalização e testes de hipóteses para validação.",
                                "Programação: Manipulação de dados em Python com pandas e pipelines.",
                                "Matemática: Álgebra linear em escalonamento e vetores de features.",
                                "Visualização: Gráficos para inspeção de dados e distribuições.",
                                "Ética em IA: Tratamento de bias em desbalanceamento para fairness."
                              ],
                              "realWorldApplication": "Em saúde, preparar dados de pacientes para detectar câncer (ex: mamografias desbalanceadas), garantindo modelos precisos que evitam falsos negativos em minorias, ou em finanças para prever fraudes raras sem oversampling no teste real."
                            },
                            "estimatedTime": "2 horas",
                            "difficulty": "intermediate",
                            "status": "not_started",
                            "prerequisites": [
                              "10.1.4.6.1.1"
                            ]
                          },
                          {
                            "id": "10.1.4.6.1.3",
                            "name": "Treinar modelos iniciais",
                            "description": "Implementar e treinar modelos simples como regressão logística ou árvores de decisão usando train_test_split e fit() do scikit-learn, interpretando saídas básicas.",
                            "atomicExpansion": {
                              "steps": [
                                {
                                  "stepNumber": 1,
                                  "title": "Preparar o ambiente e carregar o dataset",
                                  "subSteps": [
                                    "Instale as bibliotecas necessárias: scikit-learn, pandas e numpy via pip ou conda.",
                                    "Inicie um Jupyter Notebook ou script Python.",
                                    "Importe as bibliotecas: from sklearn.datasets import load_iris; import pandas as pd; from sklearn.model_selection import train_test_split.",
                                    "Carregue o dataset Iris: iris = load_iris(); X = pd.DataFrame(iris.data, columns=iris.feature_names); y = iris.target.",
                                    "Visualize os dados básicos: print(X.head()); print(y.shape)."
                                  ],
                                  "verification": "Dataset carregado com sucesso, shapes de X e y verificados (150 amostras, 4 features).",
                                  "estimatedTime": "15 minutos",
                                  "materials": [
                                    "Jupyter Notebook ou IDE Python",
                                    "scikit-learn >=1.0",
                                    "pandas",
                                    "numpy"
                                  ],
                                  "tips": "Sempre verifique a versão do scikit-learn com import sklearn; print(sklearn.__version__) para compatibilidade.",
                                  "learningObjective": "Configurar um ambiente Python pronto para treinamento de modelos de ML com dados limpos.",
                                  "commonMistakes": [
                                    "Esquecer de instalar dependências",
                                    "Confundir features (X) com target (y)",
                                    "Não converter para DataFrame para melhor visualização"
                                  ]
                                },
                                {
                                  "stepNumber": 2,
                                  "title": "Dividir os dados em conjuntos de treino e teste",
                                  "subSteps": [
                                    "Defina a proporção de divisão: tipicamente test_size=0.2 ou 20%.",
                                    "Execute train_test_split: X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42).",
                                    "Verifique os shapes: print(X_train.shape, X_test.shape, y_train.shape, y_test.shape).",
                                    "Confira a distribuição das classes: print(pd.Series(y_train).value_counts()).",
                                    "Salve as variáveis em um dicionário para organização: data_splits = {'X_train': X_train, ...}."
                                  ],
                                  "verification": "Conjuntos de treino e teste criados com shapes corretos (120 treino, 30 teste) e random_state fixo para reprodutibilidade.",
                                  "estimatedTime": "10 minutos",
                                  "materials": [
                                    "Código Python do Step 1",
                                    "sklearn.model_selection"
                                  ],
                                  "tips": "Use random_state=42 para resultados consistentes em execuções múltiplas.",
                                  "learningObjective": "Entender e aplicar divisão de dados para evitar overfitting e permitir avaliação imparcial.",
                                  "commonMistakes": [
                                    "Não usar random_state levando a resultados variáveis",
                                    "Invertir train e test",
                                    "test_size maior que 0.3 desbalanceando dados"
                                  ]
                                },
                                {
                                  "stepNumber": 3,
                                  "title": "Instanciar e treinar modelos simples",
                                  "subSteps": [
                                    "Importe os modelos: from sklearn.linear_model import LogisticRegression; from sklearn.tree import DecisionTreeClassifier.",
                                    "Instancie regressão logística: log_reg = LogisticRegression(max_iter=200, random_state=42).",
                                    "Treine o modelo: log_reg.fit(X_train, y_train).",
                                    "Repita para árvore de decisão: tree = DecisionTreeClassifier(random_state=42); tree.fit(X_train, y_train).",
                                    "Faça predições iniciais: y_pred_log = log_reg.predict(X_test); y_pred_tree = tree.predict(X_test)."
                                  ],
                                  "verification": "Modelos treinados sem erros de convergência e predições geradas para X_test.",
                                  "estimatedTime": "20 minutos",
                                  "materials": [
                                    "sklearn.linear_model",
                                    "sklearn.tree",
                                    "Dados divididos do Step 2"
                                  ],
                                  "tips": "Aumente max_iter em LogisticRegression se houver warnings de convergência.",
                                  "learningObjective": "Implementar o método fit() para treinamento e gerar predições básicas em modelos supervisionados.",
                                  "commonMistakes": [
                                    "Chamar fit() com dados errados (ex: X_test)",
                                    "Esquecer random_state",
                                    "Não ajustar hiperparâmetros básicos como max_iter"
                                  ]
                                },
                                {
                                  "stepNumber": 4,
                                  "title": "Interpretar saídas básicas dos modelos",
                                  "subSteps": [
                                    "Calcule acurácia básica: from sklearn.metrics import accuracy_score; print(accuracy_score(y_test, y_pred_log)).",
                                    "Compare scores de treino e teste: print('Train score:', log_reg.score(X_train, y_train)); print('Test score:', log_reg.score(X_test, y_test)).",
                                    "Visualize árvore de decisão: from sklearn.tree import plot_tree; plot_tree(tree, max_depth=3).",
                                    "Analise coeficientes da regressão: print(pd.DataFrame(log_reg.coef_, columns=iris.feature_names)).",
                                    "Registre insights: 'Acurácia >90% indica bom ajuste inicial; verifique overfitting se train >> test'."
                                  ],
                                  "verification": "Acurácias calculadas e interpretadas, com identificação de possível overfitting (diferença train/test <10%).",
                                  "estimatedTime": "15 minutos",
                                  "materials": [
                                    "sklearn.metrics",
                                    "sklearn.tree.plot_tree",
                                    "Modelos treinados"
                                  ],
                                  "tips": "Use score() para rapidez; accuracy_score para mais controle.",
                                  "learningObjective": "Ler e interpretar métricas básicas como acurácia e scores para validar modelos iniciais.",
                                  "commonMistakes": [
                                    "Interpretar acurácia sem contexto de baseline",
                                    "Ignorar warnings de overfitting",
                                    "Confundir coeficientes com importância de features"
                                  ]
                                }
                              ],
                              "practicalExample": "Usando o dataset Iris (150 amostras de flores com 4 features: comprimento/largura de sépala/pétala), divida em 80/20 treino/teste. Treine LogisticRegression (fit em 120 amostras) para prever espécies (setosa/versicolor/virginica), obtendo ~95% acurácia no teste. Treine DecisionTreeClassifier e plote a árvore para visualizar decisões baseadas em pétala.length > 2.45.",
                              "finalVerifications": [
                                "Código completo executa sem erros do início ao fim.",
                                "Modelos LogisticRegression e DecisionTreeClassifier treinados com fit().",
                                "Predições e acurácias calculadas para treino e teste.",
                                "Interpretação escrita de pelo menos uma métrica por modelo.",
                                "Overfitting detectado ou refutado com scores comparados.",
                                "Gráfico da árvore de decisão gerado e salvo."
                              ],
                              "assessmentCriteria": [
                                "Uso correto e completo de train_test_split com test_size=0.2 e random_state.",
                                "Chamadas adequadas de fit() apenas nos dados de treino.",
                                "Cálculo preciso de pelo menos accuracy_score ou score() em treino/teste.",
                                "Interpretação coerente das saídas (ex: features mais importantes).",
                                "Código limpo, comentado e reprodutível.",
                                "Identificação de pelo menos um erro comum evitado."
                              ],
                              "crossCurricularConnections": [
                                "Estatística: Entender probabilidades e acurácia como métricas de erro.",
                                "Programação: Manipulação de arrays NumPy/Pandas e OOP em scikit-learn.",
                                "Matemática: Álgebra linear nos coeficientes da regressão logística.",
                                "Visualização: Plots de árvores e dados para insights intuitivos."
                              ],
                              "realWorldApplication": "Em detecção de spam de e-mails (classificação binária com regressão logística), previsão de churn de clientes em bancos (árvores de decisão) ou diagnóstico inicial de doenças via sintomas, permitindo protótipos rápidos que evoluem para sistemas de produção em saúde, finanças e marketing."
                            },
                            "estimatedTime": "2 horas",
                            "difficulty": "intermediate",
                            "status": "not_started",
                            "prerequisites": [
                              "10.1.4.6.1.2"
                            ]
                          }
                        ]
                      },
                      {
                        "id": "10.1.4.6.2",
                        "name": "Ajuste de Hiperparâmetros",
                        "description": "Técnicas para otimizar parâmetros do modelo, como grid search e random search, visando maximizar performance preditiva e evitar overfitting.",
                        "specificSkills": [
                          {
                            "id": "10.1.4.6.2.1",
                            "name": "Compreender overfitting e underfitting",
                            "description": "Analisar curvas de aprendizado para detectar overfitting (alta variância) e underfitting (alto viés), utilizando validação cruzada para diagnóstico.",
                            "atomicExpansion": {
                              "steps": [
                                {
                                  "stepNumber": 1,
                                  "title": "Entender os Conceitos de Viés e Variância",
                                  "subSteps": [
                                    "Ler definições: viés é o erro sistemático devido a premissas inadequadas no modelo.",
                                    "Estudar variância: sensibilidade do modelo a variações nos dados de treinamento.",
                                    "Analisar o trade-off bias-variance: erro total = viés² + variância + ruído.",
                                    "Visualizar com exemplos gráficos de modelos simples vs. complexos.",
                                    "Explicar como alto viés leva a underfitting e alta variância a overfitting."
                                  ],
                                  "verification": "Escrever um parágrafo explicando viés e variância com exemplos.",
                                  "estimatedTime": "25 minutos",
                                  "materials": [
                                    "Notebook Jupyter",
                                    "Artigo 'Bias-Variance Tradeoff' (ex: scikit-learn docs)",
                                    "Gráficos ilustrativos online"
                                  ],
                                  "tips": [
                                    "Use analogias: viés como mirar sempre no centro do alvo, variância como tremer a mão."
                                  ],
                                  "learningObjective": "Diferenciar viés e variância e compreender seu impacto no erro de generalização.",
                                  "commonMistakes": [
                                    "Confundir viés com variância",
                                    "Ignorar o ruído irreducible nos dados"
                                  ]
                                },
                                {
                                  "stepNumber": 2,
                                  "title": "Compreender Overfitting e Underfitting",
                                  "subSteps": [
                                    "Definir overfitting: modelo se ajusta demais aos dados de treino (alta variância, baixo viés).",
                                    "Definir underfitting: modelo simples demais (alto viés, baixa variância).",
                                    "Comparar métricas: erro treino baixo + erro teste alto = overfitting.",
                                    "Estudar exemplos: polinômio grau alto (overfitting) vs. linear (underfitting).",
                                    "Discutir consequências: pobre generalização para dados novos."
                                  ],
                                  "verification": "Classificar cenários hipotéticos como over ou underfitting.",
                                  "estimatedTime": "20 minutos",
                                  "materials": [
                                    "Vídeo explicativo (ex: StatQuest no YouTube)",
                                    "Dataset simples como Iris ou Boston Housing"
                                  ],
                                  "tips": [
                                    "Lembre: 'memorizar treino sem generalizar = overfitting'."
                                  ],
                                  "learningObjective": "Identificar overfitting e underfitting com base em viés e variância.",
                                  "commonMistakes": [
                                    "Achar que overfitting sempre tem erro treino zero",
                                    "Não considerar tamanho do dataset"
                                  ]
                                },
                                {
                                  "stepNumber": 3,
                                  "title": "Analisar Curvas de Aprendizado",
                                  "subSteps": [
                                    "Gerar curvas de aprendizado: plotar erro treino e validação vs. tamanho do treino.",
                                    "Interpretar: convergência rápida treino + alta validação = underfitting.",
                                    "Identificar overfitting: gap crescente entre treino e validação.",
                                    "Implementar em Python com scikit-learn: learning_curve().",
                                    "Praticar com dataset real, variando tamanho do treino."
                                  ],
                                  "verification": "Plotar e interpretar uma curva de aprendizado de um modelo.",
                                  "estimatedTime": "40 minutos",
                                  "materials": [
                                    "Python com scikit-learn e matplotlib",
                                    "Dataset California Housing (sklearn.datasets)"
                                  ],
                                  "tips": [
                                    "Use mais pontos de amostragem para curvas suaves.",
                                    "Shuffle os dados para evitar bias."
                                  ],
                                  "learningObjective": "Diagnosticar problemas de modelo via curvas de aprendizado.",
                                  "commonMistakes": [
                                    "Não normalizar dados antes",
                                    "Interpretar gap pequeno como problema"
                                  ]
                                },
                                {
                                  "stepNumber": 4,
                                  "title": "Utilizar Validação Cruzada para Diagnóstico",
                                  "subSteps": [
                                    "Implementar K-fold cross-validation (k=5 ou 10).",
                                    "Calcular médias e desvios de scores de treino e validação.",
                                    "Comparar: variância alta em CV scores indica overfitting.",
                                    "Plotar boxplots de CV scores para visualização.",
                                    "Aplicar em modelo real e diagnosticar."
                                  ],
                                  "verification": "Executar CV e relatar diagnóstico de over/underfitting.",
                                  "estimatedTime": "35 minutos",
                                  "materials": [
                                    "Scikit-learn cross_val_score",
                                    "Mesmo dataset da etapa anterior"
                                  ],
                                  "tips": [
                                    "StratifiedKFold para datasets desbalanceados.",
                                    "Monitore tempo de computação com k alto."
                                  ],
                                  "learningObjective": "Usar validação cruzada para validar diagnósticos de curvas.",
                                  "commonMistakes": [
                                    "Usar dados de teste na CV",
                                    "Ignorar warnings de leakage"
                                  ]
                                }
                              ],
                              "practicalExample": "Treine um regressor de árvores de decisão em dados de preços de casas (California Housing). Varie max_depth de 1 a 20, plote curvas de aprendizado e CV scores. Observe: baixa profundidade mostra underfitting (erros altos e convergentes), alta profundidade mostra overfitting (gap treino-validação crescente).",
                              "finalVerifications": [
                                "Explicar corretamente viés, variância, overfitting e underfitting.",
                                "Plotar e interpretar uma curva de aprendizado com diagnóstico preciso.",
                                "Implementar validação cruzada e identificar padrões de variância.",
                                "Classificar corretamente exemplos reais de over/underfitting.",
                                "Sugerir mitigações básicas (ex: regularização para overfitting)."
                              ],
                              "assessmentCriteria": [
                                "Precisão conceitual (80%+ correto em definições).",
                                "Qualidade das curvas plotadas (legendas, eixos corretos).",
                                "Interpretação diagnóstica coerente com evidências.",
                                "Código funcional e reproduzível sem erros.",
                                "Criatividade em exemplos práticos fornecidos."
                              ],
                              "crossCurricularConnections": [
                                "Estatística: decomposição do erro quadrático médio.",
                                "Programação: manipulação de dados e visualização com Python.",
                                "Matemática Computacional: otimização e funções de perda.",
                                "Ciência de Dados: pipeline completo de modelagem."
                              ],
                              "realWorldApplication": "Em empresas como bancos ou e-commerces, diagnosticar overfitting em modelos de crédito ou recomendação previne perdas financeiras ao garantir generalização para novos clientes, evitando decisões baseadas em ruído dos dados históricos."
                            },
                            "estimatedTime": "1.5 horas",
                            "difficulty": "intermediate",
                            "status": "not_started",
                            "prerequisites": [
                              "10.1.4.6.1.3"
                            ]
                          },
                          {
                            "id": "10.1.4.6.2.2",
                            "name": "Aplicar GridSearchCV",
                            "description": "Configurar e executar GridSearchCV com cross-validation para otimizar hiperparâmetros como C em SVM ou max_depth em árvores de decisão.",
                            "atomicExpansion": {
                              "steps": [
                                {
                                  "stepNumber": 1,
                                  "title": "Preparar o Dataset e Modelo Base",
                                  "subSteps": [
                                    "Carregue um dataset adequado usando pandas ou sklearn.datasets (ex: Iris ou Wine).",
                                    "Divida o dataset em treino e teste com train_test_split (test_size=0.2, random_state=42).",
                                    "Instancie o modelo base sem hiperparâmetros otimizados (ex: SVC() ou DecisionTreeClassifier()).",
                                    "Treine o modelo base e avalie com cross_val_score para obter baseline.",
                                    "Registre o score baseline para comparação futura."
                                  ],
                                  "verification": "Verifique se o modelo base treina sem erros e o score baseline é calculado (ex: >0.8 para Iris).",
                                  "estimatedTime": "20 minutos",
                                  "materials": "Python, scikit-learn, pandas, Jupyter Notebook",
                                  "tips": "Use random_state para reprodutibilidade em todas as divisões.",
                                  "learningObjective": "Entender a importância de um baseline antes da otimização.",
                                  "commonMistakes": "Esquecer de importar bibliotecas ou usar dataset desbalanceado sem tratamento."
                                },
                                {
                                  "stepNumber": 2,
                                  "title": "Definir o Grid de Hiperparâmetros",
                                  "subSteps": [
                                    "Identifique hiperparâmetros relevantes (ex: 'C' para SVM: [0.1, 1, 10, 100]; 'max_depth' para árvore: [3,5,7,None]).",
                                    "Crie um dicionário param_grid com sklearn.model_selection.ParameterGrid ou manualmente.",
                                    "Escolha cv=5 para cross-validationStratifiedKFold se aplicável.",
                                    "Defina scoring='accuracy' ou métrica apropriada (ex: 'f1_macro' para multiclasse).",
                                    "Visualize o grid com print(len(list(ParameterGrid(param_grid)))) para confirmar tamanho."
                                  ],
                                  "verification": "Confirme que param_grid tem pelo menos 4-10 combinações únicas.",
                                  "estimatedTime": "15 minutos",
                                  "materials": "scikit-learn ParameterGrid, documentação de hiperparâmetros do modelo",
                                  "tips": "Comece com grids pequenos para evitar explosão combinatória.",
                                  "learningObjective": "Saber selecionar e parametrizar grids eficientes.",
                                  "commonMistakes": "Incluir valores extremos que causem overflow ou underflow."
                                },
                                {
                                  "stepNumber": 3,
                                  "title": "Configurar e Executar GridSearchCV",
                                  "subSteps": [
                                    "Instancie GridSearchCV(estimator=modelo_base, param_grid=param_grid, cv=5, scoring='accuracy').",
                                    "Execute grid_search.fit(X_train, y_train) e monitore o tempo de execução.",
                                    "Acesse grid_search.best_params_ e grid_search.best_score_.",
                                    "Compare best_score_ com o baseline anterior.",
                                    "Salve o melhor modelo com joblib.dump para reutilização."
                                  ],
                                  "verification": "Verifique se best_score_ > baseline e sem warnings de convergência.",
                                  "estimatedTime": "30-60 minutos (depende do grid)",
                                  "materials": "scikit-learn GridSearchCV, joblib",
                                  "tips": "Use n_jobs=-1 para paralelização em múltiplos cores.",
                                  "learningObjective": "Executar busca exaustiva com validação cruzada.",
                                  "commonMistakes": "Não usar cv adequado ou esquecer de fit no treino."
                                },
                                {
                                  "stepNumber": 4,
                                  "title": "Analisar Resultados e Avaliar no Teste",
                                  "subSteps": [
                                    "Preveja no conjunto de teste com grid_search.predict(X_test) e calcule score final.",
                                    "Plote matriz de confusão e curva de aprendizado com matplotlib/seaborn.",
                                    "Analise grid_search.cv_results_ para top 3 melhores params.",
                                    "Teste overfitting comparando train vs test scores.",
                                    "Documente insights em um relatório curto (melhor C=10, acurácia=0.95)."
                                  ],
                                  "verification": "Score no teste próximo ao best_score_ CV (diff <0.05).",
                                  "estimatedTime": "25 minutos",
                                  "materials": "matplotlib, seaborn, pandas para cv_results_",
                                  "tips": "Ordene cv_results_ por 'rank_test_score' para análise rápida.",
                                  "learningObjective": "Interpretar resultados e detectar overfitting.",
                                  "commonMistakes": "Avaliar só no treino ou ignorar std nos scores CV."
                                }
                              ],
                              "practicalExample": "No dataset Iris, use SVC com param_grid={'C': [0.1,1,10], 'kernel':['linear','rbf']}, cv=5. Baseline: 0.95 acc. Após GridSearchCV: best C=1, kernel='linear', best_score=0.97. Test acc=0.96.",
                              "finalVerifications": [
                                "GridSearchCV completa sem erros e best_params_ é acessível.",
                                "Best CV score supera baseline em pelo menos 2-5%.",
                                "Score no teste é estável (desvio <0.05 do CV).",
                                "Matriz de confusão mostra equilíbrio de classes.",
                                "Modelo salvo e carregável com joblib.",
                                "Relatório com params e scores gerado."
                              ],
                              "assessmentCriteria": [
                                "Correta definição de param_grid com variedade relevante.",
                                "Execução eficiente com cv=5 e n_jobs=-1.",
                                "Análise comparativa baseline vs otimizado.",
                                "Detecção de overfitting via train/test gap.",
                                "Visualizações claras de resultados.",
                                "Documentação de insights acionáveis."
                              ],
                              "crossCurricularConnections": [
                                "Estatística: Compreensão de variância em validação cruzada.",
                                "Programação: Manipulação avançada de dicionários e loops em Python.",
                                "Matemática: Otimização de funções custo em SVM.",
                                "Ciência de Dados: Integração com pipelines de ML.",
                                "Ética: Discussão de viés em grids desbalanceados."
                              ],
                              "realWorldApplication": "Em empresas como bancos ou saúde, otimizar hiperparâmetros de SVM para detecção de fraudes eleva acurácia de 85% para 95%, reduzindo falsos positivos e custos operacionais."
                            },
                            "estimatedTime": "2 horas",
                            "difficulty": "intermediate",
                            "status": "not_started",
                            "prerequisites": [
                              "10.1.4.6.2.1"
                            ]
                          },
                          {
                            "id": "10.1.4.6.2.3",
                            "name": "Utilizar RandomizedSearchCV",
                            "description": "Implementar busca aleatória de hiperparâmetros para eficiência computacional em espaços grandes, comparando resultados com grid search.",
                            "atomicExpansion": {
                              "steps": [
                                {
                                  "stepNumber": 1,
                                  "title": "Compreender os conceitos de RandomizedSearchCV e GridSearchCV",
                                  "subSteps": [
                                    "Ler a documentação oficial do scikit-learn sobre RandomizedSearchCV e GridSearchCV.",
                                    "Identificar diferenças: amostragem aleatória (n_iter combinações) vs busca exaustiva.",
                                    "Analisar vantagens em espaços de hiperparâmetros grandes e dimensionais.",
                                    "Estudar parâmetros chave: param_distributions, n_iter, cv, scoring.",
                                    "Exemplificar com cenários onde RandomizedSearchCV economiza tempo computacional."
                                  ],
                                  "verification": "Explicar verbalmente ou por escrito as diferenças e quando usar cada um.",
                                  "estimatedTime": "25 minutos",
                                  "materials": [
                                    "Documentação scikit-learn (RandomizedSearchCV)",
                                    "Notebook Jupyter para anotações"
                                  ],
                                  "tips": "Use diagramas para visualizar busca exaustiva vs aleatória.",
                                  "learningObjective": "Diferenciar RandomizedSearchCV de GridSearchCV e justificar sua eficiência.",
                                  "commonMistakes": [
                                    "Confundir n_iter com número total de combinações",
                                    "Ignorar impacto do tamanho do espaço de busca"
                                  ]
                                },
                                {
                                  "stepNumber": 2,
                                  "title": "Preparar o ambiente, dados e modelo base",
                                  "subSteps": [
                                    "Instalar e importar bibliotecas: scikit-learn, pandas, numpy.",
                                    "Carregar um dataset padrão (ex: Iris ou Breast Cancer).",
                                    "Dividir dados em treino/teste com train_test_split.",
                                    "Definir um modelo base (ex: RandomForestClassifier).",
                                    "Preparar dicionário de distribuições de hiperparâmetros (param_distributions)."
                                  ],
                                  "verification": "Executar código sem erros e visualizar shape dos splits de dados.",
                                  "estimatedTime": "20 minutos",
                                  "materials": [
                                    "Python 3.x",
                                    "Jupyter Notebook",
                                    "Dataset Iris do sklearn.datasets"
                                  ],
                                  "tips": "Use random_state para reprodutibilidade em todos os splits.",
                                  "learningObjective": "Configurar ambiente pronto para otimização de hiperparâmetros.",
                                  "commonMistakes": [
                                    "Não definir random_state",
                                    "Usar dataset desbalanceado sem tratamento"
                                  ]
                                },
                                {
                                  "stepNumber": 3,
                                  "title": "Implementar e executar RandomizedSearchCV",
                                  "subSteps": [
                                    "Instanciar RandomizedSearchCV com modelo, param_distributions, n_iter=20, cv=5.",
                                    "Ajustar o search com fit(X_train, y_train).",
                                    "Extrair melhores hiperparâmetros com best_params_ e melhor score com best_score_.",
                                    "Prever no conjunto de teste e calcular métricas (accuracy, f1-score).",
                                    "Registrar tempo de execução com time.time()."
                                  ],
                                  "verification": "Obter best_params_ não nulos e score > baseline do modelo sem tuning.",
                                  "estimatedTime": "30 minutos",
                                  "materials": [
                                    "Código preparado do step 2",
                                    "Jupyter Notebook"
                                  ],
                                  "tips": "Comece com n_iter baixo para testes rápidos; aumente para produção.",
                                  "learningObjective": "Executar RandomizedSearchCV corretamente e interpretar saídas iniciais.",
                                  "commonMistakes": [
                                    "Esquecer cv na instanciação",
                                    "Não usar scoring adequado ao problema"
                                  ]
                                },
                                {
                                  "stepNumber": 4,
                                  "title": "Comparar RandomizedSearchCV com GridSearchCV",
                                  "subSteps": [
                                    "Definir param_grid para GridSearchCV com subconjunto pequeno dos parâmetros.",
                                    "Instanciar e executar GridSearchCV com mesmos cv e scoring.",
                                    "Comparar tempos de execução, best_params_ e scores entre os dois.",
                                    "Visualizar resultados com DataFrame (tempos, scores, params).",
                                    "Analisar se RandomizedSearchCV aproxima resultados com menos tempo."
                                  ],
                                  "verification": "Gerar tabela comparativa mostrando RandomizedSearchCV mais rápido.",
                                  "estimatedTime": "35 minutos",
                                  "materials": [
                                    "Código do step 3",
                                    "matplotlib ou pandas para visualização"
                                  ],
                                  "tips": "Limite param_grid para evitar explosão computacional.",
                                  "learningObjective": "Demonstrar superioridade computacional do RandomizedSearchCV empiricamente.",
                                  "commonMistakes": [
                                    "Grid muito grande causando timeout",
                                    "Comparar sem mesmos cv folds"
                                  ]
                                },
                                {
                                  "stepNumber": 5,
                                  "title": "Analisar resultados e iterar melhorias",
                                  "subSteps": [
                                    "Plotar distribuição de scores por iteração (histograma).",
                                    "Testar refinamento: usar best_params_ para novo RandomizedSearchCV mais fino.",
                                    "Avaliar modelo final no test set com cross_val_score.",
                                    "Documentar insights: trade-off precisão vs tempo.",
                                    "Salvar modelo com joblib para reutilização."
                                  ],
                                  "verification": "Modelo final com score estável e documentação de insights.",
                                  "estimatedTime": "25 minutos",
                                  "materials": [
                                    "Bibliotecas: matplotlib, joblib",
                                    "Resultados anteriores"
                                  ],
                                  "tips": "Use verbose=2 para logs detalhados durante fit.",
                                  "learningObjective": "Interpretar e refinar resultados de busca aleatória.",
                                  "commonMistakes": [
                                    "Overfitting ao ignorar test set",
                                    "Não salvar artefatos"
                                  ]
                                }
                              ],
                              "practicalExample": "No dataset Breast Cancer do sklearn, use RandomForestClassifier com param_distributions={'n_estimators': [10, 50, 100], 'max_depth': [None, 10, 20], 'min_samples_split': [2, 5, 10]}. Execute RandomizedSearchCV(n_iter=30, cv=5), compare tempo e accuracy com GridSearchCV em subgrid, obtendo ~95% accuracy em 2min vs 10min do grid.",
                              "finalVerifications": [
                                "Código roda sem erros e produz best_params_ válidos.",
                                "Tempo de RandomizedSearchCV < 50% do GridSearchCV para espaço similar.",
                                "Score do modelo tunado > 5% melhor que baseline.",
                                "Tabela comparativa clara com tempos, scores e params.",
                                "Gráficos mostram convergência de scores.",
                                "Modelo salvo e testado em hold-out set."
                              ],
                              "assessmentCriteria": [
                                "Precisão na implementação (80%+ score em dataset padrão).",
                                "Eficiência demonstrada (redução temporal >30%).",
                                "Análise qualitativa das diferenças entre métodos.",
                                "Uso correto de CV e métricas apropriadas.",
                                "Documentação clara de código e resultados.",
                                "Capacidade de iterar e refinar buscas."
                              ],
                              "crossCurricularConnections": [
                                "Estatística: Amostragem aleatória e distribuições paramétricas.",
                                "Programação: Manipulação de dicionários e funções Python avançadas.",
                                "Matemática Computacional: Otimização estocástica em espaços de alta dimensão.",
                                "Ciência de Dados: Pipelines de ML e validação cruzada.",
                                "Computação: Gerenciamento de recursos computacionais em tarefas intensivas."
                              ],
                              "realWorldApplication": "Em competições Kaggle ou produção de modelos de ML (ex: detecção de fraudes em bancos ou sistemas de recomendação Netflix), onde espaços de hiperparâmetros são vastos e grid search demoraria horas/dias, RandomizedSearchCV permite tuning rápido e escalável, aproximando performance ótima com fração do custo computacional."
                            },
                            "estimatedTime": "1.5 horas",
                            "difficulty": "advanced",
                            "status": "not_started",
                            "prerequisites": [
                              "10.1.4.6.2.2"
                            ]
                          }
                        ]
                      },
                      {
                        "id": "10.1.4.6.3",
                        "name": "Avaliação de Modelos com Métricas",
                        "description": "Seleção e cálculo de métricas adequadas para validação de modelos preditivos em tarefas de classificação, regressão e clustering, incluindo validação cruzada e análise de confusão.",
                        "specificSkills": [
                          {
                            "id": "10.1.4.6.3.1",
                            "name": "Calcular métricas para classificação",
                            "description": "Computar accuracy, precision, recall, F1-score e matriz de confusão usando classification_report e confusion_matrix do scikit-learn.",
                            "atomicExpansion": {
                              "steps": [
                                {
                                  "stepNumber": 1,
                                  "title": "Preparar ambiente e dados para avaliação",
                                  "subSteps": [
                                    "Instale o scikit-learn se necessário: pip install scikit-learn",
                                    "Importe pandas, numpy e sklearn.metrics",
                                    "Carregue um dataset de classificação, como o Iris do sklearn.datasets",
                                    "Divida os dados em treino e teste usando train_test_split"
                                  ],
                                  "verification": "Confirme que os dados estão divididos corretamente (X_train, X_test, y_train, y_test) e sem erros de importação",
                                  "estimatedTime": "20 minutos",
                                  "materials": [
                                    "Python 3.x",
                                    "Jupyter Notebook ou IDE como VS Code",
                                    "Bibliotecas: scikit-learn, pandas, numpy"
                                  ],
                                  "tips": "Use random_state=42 para reprodutibilidade nos splits",
                                  "learningObjective": "Configurar corretamente o ambiente e preparar dados para predições",
                                  "commonMistakes": [
                                    "Esquecer de importar bibliotecas",
                                    "Não dividir dados em treino/teste",
                                    "Usar dados desbalanceados sem tratamento"
                                  ]
                                },
                                {
                                  "stepNumber": 2,
                                  "title": "Treinar modelo e gerar predições",
                                  "subSteps": [
                                    "Escolha um classificador simples como LogisticRegression do sklearn.linear_model",
                                    "Treine o modelo com X_train e y_train: model.fit(X_train, y_train)",
                                    "Faça predições no conjunto de teste: y_pred = model.predict(X_test)",
                                    "Verifique o shape de y_pred e y_test para compatibilidade"
                                  ],
                                  "verification": "y_pred deve ter o mesmo comprimento que y_test e valores válidos (rótulos das classes)",
                                  "estimatedTime": "15 minutos",
                                  "materials": [
                                    "Dataset Iris carregado",
                                    "Modelo LogisticRegression importado"
                                  ],
                                  "tips": "Padronize os dados com StandardScaler se necessário para melhor performance",
                                  "learningObjective": "Gerar predições confiáveis a partir de um modelo treinado",
                                  "commonMistakes": [
                                    "Treinar com dados de teste",
                                    "Não ajustar hiperparâmetros básicos",
                                    "Ignorar warnings de convergência"
                                  ]
                                },
                                {
                                  "stepNumber": 3,
                                  "title": "Calcular matriz de confusão",
                                  "subSteps": [
                                    "Importe confusion_matrix de sklearn.metrics",
                                    "Gere a matriz: cm = confusion_matrix(y_test, y_pred)",
                                    "Visualize com seaborn: sns.heatmap(cm, annot=True, fmt='d')",
                                    "Calcule métricas básicas manualmente: TP, TN, FP, FN da matriz"
                                  ],
                                  "verification": "Matriz deve ser quadrada (n_classes x n_classes) com somas corretas nas margens",
                                  "estimatedTime": "20 minutos",
                                  "materials": [
                                    "sklearn.metrics.confusion_matrix",
                                    "seaborn para visualização",
                                    "matplotlib.pyplot"
                                  ],
                                  "tips": "Use labels=classes únicas para rotular corretamente a heatmap",
                                  "learningObjective": "Entender e visualizar erros de classificação via matriz de confusão",
                                  "commonMistakes": [
                                    "Confundir y_test e y_pred na função",
                                    "Não normalizar a matriz para porcentagens",
                                    "Ignorar classes desbalanceadas"
                                  ]
                                },
                                {
                                  "stepNumber": 4,
                                  "title": "Gerar relatório de métricas com classification_report",
                                  "subSteps": [
                                    "Importe classification_report de sklearn.metrics",
                                    "Gere o relatório: report = classification_report(y_test, y_pred)",
                                    "Imprima o relatório e interprete accuracy, precision, recall, f1-score por classe",
                                    "Compare com matriz de confusão para validação cruzada"
                                  ],
                                  "verification": "Relatório mostra valores entre 0-1 para métricas e accuracy global > 0.8 no Iris",
                                  "estimatedTime": "15 minutos",
                                  "materials": [
                                    "sklearn.metrics.classification_report"
                                  ],
                                  "tips": "Use output_dict=True para acessar métricas como dicionário para análises adicionais",
                                  "learningObjective": "Computar e interpretar métricas padrão de classificação automaticamente",
                                  "commonMistakes": [
                                    "Não especificar target_names para rótulos legíveis",
                                    "Confundir macro vs weighted averages",
                                    "Ignorar support para classes desbalanceadas"
                                  ]
                                },
                                {
                                  "stepNumber": 5,
                                  "title": "Interpretar e salvar resultados",
                                  "subSteps": [
                                    "Extraia métricas chave: accuracy = report['accuracy']",
                                    "Crie um DataFrame com métricas por classe para exportação",
                                    "Salve cm e report em arquivos: pd.DataFrame(cm).to_csv('confusion_matrix.csv')",
                                    "Discuta implicações: alta precision baixa recall significa?"
                                  ],
                                  "verification": "Arquivos salvos corretamente e interpretação escrita em um markdown ou print",
                                  "estimatedTime": "10 minutos",
                                  "materials": [
                                    "pandas para DataFrame e CSV"
                                  ],
                                  "tips": "Sempre relate support para contextualizar métricas",
                                  "learningObjective": "Aplicar métricas em contexto e documentar para relatórios",
                                  "commonMistakes": [
                                    "Focar só em accuracy ignorando precision/recall",
                                    "Não salvar resultados para reutilização",
                                    "Métricas erradas por labels inconsistentes"
                                  ]
                                }
                              ],
                              "practicalExample": "Usando o dataset Iris: treine LogisticRegression, prediga classes de flores, gere matriz de confusão mostrando 2 falsos positivos em setosa/versicolor, e classification_report com accuracy=0.95, precision=0.96 por classe.",
                              "finalVerifications": [
                                "Código roda sem erros e produz matriz de confusão visualizável",
                                "Classification_report imprime com todas métricas (accuracy, precision, recall, F1, support)",
                                "Métricas manuais da CM batem com o report",
                                "Interpretação correta: ex. 'Alta recall em classe X significa poucos falsos negativos'",
                                "Resultados salvos em CSV/JSON para inspeção",
                                "Teste com dataset desbalanceado mostra impacto em métricas weighted"
                              ],
                              "assessmentCriteria": [
                                "Precisão nas importações e uso correto de funções (100%)",
                                "Cálculo exato de CM e report com valores válidos (>90% accuracy em Iris)",
                                "Visualização clara da CM com anotações",
                                "Interpretação qualitativa das métricas em 1-2 frases por métrica",
                                "Eficiência: código modular e comentado",
                                "Tratamento de edge cases como classes únicas ou desbalanceadas"
                              ],
                              "crossCurricularConnections": [
                                "Estatística: Entender TP/FP/TN/FN como base para testes de hipótese",
                                "Programação: Manipulação de arrays NumPy e DataFrames Pandas",
                                "Matemática: Cálculos de médias harmônicas (F1-score) e proporções",
                                "Visualização de Dados: Uso de heatmaps Seaborn/Matplotlib",
                                "Ética em IA: Discussão de vieses em métricas para classes minoritárias"
                              ],
                              "realWorldApplication": "Em detecção de fraudes bancárias, calcular precision alta evita falsos alarmes caros; em diagnósticos médicos, recall alto minimiza falsos negativos salvando vidas; empresas usam para avaliar chatbots (accuracy em intenções) ou recomendadores (F1 em categorias)."
                            },
                            "estimatedTime": "2 horas",
                            "difficulty": "intermediate",
                            "status": "not_started",
                            "prerequisites": [
                              "10.1.4.6.2.3"
                            ]
                          },
                          {
                            "id": "10.1.4.6.3.2",
                            "name": "Aplicar métricas para regressão",
                            "description": "Avaliar modelos com MSE, RMSE, MAE e R², interpretando erros residuais e comparando baselines.",
                            "atomicExpansion": {
                              "steps": [
                                {
                                  "stepNumber": 1,
                                  "title": "Compreender e Calcular Métricas de Erro: MSE, RMSE e MAE",
                                  "subSteps": [
                                    "Defina MSE como a média dos quadrados dos resíduos (y_true - y_pred)^2",
                                    "Calcule MSE manualmente para um conjunto pequeno de 5-10 previsões",
                                    "Implemente MSE usando NumPy em um dataset real",
                                    "Calcule RMSE como a raiz quadrada do MSE para escalar ao mesmo nível dos dados",
                                    "Calcule MAE como a média dos valores absolutos dos resíduos para medir erro médio"
                                  ],
                                  "verification": "Execute código que retorna MSE=2.5, RMSE≈1.58 e MAE=1.2 para um conjunto de teste conhecido com resíduos [1,2,1.5,-1,-2].",
                                  "estimatedTime": "1 hora",
                                  "materials": [
                                    "Python 3.x",
                                    "NumPy",
                                    "Jupyter Notebook",
                                    "Dataset de exemplo (e.g., Boston Housing do sklearn)"
                                  ],
                                  "tips": "Use np.mean() para médias e np.sqrt() para RMSE; sempre verifique unidades dos dados.",
                                  "learningObjective": "Dominar o cálculo preciso de métricas de erro absoluto e quadrático.",
                                  "commonMistakes": [
                                    "Esquecer de dividir pelo número de amostras em MSE",
                                    "Não tirar raiz quadrada para RMSE",
                                    "Confundir MAE com mediana dos erros"
                                  ]
                                },
                                {
                                  "stepNumber": 2,
                                  "title": "Calcular e Interpretar R² (Coeficiente de Determinação)",
                                  "subSteps": [
                                    "Explique R² como 1 - (SS_res / SS_tot), onde SS_res é soma de quadrados residuais e SS_tot é soma total",
                                    "Calcule R² manualmente comparando variância explicada vs. total",
                                    "Implemente R² usando sklearn.metrics.r2_score()",
                                    "Interprete valores: R²=1 (perfeito), R²=0 (baseline de média), R²<0 (pior que baseline)",
                                    "Compare R² ajustado para penalizar complexidade em múltiplas features"
                                  ],
                                  "verification": "Para um modelo com SS_res=50 e SS_tot=100, confirme R²=0.5; teste com sklearn.",
                                  "estimatedTime": "45 minutos",
                                  "materials": [
                                    "Python",
                                    "scikit-learn",
                                    "NumPy",
                                    "Dataset com y_true e y_pred"
                                  ],
                                  "tips": "R² alto não garante causalidade; sempre cheque resíduos.",
                                  "learningObjective": "Avaliar quão bem o modelo explica a variância dos dados.",
                                  "commonMistakes": [
                                    "Interpretar R² como proporção de correlação",
                                    "Ignorar R² negativo como sinal de mau modelo",
                                    "Usar R² sem ajustar para número de features"
                                  ]
                                },
                                {
                                  "stepNumber": 3,
                                  "title": "Interpretar Erros Residuais",
                                  "subSteps": [
                                    "Gere resíduos como y_true - y_pred e plote histograma para normalidade",
                                    "Crie scatter plot de resíduos vs. predições para detectar heteroscedasticidade",
                                    "Verifique independência com Durbin-Watson ou plot ACF",
                                    "Identifique outliers usando Q-Q plot ou boxplot de resíduos",
                                    "Ajuste modelo se resíduos mostrarem padrões (e.g., não linearidade)"
                                  ],
                                  "verification": "Analise plot de resíduos sem padrões sistemáticos e confirme normalidade via Shapiro-Wilk test (p>0.05).",
                                  "estimatedTime": "1 hora",
                                  "materials": [
                                    "Python",
                                    "Matplotlib/Seaborn",
                                    "SciPy para testes",
                                    "Pandas"
                                  ],
                                  "tips": "Resíduos ideais: aleatórios, média zero, variância constante.",
                                  "learningObjective": "Diagnosticar problemas no modelo via análise de resíduos.",
                                  "commonMistakes": [
                                    "Assumir resíduos normais sem teste",
                                    "Ignorar heteroscedasticidade em regressões simples",
                                    "Não escalar resíduos antes de plots"
                                  ]
                                },
                                {
                                  "stepNumber": 4,
                                  "title": "Comparar Modelo com Baselines e Avaliar Holisticamente",
                                  "subSteps": [
                                    "Calcule baseline simples: previsão constante = média(y_train)",
                                    "Compare métricas do modelo vs. baseline (e.g., RMSE_model < RMSE_baseline)",
                                    "Use validação cruzada (cross_val_score) para métricas robustas",
                                    "Crie tabela comparativa de todas métricas (MSE, RMSE, MAE, R²)",
                                    "Decida se o modelo é viável baseado em thresholds (e.g., R²>0.7, RMSE<10% de escala y)"
                                  ],
                                  "verification": "Mostre tabela onde modelo supera baseline em todas métricas e R²>0.6.",
                                  "estimatedTime": "1 hora",
                                  "materials": [
                                    "scikit-learn (mean_squared_error, r2_score, cross_val_score)",
                                    "Pandas para tabelas",
                                    "Dataset split train/test"
                                  ],
                                  "tips": "Sempre use train/test split para evitar overfitting.",
                                  "learningObjective": "Selecionar modelos superiores via comparação quantitativa.",
                                  "commonMistakes": [
                                    "Comparar apenas com zero, não com média",
                                    "Usar train set para baselines",
                                    "Ignorar custo computacional em baselines complexas"
                                  ]
                                }
                              ],
                              "practicalExample": "No dataset California Housing (sklearn), treine regressão linear para prever preço mediano de casas (MEDV). Calcule MSE=25.3, RMSE=5.03, MAE=3.8, R²=0.62. Plote resíduos sem padrões, confirme RMSE < baseline (média=4.5k USD, RMSE_baseline=6.8k).",
                              "finalVerifications": [
                                "Calcula corretamente MSE, RMSE, MAE e R² para modelo dado.",
                                "Interpreta R²=0.75 como 'modelo explica 75% da variância'.",
                                "Identifica heteroscedasticidade em plot de resíduos.",
                                "Compara RMSE_modelo (4.2) < RMSE_baseline (5.1).",
                                "Gera relatório com tabela de métricas e plots.",
                                "Aplica cross-validation com média R²=0.60±0.05."
                              ],
                              "assessmentCriteria": [
                                "Precisão nos cálculos (MSE erro <1%)",
                                "Interpretação correta de métricas (R², baselines)",
                                "Qualidade dos plots de resíduos (sem erros de eixo/label)",
                                "Comparação robusta com baselines e CV",
                                "Relatório claro com conclusões acionáveis",
                                "Tempo e eficiência no código"
                              ],
                              "crossCurricularConnections": [
                                "Estatística: Testes de normalidade e variância de resíduos",
                                "Programação: Manipulação de dados com Pandas/NumPy",
                                "Matemática Computacional: Álgebra linear em regressão",
                                "Ciência de Dados: Pipelines de ML com scikit-learn",
                                "Análise de Dados: Visualização com Matplotlib/Seaborn"
                              ],
                              "realWorldApplication": "Em finanças, prever preços de ações com regressão linear, usando RMSE para medir risco de erro em milhões de dólares, comparando com baseline histórica para decisões de investimento."
                            },
                            "estimatedTime": "1.5 horas",
                            "difficulty": "intermediate",
                            "status": "not_started",
                            "prerequisites": [
                              "10.1.4.6.3.1"
                            ]
                          },
                          {
                            "id": "10.1.4.6.3.3",
                            "name": "Executar validação cruzada",
                            "description": "Implementar K-Fold Cross-Validation com cross_val_score para estimar performance robusta e selecionar melhores modelos.",
                            "atomicExpansion": {
                              "steps": [
                                {
                                  "stepNumber": 1,
                                  "title": "Preparar o ambiente e os dados para validação cruzada",
                                  "subSteps": [
                                    "Instale as bibliotecas necessárias: scikit-learn, pandas e numpy via pip.",
                                    "Carregue um dataset relevante, como o Boston Housing do sklearn.datasets.",
                                    "Separe as features (X) e o target (y), e divida em treino/teste inicial com train_test_split.",
                                    "Verifique a forma dos dados e estatísticas descritivas com .describe().",
                                    "Padronize os dados se necessário usando StandardScaler."
                                  ],
                                  "verification": "Execute o código e confirme que X_train, X_test, y_train, y_test estão definidos sem erros e com dimensões corretas.",
                                  "estimatedTime": "20 minutos",
                                  "materials": [
                                    "Python 3.x",
                                    "scikit-learn",
                                    "pandas",
                                    "numpy",
                                    "Jupyter Notebook ou IDE como VS Code"
                                  ],
                                  "tips": [
                                    "Use random_state=42 para reprodutibilidade.",
                                    "Sempre verifique missing values antes de prosseguir."
                                  ],
                                  "learningObjective": "Configurar um ambiente Python pronto para validação cruzada com dados preparados adequadamente.",
                                  "commonMistakes": [
                                    "Esquecer de importar train_test_split.",
                                    "Não padronizar features numéricas.",
                                    "Confundir X e y na separação."
                                  ]
                                },
                                {
                                  "stepNumber": 2,
                                  "title": "Selecionar e instanciar o modelo para avaliação",
                                  "subSteps": [
                                    "Importe o modelo desejado, ex: from sklearn.linear_model import LinearRegression ou RandomForestRegressor.",
                                    "Defina o número de folds K (ex: K=5 ou 10) e a métrica de scoring (ex: 'r2' para regressão, 'accuracy' para classificação).",
                                    "Instancie o modelo sem fit inicial: model = LinearRegression().",
                                    "Opcionalmente, defina hiperparâmetros iniciais para o modelo.",
                                    "Confirme a compatibilidade do modelo com cross_val_score."
                                  ],
                                  "verification": "Inspecione o objeto do modelo com print(model) e verifique se os parâmetros estão corretos.",
                                  "estimatedTime": "15 minutos",
                                  "materials": [
                                    "Documentação scikit-learn para modelos",
                                    "Referência de métricas de scoring"
                                  ],
                                  "tips": [
                                    "Comece com K=5 para equilíbrio entre viés e variância.",
                                    "Use 'neg_mean_squared_error' para regressão se quiser penalizar erros maiores."
                                  ],
                                  "learningObjective": "Escolher e configurar um modelo scikit-learn adequado para validação cruzada.",
                                  "commonMistakes": [
                                    "Fit o modelo antes da CV.",
                                    "Escolher métrica incompatível com o tipo de problema.",
                                    "Ignorar hiperparâmetros que afetam a performance."
                                  ]
                                },
                                {
                                  "stepNumber": 3,
                                  "title": "Executar K-Fold Cross-Validation com cross_val_score",
                                  "subSteps": [
                                    "Importe cross_val_score: from sklearn.model_selection import cross_val_score.",
                                    "Execute: scores = cross_val_score(model, X, y, cv=5, scoring='r2').",
                                    "Calcule estatísticas: media = scores.mean(), std = scores.std().",
                                    "Registre os scores individuais em uma lista ou array.",
                                    "Repita para múltiplos modelos se necessário, armazenando em um dicionário."
                                  ],
                                  "verification": "Print(scores) deve retornar um array de 5 valores (para K=5) sem warnings ou erros.",
                                  "estimatedTime": "25 minutos",
                                  "materials": [
                                    "sklearn.model_selection.cross_val_score docs",
                                    "Dataset preparado do Step 1"
                                  ],
                                  "tips": [
                                    "Use cv=KFold(n_splits=5, shuffle=True, random_state=42) para mais controle.",
                                    "Aumente K para datasets maiores."
                                  ],
                                  "learningObjective": "Implementar corretamente cross_val_score para obter estimativas robustas de performance.",
                                  "commonMistakes": [
                                    "Passar X_train em vez de X total.",
                                    "Esquecer shuffle=True em dados não ordenados.",
                                    "Interpretar scores negativos incorretamente."
                                  ]
                                },
                                {
                                  "stepNumber": 4,
                                  "title": "Analisar resultados e selecionar o melhor modelo",
                                  "subSteps": [
                                    "Calcule e plote os scores com matplotlib: plt.boxplot(scores).",
                                    "Compare médias e desvios padrão entre modelos.",
                                    "Identifique o modelo com maior média e menor std como o melhor candidato.",
                                    "Salve resultados em um DataFrame para relatório.",
                                    "Decida próximos passos: tuning de hiperparâmetros ou teste final."
                                  ],
                                  "verification": "Gere um gráfico de boxplot e confirme que a média dos scores está acima de um threshold razoável (ex: 0.7 para R2).",
                                  "estimatedTime": "20 minutos",
                                  "materials": [
                                    "matplotlib",
                                    "pandas para DataFrames"
                                  ],
                                  "tips": [
                                    "Priorize modelos com std < 0.05 para estabilidade.",
                                    "Registre tudo em um notebook para reprodutibilidade."
                                  ],
                                  "learningObjective": "Interpretar resultados de CV para tomada de decisão em modelagem.",
                                  "commonMistakes": [
                                    "Ignorar variância alta.",
                                    "Comparar modelos sem normalização.",
                                    "Selecionar baseado apenas na média."
                                  ]
                                }
                              ],
                              "practicalExample": "No dataset California Housing (fetch_california_housing), use LinearRegression e RandomForestRegressor com 5-fold CV e scoring='neg_mean_squared_error'. Compare as médias: o RandomForest deve ter MSE menor (~0.35 vs 0.5), indicando melhor performance geral.",
                              "finalVerifications": [
                                "cross_val_score executado sem erros para pelo menos dois modelos.",
                                "Média e desvio padrão dos scores calculados e reportados.",
                                "Boxplot ou tabela comparativa dos resultados gerada.",
                                "Modelo selecionado com justificativa baseada em métricas.",
                                "Código completo rodável e reprodutível com random_state.",
                                "Interpretação escrita dos resultados (viés/variância)."
                              ],
                              "assessmentCriteria": [
                                "Precisão na implementação de cross_val_score (cv, scoring corretos).",
                                "Qualidade da análise estatística (média, std, intervalos).",
                                "Uso apropriado de visualizações para insights.",
                                "Seleção de modelo justificada e robusta.",
                                "Código limpo, comentado e modular.",
                                "Tratamento de edge cases como dados desbalanceados."
                              ],
                              "crossCurricularConnections": [
                                "Estatística: Conceitos de variância, viés e intervalos de confiança.",
                                "Programação: Manipulação de arrays NumPy e DataFrames Pandas.",
                                "Matemática: Métricas de erro (MSE, R2) e otimização.",
                                "Ciência de Dados: Pipeline completo de ML.",
                                "Visualização: Gráficos com Matplotlib/Seaborn."
                              ],
                              "realWorldApplication": "Em pipelines de ML para previsões financeiras (ex: risco de crédito no Itaú), K-Fold CV garante estimativas confiáveis de performance antes de deploy, evitando overfitting e permitindo comparação justa entre modelos como XGBoost vs SVM em produção."
                            },
                            "estimatedTime": "2 horas",
                            "difficulty": "advanced",
                            "status": "not_started",
                            "prerequisites": [
                              "10.1.4.6.3.2"
                            ]
                          }
                        ]
                      },
                      {
                        "id": "10.1.4.6.4",
                        "name": "Exemplos e Estudos de Caso",
                        "description": "Aplicação prática das etapas anteriores em cenários reais, como previsão de churn ou classificação de imagens, com discussão ética na manipulação de dados.",
                        "specificSkills": [
                          {
                            "id": "10.1.4.6.4.1",
                            "name": "Analisar estudo de caso em churn prediction",
                            "description": "Reproduzir pipeline completo (treino, ajuste, avaliação) no dataset Telco-Churn, interpretando métricas e impactos éticos.",
                            "atomicExpansion": {
                              "steps": [
                                {
                                  "stepNumber": 1,
                                  "title": "Compreender o Problema e Explorar o Dataset Telco-Churn",
                                  "subSteps": [
                                    "Baixar o dataset Telco-Churn do Kaggle ou repositórios públicos.",
                                    "Carregar o dataset usando pandas e inspecionar estrutura (shape, dtypes, head).",
                                    "Analisar variáveis: identificar target (Churn?), features categóricas/numéricas, missing values.",
                                    "Visualizar distribuições com histograms, boxplots e countplots para entender desbalanceamento.",
                                    "Documentar insights iniciais sobre o problema de churn em telecomunicações."
                                  ],
                                  "verification": "Relatório inicial gerado com estatísticas descritivas e visualizações salvas como imagens ou notebook.",
                                  "estimatedTime": "1 hora",
                                  "materials": [
                                    "Dataset Telco-Churn (Kaggle: https://www.kaggle.com/datasets/blastchar/telco-customer-churn)",
                                    "Python com pandas, matplotlib, seaborn",
                                    "Jupyter Notebook"
                                  ],
                                  "tips": "Use df.info() e df.describe() para visão rápida; priorize visualizações para insights intuitivos.",
                                  "learningObjective": "Entender o contexto do problema de churn e características do dataset para preparar análise eficaz.",
                                  "commonMistakes": [
                                    "Ignorar desbalanceamento de classes (churn é minoritário).",
                                    "Não checar missing values ou inconsistências nos dados."
                                  ]
                                },
                                {
                                  "stepNumber": 2,
                                  "title": "Preparar e Engenharia de Features",
                                  "subSteps": [
                                    "Tratar missing values: imputar ou remover com base na análise.",
                                    "Codificar variáveis categóricas: one-hot encoding para nominais, ordinal para ordinais.",
                                    "Criar novas features: tenure groups, total charges normalizado, interações relevantes.",
                                    "Dividir dataset em train/test (80/20) com stratify no target.",
                                    "Escalonar features numéricas com StandardScaler."
                                  ],
                                  "verification": "Dataset preparado salvo como X_train, X_test, y_train, y_test; shape e dtypes verificados.",
                                  "estimatedTime": "1.5 horas",
                                  "materials": [
                                    "Scikit-learn (preprocessing, model_selection)",
                                    "Pandas para manipulação"
                                  ],
                                  "tips": "Use Pipeline para automatizar preprocessing; sempre valide splits com proporções iguais.",
                                  "learningObjective": "Transformar dados brutos em formato pronto para modelagem, evitando vazamento de dados.",
                                  "commonMistakes": [
                                    "Aplicar scaler no full dataset antes do split.",
                                    "Esquecer de tratar outliers em features como MonthlyCharges."
                                  ]
                                },
                                {
                                  "stepNumber": 3,
                                  "title": "Treinar Modelos Iniciais",
                                  "subSteps": [
                                    "Selecionar modelos baseline: LogisticRegression, RandomForestClassifier, XGBoost.",
                                    "Treinar cada modelo nos dados de treino.",
                                    "Prever no test set e calcular métricas básicas: accuracy, precision, recall, F1.",
                                    "Usar classification_report e confusion_matrix para análise.",
                                    "Plotar ROC curve e calcular AUC."
                                  ],
                                  "verification": "Tabela comparativa de métricas para baselines gerada e salva.",
                                  "estimatedTime": "1 hora",
                                  "materials": [
                                    "Scikit-learn classifiers",
                                    "XGBoost library",
                                    "Matplotlib para plots"
                                  ],
                                  "tips": "Comece com LogisticRegression por interpretabilidade; foque em recall para churn (capturar cancelamentos).",
                                  "learningObjective": "Implementar e comparar modelos iniciais para baseline sólido.",
                                  "commonMistakes": [
                                    "Treinar sem validação cruzada, levando a overfitting.",
                                    "Ignorar métricas além de accuracy em dados desbalanceados."
                                  ]
                                },
                                {
                                  "stepNumber": 4,
                                  "title": "Ajustar Hiperparâmetros e Avaliar Final",
                                  "subSteps": [
                                    "Usar GridSearchCV ou RandomizedSearchCV para otimizar hiperparâmetros do melhor baseline.",
                                    "Aplicar validação cruzada (5-fold) com scoring='f1' ou 'roc_auc'.",
                                    "Re-treinar modelo otimizado e avaliar no test set.",
                                    "Analisar feature importance (SHAP ou built-in).",
                                    "Comparar performance final vs baseline."
                                  ],
                                  "verification": "Melhor modelo salvo com pickle; relatório de CV scores e feature importance plotado.",
                                  "estimatedTime": "2 horas",
                                  "materials": [
                                    "Scikit-learn GridSearchCV",
                                    "SHAP library para interpretabilidade"
                                  ],
                                  "tips": "Limite grid size para evitar tempo excessivo; priorize params como n_estimators e max_depth.",
                                  "learningObjective": "Otimizar modelo para performance robusta e interpretável.",
                                  "commonMistakes": [
                                    "Overfitting por grid muito amplo sem CV adequada.",
                                    "Não reportar std de CV scores."
                                  ]
                                },
                                {
                                  "stepNumber": 5,
                                  "title": "Interpretar Resultados e Analisar Impactos Éticos",
                                  "subSteps": [
                                    "Interpretar métricas: trade-offs entre precision/recall no contexto de churn.",
                                    "Discutir feature importance: quais fatores predizem churn (ex: tenure baixo)?",
                                    "Analisar vieses: checar fairness por grupos demográficos (SeniorCitizen, gender).",
                                    "Considerar ética: privacidade de dados, ações discriminatórias baseadas em predições.",
                                    "Redigir relatório final com recomendações de negócios."
                                  ],
                                  "verification": "Relatório ético e interpretativo escrito (Markdown ou PDF).",
                                  "estimatedTime": "1 hora",
                                  "materials": [
                                    "SHAP ou LIME para explicabilidade",
                                    "Notebook para relatório"
                                  ],
                                  "tips": "Use group fairness metrics; pense em impactos reais como targeting injusto.",
                                  "learningObjective": "Conectar resultados técnicos a implicações práticas e éticas.",
                                  "commonMistakes": [
                                    "Ignorar vieses em features sensíveis.",
                                    "Focar só em performance numérica sem contexto."
                                  ]
                                }
                              ],
                              "practicalExample": "Em uma empresa de telecom como a AT&T, reproduza o pipeline no dataset Telco-Churn para prever churn de clientes com baixa tenure e altos charges, permitindo campanhas de retenção personalizadas que reduzem perda em 15%.",
                              "finalVerifications": [
                                "Pipeline completo reproduzido com F1-score > 0.75 no test set.",
                                "Relatório de métricas, feature importance e ROC AUC gerado.",
                                "Análise ética documentada com pelo menos 3 riscos identificados.",
                                "Modelo salvo e carregável para predições.",
                                "Notebook reproduzível com todas visualizações.",
                                "Comparação baseline vs otimizado clara."
                              ],
                              "assessmentCriteria": [
                                "Precisão na preparação de dados e handling de desbalanceamento.",
                                "Uso correto de CV e hiperparâmetro tuning.",
                                "Interpretação profunda de métricas e feature importance.",
                                "Análise ética abrangente e relevante.",
                                "Código limpo, comentado e reproduzível.",
                                "Relatório final estruturado e acionável."
                              ],
                              "crossCurricularConnections": [
                                "Estatística: Métricas de classificação e validação cruzada.",
                                "Programação: Python, pandas, scikit-learn avançado.",
                                "Ética em IA: Fairness, bias e privacidade.",
                                "Negócios: Estratégias de retenção de clientes.",
                                "Visualização de Dados: Plots interpretativos."
                              ],
                              "realWorldApplication": "Empresas de telecom e SaaS usam churn prediction para identificar clientes em risco, enviando ofertas personalizadas e reduzindo churn em 10-20%, gerando milhões em receita retida anualmente."
                            },
                            "estimatedTime": "3 horas",
                            "difficulty": "advanced",
                            "status": "not_started",
                            "prerequisites": [
                              "10.1.4.6.3.3"
                            ]
                          },
                          {
                            "id": "10.1.4.6.4.2",
                            "name": "Explorar caso de regressão preditiva",
                            "description": "Implementar modelo para previsão de preços de casas (Boston Housing), ajustando hiperparâmetros e avaliando com R² e RMSE.",
                            "atomicExpansion": {
                              "steps": [
                                {
                                  "stepNumber": 1,
                                  "title": "Carregar e Explorar o Dataset Boston Housing",
                                  "subSteps": [
                                    "Importar bibliotecas necessárias: pandas, numpy, sklearn.datasets e matplotlib/seaborn para visualizações.",
                                    "Carregar o dataset Boston usando load_boston() do sklearn.",
                                    "Examinar estrutura dos dados: shape, describe(), head() e info().",
                                    "Visualizar distribuições de features chave (ex: RM, LSTAT) e target (MEDV) com histograms e scatter plots.",
                                    "Identificar correlações com heatmap para entender relações com o preço das casas."
                                  ],
                                  "verification": "Dataset carregado corretamente, com summary statistics impressos e gráficos gerados sem erros.",
                                  "estimatedTime": "30 minutos",
                                  "materials": "Python (Jupyter Notebook), bibliotecas: scikit-learn, pandas, numpy, matplotlib, seaborn.",
                                  "tips": "Use %matplotlib inline no Jupyter para exibir gráficos inline.",
                                  "learningObjective": "Compreender a estrutura e características do dataset Boston Housing.",
                                  "commonMistakes": "Esquecer de importar bibliotecas ou usar dataset desatualizado (note: Boston foi removido do sklearn recente; use fetch_openml ou alternativa)."
                                },
                                {
                                  "stepNumber": 2,
                                  "title": "Preparar os Dados para Modelagem",
                                  "subSteps": [
                                    "Dividir dados em features (X) e target (y = MEDV).",
                                    "Separar em treino e teste usando train_test_split (80/20).",
                                    "Aplicar StandardScaler para normalizar features.",
                                    "Verificar balanceamento e tratar outliers se necessário (ex: boxplots).",
                                    "Salvar conjuntos de treino e teste em variáveis claras."
                                  ],
                                  "verification": "Conjuntos X_train, X_test, y_train, y_test criados e escalados, com shapes confirmados.",
                                  "estimatedTime": "20 minutos",
                                  "materials": "sklearn.model_selection (train_test_split), sklearn.preprocessing (StandardScaler).",
                                  "tips": "Defina random_state=42 para reprodutibilidade.",
                                  "learningObjective": "Preparar dados adequadamente para evitar vazamento e melhorar performance do modelo.",
                                  "commonMistakes": "Não escalar features, levando a bias em algoritmos sensíveis à escala."
                                },
                                {
                                  "stepNumber": 3,
                                  "title": "Implementar e Treinar Modelo Inicial de Regressão Linear",
                                  "subSteps": [
                                    "Importar LinearRegression do sklearn.linear_model.",
                                    "Instanciar e treinar o modelo com fit(X_train, y_train).",
                                    "Fazer previsões em treino e teste com predict().",
                                    "Calcular métricas iniciais: R² e RMSE para treino e teste.",
                                    "Visualizar resíduos com scatter plot (y_test vs residuals)."
                                  ],
                                  "verification": "Modelo treinado, previsões geradas e métricas iniciais impressas (ex: R² > 0.7 no treino).",
                                  "estimatedTime": "25 minutos",
                                  "materials": "sklearn.linear_model (LinearRegression), sklearn.metrics (r2_score, mean_squared_error).",
                                  "tips": "Use np.sqrt(mean_squared_error) para RMSE.",
                                  "learningObjective": "Treinar um baseline de regressão linear e interpretar métricas iniciais.",
                                  "commonMistakes": "Confundir R² de treino com teste, ignorando overfitting."
                                },
                                {
                                  "stepNumber": 4,
                                  "title": "Ajustar Hiperparâmetros e Otimizar o Modelo",
                                  "subSteps": [
                                    "Explorar Ridge ou Lasso para regularização (alpha como hiperparâmetro).",
                                    "Usar GridSearchCV para testar valores de alpha (ex: [0.1, 1, 10]).",
                                    "Treinar o melhor modelo encontrado pelo GridSearch.",
                                    "Comparar métricas antes/depois do ajuste.",
                                    "Plotar curva de aprendizado ou validação cruzada scores."
                                  ],
                                  "verification": "Melhor modelo selecionado via GridSearch, com R² e RMSE melhorados no teste.",
                                  "estimatedTime": "40 minutos",
                                  "materials": "sklearn.linear_model (Ridge), sklearn.model_selection (GridSearchCV, cross_val_score).",
                                  "tips": "Use cv=5 para validação cruzada robusta.",
                                  "learningObjective": "Otimizar hiperparâmetros para reduzir overfitting e melhorar generalização.",
                                  "commonMistakes": "Grid muito amplo causando tempo excessivo; comece com poucos valores."
                                },
                                {
                                  "stepNumber": 5,
                                  "title": "Avaliar e Interpretar o Modelo Final",
                                  "subSteps": [
                                    "Calcular R² e RMSE finais no conjunto de teste.",
                                    "Analisar importância de features com coef_ do modelo.",
                                    "Gerar previsões e comparar com valores reais em tabela.",
                                    "Testar robustez com validação cruzada completa.",
                                    "Documentar insights (ex: features mais impactantes como LSTAT)."
                                  ],
                                  "verification": "Relatório final com métricas, coeficientes e gráficos de avaliação salvos.",
                                  "estimatedTime": "25 minutos",
                                  "materials": "sklearn.metrics, pandas para tabelas de comparação.",
                                  "tips": "Salve modelo com joblib para reutilização.",
                                  "learningObjective": "Avaliar criticamente o modelo e extrair insights acionáveis.",
                                  "commonMistakes": "Ignorar interpretação de coeficientes, focando só em métricas numéricas."
                                }
                              ],
                              "practicalExample": "Usando o dataset Boston Housing, carregue os dados, treine uma Ridge Regression com alpha=1.0 otimizado via GridSearchCV, obtenha R²=0.85 e RMSE=3.5 no teste, identificando LSTAT como feature mais negativa no preço.",
                              "finalVerifications": [
                                "Dataset Boston carregado e explorado com visualizações corretas.",
                                "Modelo otimizado com R² > 0.8 e RMSE < 4.0 no conjunto de teste.",
                                "Hiperparâmetros ajustados via GridSearchCV com scores de CV reportados.",
                                "Coeficientes de features interpretados e plotados.",
                                "Resíduos analisados sem padrões de heteroscedasticidade.",
                                "Código completo executável e documentado em Jupyter Notebook."
                              ],
                              "assessmentCriteria": [
                                "Precisão das métricas R² e RMSE alinhadas com benchmarks (R² ~0.85).",
                                "Qualidade da preparação de dados (escalonamento e split corretos).",
                                "Eficácia do ajuste de hiperparâmetros (melhoria mensurável).",
                                "Profundidade da análise exploratória e interpretação.",
                                "Código limpo, reprodutível e com visualizações claras.",
                                "Identificação correta de overfitting/underfitting."
                              ],
                              "crossCurricularConnections": [
                                "Estatística: Correlação, regressão linear e métricas de erro.",
                                "Programação: Manipulação de dados com Python/pandas e scikit-learn.",
                                "Matemática: Álgebra linear (matrizes de features) e otimização.",
                                "Economia: Modelagem preditiva para valuation de imóveis."
                              ],
                              "realWorldApplication": "Previsão de preços de imóveis para corretores, bancos (concessão de hipotecas) e investidores, integrando em apps como Zillow ou ferramentas de análise urbana."
                            },
                            "estimatedTime": "2.5 horas",
                            "difficulty": "advanced",
                            "status": "not_started",
                            "prerequisites": [
                              "10.1.4.6.4.1"
                            ]
                          }
                        ]
                      }
                    ]
                  }
                ]
              },
              {
                "id": "10.1.5",
                "name": "Introdução ao Aprendizado de Máquina",
                "description": "Visão introdutória ao aprendizado de máquina supervisionado e não-supervisionado.",
                "totalSkills": 47,
                "atomicTopics": [
                  {
                    "id": "10.1.5.1",
                    "name": "Aprendizado de Máquina Supervisionado",
                    "description": "Conceitos básicos, exemplos e diferenças em relação a outros tipos de aprendizado.",
                    "individualConcepts": [
                      {
                        "id": "10.1.5.1.1",
                        "name": "Conceitos Básicos do Aprendizado de Máquina Supervisionado",
                        "description": "Definições fundamentais, incluindo dados rotulados, ambiente i.i.d. (independente e identicamente distribuído), e o processo de treinamento e predição.",
                        "specificSkills": [
                          {
                            "id": "10.1.5.1.1.1",
                            "name": "Definir Aprendizado Supervisionado",
                            "description": "Explicar que o aprendizado supervisionado utiliza conjuntos de dados rotulados (pares entrada-saída) para treinar modelos que preveem saídas para novas entradas, diferenciando de modelagem estatística tradicional.",
                            "atomicExpansion": {
                              "steps": [
                                {
                                  "stepNumber": 1,
                                  "title": "Compreender Dados Rotulados",
                                  "subSteps": [
                                    "Defina pares entrada-saída como exemplos onde cada entrada (features) tem uma saída conhecida (label).",
                                    "Identifique exemplos: imagem de gato com label 'gato'; preço de casa com label 'vendida'.",
                                    "Explique por que rotulados são essenciais: fornecem 'respostas corretas' para o modelo aprender.",
                                    "Compare com dados não rotulados: sem labels, usado em aprendizado não supervisionado.",
                                    "Crie 3 exemplos pessoais de dados rotulados do dia a dia."
                                  ],
                                  "verification": "Liste 3 exemplos de pares entrada-saída rotulados e explique sua importância.",
                                  "estimatedTime": "20 minutos",
                                  "materials": [
                                    "Papel e caneta para anotações",
                                    "Vídeo introdutório sobre dados em ML (ex: Khan Academy)"
                                  ],
                                  "tips": "Use analogias como 'professor corrigindo provas' para visualizar labels.",
                                  "learningObjective": "O aluno definirá dados rotulados e identificará exemplos corretos.",
                                  "commonMistakes": [
                                    "Confundir rotulados com não rotulados",
                                    "Pensar que labels são opcionais"
                                  ]
                                },
                                {
                                  "stepNumber": 2,
                                  "title": "Descrever o Processo de Treinamento Supervisionado",
                                  "subSteps": [
                                    "Divida o dataset: 70-80% para treinamento, 20-30% para teste.",
                                    "Explique treinamento: modelo ajusta parâmetros minimizando erro entre predições e labels reais.",
                                    "Descreva previsão: use modelo treinado em novas entradas sem labels.",
                                    "Ilustre com fluxograma simples: dados rotulados → treinar → prever novas entradas.",
                                    "Simule manualmente com 5 exemplos de classificação binária."
                                  ],
                                  "verification": "Desenhe um fluxograma do processo e simule uma predição.",
                                  "estimatedTime": "25 minutos",
                                  "materials": [
                                    "Ferramenta de desenho online (ex: Draw.io)",
                                    "Notebook Jupyter com dataset Iris (opcional)"
                                  ],
                                  "tips": "Pense no modelo como um aluno estudando exemplos resolvidos antes da prova.",
                                  "learningObjective": "O aluno explicará o ciclo de treinamento e previsão em aprendizado supervisionado.",
                                  "commonMistakes": [
                                    "Ignorar divisão train/test",
                                    "Confundir treinamento com previsão"
                                  ]
                                },
                                {
                                  "stepNumber": 3,
                                  "title": "Diferenciar de Modelagem Estatística Tradicional",
                                  "subSteps": [
                                    "Defina modelagem estatística: assume distribuições conhecidas, foca em inferência (ex: regressão linear clássica).",
                                    "Compare: supervisionado lida com dados complexos/high-dimensional sem assumir distribuições; foca em predição.",
                                    "Exemplo: regressão linear tradicional vs. árvore de decisão supervisionada em dados não lineares.",
                                    "Liste vantagens do supervisionado: flexibilidade, performance em big data.",
                                    "Debata limitações: requer muitos dados rotulados vs. métodos estatísticos mais interpretáveis."
                                  ],
                                  "verification": "Escreva uma tabela comparativa com 4 diferenças chave.",
                                  "estimatedTime": "30 minutos",
                                  "materials": [
                                    "Planilha Excel ou Google Sheets para tabela",
                                    "Artigo curto sobre regressão vs ML"
                                  ],
                                  "tips": "Foque em 'predição vs explicação' como diferença central.",
                                  "learningObjective": "O aluno distinguirá aprendizado supervisionado de modelagem estatística tradicional.",
                                  "commonMistakes": [
                                    "Equiparar completamente ML a estatística",
                                    "Ignorar foco preditivo do ML"
                                  ]
                                },
                                {
                                  "stepNumber": 4,
                                  "title": "Sintetizar a Definição Completa",
                                  "subSteps": [
                                    "Escreva definição em próprias palavras: 'Aprendizado supervisionado usa dados rotulados para treinar modelos preditivos, diferindo de estatística por flexibilidade em dados complexos'.",
                                    "Crie um quiz de 5 perguntas para testar compreensão.",
                                    "Responda quiz e autoavalie.",
                                    "Discuta em fórum ou com par: explique para outro.",
                                    "Registre definição final em portfólio."
                                  ],
                                  "verification": "Produza uma definição escrita de 100 palavras e responda quiz com 100% acerto.",
                                  "estimatedTime": "15 minutos",
                                  "materials": [
                                    "Documento Google Docs",
                                    "Quiz online (ex: Google Forms)"
                                  ],
                                  "tips": "Mantenha simples e memorável: foque em 'rotulados → treinar → prever'.",
                                  "learningObjective": "O aluno formulará uma definição precisa e acionável de aprendizado supervisionado.",
                                  "commonMistakes": [
                                    "Omitir diferenciação da estatística",
                                    "Definição vaga sem exemplos"
                                  ]
                                }
                              ],
                              "practicalExample": "Classificar e-mails como 'spam' ou 'não spam': entradas são palavras-chave e remetente (features rotuladas), modelo treinado prevê para novos e-mails, filtrando inbox automaticamente como no Gmail.",
                              "finalVerifications": [
                                "Defina aprendizado supervisionado em 1 frase precisa.",
                                "Liste 3 exemplos de dados rotulados.",
                                "Descreva o processo de treinamento em 4 etapas.",
                                "Diferencie de modelagem estatística com 2 vantagens.",
                                "Simule uma predição com dados fictícios.",
                                "Crie fluxograma do ciclo supervisionado."
                              ],
                              "assessmentCriteria": [
                                "Precisão conceitual: inclui dados rotulados e predição (30%)",
                                "Diferenciação clara de estatística tradicional (25%)",
                                "Uso de exemplos concretos e relevantes (20%)",
                                "Compreensão do processo (train/test/predict) (15%)",
                                "Capacidade de síntese em definição própria (10%)"
                              ],
                              "crossCurricularConnections": [
                                "Estatística: conceitos de regressão e inferência probabilística",
                                "Programação: implementação em Python com scikit-learn",
                                "Matemática: otimização e funções de perda (ex: MSE)",
                                "Ética: viés em dados rotulados e fairness em ML",
                                "Biologia: classificação de espécies ou diagnósticos médicos"
                              ],
                              "realWorldApplication": "Previsão de churn de clientes em empresas como bancos (usando histórico rotulado para prever quem cancela), detecção de fraudes em cartões de crédito e recomendação de produtos na Amazon."
                            },
                            "estimatedTime": "1 hora",
                            "difficulty": "beginner",
                            "status": "not_started",
                            "prerequisites": []
                          },
                          {
                            "id": "10.1.5.1.1.2",
                            "name": "Entender o Ambiente i.i.d.",
                            "description": "Descrever o pressuposto de Independente e Identicamente Distribuído (i.i.d.), essencial para a validade dos modelos supervisionados, e discutir implicações em dados reais.",
                            "atomicExpansion": {
                              "steps": [
                                {
                                  "stepNumber": 1,
                                  "title": "Compreender a Definição Básica de i.i.d.",
                                  "subSteps": [
                                    "Ler a definição formal: amostras independentes e identicamente distribuídas (i.i.d.) significa que cada observação é independente das outras e vem da mesma distribuição de probabilidade.",
                                    "Identificar os dois componentes principais: Independência e Distribuição Idêntica.",
                                    "Estudar exemplos simples, como lançar uma moeda justa múltiplas vezes.",
                                    "Diferenciar i.i.d. de amostras dependentes, como em sequências temporais.",
                                    "Anotar a fórmula matemática básica: P(X1, X2, ..., Xn) = ∏ P(Xi) para independência."
                                  ],
                                  "verification": "Escrever uma definição em suas próprias palavras e fornecer um exemplo simples.",
                                  "estimatedTime": "20 minutos",
                                  "materials": [
                                    "Livro de probabilidade ou artigo online sobre i.i.d.",
                                    "Notebook para anotações"
                                  ],
                                  "tips": "Use analogias cotidianas, como sorteios independentes de loteria.",
                                  "learningObjective": "Definir precisamente o conceito de i.i.d. e seus componentes.",
                                  "commonMistakes": [
                                    "Confundir independência com correlação zero.",
                                    "Ignorar que 'identicamente distribuída' requer a mesma distribuição para todas as amostras."
                                  ]
                                },
                                {
                                  "stepNumber": 2,
                                  "title": "Explorar o Conceito de Independência",
                                  "subSteps": [
                                    "Estudar independência estatística: eventos A e B são independentes se P(A∩B) = P(A)P(B).",
                                    "Simular dados independentes em Python usando numpy.random.",
                                    "Calcular covariância entre pares de amostras i.i.d. para verificar independência.",
                                    "Comparar com dados dependentes, como AR(1) process.",
                                    "Visualizar com scatter plots para observar ausência de padrões."
                                  ],
                                  "verification": "Gerar 1000 amostras i.i.d. e calcular a correlação entre elas (deve ser próxima de zero).",
                                  "estimatedTime": "30 minutos",
                                  "materials": [
                                    "Python com numpy e matplotlib",
                                    "Jupyter Notebook"
                                  ],
                                  "tips": "Sempre teste empiricamente com simulações para internalizar o conceito.",
                                  "learningObjective": "Diferenciar amostras independentes de dependentes através de exemplos e testes.",
                                  "commonMistakes": [
                                    "Assumir que baixa correlação implica independência completa.",
                                    "Não considerar dependências não-lineares."
                                  ]
                                },
                                {
                                  "stepNumber": 3,
                                  "title": "Analisar Distribuição Idêntica",
                                  "subSteps": [
                                    "Verificar se todas as amostras têm a mesma PDF/PMF usando testes KS ou QQ-plots.",
                                    "Gerar múltiplos conjuntos de dados da mesma distribuição (ex: Normal(0,1)) e comparar histogramas.",
                                    "Discutir consequências de distribuições não-idênticas, como concept drift.",
                                    "Estudar teoremas que dependem de i.i.d., como LLN e CLT.",
                                    "Simular violações: mudar a média ao longo das amostras."
                                  ],
                                  "verification": "Criar QQ-plot de duas subamostras i.i.d. e confirmar alinhamento linear.",
                                  "estimatedTime": "25 minutos",
                                  "materials": [
                                    "Python com scipy.stats para testes KS",
                                    "Matplotlib para plots"
                                  ],
                                  "tips": "Use testes estatísticos para quantificar 'idêntico' em prática.",
                                  "learningObjective": "Identificar e testar se amostras seguem a mesma distribuição.",
                                  "commonMistakes": [
                                    "Confundir variância similar com distribuição idêntica.",
                                    "Ignorar caudas pesadas em distribuições não-idênticas."
                                  ]
                                },
                                {
                                  "stepNumber": 4,
                                  "title": "Discutir Implicações em Aprendizado de Máquina Supervisionado",
                                  "subSteps": [
                                    "Explicar por que i.i.d. é crucial para validade de modelos: estimação de risco empírico converge para risco verdadeiro.",
                                    "Analisar violações comuns em dados reais: séries temporais, dados de redes sociais.",
                                    "Estudar soluções: bootstrapping, time-series cross-validation.",
                                    "Ler papers ou docs sobre quando ML falha sem i.i.d. (ex: stock prices).",
                                    "Debater trade-offs: assumir i.i.d. para simplicidade vs modelar dependências."
                                  ],
                                  "verification": "Escrever um parágrafo sobre por que i.i.d. falha em dados de tempo e propor uma alternativa.",
                                  "estimatedTime": "35 minutos",
                                  "materials": [
                                    "Artigo 'Assumptions of Machine Learning' ou docs Scikit-learn",
                                    "Exemplos de datasets reais como UCI ML repo"
                                  ],
                                  "tips": "Pense em aplicações pessoais: dados de sensores IoT são i.i.d.?",
                                  "learningObjective": "Avaliar impactos da violação de i.i.d. em modelos supervisionados e discutir mitigação.",
                                  "commonMistakes": [
                                    "Superestimar robustez de modelos a violações i.i.d.",
                                    "Não considerar custo computacional de modelar dependências."
                                  ]
                                }
                              ],
                              "practicalExample": "Gere 1000 amostras de uma Normal(0,1) i.i.d. usando numpy.random.normal. Treine um modelo linear simples nelas e avalie o erro. Agora, crie dados não-i.i.d. adicionando tendência temporal (ex: X_i = i/1000 + noise). Re-treine e compare o overfitting/generalização pobre.",
                              "finalVerifications": [
                                "Definir i.i.d. corretamente em termos matemáticos.",
                                "Simular e plotar dados i.i.d. vs não-i.i.d.",
                                "Explicar por que LLN requer i.i.d.",
                                "Identificar 3 violações comuns em dados reais.",
                                "Propor teste para checar i.i.d. em um dataset.",
                                "Discutir impacto em risco empírico de ML."
                              ],
                              "assessmentCriteria": [
                                "Precisão na definição de independência e distribuição idêntica (80%+ correto).",
                                "Uso correto de simulações e visualizações para demonstração.",
                                "Profundidade na discussão de implicações para ML supervisionado.",
                                "Identificação de pelo menos 2 erros comuns e como evitá-los.",
                                "Criatividade em exemplos reais e conexões interdisciplinares.",
                                "Clareza e estrutura na explicação escrita/oral."
                              ],
                              "crossCurricularConnections": [
                                "Probabilidade e Estatística: Teoremas fundamentais como Lei dos Grandes Números.",
                                "Ciência de Dados: Pré-processamento e validação de dados.",
                                "Programação: Simulações em Python/R para testes estatísticos.",
                                "Física: Modelos estocásticos em sistemas caóticos.",
                                "Economia: Análise de séries temporais financeiras."
                              ],
                              "realWorldApplication": "Em treinamento de modelos de ML para previsão de demanda, assumir i.i.d. permite usar validação cruzada padrão; violações em dados sazonais requerem técnicas como Prophet ou validação walk-forward para evitar previsões enviesadas em e-commerce."
                            },
                            "estimatedTime": "45 minutos",
                            "difficulty": "beginner",
                            "status": "not_started",
                            "prerequisites": []
                          },
                          {
                            "id": "10.1.5.1.1.3",
                            "name": "Identificar Etapas do Processo Supervisionado",
                            "description": "Listar e explicar as etapas: coleta de dados rotulados, divisão em treino/teste, treinamento do modelo, avaliação e predição.",
                            "atomicExpansion": {
                              "steps": [
                                {
                                  "stepNumber": 1,
                                  "title": "Compreender a Coleta de Dados Rotulados",
                                  "subSteps": [
                                    "Explicar o que são dados rotulados: pares de entrada (features) e saída (labels) conhecidos.",
                                    "Identificar fontes de dados rotulados, como datasets públicos (ex: Iris, MNIST).",
                                    "Discutir a importância da qualidade e quantidade dos dados para o sucesso do modelo.",
                                    "Explorar métodos de rotulagem manual ou semi-automatizada.",
                                    "Verificar balanceamento das classes nos dados."
                                  ],
                                  "verification": "Listar 3 exemplos de dados rotulados e explicar por que são essenciais.",
                                  "estimatedTime": "45 minutos",
                                  "materials": "Notebook Jupyter, dataset Iris do scikit-learn, documentação de datasets Kaggle.",
                                  "tips": "Sempre priorize dados limpos e representativos para evitar viés.",
                                  "learningObjective": "Identificar e justificar a necessidade de dados rotulados no aprendizado supervisionado.",
                                  "commonMistakes": "Confundir dados rotulados com não rotulados ou ignorar desbalanceamento de classes."
                                },
                                {
                                  "stepNumber": 2,
                                  "title": "Realizar a Divisão em Conjuntos de Treino e Teste",
                                  "subSteps": [
                                    "Explicar a proporção comum: 80% treino, 20% teste (ou 70/15/15 com validação).",
                                    "Usar funções como train_test_split do scikit-learn para divisão estratificada.",
                                    "Garantir que a divisão preserve a distribuição das classes.",
                                    "Discutir o papel do conjunto de treino (aprendizado) e teste (generalização).",
                                    "Evitar vazamento de dados (data leakage) entre conjuntos."
                                  ],
                                  "verification": "Executar divisão em um dataset e mostrar os shapes dos conjuntos resultantes.",
                                  "estimatedTime": "30 minutos",
                                  "materials": "Python com scikit-learn instalado, dataset exemplo carregado.",
                                  "tips": "Use random_state para reprodutibilidade nos experimentos.",
                                  "learningObjective": "Aplicar e explicar a divisão correta de dados para avaliação imparcial.",
                                  "commonMistakes": "Dividir aleatoriamente sem estratificação, causando viés em classes minoritárias."
                                },
                                {
                                  "stepNumber": 3,
                                  "title": "Executar o Treinamento do Modelo",
                                  "subSteps": [
                                    "Selecionar um algoritmo supervisionado simples (ex: regressão logística ou árvore de decisão).",
                                    "Ajustar hiperparâmetros iniciais e treinar usando fit() no conjunto de treino.",
                                    "Monitorar métricas durante o treinamento, como loss.",
                                    "Salvar o modelo treinado para reutilização.",
                                    "Interpretar o que significa 'treinar' (otimização de pesos)."
                                  ],
                                  "verification": "Treinar um modelo e imprimir os coeficientes ou importância de features.",
                                  "estimatedTime": "40 minutos",
                                  "materials": "Scikit-learn, pandas, Jupyter Notebook.",
                                  "tips": "Comece com modelos simples antes de complexos para entender o processo.",
                                  "learningObjective": "Compreender como o modelo aprende padrões dos dados de treino.",
                                  "commonMistakes": "Treinar no conjunto de teste, invalidando a avaliação."
                                },
                                {
                                  "stepNumber": 4,
                                  "title": "Avaliar o Desempenho do Modelo",
                                  "subSteps": [
                                    "Calcular métricas como acurácia, precisão, recall e F1-score no conjunto de teste.",
                                    "Usar funções como accuracy_score, classification_report do scikit-learn.",
                                    "Visualizar matriz de confusão e curva ROC.",
                                    "Comparar com baseline (ex: maioria da classe).",
                                    "Identificar overfitting ou underfitting pelos resultados."
                                  ],
                                  "verification": "Gerar relatório de métricas e interpretar se o modelo é bom (>80% acurácia em baseline).",
                                  "estimatedTime": "35 minutos",
                                  "materials": "Scikit-learn metrics, matplotlib para plots.",
                                  "tips": "Não use apenas acurácia; avalie por classe em problemas desbalanceados.",
                                  "learningObjective": "Selecionar e interpretar métricas adequadas para avaliação.",
                                  "commonMistakes": "Ignorar métricas além da acurácia ou confundir treino com teste."
                                },
                                {
                                  "stepNumber": 5,
                                  "title": "Realizar Predições com o Modelo Treinado",
                                  "subSteps": [
                                    "Carregar novos dados não vistos e aplicar predict().",
                                    "Interpretar probabilidades com predict_proba() se aplicável.",
                                    "Discutir deployment em produção (ex: API Flask).",
                                    "Testar com dados reais para validar generalização.",
                                    "Documentar limitações da predição."
                                  ],
                                  "verification": "Fazer predições em 5 amostras novas e explicar os resultados.",
                                  "estimatedTime": "25 minutos",
                                  "materials": "Modelo salvo, novos dados sintéticos ou reais.",
                                  "tips": "Sempre valide predições com domínio experto.",
                                  "learningObjective": "Usar o modelo para inferência em cenários reais.",
                                  "commonMistakes": "Predizer em dados de treino, superestimando performance."
                                }
                              ],
                              "practicalExample": "Usando o dataset Iris: coletar dados rotulados de flores (features: pétalas/sépalas, label: espécie); dividir 80/20; treinar KNN; avaliar com acurácia >95%; predizer espécie de nova flor.",
                              "finalVerifications": [
                                "Listar corretamente as 5 etapas em ordem.",
                                "Explicar o propósito de cada etapa com exemplos.",
                                "Executar pipeline completo em código Python sem erros.",
                                "Identificar erros comuns em um fluxograma dado.",
                                "Comparar com processo não supervisionado.",
                                "Aplicar em dataset diferente (ex: dígitos MNIST)."
                              ],
                              "assessmentCriteria": [
                                "Precisão na sequência e explicação das etapas (90%+).",
                                "Uso correto de terminologia técnica.",
                                "Demonstração prática com código funcional.",
                                "Análise crítica de potenciais falhas.",
                                "Criatividade em exemplos reais.",
                                "Clareza na comunicação escrita/oral."
                              ],
                              "crossCurricularConnections": [
                                "Estatística: Análise descritiva e testes de hipóteses nos dados.",
                                "Programação: Implementação em Python/R com bibliotecas ML.",
                                "Matemática: Otimização e cálculo de gradientes no treinamento.",
                                "Ética: Viés em dados rotulados e privacidade.",
                                "Informática: Gerenciamento de bancos de dados para coleta."
                              ],
                              "realWorldApplication": "Em detecção de spam no email (dados rotulados: spam/não spam), previsão de churn em clientes de bancos, ou diagnóstico de imagens médicas (ex: câncer via raio-X)."
                            },
                            "estimatedTime": "1 hora",
                            "difficulty": "beginner",
                            "status": "not_started",
                            "prerequisites": []
                          }
                        ]
                      },
                      {
                        "id": "10.1.5.1.2",
                        "name": "Exemplos de Aplicações Supervisionadas",
                        "description": "Casos práticos de classificação e regressão, com ênfase em problemas reais da Ciência de Dados.",
                        "specificSkills": [
                          {
                            "id": "10.1.5.1.2.1",
                            "name": "Exemplificar Classificação",
                            "description": "Descrever exemplos como detecção de spam em e-mails ou diagnóstico médico (classe: positivo/negativo), destacando algoritmos como árvores de decisão.",
                            "atomicExpansion": {
                              "steps": [
                                {
                                  "stepNumber": 1,
                                  "title": "Revisar Conceitos Fundamentais de Classificação Supervisionada",
                                  "subSteps": [
                                    "Defina classificação como uma tarefa de aprendizado de máquina supervisionado onde o modelo prevê categorias discretas (classes) baseadas em dados rotulados.",
                                    "Diferencie classificação de regressão: classificação usa labels categóricos (ex: sim/não), regressão usa valores contínuos.",
                                    "Identifique componentes chave: features (entradas), labels (saídas rotuladas) e algoritmo de classificação.",
                                    "Liste exemplos iniciais: detecção de spam (spam/não spam) e diagnóstico médico (doente/saudável).",
                                    "Explique o papel dos dados de treinamento rotulados na classificação."
                                  ],
                                  "verification": "Escreva uma definição de classificação em suas palavras e liste 2 exemplos com features e labels.",
                                  "estimatedTime": "25 minutos",
                                  "materials": [
                                    "Notebook para anotações",
                                    "Diagrama de ML supervisionado (impresso ou digital)",
                                    "Vídeo introdutório de 5 minutos sobre classificação (ex: Khan Academy)"
                                  ],
                                  "tips": "Use analogias cotidianas, como classificar frutas (maçã/banana) por cor e tamanho, para fixar o conceito.",
                                  "learningObjective": "Compreender os pilares da classificação supervisionada e diferenciá-la de outras tarefas de ML.",
                                  "commonMistakes": [
                                    "Confundir classificação com regressão",
                                    "Ignorar a necessidade de dados rotulados",
                                    "Pensar que classes devem ser numéricas"
                                  ]
                                },
                                {
                                  "stepNumber": 2,
                                  "title": "Selecionar e Descrever um Exemplo Prático de Classificação",
                                  "subSteps": [
                                    "Escolha detecção de spam em e-mails como exemplo principal: classe positiva (spam), negativa (não spam).",
                                    "Identifique features típicas: presença de palavras como 'grátis' ou 'urgente', endereço de remetente desconhecido, número de links.",
                                    "Descreva o dataset: e-mails rotulados manualmente como spam ou não.",
                                    "Explique o objetivo: prever a classe de um novo e-mail não visto.",
                                    "Discuta um segundo exemplo: diagnóstico médico (positivo para doença/negativo)."
                                  ],
                                  "verification": "Crie um diagrama simples com features, labels e predição para detecção de spam.",
                                  "estimatedTime": "30 minutos",
                                  "materials": [
                                    "Exemplo de dataset de e-mails spam (CSV simples com 10 linhas)",
                                    "Ferramenta de desenho como Draw.io ou papel e caneta"
                                  ],
                                  "tips": "Foque em 4-6 features realistas para manter simples; priorize features intuitivas.",
                                  "learningObjective": "Mapear problemas reais a tarefas de classificação, identificando features e labels adequadamente.",
                                  "commonMistakes": [
                                    "Escolher features irrelevantes",
                                    "Confundir features com labels",
                                    "Usar exemplos muito complexos no início"
                                  ]
                                },
                                {
                                  "stepNumber": 3,
                                  "title": "Explicar o Funcionamento com Algoritmo de Árvore de Decisão",
                                  "subSteps": [
                                    "Descreva árvore de decisão: modelo que divide dados em ramos baseados em regras (ex: se 'palavra grátis' > 1, ramo spam).",
                                    "Simule treinamento: use dados rotulados para construir a árvore, calculando impureza (ex: Gini).",
                                    "Demonstre predição: para novo e-mail, siga os ramos até a folha (classe final).",
                                    "Compare com outro algoritmo simples: regressão logística para probabilidade de classe.",
                                    "Ilustre com fluxograma: raiz (feature mais importante) até folhas."
                                  ],
                                  "verification": "Desenhe uma árvore de decisão simples para 5 e-mails de exemplo e preveja classes.",
                                  "estimatedTime": "40 minutos",
                                  "materials": [
                                    "Papel para desenhar árvore",
                                    "Ferramenta online como Orange Data Mining (versão gratuita)",
                                    "Dataset pequeno de spam (10-20 amostras)"
                                  ],
                                  "tips": "Comece com 2-3 features para a árvore; teste predições manualmente antes de automatizar.",
                                  "learningObjective": "Entender mecanicamente como algoritmos de classificação, como árvores de decisão, processam dados.",
                                  "commonMistakes": [
                                    "Sobrecarregar a árvore com muitas features",
                                    "Ignorar o conceito de impureza nos splits",
                                    "Confundir treinamento com predição"
                                  ]
                                },
                                {
                                  "stepNumber": 4,
                                  "title": "Analisar Vantagens, Limitações e Verificações",
                                  "subSteps": [
                                    "Liste vantagens da classificação: interpretável (árvores), lida com dados categóricos, útil para decisões binárias.",
                                    "Discuta limitações: overfitting em árvores profundas, necessidade de dados balanceados.",
                                    "Compare aplicações: spam (alta precisão) vs. médico (alta recall para positivos).",
                                    "Planeje métricas: acurácia, precisão, recall.",
                                    "Conclua com quando usar classificação vs. outros métodos."
                                  ],
                                  "verification": "Escreva um parágrafo resumindo prós/contras e sugira melhorias para o exemplo de spam.",
                                  "estimatedTime": "25 minutos",
                                  "materials": [
                                    "Tabela de métricas de avaliação (template)",
                                    "Artigos curtos sobre bias em ML (1 página)"
                                  ],
                                  "tips": "Use matriz de confusão para visualizar erros em exemplos binários.",
                                  "learningObjective": "Avaliar criticamente aplicações de classificação e identificar cenários ideais.",
                                  "commonMistakes": [
                                    "Ignorar desbalanceamento de classes",
                                    "Superestimar interpretabilidade de modelos black-box",
                                    "Não considerar ética em diagnósticos"
                                  ]
                                }
                              ],
                              "practicalExample": "Na detecção de spam: features incluem contagem de palavras suspeitas ('viagra', 'loteria'), remetente desconhecido e anexos. Labels: 'spam' (1) ou 'não spam' (0). Uma árvore de decisão treina com 1000 e-mails rotulados, criando regras como: se palavras_suspeitas > 2 E remetente_desconhecido = true, classificar como spam. Para um novo e-mail com 3 palavras suspeitas e remetente estranho, prediz 'spam'.",
                              "finalVerifications": [
                                "Pode explicar classificação com um exemplo próprio além de spam/diagnóstico?",
                                "Desenha corretamente uma árvore de decisão para um dataset pequeno?",
                                "Identifica features, labels e predição em um cenário real?",
                                "Discute pelo menos 2 limitações de árvores de decisão?",
                                "Calcula acurácia simples de um modelo de exemplo?",
                                "Diferencia classificação de regressão com precisão?"
                              ],
                              "assessmentCriteria": [
                                "Clareza e precisão na definição de classificação (20%)",
                                "Qualidade da descrição de exemplos reais com features/labels (25%)",
                                "Correção na simulação de algoritmo (árvore de decisão) (25%)",
                                "Análise de vantagens/limitações e métricas (20%)",
                                "Criatividade em conexões com mundo real (10%)"
                              ],
                              "crossCurricularConnections": [
                                "Estatística: Cálculo de probabilidades e impureza (Gini/Entropia)",
                                "Programação: Implementação em Python com scikit-learn",
                                "Ética e Sociedade: Bias em diagnósticos médicos e privacidade em e-mails",
                                "Biologia/Medicina: Aplicações em triagem de doenças",
                                "Lógica Matemática: Estruturas de decisão em árvores"
                              ],
                              "realWorldApplication": "Classificação é essencial em filtros de spam (Gmail bloqueia bilhões/dia), diagnósticos médicos (IA detecta câncer em imagens com 95% acurácia), aprovação de cartões de crédito (fraude detection), recomendação de conteúdo (YouTube classifica vídeos como apropriados) e moderação de redes sociais (detectar hate speech)."
                            },
                            "estimatedTime": "45 minutos",
                            "difficulty": "beginner",
                            "status": "not_started",
                            "prerequisites": [
                              "10.1.5.1.1.1"
                            ]
                          },
                          {
                            "id": "10.1.5.1.2.2",
                            "name": "Exemplificar Regressão",
                            "description": "Apresentar casos como previsão de preços de imóveis ou vendas futuras, utilizando regressão linear como exemplo básico.",
                            "atomicExpansion": {
                              "steps": [
                                {
                                  "stepNumber": 1,
                                  "title": "Introduzir o Conceito de Regressão e Regressão Linear",
                                  "subSteps": [
                                    "Definir regressão como um tipo de aprendizado supervisionado que prevê valores contínuos.",
                                    "Explicar a diferença entre regressão e classificação (contínuo vs categórico).",
                                    "Apresentar a regressão linear simples: equação y = mx + b, onde m é a inclinação e b o intercepto.",
                                    "Discutir premissas básicas: linearidade, independência dos erros e homocedasticidade.",
                                    "Mostrar um gráfico simples de dispersão com linha de tendência."
                                  ],
                                  "verification": "O aluno explica verbalmente ou por escrito a equação da regressão linear e suas premissas.",
                                  "estimatedTime": "20 minutos",
                                  "materials": [
                                    "Quadro branco ou ferramenta de desenho online (ex: Draw.io)",
                                    "Gráficos de exemplo impressos ou digitais"
                                  ],
                                  "tips": "Use analogias cotidianas, como prever altura de uma planta baseado em dias de crescimento.",
                                  "learningObjective": "Compreender os fundamentos teóricos da regressão linear como ferramenta preditiva.",
                                  "commonMistakes": [
                                    "Confundir regressão com classificação",
                                    "Ignorar premissas do modelo",
                                    "Interpretar m como causalidade direta"
                                  ]
                                },
                                {
                                  "stepNumber": 2,
                                  "title": "Selecionar e Preparar um Caso de Exemplo Prático: Previsão de Preços de Imóveis",
                                  "subSteps": [
                                    "Escolher variáveis: feature X (ex: tamanho em m²), target Y (preço em R$).",
                                    "Criar ou usar um dataset pequeno fictício com 10-15 amostras (ex: casas de 50-200m² custando 200k-800k).",
                                    "Explorar dados: calcular média, visualização scatter plot.",
                                    "Limpar dados: remover outliers ou valores ausentes.",
                                    "Dividir em treino e teste (80/20)."
                                  ],
                                  "verification": "Dataset preparado com scatter plot gerado e estatísticas básicas calculadas.",
                                  "estimatedTime": "25 minutos",
                                  "materials": [
                                    "Planilha Excel ou Google Sheets",
                                    "Python com Pandas e Matplotlib (opcional)"
                                  ],
                                  "tips": "Mantenha o dataset simples para foco no conceito, não na complexidade.",
                                  "learningObjective": "Preparar dados reais para aplicação de regressão linear.",
                                  "commonMistakes": [
                                    "Usar features irrelevantes",
                                    "Não visualizar dados antes do modelo",
                                    "Ignorar divisão treino/teste"
                                  ]
                                },
                                {
                                  "stepNumber": 3,
                                  "title": "Construir e Ajustar o Modelo de Regressão Linear",
                                  "subSteps": [
                                    "Implementar regressão linear usando fórmula ou biblioteca (ex: sklearn.linear_model.LinearRegression).",
                                    "Treinar o modelo nos dados de treino.",
                                    "Visualizar a linha de regressão no scatter plot.",
                                    "Calcular métricas: coeficiente de determinação (R²), inclinação (m) e intercepto (b).",
                                    "Interpretar coeficientes: 'por cada m² extra, preço aumenta R$X'."
                                  ],
                                  "verification": "Modelo treinado com R² calculado (>0.7 ideal para exemplo) e gráfico com linha ajustada.",
                                  "estimatedTime": "30 minutos",
                                  "materials": [
                                    "Jupyter Notebook ou Google Colab",
                                    "Biblioteca scikit-learn instalada"
                                  ],
                                  "tips": "Sempre plote os resíduos para checar premissas visualmente.",
                                  "learningObjective": "Aplicar e interpretar um modelo de regressão linear em dados reais.",
                                  "commonMistakes": [
                                    "Sobreajuste por dataset pequeno",
                                    "Não interpretar coeficientes",
                                    "Confundir correlação com causalidade"
                                  ]
                                },
                                {
                                  "stepNumber": 4,
                                  "title": "Realizar Previsões, Avaliar e Exemplificar Aplicações",
                                  "subSteps": [
                                    "Prever preços para novos imóveis (ex: casa de 120m²).",
                                    "Calcular erros: MSE ou MAE nos dados de teste.",
                                    "Comparar previsões com valores reais.",
                                    "Discutir limitações: não captura não-linearidades.",
                                    "Estender para múltiplas variáveis (regressão múltipla básica)."
                                  ],
                                  "verification": "Previsões feitas para 3 imóveis novos com erros calculados e explicação de resultados.",
                                  "estimatedTime": "25 minutos",
                                  "materials": [
                                    "Mesmo ambiente do Step 3",
                                    "Dataset de teste"
                                  ],
                                  "tips": "Use previsões para perguntas 'e se?' para engajar.",
                                  "learningObjective": "Usar o modelo para previsões e avaliar sua performance.",
                                  "commonMistakes": [
                                    "Aceitar R² baixo sem questionar",
                                    "Ignorar erros de previsão",
                                    "Generalizar demais o modelo simples"
                                  ]
                                }
                              ],
                              "practicalExample": "Em um dataset de 12 casas, com tamanho (m²) como feature e preço (R$) como target. Exemplo: casa de 100m² prevista em R$500.000 (real: R$480.000, erro baixo). Gráfico mostra linha ajustada com R²=0.85.",
                              "finalVerifications": [
                                "Explica regressão linear vs classificação corretamente.",
                                "Interpreta coeficientes de um modelo treinado.",
                                "Gera previsões precisas para dados novos.",
                                "Identifica pelo menos uma limitação do modelo.",
                                "Visualiza scatter plot com linha de regressão.",
                                "Calcula e interpreta R² adequadamente."
                              ],
                              "assessmentCriteria": [
                                "Clareza na explicação conceitual (30%)",
                                "Preparação e exploração de dados correta (20%)",
                                "Implementação e ajuste do modelo (25%)",
                                "Interpretação de resultados e previsões (15%)",
                                "Uso de visualizações e métricas (10%)"
                              ],
                              "crossCurricularConnections": [
                                "Estatística: Correlação e análise de regressão.",
                                "Programação: Uso de Python/scikit-learn para ML.",
                                "Economia/Negócios: Previsão de vendas e preços.",
                                "Visualização de Dados: Gráficos scatter e linhas de tendência."
                              ],
                              "realWorldApplication": "Previsão de preços de imóveis por corretores (ex: Zillow), previsão de vendas futuras por empresas de varejo (ex: Amazon), ou estimativa de salários baseado em experiência em RH."
                            },
                            "estimatedTime": "45 minutos",
                            "difficulty": "beginner",
                            "status": "not_started",
                            "prerequisites": [
                              "10.1.5.1.1.1"
                            ]
                          },
                          {
                            "id": "10.1.5.1.2.3",
                            "name": "Relacionar com Etapas da Ciência de Dados",
                            "description": "Conectar exemplos supervisionados às etapas de coleta, limpeza, análise exploratória e avaliação de modelos, citando estudos de caso simples.",
                            "atomicExpansion": {
                              "steps": [
                                {
                                  "stepNumber": 1,
                                  "title": "Revisar as Etapas Principais da Ciência de Dados",
                                  "subSteps": [
                                    "Liste as quatro etapas chave: coleta de dados, limpeza de dados, análise exploratória de dados (EDA) e avaliação de modelos.",
                                    "Descreva brevemente cada etapa em suas próprias palavras, focando em seu papel no pipeline de ML.",
                                    "Crie um diagrama simples ou fluxograma ilustrando a sequência das etapas.",
                                    "Identifique como o aprendizado supervisionado se encaixa nesse pipeline geral.",
                                    "Pesquise definições padrão de fontes confiáveis como Towards Data Science ou livros como 'Hands-On Machine Learning'."
                                  ],
                                  "verification": "Crie um fluxograma ou tabela resumindo as 4 etapas com descrições curtas; revise se cobre todos os aspectos essenciais.",
                                  "estimatedTime": "45 minutos",
                                  "materials": "Papel/caneta ou ferramenta como Draw.io, acesso à internet para referências (ex: artigos no Medium sobre CRISP-DM).",
                                  "tips": "Use mnemônicos como 'CLAE' (Coleta, Limpeza, Análise, Avaliação) para memorizar as etapas.",
                                  "learningObjective": "Compreender e articular as etapas fundamentais da ciência de dados como base para integrações com ML supervisionado.",
                                  "commonMistakes": "Confundir limpeza com coleta ou ignorar EDA como etapa distinta de modelagem."
                                },
                                {
                                  "stepNumber": 2,
                                  "title": "Identificar Exemplos de Aprendizado Supervisionado",
                                  "subSteps": [
                                    "Liste 3-5 exemplos comuns de ML supervisionado, como regressão linear para previsão de preços de casas, classificação para detecção de spam ou regressão logística para churn de clientes.",
                                    "Para cada exemplo, anote o tipo de tarefa (regressão ou classificação) e o objetivo de negócio.",
                                    "Classifique-os por domínio (saúde, finanças, marketing) para facilitar conexões futuras.",
                                    "Selecione dois exemplos para aprofundamento: um de regressão e um de classificação.",
                                    "Documente fontes ou datasets públicos como Boston Housing ou Iris para suporte."
                                  ],
                                  "verification": "Compile uma tabela com colunas: Exemplo, Tipo, Domínio, Dataset Exemplo; confirme pelo menos 4 entradas completas.",
                                  "estimatedTime": "30 minutos",
                                  "materials": "Planilha Google Sheets ou Excel, Kaggle datasets explorer.",
                                  "tips": "Priorize exemplos simples e relacionáveis para evitar sobrecarga cognitiva.",
                                  "learningObjective": "Reconhecer e catalogar aplicações práticas de ML supervisionado que se integram ao pipeline de dados.",
                                  "commonMistakes": "Focar apenas em exemplos avançados como deep learning, ignorando os básicos supervisionados."
                                },
                                {
                                  "stepNumber": 3,
                                  "title": "Mapear Exemplos às Etapas da Ciência de Dados",
                                  "subSteps": [
                                    "Para cada exemplo selecionado, descreva como a coleta de dados ocorre (fontes, volume).",
                                    "Detalhe a limpeza: tratamento de missing values, outliers, normalização.",
                                    "Explique EDA: visualizações (histogramas, scatter plots), correlações.",
                                    "Descreva avaliação: métricas como accuracy, precision/recall, MSE/R².",
                                    "Crie um mapa conceitual ligando cada etapa ao exemplo específico."
                                  ],
                                  "verification": "Gere um documento ou slide com mapeamentos visuais para dois exemplos; cheque se todas as 4 etapas estão cobertas.",
                                  "estimatedTime": "60 minutos",
                                  "materials": "Ferramentas como Lucidchart para mapas, Python/Jupyter se disponível para sketches rápidos.",
                                  "tips": "Use setas em um fluxograma para mostrar fluxo de dados através das etapas.",
                                  "learningObjective": "Estabelecer conexões claras entre tarefas supervisionadas e o ciclo completo de ciência de dados.",
                                  "commonMistakes": "Pular etapas como limpeza, assumindo dados 'prontos', ou superestimar EDA sem exemplos concretos."
                                },
                                {
                                  "stepNumber": 4,
                                  "title": "Analisar Estudos de Caso Simples e Sintetizar",
                                  "subSteps": [
                                    "Escolha um estudo de caso simples, ex: previsão de preços de casas com dataset Ames Housing.",
                                    "Passe pelas 4 etapas aplicando o caso: coleta (Kaggle), limpeza (pandas dropna), EDA (seaborn plots), avaliação (cross_val_score).",
                                    "Cite métricas reais de um tutorial (ex: R²=0.85).",
                                    "Repita com outro caso breve, como classificação de vinho.",
                                    "Escreva um parágrafo sintetizando as conexões observadas."
                                  ],
                                  "verification": "Produza um relatório de 1 página com o estudo de caso mapeado; inclua pelo menos uma métrica citada.",
                                  "estimatedTime": "75 minutos",
                                  "materials": "Jupyter Notebook ou Google Colab, datasets Kaggle (Ames Housing, Wine Quality).",
                                  "tips": "Comece com notebooks prontos no Kaggle e modifique para focar nas etapas.",
                                  "learningObjective": "Aplicar mapeamentos teóricos a casos reais, reforçando compreensão prática.",
                                  "commonMistakes": "Escolher casos complexos sem código simples; não citar fontes ou métricas específicas."
                                },
                                {
                                  "stepNumber": 5,
                                  "title": "Verificar e Reforçar Conexões Interdisciplinares",
                                  "subSteps": [
                                    "Revise todos os mapeamentos para consistência e completude.",
                                    "Identifique conexões com outras áreas (ex: estatística na EDA).",
                                    "Crie perguntas de autoavaliação: 'Como a limpeza afeta a avaliação?'",
                                    "Atualize o fluxograma inicial com insights dos casos.",
                                    "Prepare um resumo de 200 palavras para apresentação."
                                  ],
                                  "verification": "Responda a 5 perguntas de autoavaliação corretamente; compartilhe resumo com peer para feedback.",
                                  "estimatedTime": "30 minutos",
                                  "materials": "Documento de revisão, timer para autoavaliação.",
                                  "tips": "Use a técnica Feynman: explique como se fosse para uma criança de 10 anos.",
                                  "learningObjective": "Consolidar conhecimento através de verificação e extensão interdisciplinar.",
                                  "commonMistakes": "Ignorar feedback ou não conectar a domínios além de dados (ex: ética em avaliação)."
                                }
                              ],
                              "practicalExample": "No dataset Ames Housing para previsão de preços (regressão supervisionada): Coleta (baixar de Kaggle com 79 features); Limpeza (remover outliers com IQR, imputar missings); EDA (boxplots de preço por bairro, heatmap de correlações); Avaliação (R² de 0.88 via Random Forest com cross-validation).",
                              "finalVerifications": [
                                "Pode listar e descrever as 4 etapas da ciência de dados sem hesitação?",
                                "Mapeia corretamente pelo menos 3 exemplos supervisionados às etapas?",
                                "Analisa um estudo de caso com métricas específicas citadas?",
                                "Identifica pelo menos 2 conexões interdisciplinares?",
                                "Explica o impacto de uma etapa falha (ex: limpeza ruim) em uma aplicação real?",
                                "Produz um fluxograma ou mapa conceitual claro e completo?"
                              ],
                              "assessmentCriteria": [
                                "Clareza e precisão no mapeamento de exemplos às 4 etapas (30%)",
                                "Profundidade na análise de estudos de caso com exemplos concretos e métricas (25%)",
                                "Uso de visualizações ou diagramas para ilustrar conexões (20%)",
                                "Incorporação de dicas práticas e erros comuns evitados (15%)",
                                "Síntese interdisciplinar e aplicação real demonstrada (10%)"
                              ],
                              "crossCurricularConnections": [
                                "Estatística: Correlações e testes de hipótese na EDA",
                                "Programação: Uso de Python/R para limpeza e avaliação (pandas, scikit-learn)",
                                "Negócios: Interpretação de modelos para decisões (ex: churn prediction)",
                                "Ética: Viés em coleta e limpeza de dados",
                                "Visualização: Gráficos em EDA com matplotlib/seaborn"
                              ],
                              "realWorldApplication": "Em marketing, conectar coleta de dados de clientes (CRM), limpeza (remover duplicatas), EDA (segmentação RFM) e avaliação de modelo de churn (AUC-ROC >0.8) para reter 15% mais clientes, como feito por empresas como Netflix em recomendações personalizadas."
                            },
                            "estimatedTime": "1 hora",
                            "difficulty": "intermediate",
                            "status": "not_started",
                            "prerequisites": [
                              "10.1.5.1.1.3"
                            ]
                          }
                        ]
                      },
                      {
                        "id": "10.1.5.1.3",
                        "name": "Diferenças em Relação a Outros Tipos de Aprendizado",
                        "description": "Comparação com aprendizado não-supervisionado, semi-supervisionado e por reforço, incluindo considerações éticas.",
                        "specificSkills": [
                          {
                            "id": "10.1.5.1.3.1",
                            "name": "Comparar com Aprendizado Não-Supervisionado",
                            "description": "Explicar que o não-supervisionado usa dados não rotulados para clustering ou redução de dimensionalidade, sem predições diretas, ao contrário do supervisionado.",
                            "atomicExpansion": {
                              "steps": [
                                {
                                  "stepNumber": 1,
                                  "title": "Revisar Conceitos Básicos do Aprendizado Supervisionado",
                                  "subSteps": [
                                    "Defina aprendizado supervisionado: usa dados rotulados (inputs e outputs conhecidos) para treinar modelos preditivos.",
                                    "Identifique exemplos: regressão linear para prever preços, classificação para spam/não-spam.",
                                    "Explique o processo: treinamento com labels, validação e predição em novos dados.",
                                    "Destaque dependência: requer rótulos manuais, caros e demorados.",
                                    "Anote as métricas comuns: acurácia, precisão, recall."
                                  ],
                                  "verification": "Resuma em 3 frases as características principais do supervisionado e liste 2 exemplos.",
                                  "estimatedTime": "20 minutos",
                                  "materials": [
                                    "Notebook para anotações",
                                    "Artigo introdutório sobre ML supervisionado (ex: Towards Data Science)"
                                  ],
                                  "tips": "Use diagramas simples para visualizar o fluxo de dados rotulados.",
                                  "learningObjective": "Compreender os fundamentos do aprendizado supervisionado como base para comparação.",
                                  "commonMistakes": "Confundir com reforço, achando que supervisionado não precisa de labels."
                                },
                                {
                                  "stepNumber": 2,
                                  "title": "Introduzir o Aprendizado Não-Supervisionado",
                                  "subSteps": [
                                    "Defina aprendizado não-supervisionado: usa dados não rotulados para encontrar padrões intrínsecos.",
                                    "Liste técnicas principais: clustering (K-means), redução de dimensionalidade (PCA).",
                                    "Explique objetivos: agrupar dados semelhantes ou simplificar datasets sem predições diretas.",
                                    "Descreva o processo: análise exploratória, detecção de estruturas ocultas.",
                                    "Compare superficialmente: sem labels, foca em descoberta, não predição."
                                  ],
                                  "verification": "Crie uma tabela com 3 técnicas não-supervisionadas e suas finalidades.",
                                  "estimatedTime": "25 minutos",
                                  "materials": [
                                    "Planilha ou papel para tabela",
                                    "Vídeo curto sobre K-means no YouTube (ex: canal StatQuest)"
                                  ],
                                  "tips": "Pense em dados como 'caixas misturadas' sem etiquetas para visualizar.",
                                  "learningObjective": "Dominar os princípios do não-supervisionado isoladamente.",
                                  "commonMistakes": "Achar que não-supervisionado faz predições como o supervisionado."
                                },
                                {
                                  "stepNumber": 3,
                                  "title": "Comparar Diretamente os Dois Tipos de Aprendizado",
                                  "subSteps": [
                                    "Compare dados: supervisionado (rotulados) vs não-supervisionado (não rotulados).",
                                    "Diferenças em objetivos: predição/targeted vs descoberta/padrões.",
                                    "Aborde aplicações: superv. para forecasting, não-sup. para segmentação.",
                                    "Discuta prós/contras: superv. preciso mas caro; não-sup. barato mas interpretativo.",
                                    "Crie uma tabela de comparação com 5 colunas: tipo, dados, técnicas, outputs, usos."
                                  ],
                                  "verification": "Preencha e revise a tabela de comparação, garantindo pelo menos 4 diferenças chave.",
                                  "estimatedTime": "30 minutos",
                                  "materials": [
                                    "Ferramenta de tabela (Google Sheets ou papel)",
                                    "Infográfico de ML tipos (ex: de Scikit-learn docs)"
                                  ],
                                  "tips": "Use setas para mostrar fluxos contrastantes em um diagrama.",
                                  "learningObjective": "Identificar e articular diferenças fundamentais entre supervisionado e não-supervisionado.",
                                  "commonMistakes": "Ignorar custo de rótulos ou superestimar precisão do não-supervisionado."
                                },
                                {
                                  "stepNumber": 4,
                                  "title": "Aplicar a Comparação em Exemplos Práticos",
                                  "subSteps": [
                                    "Escolha dataset: Iris para superv. (classificação) vs não-sup. (clustering).",
                                    "Simule mentalmente: superv. prevê espécie; não-sup. agrupa sem saber espécies.",
                                    "Discuta trade-offs: quando usar cada um em cenários reais.",
                                    "Registre 3 cenários onde não-supervisionado é preferível.",
                                    "Autoavalie compreensão com perguntas: 'Por que não-sup. não prediz?'"
                                  ],
                                  "verification": "Escreva um parágrafo comparando os dois em um dataset específico.",
                                  "estimatedTime": "25 minutos",
                                  "materials": [
                                    "Dataset Iris (disponível no Kaggle)",
                                    "Notebook Jupyter para visualização opcional"
                                  ],
                                  "tips": "Visualize clusters com scatter plots para intuitividade.",
                                  "learningObjective": "Consolidar comparação através de exemplos concretos.",
                                  "commonMistakes": "Confundir outputs: clustering não é classificação rotulada."
                                }
                              ],
                              "practicalExample": "Em marketing, use supervisionado para prever churn de clientes (dados com label 'churn/sim/não'); use não-supervisionado para clusterizar clientes por comportamento de compra sem labels prévios, descobrindo segmentos como 'compradores impulsivos'.",
                              "finalVerifications": [
                                "Explica corretamente uso de dados rotulados vs não rotulados.",
                                "Lista pelo menos 3 técnicas não-supervisionadas (ex: K-means, PCA).",
                                "Diferencia predição/descoberta como outputs principais.",
                                "Identifica cenários ideais para cada tipo.",
                                "Cria tabela de comparação precisa.",
                                "Evita confusões comuns como achar não-supervisionado preditivo."
                              ],
                              "assessmentCriteria": [
                                "Clareza na distinção de dados (rotulados vs não).",
                                "Precisão nas técnicas e objetivos de cada tipo.",
                                "Profundidade na comparação de prós/contras.",
                                "Uso de exemplos relevantes e corretos.",
                                "Capacidade de articular diferenças em linguagem acessível.",
                                "Ausência de erros conceituais fundamentais."
                              ],
                              "crossCurricularConnections": [
                                "Estatística: distribuições e análise exploratória de dados.",
                                "Programação: implementação em Python com Scikit-learn.",
                                "Matemática Computacional: vetores e álgebra linear em PCA.",
                                "Ciência de Dados: pipelines de pré-processamento compartilhados."
                              ],
                              "realWorldApplication": "Na recomendação de produtos da Netflix, supervisionado prevê ratings (com histórico rotulado); não-supervisionado agrupa usuários por preferências ocultas para sugestões personalizadas sem labels explícitos."
                            },
                            "estimatedTime": "45 minutos",
                            "difficulty": "beginner",
                            "status": "not_started",
                            "prerequisites": [
                              "10.1.5.1.1.1"
                            ]
                          },
                          {
                            "id": "10.1.5.1.3.2",
                            "name": "Diferenciar de Aprendizado por Reforço",
                            "description": "Destacar que o reforço envolve agente aprendendo via recompensas em ambiente dinâmico, sem dados rotulados fixos, versus predições estáticas do supervisionado.",
                            "atomicExpansion": {
                              "steps": [
                                {
                                  "stepNumber": 1,
                                  "title": "Revisar Fundamentos do Aprendizado Supervisionado",
                                  "subSteps": [
                                    "Lembre-se que o aprendizado supervisionado usa conjuntos de dados rotulados com entradas e saídas conhecidas.",
                                    "Identifique exemplos como classificação de imagens (ex: gato vs. cachorro) ou regressão de preços de casas.",
                                    "Note que o modelo faz predições estáticas baseadas em padrões aprendidos dos dados fixos.",
                                    "Entenda que não há interação dinâmica; o treinamento é feito uma vez e o modelo é usado para inferência.",
                                    "Registre as limitações: requer grandes volumes de dados rotulados, que são caros de obter."
                                  ],
                                  "verification": "Escreva um parágrafo resumindo as características principais do aprendizado supervisionado e liste 2 exemplos.",
                                  "estimatedTime": "15 minutes",
                                  "materials": [
                                    "Notebook para anotações",
                                    "Artigo ou vídeo introdutório sobre ML supervisionado (opcional)"
                                  ],
                                  "tips": "Use analogias cotidianas, como um professor corrigindo provas com respostas certas, para fixar o conceito.",
                                  "learningObjective": "Compreender os pilares do aprendizado supervisionado para estabelecer base de comparação.",
                                  "commonMistakes": "Confundir com não-supervisionado, achando que não precisa de labels."
                                },
                                {
                                  "stepNumber": 2,
                                  "title": "Explorar Conceitos Básicos do Aprendizado por Reforço",
                                  "subSteps": [
                                    "Defina RL como um agente que aprende tomando ações em um ambiente dinâmico e recebendo recompensas ou penalidades.",
                                    "Identifique componentes chave: agente, ambiente, estados, ações, recompensas e política.",
                                    "Entenda o processo de trial-and-error: o agente explora e explora para maximizar recompensa cumulativa.",
                                    "Destaque a ausência de dados rotulados fixos; o aprendizado ocorre em tempo real via feedback.",
                                    "Examine exemplos iniciais como o agente aprendendo a equilibrar um poste invertido."
                                  ],
                                  "verification": "Desenhe um diagrama simples do ciclo agente-ambiente-ação-recompensa.",
                                  "estimatedTime": "20 minutes",
                                  "materials": [
                                    "Papel e caneta para diagrama",
                                    "Vídeo curto sobre RL (ex: de Andrew Ng)"
                                  ],
                                  "tips": "Pense no RL como treinar um cachorro com petiscos: recompensas guiam o comportamento.",
                                  "learningObjective": "Graspar os elementos fundamentais do RL para contrastar com o supervisionado.",
                                  "commonMistakes": "Achar que RL usa labels como no supervisionado, ignorando o feedback dinâmico."
                                },
                                {
                                  "stepNumber": 3,
                                  "title": "Identificar Diferenças Chave Entre os Dois Paradigmas",
                                  "subSteps": [
                                    "Compare dados: supervisionado usa dataset fixo rotulado vs. RL usa ambiente interativo sem labels pré-fixos.",
                                    "Analise o objetivo: predições precisas em dados novos (sup.) vs. maximizar recompensa ao longo do tempo (RL).",
                                    "Discuta o ambiente: estático e observável (sup.) vs. dinâmico, parcialmente observável e estocástico (RL).",
                                    "Examine o treinamento: batch learning offline (sup.) vs. online learning com exploração (RL).",
                                    "Registre métricas: acurácia/loss (sup.) vs. recompensa acumulada/episódios (RL)."
                                  ],
                                  "verification": "Crie uma tabela de comparação com pelo menos 5 diferenças em colunas lado a lado.",
                                  "estimatedTime": "25 minutes",
                                  "materials": [
                                    "Planilha ou documento para tabela",
                                    "Referências online sobre comparações ML"
                                  ],
                                  "tips": "Use cores na tabela para destacar contrastes: verde para similaridades, vermelho para diferenças.",
                                  "learningObjective": "Discernir precisamente as distinções conceituais e operacionais entre supervisionado e RL.",
                                  "commonMistakes": "Subestimar a interatividade do RL, tratando-o como 'supervisionado sequencial'."
                                },
                                {
                                  "stepNumber": 4,
                                  "title": "Aplicar Diferenças em Cenários Práticos",
                                  "subSteps": [
                                    "Escolha um problema e explique por que supervisionado falha (ex: jogo onde regras mudam dinamicamente).",
                                    "Simule um agente RL em um cenário simples, como labirinto com recompensas.",
                                    "Discuta trade-offs: RL é mais flexível mas computacionalmente caro e instável.",
                                    "Teste compreensão adaptando um exemplo supervisionado para RL.",
                                    "Reflita sobre quando usar cada um baseado nas diferenças."
                                  ],
                                  "verification": "Resolva 2 exercícios: classifique problemas como sup. ou RL e justifique.",
                                  "estimatedTime": "20 minutes",
                                  "materials": [
                                    "Exercícios impressos ou online",
                                    "Simulador RL básico como Gym (opcional)"
                                  ],
                                  "tips": "Comece com problemas do dia a dia, como dirigir um carro (RL para adaptação real-time).",
                                  "learningObjective": "Aplicar as diferenças para selecionar paradigmas adequados em contextos reais.",
                                  "commonMistakes": "Ignorar custos de dados no supervisionado vs. tempo de convergência no RL."
                                }
                              ],
                              "practicalExample": "Em um robô aspirador de pó: no supervisionado, treina com mapas rotulados de salas limpas/sujas para navegar estático; no RL, o robô explora a casa real, recebe +1 por sujeira aspirada e -1 por bater em móveis, aprendendo política ótima dinamicamente sem mapas prévios.",
                              "finalVerifications": [
                                "Explique verbalmente 4 diferenças chave sem consultar notas.",
                                "Classifique corretamente 5 problemas de ML como supervisionado ou RL.",
                                "Descreva o ciclo de RL com diagrama preciso.",
                                "Identifique por que RL é inadequado para predições pontuais como classificação de spam.",
                                "Compare métricas de sucesso de ambos os métodos.",
                                "Adapte um exemplo supervisionado para RL e vice-versa."
                              ],
                              "assessmentCriteria": [
                                "Precisão na distinção de dados rotulados vs. recompensas dinâmicas (peso: 25%).",
                                "Compreensão de ambientes estáticos vs. interativos (peso: 20%).",
                                "Capacidade de exemplos relevantes e corretos (peso: 20%).",
                                "Uso correto de terminologia técnica (ex: agente, política, loss) (peso: 15%).",
                                "Identificação de trade-offs e limitações (peso: 10%).",
                                "Clareza na tabela/comparação visual (peso: 10%)"
                              ],
                              "crossCurricularConnections": [
                                "Psicologia: Behaviorismo de Skinner com reforço positivo/negativo.",
                                "Jogos e Simulações: Design de IA em videogames como AlphaGo.",
                                "Economia: Modelos de decisão sequencial e otimização de utilidade.",
                                "Física: Dinâmica de sistemas estocásticos e Markov.",
                                "Robótica: Controle adaptativo em ambientes incertos."
                              ],
                              "realWorldApplication": "Desenvolver agentes autônomos em robótica industrial, onde o RL permite adaptação a falhas de máquina imprevisíveis via recompensas por eficiência, contrastando com supervisionado usado para inspeção visual estática de defeitos em linhas de produção."
                            },
                            "estimatedTime": "45 minutos",
                            "difficulty": "intermediate",
                            "status": "not_started",
                            "prerequisites": [
                              "10.1.5.1.1.1"
                            ]
                          },
                          {
                            "id": "10.1.5.1.3.3",
                            "name": "Discutir Ética no Aprendizado Supervisionado",
                            "description": "Abordar questões como viés em dados rotulados, privacidade e manipulação ética, relacionando com bibliografia como Witten & Frank.",
                            "atomicExpansion": {
                              "steps": [
                                {
                                  "stepNumber": 1,
                                  "title": "Introduzir Conceitos Éticos Básicos no Aprendizado Supervisionado",
                                  "subSteps": [
                                    "Definir ética aplicada ao machine learning e seu foco em aprendizado supervisionado com dados rotulados",
                                    "Revisar o funcionamento básico do aprendizado supervisionado (treinamento com labels e predições)",
                                    "Identificar dilemas éticos iniciais, como responsabilidade do desenvolvedor e impactos sociais",
                                    "Relacionar com princípios éticos gerais (justiça, transparência, não-maleficência)",
                                    "Mapear questões específicas: viés, privacidade e manipulação"
                                  ],
                                  "verification": "Escrever um parágrafo resumindo 3 dilemas éticos principais no aprendizado supervisionado e compartilhá-lo com um colega para feedback.",
                                  "estimatedTime": "25 minutos",
                                  "materials": [
                                    "Capítulo relevante de 'Data Mining: Practical Machine Learning Tools and Techniques' de Witten & Frank",
                                    "Artigo introdutório sobre ética em IA (ex: de arXiv)",
                                    "Caderno de anotações"
                                  ],
                                  "tips": "Use analogias cotidianas, como julgar alguém por rótulos enviesados, para fixar conceitos.",
                                  "learningObjective": "Compreender os fundamentos éticos e sua relevância específica ao aprendizado supervisionado.",
                                  "commonMistakes": [
                                    "Confundir ética de ML supervisionado com não-supervisionado",
                                    "Ignorar o papel central dos dados rotulados",
                                    "Generalizar demais sem exemplos concretos"
                                  ]
                                },
                                {
                                  "stepNumber": 2,
                                  "title": "Analisar Viés em Dados Rotulados",
                                  "subSteps": [
                                    "Explicar como viés surge em dados rotulados (seleção enviesada, labeling subjetivo)",
                                    "Discutir tipos de viés: seleção, confirmação, histórico",
                                    "Explorar impactos: predições injustas em grupos minoritários",
                                    "Referenciar Witten & Frank sobre avaliação de modelos e detecção de viés",
                                    "Propor técnicas iniciais de mitigação (diversificação de dados, auditoria)"
                                  ],
                                  "verification": "Identificar e descrever um exemplo de viés em um dataset hipotético de recrutamento, propondo uma correção.",
                                  "estimatedTime": "30 minutos",
                                  "materials": [
                                    "Dataset exemplo (ex: UCI Adult Income)",
                                    "Seção de Witten & Frank sobre avaliação de modelos",
                                    "Ferramenta online como AIF360 para demo de viés"
                                  ],
                                  "tips": "Visualize viés com gráficos de distribuição de labels por grupo demográfico.",
                                  "learningObjective": "Dominar as causas, impactos e abordagens iniciais para viés em dados rotulados.",
                                  "commonMistakes": [
                                    "Atribuir viés apenas a algoritmos, ignorando dados",
                                    "Subestimar viés histórico em labels",
                                    "Não conectar com métricas de performance desiguais"
                                  ]
                                },
                                {
                                  "stepNumber": 3,
                                  "title": "Explorar Privacidade e Manipulação Ética",
                                  "subSteps": [
                                    "Definir privacidade em contexto de dados rotulados (anonimização, re-identificação)",
                                    "Discutir riscos: vazamento de dados sensíveis em treinamento",
                                    "Abordar manipulação ética: overfitting intencional ou gaming de métricas",
                                    "Relacionar com bibliografia: discussões em Witten & Frank sobre validação ética",
                                    "Analisar regulamentações como GDPR e seu impacto em ML supervisionado"
                                  ],
                                  "verification": "Criar um fluxograma mostrando fluxo de dados rotulados e pontos de risco de privacidade/manipulação.",
                                  "estimatedTime": "35 minutos",
                                  "materials": [
                                    "Resumo do GDPR",
                                    "Capítulo de Witten & Frank sobre cross-validation e overfitting",
                                    "Vídeo curto sobre ataques de privacidade em ML (ex: TED Talk)"
                                  ],
                                  "tips": "Pense em cenários reais: dados médicos rotulados expostos.",
                                  "learningObjective": "Identificar e mitigar riscos de privacidade e manipulação em pipelines de ML supervisionado.",
                                  "commonMistakes": [
                                    "Confundir privacidade com segurança de modelo",
                                    "Ignorar manipulação sutil como label flipping",
                                    "Não considerar consentimento em labeling"
                                  ]
                                },
                                {
                                  "stepNumber": 4,
                                  "title": "Sintetizar Discussão Ética e Implicações",
                                  "subSteps": [
                                    "Integrar viés, privacidade e manipulação em uma discussão coesa",
                                    "Referenciar estudos de caso de Witten & Frank ou similares",
                                    "Debater trade-offs: acurácia vs. ética",
                                    "Propor framework pessoal para ética em projetos de ML",
                                    "Planejar como aplicar em projetos futuros"
                                  ],
                                  "verification": "Redigir um ensaio curto (300 palavras) discutindo ética em um projeto supervisionado hipotético.",
                                  "estimatedTime": "30 minutos",
                                  "materials": [
                                    "Witten & Frank completo para referências",
                                    "Template de ensaio ético",
                                    "Fórum online para discussão (ex: Reddit r/MachineLearning)"
                                  ],
                                  "tips": "Estruture como problema-solução-impacto para clareza.",
                                  "learningObjective": "Sintetizar conhecimentos éticos e aplicá-los criticamente.",
                                  "commonMistakes": [
                                    "Focar só em problemas sem soluções",
                                    "Não citar bibliografia adequadamente",
                                    "Ser superficial nas implicações reais"
                                  ]
                                }
                              ],
                              "practicalExample": "Analise o caso do algoritmo COMPAS usado em decisões judiciais nos EUA: dados rotulados históricos continham viés racial (maior risco previsto para minorias), violando privacidade de réus e permitindo manipulação ética em scores. Discuta mitigação via re-rotulagem diversa e auditorias, referenciando Witten & Frank sobre avaliação imparcial.",
                              "finalVerifications": [
                                "Explicar com precisão como viés em labels afeta predições em grupos sub-representados",
                                "Descrever 3 técnicas para proteger privacidade em dados de treinamento supervisionado",
                                "Identificar manipulações éticas comuns e propor contra-medidas",
                                "Citar corretamente seções relevantes de Witten & Frank",
                                "Debater trade-offs éticos em um cenário real",
                                "Propor um checklist ético para projetos de ML supervisionado"
                              ],
                              "assessmentCriteria": [
                                "Profundidade na análise de viés, com exemplos quantitativos",
                                "Clareza na distinção entre privacidade, viés e manipulação",
                                "Integração precisa de referências bibliográficas (Witten & Frank)",
                                "Criatividade em soluções e mitigação prática",
                                "Capacidade de conectar ética a impactos sociais amplos",
                                "Estrutura lógica e coesa na discussão final",
                                "Uso de linguagem precisa e livre de jargões desnecessários"
                              ],
                              "crossCurricularConnections": [
                                "Filosofia: Aplicação de utilitarismo e deontologia em decisões algorítmicas",
                                "Direito: Conformidade com GDPR, LGPD e leis anti-discriminação",
                                "Sociologia: Impactos de viés algorítmico em desigualdades sociais",
                                "Psicologia: Efeitos cognitivos de labels enviesados em humanos e máquinas",
                                "Gestão: Ética corporativa em desenvolvimento de IA"
                              ],
                              "realWorldApplication": "Em ferramentas de recrutamento como LinkedIn ou ATS, onde dados rotulados evitam viés de gênero/etnia; em saúde (predição de doenças com dados pacientes protegendo privacidade); em finanças (scoring de crédito ético evitando manipulação), garantindo decisões justas e regulatórias."
                            },
                            "estimatedTime": "30 minutos",
                            "difficulty": "intermediate",
                            "status": "not_started",
                            "prerequisites": [
                              "10.1.5.1.1.1"
                            ]
                          }
                        ]
                      }
                    ]
                  },
                  {
                    "id": "10.1.5.2",
                    "name": "Aprendizado de Máquina Não-Supervisionado",
                    "description": "Introdução aos métodos que não utilizam rótulos, como clustering e redução de dimensionalidade.",
                    "individualConcepts": [
                      {
                        "id": "10.1.5.2.1",
                        "name": "Conceitos Fundamentais do Aprendizado Não-Supervisionado",
                        "description": "Definição e características principais do aprendizado de máquina não-supervisionado, incluindo a ausência de rótulos nos dados, objetivos como descoberta de padrões e estrutura oculta, e comparação com o aprendizado supervisionado.",
                        "specificSkills": [
                          {
                            "id": "10.1.5.2.1.1",
                            "name": "Diferenciar Aprendizado Supervisionado e Não-Supervisionado",
                            "description": "Identificar as diferenças chave entre métodos supervisionados (com rótulos) e não-supervisionados (sem rótulos), reconhecendo cenários de aplicação como classificação vs. agrupamento de dados.",
                            "atomicExpansion": {
                              "steps": [
                                {
                                  "stepNumber": 1,
                                  "title": "Compreender os Fundamentos do Aprendizado Supervisionado",
                                  "subSteps": [
                                    "Defina aprendizado supervisionado como um método onde o modelo é treinado com dados rotulados (inputs e outputs conhecidos).",
                                    "Identifique exemplos principais: classificação (prever categorias, ex: spam/não-spam) e regressão (prever valores contínuos, ex: preço de casas).",
                                    "Explique o processo: o algoritmo aprende padrões dos rótulos para fazer previsões em novos dados.",
                                    "Discuta a necessidade de um conjunto de dados rotulado de alta qualidade.",
                                    "Visualize com um diagrama simples: dados de entrada → rótulos → modelo treinado → previsões."
                                  ],
                                  "verification": "Crie um diagrama ou resumo escrito explicando o fluxo do aprendizado supervisionado e liste 2 exemplos.",
                                  "estimatedTime": "20 minutos",
                                  "materials": [
                                    "Notebook e caneta para diagramas",
                                    "Vídeo introdutório sobre ML supervisionado (ex: YouTube - StatQuest)"
                                  ],
                                  "tips": "Use analogias cotidianas, como ensinar uma criança a identificar frutas com exemplos rotulados.",
                                  "learningObjective": "Entender o conceito central e exemplos do aprendizado supervisionado.",
                                  "commonMistakes": [
                                    "Confundir com regressão linear simples sem contexto de ML",
                                    "Ignorar a importância dos rótulos de qualidade"
                                  ]
                                },
                                {
                                  "stepNumber": 2,
                                  "title": "Compreender os Fundamentos do Aprendizado Não-Supervisionado",
                                  "subSteps": [
                                    "Defina aprendizado não-supervisionado como um método onde o modelo trabalha com dados não rotulados (apenas inputs).",
                                    "Identifique exemplos principais: agrupamento (clustering, ex: K-means para segmentar clientes) e redução de dimensionalidade (ex: PCA).",
                                    "Explique o processo: o algoritmo descobre padrões, estruturas ou relações ocultas nos dados.",
                                    "Discuta cenários onde rótulos não existem ou são caros de obter.",
                                    "Visualize com um diagrama: dados de entrada → algoritmo → grupos ou representações reduzidas."
                                  ],
                                  "verification": "Desenhe um diagrama do fluxo não-supervisionado e cite 2 exemplos de aplicação.",
                                  "estimatedTime": "20 minutos",
                                  "materials": [
                                    "Notebook para diagramas",
                                    "Artigo ou vídeo sobre clustering (ex: Towards Data Science)"
                                  ],
                                  "tips": "Pense em explorar uma sala escura sem mapa: o algoritmo 'sente' padrões pelos dados.",
                                  "learningObjective": "Dominar o conceito e exemplos do aprendizado não-supervisionado.",
                                  "commonMistakes": [
                                    "Achar que não-supervisionado é 'aleatório'",
                                    "Confundir com reinforcement learning"
                                  ]
                                },
                                {
                                  "stepNumber": 3,
                                  "title": "Identificar Diferenças Chave Entre os Dois Métodos",
                                  "subSteps": [
                                    "Compare presença de rótulos: supervisionado (sim) vs. não-supervisionado (não).",
                                    "Compare objetivos: supervisionado (previsão precisa) vs. não-supervisionado (descoberta de padrões).",
                                    "Compare avaliação: supervisionado (métricas como acurácia) vs. não-supervisionado (silhueta score, validação interna).",
                                    "Crie uma tabela de comparação com pelo menos 5 diferenças (ex: custo de dados, interpretabilidade).",
                                    "Discuta trade-offs: supervisionado é mais preciso mas requer mais preparação de dados."
                                  ],
                                  "verification": "Preencha uma tabela de comparação com 5+ diferenças e explique verbalmente.",
                                  "estimatedTime": "25 minutos",
                                  "materials": [
                                    "Planilha ou papel para tabela",
                                    "Template de tabela de comparação ML (Google Docs)"
                                  ],
                                  "tips": "Use cores diferentes na tabela para destacar supervisionado (verde) vs. não-supervisionado (azul).",
                                  "learningObjective": "Comparar criticamente as diferenças fundamentais.",
                                  "commonMistakes": [
                                    "Ignorar diferenças na avaliação de modelos",
                                    "Generalizar que um é sempre melhor"
                                  ]
                                },
                                {
                                  "stepNumber": 4,
                                  "title": "Reconhecer Cenários de Aplicação Adequados",
                                  "subSteps": [
                                    "Liste cenários para supervisionado: detecção de fraudes (com histórico rotulado), previsão de churn.",
                                    "Liste cenários para não-supervisionado: segmentação de mercado (sem labels prévios), detecção de anomalias.",
                                    "Analise um caso híbrido: usar não-supervisionado para gerar labels iniciais para supervisionado.",
                                    "Pratique escolhendo o método certo para 3 cenários hipotéticos.",
                                    "Resuma quando escolher cada um baseado em disponibilidade de dados."
                                  ],
                                  "verification": "Classifique 3 cenários reais como supervisionado ou não-supervisionado com justificativa.",
                                  "estimatedTime": "25 minutos",
                                  "materials": [
                                    "Lista de cenários de ML (ex: Kaggle datasets)",
                                    "Caneta para anotações"
                                  ],
                                  "tips": "Pergunte: 'Tenho labels?' Se sim → supervisionado; se não → não-supervisionado.",
                                  "learningObjective": "Aplicar o conhecimento para selecionar o método correto.",
                                  "commonMistakes": [
                                    "Forçar supervisionado quando dados não têm labels",
                                    "Subestimar usos exploratórios do não-supervisionado"
                                  ]
                                }
                              ],
                              "practicalExample": "Use o dataset Iris: Para supervisionado, treine um classificador (ex: KNN) com flores rotuladas por espécie para prever novas flores. Para não-supervisionado, aplique K-means para agrupar flores por similaridades sem saber espécies prévias, revelando clusters naturais.",
                              "finalVerifications": [
                                "Explique verbalmente 3 diferenças chave sem consultar notas.",
                                "Crie uma tabela de comparação precisa.",
                                "Classifique corretamente 5 cenários de aplicação.",
                                "Descreva prós e contras de cada método.",
                                "Identifique um exemplo real para cada tipo."
                              ],
                              "assessmentCriteria": [
                                "Precisão na definição de conceitos (80%+ correto).",
                                "Capacidade de listar e explicar ≥3 diferenças chave.",
                                "Correta identificação de cenários de aplicação (sem erros).",
                                "Uso de exemplos relevantes e concretos.",
                                "Clareza na comunicação (diagramas/tabelas bem estruturados).",
                                "Compreensão de trade-offs e limitações."
                              ],
                              "crossCurricularConnections": [
                                "Estatística: Métricas de avaliação e distribuições de dados.",
                                "Programação: Implementação em Python com scikit-learn.",
                                "Matemática: Álgebra linear em PCA e distâncias em clustering.",
                                "Negócios: Aplicações em análise de clientes e marketing."
                              ],
                              "realWorldApplication": "Em e-commerce, use supervisionado para prever compras (com histórico rotulado) e não-supervisionado para segmentar usuários em grupos de comportamento sem labels prévios, otimizando recomendações personalizadas."
                            },
                            "estimatedTime": "1 hora",
                            "difficulty": "beginner",
                            "status": "not_started",
                            "prerequisites": []
                          },
                          {
                            "id": "10.1.5.2.1.2",
                            "name": "Reconhecer Problemas Não-Supervisionados",
                            "description": "Analisar conjuntos de dados sem rótulos e identificar problemas adequados para métodos não-supervisionados, como detecção de anomalias ou segmentação de clientes.",
                            "atomicExpansion": {
                              "steps": [
                                {
                                  "stepNumber": 1,
                                  "title": "Diferenciar Problemas Supervisionados de Não-Supervisionados",
                                  "subSteps": [
                                    "Revise definições: Problemas supervisionados usam dados rotulados (ex: classificação de emails como spam/não-spam).",
                                    "Entenda não-supervisionados: Dados sem rótulos, foco em padrões intrínsecos (ex: agrupar clientes similares).",
                                    "Compare exemplos: Liste 3 problemas supervisionados e 3 não-supervisionados de domínios reais.",
                                    "Analise trade-offs: Discuta quando usar cada um (custo de rotulagem vs. descoberta de padrões).",
                                    "Crie um fluxograma simples para decidir o tipo de problema."
                                  ],
                                  "verification": "Crie um fluxograma ou tabela comparativa que distingue corretamente 5 exemplos de cada tipo.",
                                  "estimatedTime": "45 minutos",
                                  "materials": [
                                    "Notebook para anotações",
                                    "Artigos introdutórios sobre ML supervisionado vs. não-supervisionado (ex: Scikit-learn docs)"
                                  ],
                                  "tips": [
                                    "Use analogias cotidianas, como rotular fotos (supervisionado) vs. organizar fotos por similaridade (não-supervisionado)."
                                  ],
                                  "learningObjective": "Compreender as diferenças fundamentais entre aprendizado supervisionado e não-supervisionado.",
                                  "commonMistakes": [
                                    "Confundir clustering com classificação rotulada.",
                                    "Ignorar o custo de rotulagem em dados reais."
                                  ]
                                },
                                {
                                  "stepNumber": 2,
                                  "title": "Identificar Características de Dados Sem Rótulos",
                                  "subSteps": [
                                    "Examine um dataset sem rótulos: Baixe e explore um dataset como Iris sem labels ou transações bancárias.",
                                    "Calcule estatísticas básicas: Média, desvio padrão, histogramas para variáveis numéricas.",
                                    "Visualize dados: Crie scatter plots e histograms para detectar padrões ou outliers.",
                                    "Questione a ausência de labels: Pergunte 'O que os dados revelam sozinhos?' vs. 'Preciso de labels para responder?'",
                                    "Documente observações: Registre potenciais padrões (grupos, anomalias) sem assumir labels."
                                  ],
                                  "verification": "Gere visualizações e um relatório de 1 página descrevendo padrões observados sem rótulos.",
                                  "estimatedTime": "60 minutos",
                                  "materials": [
                                    "Python/Jupyter Notebook",
                                    "Bibliotecas: Pandas, Matplotlib",
                                    "Dataset sem rótulos (ex: Kaggle 'Bank Transactions')"
                                  ],
                                  "tips": [
                                    "Comece com visualizações univariadas antes de multivariadas para evitar sobrecarga."
                                  ],
                                  "learningObjective": "Reconhecer sinais de dados não-rotulados e padrões intrínsecos.",
                                  "commonMistakes": [
                                    "Assumir labels implícitos nos dados.",
                                    "Ignorar variáveis categóricas em análises iniciais."
                                  ]
                                },
                                {
                                  "stepNumber": 3,
                                  "title": "Reconhecer Tipos Comuns de Problemas Não-Supervisionados",
                                  "subSteps": [
                                    "Estude detecção de anomalias: Identifique cenários onde itens raros indicam problemas (ex: fraudes).",
                                    "Aprenda clustering: Agrupar dados similares sem labels prévios (ex: segmentação de clientes).",
                                    "Explore redução de dimensionalidade: Quando dados têm muitas features (ex: PCA para visualização).",
                                    "Mapeie problemas reais: Associe 5 cenários comuns a técnicas não-supervisionadas.",
                                    "Pratique classificação: Dado um problema descrito, categorize como clustering, anomalias ou outro."
                                  ],
                                  "verification": "Classifique corretamente 10 problemas descritos como adequados para métodos específicos não-supervisionados.",
                                  "estimatedTime": "50 minutos",
                                  "materials": [
                                    "Lista de problemas reais (ex: 'detectar fraudes em cartões', 'agrupar notícias')",
                                    "Vídeos curtos sobre K-Means e Isolation Forest"
                                  ],
                                  "tips": [
                                    "Lembre: Não-supervisionado responde 'O que está acontecendo?' vs. supervisionado 'Qual é a label?'."
                                  ],
                                  "learningObjective": "Associar problemas de dados a técnicas não-supervisionadas apropriadas.",
                                  "commonMistakes": [
                                    "Confundir anomalias com outliers normais.",
                                    "Aplicar clustering a dados lineares."
                                  ]
                                },
                                {
                                  "stepNumber": 4,
                                  "title": "Aplicar Análise a um Dataset Real",
                                  "subSteps": [
                                    "Selecione um dataset: Escolha um sem rótulos, como dados de sensores IoT ou e-commerce.",
                                    "Analise o contexto: Pergunte 'Qual o objetivo? Há labels? Quais padrões esperados?'",
                                    "Identifique o problema: Decida se é anomalias, clustering, etc., e justifique.",
                                    "Proponha solução: Descreva método não-supervisionado e passos iniciais.",
                                    "Valide decisão: Compare com problemas supervisionados alternativos e explique por quê não."
                                  ],
                                  "verification": "Produza um relatório curto propondo uma técnica não-supervisionada com justificativa para o dataset.",
                                  "estimatedTime": "75 minutos",
                                  "materials": [
                                    "Dataset real sem rótulos (ex: Kaggle 'Credit Card Fraud' sem labels)",
                                    "Jupyter Notebook"
                                  ],
                                  "tips": [
                                    "Considere o custo: Rotular é caro? Use não-supervisionado."
                                  ],
                                  "learningObjective": "Aplicar reconhecimento de problemas não-supervisionados a dados reais.",
                                  "commonMistakes": [
                                    "Forçar labels onde não há.",
                                    "Escolher técnica errada por buzzwords."
                                  ]
                                }
                              ],
                              "practicalExample": "Analisando um dataset de transações bancárias sem rótulos: Visualize saldos e valores para detectar anomalias (transações fraudulentas raras) ou clusterizar clientes por padrões de gasto para segmentação de marketing.",
                              "finalVerifications": [
                                "Explica corretamente a diferença entre dados rotulados e não-rotulados com exemplos.",
                                "Identifica detecção de anomalias em um dataset de sensores sem labels.",
                                "Propõe clustering para segmentação de clientes em dados de e-commerce.",
                                "Justifica por que um problema não é supervisionado (ausência de labels viáveis).",
                                "Cria um fluxograma para decidir tipo de ML baseado em dados.",
                                "Analisa um dataset real e recomenda técnica não-supervisionada apropriada."
                              ],
                              "assessmentCriteria": [
                                "Precisão na distinção supervisionado vs. não-supervisionado (90%+ acurácia em exemplos).",
                                "Profundidade na análise de dados sem rótulos (visualizações + estatísticas).",
                                "Correta associação de problemas a técnicas (anomalias, clustering, etc.).",
                                "Justificativas lógicas e baseadas em contexto real.",
                                "Criatividade em aplicações práticas e conexões interdisciplinares.",
                                "Relatórios claros e estruturados sem erros conceituais."
                              ],
                              "crossCurricularConnections": [
                                "Estatística: Uso de distribuições e testes de outlier.",
                                "Programação: Manipulação de dados com Python/Pandas.",
                                "Negócios: Segmentação de mercado e detecção de fraudes.",
                                "Visualização de Dados: Criação de plots para insights.",
                                "Ética em IA: Viés em clustering não-supervisionado."
                              ],
                              "realWorldApplication": "Em bancos, detectar fraudes em transações sem labels históricos; em marketing, segmentar clientes por comportamento de compra para campanhas personalizadas; em manufatura, identificar falhas em sensores IoT via anomalias."
                            },
                            "estimatedTime": "1 hora",
                            "difficulty": "beginner",
                            "status": "not_started",
                            "prerequisites": []
                          },
                          {
                            "id": "10.1.5.2.1.3",
                            "name": "Entender Avaliação em Não-Supervisionado",
                            "description": "Compreender métricas internas de avaliação como silhueta e Davies-Bouldin, já que não há rótulos verdadeiros para comparação.",
                            "atomicExpansion": {
                              "steps": [
                                {
                                  "stepNumber": 1,
                                  "title": "Compreender o Desafio da Avaliação em Aprendizado Não-Supervisionado",
                                  "subSteps": [
                                    "Explique por que rótulos verdadeiros não estão disponíveis em cenários não-supervisionados.",
                                    "Discuta a necessidade de métricas internas baseadas apenas nos dados e clusters formados.",
                                    "Identifique limitações de métricas externas e quando usar internas.",
                                    "Revise conceitos básicos de clustering como K-Means para contextualizar.",
                                    "Liste exemplos reais onde avaliação não-supervisionada é essencial."
                                  ],
                                  "verification": "Escreva um parágrafo resumindo o problema e suas implicações, sem mencionar métricas específicas ainda.",
                                  "estimatedTime": "30 minutos",
                                  "materials": [
                                    "Notebook para anotações",
                                    "Artigo introdutório sobre clustering (ex: Scikit-learn docs)"
                                  ],
                                  "tips": "Use analogias como 'agrupar frutas sem saber os nomes' para fixar o conceito.",
                                  "learningObjective": "Entender a motivação fundamental para métricas internas de avaliação.",
                                  "commonMistakes": "Confundir com avaliação supervisionada ou ignorar viés em dados não-rotulados."
                                },
                                {
                                  "stepNumber": 2,
                                  "title": "Dominar a Métrica Silhouette Score",
                                  "subSteps": [
                                    "Defina o Silhouette Score: média de (b - a)/max(a,b) para cada amostra, onde a é coerência intra-cluster e b é separabilidade inter-cluster.",
                                    "Calcule manualmente para um pequeno dataset de 2D (ex: 4 pontos em 2 clusters).",
                                    "Interprete valores: próximo de 1 (bom), 0 (sobreposto), negativo (mal atribuído).",
                                    "Implemente em Python usando sklearn.metrics.silhouette_score.",
                                    "Visualize o silhouette plot com sklearn.metrics.silhouette_samples."
                                  ],
                                  "verification": "Gere um silhouette plot para clusters sintéticos e interprete os valores médios.",
                                  "estimatedTime": "45 minutos",
                                  "materials": [
                                    "Python/Jupyter Notebook",
                                    "Bibliotecas: scikit-learn, matplotlib",
                                    "Dataset iris (sem labels)"
                                  ],
                                  "tips": "Comece com K=2 para visualização simples antes de aumentar.",
                                  "learningObjective": "Calcular e interpretar o Silhouette Score para avaliar qualidade de clusters.",
                                  "commonMistakes": "Interpretar valores baixos como 'ruim' sem considerar densidade de dados."
                                },
                                {
                                  "stepNumber": 3,
                                  "title": "Explorar o Índice Davies-Bouldin",
                                  "subSteps": [
                                    "Defina o DB Index: razão entre dispersão intra-cluster e separação inter-cluster, minimizado para bom clustering.",
                                    "Entenda a fórmula: média de max(R_ij) para i≠j, onde R_ij = (s_i + s_j)/d_ij.",
                                    "Compare com Silhouette: DB é relativo, valores baixos são melhores.",
                                    "Implemente usando sklearn.metrics.davies_bouldin_score.",
                                    "Teste em diferentes K para um dataset e plote o 'elbow' com DB.",
                                    "verification\": "
                                  ],
                                  "estimatedTime": "45 minutes",
                                  "materials": [
                                    "Python/Jupyter Notebook",
                                    "Scikit-learn",
                                    "Matplotlib"
                                  ],
                                  "tips": "Normalize features antes para evitar distorções por escala.",
                                  "learningObjective": "Aplicar e interpretar DB Index como métrica complementar.",
                                  "commonMistakes": "Confundir minimização (DB baixo bom) com Silhouette (alto bom).",
                                  "verification": "Implemente e plote o DB Index para K=2 até 10 no dataset Iris (sem labels). Identifique o K com menor DB e descreva o 'elbow' observado no gráfico."
                                },
                                {
                                  "stepNumber": 4,
                                  "title": "Comparar Métricas e Aplicar em Prática",
                                  "subSteps": [
                                    "Compare Silhouette vs DB: forças, fraquezas e quando usar cada uma.",
                                    "Aplique ambas em um dataset real (ex: Iris sem labels) variando K.",
                                    "Analise trade-offs e selecione melhor K baseado nas métricas.",
                                    "Discuta limitações: sensibilidade a outliers, forma de clusters.",
                                    "Documente um relatório com plots e conclusões."
                                  ],
                                  "verification": "Produza um relatório com scores para K=2-5 e recomendação de K ótimo.",
                                  "estimatedTime": "60 minutos",
                                  "materials": [
                                    "Jupyter Notebook",
                                    "Dataset Iris/Wholesale customers",
                                    "Scikit-learn"
                                  ],
                                  "tips": "Use grid search para múltiplos K e salve plots lado a lado.",
                                  "learningObjective": "Integrar métricas para avaliação robusta de clustering.",
                                  "commonMistakes": "Escolher K só pelo menor DB sem validar com visualização."
                                }
                              ],
                              "practicalExample": "Usando o dataset Iris (sem labels verdadeiros), aplique K-Means para K=2,3,5. Calcule Silhouette (ex: 0.55 para K=2) e DB (ex: 0.68 para K=2). Visualize plots e conclua que K=2 é ótimo apesar de 3 clusters verdadeiros, destacando limitações.",
                              "finalVerifications": [
                                "Explique Silhouette e DB em suas próprias palavras.",
                                "Calcule manualmente Silhouette para 3 pontos simples.",
                                "Implemente código para ambas métricas em um notebook.",
                                "Interprete um plot de Silhouette fornecido.",
                                "Identifique cenários onde uma métrica supera a outra.",
                                "Recomende K ótimo para um dataset dado com scores."
                              ],
                              "assessmentCriteria": [
                                "Precisão na definição e fórmula das métricas (80%+ correto).",
                                "Correta implementação e plots sem erros de código.",
                                "Interpretação coerente: alto Silhouette/DB baixo = bom clustering.",
                                "Consideração de limitações e trade-offs.",
                                "Relatório claro com evidências visuais e conclusões acionáveis.",
                                "Tempo de execução dentro do estimado com qualidade."
                              ],
                              "crossCurricularConnections": [
                                "Estatística: Medidas de dispersão e distância (ex: Euclidiana).",
                                "Programação: Python/Scikit-learn para ML prático.",
                                "Visualização de Dados: Plots para interpretação de clusters.",
                                "Matemática: Álgebra linear em representações de clusters."
                              ],
                              "realWorldApplication": "Em marketing, segmentar clientes por comportamento de compra sem labels prévios usando K-Means + Silhouette/DB para otimizar número de grupos, melhorando campanhas direcionadas."
                            },
                            "estimatedTime": "1.5 horas",
                            "difficulty": "intermediate",
                            "status": "not_started",
                            "prerequisites": []
                          }
                        ]
                      },
                      {
                        "id": "10.1.5.2.2",
                        "name": "Clustering",
                        "description": "Técnicas de agrupamento de dados para descobrir grupos naturais em conjuntos sem rótulos, incluindo algoritmos particionais e hierárquicos.",
                        "specificSkills": [
                          {
                            "id": "10.1.5.2.2.1",
                            "name": "Explicar o Algoritmo K-Means",
                            "description": "Descrever o funcionamento do K-Means, incluindo inicialização de centróides, atribuição de pontos e atualização iterativa, com exemplos simples.",
                            "atomicExpansion": {
                              "steps": [
                                {
                                  "stepNumber": 1,
                                  "title": "Entender os Conceitos Fundamentais do K-Means",
                                  "subSteps": [
                                    "Defina clustering como agrupamento de dados semelhantes sem rótulos prévios.",
                                    "Explique que K-Means é um algoritmo de clustering particional que divide dados em K clusters esféricos.",
                                    "Descreva os componentes chave: centróides (centros dos clusters), pontos de dados e distância euclidiana.",
                                    "Discuta a necessidade de escolher K (número de clusters) previamente.",
                                    "Revise matemática básica: média como centroide e soma de quadrados das distâncias."
                                  ],
                                  "verification": "Resuma os conceitos em um parágrafo coerente e responda a perguntas sobre diferenças entre clustering supervisionado e não-supervisionado.",
                                  "estimatedTime": "20 minutos",
                                  "materials": [
                                    "Quadro branco ou papel para diagramas",
                                    "Vídeo introdutório sobre clustering (ex: Khan Academy)"
                                  ],
                                  "tips": "Use analogias como 'agrupar amigos em grupos baseados em interesses semelhantes'.",
                                  "learningObjective": "Compreender os princípios básicos e terminologia do algoritmo K-Means.",
                                  "commonMistakes": [
                                    "Confundir K-Means com classificação supervisionada",
                                    "Ignorar que K deve ser escolhido manualmente"
                                  ]
                                },
                                {
                                  "stepNumber": 2,
                                  "title": "Inicialização dos Centróides",
                                  "subSteps": [
                                    "Escolha K pontos aleatórios dos dados como centróides iniciais (método Forgy).",
                                    "Alternativa: Use K-Means++ para inicialização inteligente, selecionando pontos distantes.",
                                    "Visualize em 2D: Plote pontos e marque centróides iniciais com símbolos diferentes.",
                                    "Explique por que a inicialização afeta a convergência (sensibilidade local).",
                                    "Calcule distâncias iniciais de amostra entre pontos e centróides."
                                  ],
                                  "verification": "Desenhe um diagrama com 3 clusters e centróides iniciais, justificando escolhas.",
                                  "estimatedTime": "15 minutos",
                                  "materials": [
                                    "Ferramenta de plotagem como Desmos ou Python Matplotlib (simulação)",
                                    "Conjunto de dados simples em planilha"
                                  ],
                                  "tips": "Sempre plote para visualização; evite centróides muito próximos inicialmente.",
                                  "learningObjective": "Dominar o processo de inicialização e suas implicações.",
                                  "commonMistakes": [
                                    "Escolher centróides fora dos dados",
                                    "Não considerar múltiplas inicializações para melhor resultado"
                                  ]
                                },
                                {
                                  "stepNumber": 3,
                                  "title": "Atribuição de Pontos aos Clusters",
                                  "subSteps": [
                                    "Para cada ponto de dados, calcule a distância euclidiana a todos os centróides.",
                                    "Atribua o ponto ao cluster do centróide mais próximo (menor distância).",
                                    "Crie uma matriz de atribuição: cada ponto recebe um rótulo de cluster (1 a K).",
                                    "Visualize: Colorir pontos por cluster atual.",
                                    "Repita para todos os pontos simultaneamente."
                                  ],
                                  "verification": "Aplique a um conjunto de 10 pontos 2D e liste atribuições corretas.",
                                  "estimatedTime": "25 minutos",
                                  "materials": [
                                    "Calculadora ou Python para distâncias",
                                    "Papel milimetrado para plotagem manual"
                                  ],
                                  "tips": "Use fórmula da distância euclidiana: sqrt(sum((x_i - y_i)^2)).",
                                  "learningObjective": "Executar e explicar a fase de atribuição com precisão matemática.",
                                  "commonMistakes": [
                                    "Usar distância manhattan em vez de euclidiana",
                                    "Atribuir sequencialmente em vez de simultaneamente"
                                  ]
                                },
                                {
                                  "stepNumber": 4,
                                  "title": "Atualização dos Centróides e Iteração",
                                  "subSteps": [
                                    "Para cada cluster, calcule o novo centróide como a média das coordenadas dos pontos atribuídos.",
                                    "Substitua centróides antigos pelos novos.",
                                    "Verifique mudança: Se centróides mudaram menos que um limiar (ex: 0.001), pare.",
                                    "Repita atribuição e atualização até convergência ou máximo de iterações (ex: 100).",
                                    "Registre perda (inércia): Soma das distâncias quadradas intra-cluster."
                                  ],
                                  "verification": "Simule 2 iterações em um exemplo pequeno e mostre evolução dos centróides.",
                                  "estimatedTime": "30 minutos",
                                  "materials": [
                                    "Planilha Excel para cálculos iterativos",
                                    "Notebook Jupyter com scikit-learn para demo"
                                  ],
                                  "tips": "Monitore a inércia decrescente para sinal de convergência.",
                                  "learningObjective": "Compreender o loop iterativo e critério de parada.",
                                  "commonMistakes": [
                                    "Atualizar centróides sequencialmente",
                                    "Não definir limite de iterações"
                                  ]
                                },
                                {
                                  "stepNumber": 5,
                                  "title": "Interpretação e Análise Final",
                                  "subSteps": [
                                    "Analise clusters finais: tamanhos, separação e centróides.",
                                    "Avalie qualidade com métricas como Silhouette Score ou Elbow Method para K.",
                                    "Discuta limitações: Assume clusters esféricos, sensível a outliers.",
                                    "Compare com execuções diferentes para validar estabilidade.",
                                    "Documente o processo completo em um fluxograma."
                                  ],
                                  "verification": "Crie um relatório resumido com fluxograma e métricas de um exemplo.",
                                  "estimatedTime": "20 minutos",
                                  "materials": [
                                    "Ferramenta de diagrama como Draw.io",
                                    "Código Python pronto para K-Means"
                                  ],
                                  "tips": "Sempre rode múltiplas vezes e pegue o melhor resultado.",
                                  "learningObjective": "Interpretar resultados e conhecer limitações do algoritmo.",
                                  "commonMistakes": [
                                    "Assumir convergência sempre ótima",
                                    "Ignorar escolha de K"
                                  ]
                                }
                              ],
                              "practicalExample": "Considere 2D pontos representando alturas e pesos de pessoas: (170,70), (172,72), (160,55), (162,58), (180,85), (182,88). Inicialize K=2 centróides em (170,70) e (180,85). Atribua pontos, atualize médias iterativamente até estabilizar em clusters 'magros' e 'pesados'.",
                              "finalVerifications": [
                                "Descreva verbalmente os 3 passos principais do K-Means sem consulta.",
                                "Explique com diagrama a evolução de centróides em 2 iterações.",
                                "Identifique limitações como sensibilidade a K e outliers.",
                                "Calcule manualmente atribuição para 5 pontos e 2 centróides.",
                                "Compare K-Means com hierarchical clustering.",
                                "Implemente pseudocódigo completo do algoritmo."
                              ],
                              "assessmentCriteria": [
                                "Precisão na descrição dos passos iterativos (inicialização, atribuição, atualização).",
                                "Correta explicação matemática (distância euclidiana, média como centróide).",
                                "Uso de exemplos visuais e simples para ilustrar conceitos.",
                                "Identificação de critérios de parada e métricas de qualidade.",
                                "Discussão de limitações e escolhas como valor de K.",
                                "Clareza e estrutura na apresentação do fluxograma."
                              ],
                              "crossCurricularConnections": [
                                "Estatística: Cálculo de médias, variâncias e distâncias.",
                                "Programação: Implementação em Python com NumPy/Scikit-learn.",
                                "Matemática: Vetores, geometria euclidiana e otimização.",
                                "Ciência de Dados: Pré-processamento, visualização com Matplotlib.",
                                "Inteligência Artificial: Base para algoritmos mais avançados como GMM."
                              ],
                              "realWorldApplication": "Segmentação de clientes em marketing (agrupar por comportamento de compra), compressão de imagens (reduzir cores a K clusters), análise de genes em bioinformática (agrupar expressões semelhantes) e detecção de anomalias em cibersegurança."
                            },
                            "estimatedTime": "2 horas",
                            "difficulty": "intermediate",
                            "status": "not_started",
                            "prerequisites": [
                              "10.1.5.2.1"
                            ]
                          },
                          {
                            "id": "10.1.5.2.2.2",
                            "name": "Aplicar Clustering Hierárquico",
                            "description": "Entender métodos aglomerativos e divisivos, dendrogramas e escolha do número de clusters via corte no dendrograma.",
                            "atomicExpansion": {
                              "steps": [
                                {
                                  "stepNumber": 1,
                                  "title": "Compreender os Fundamentos Teóricos do Clustering Hierárquico",
                                  "subSteps": [
                                    "Estude a definição de clustering hierárquico e suas duas abordagens principais: aglomerativa (bottom-up) e divisiva (top-down).",
                                    "Aprenda as medidas de dissimilaridade comuns, como distância Euclidiana, Manhattan e linkage (single, complete, average).",
                                    "Revise exemplos conceituais de como clusters são mesclados ou divididos iterativamente.",
                                    "Compare clustering hierárquico com métodos particionais como K-means.",
                                    "Pratique desenhando manualmente um dendrograma simples com 4-5 pontos de dados."
                                  ],
                                  "verification": "Explique verbalmente ou por escrito a diferença entre métodos aglomerativos e divisivos, com um exemplo de linkage.",
                                  "estimatedTime": "1.5 horas",
                                  "materials": [
                                    "Livro 'Introduction to Data Mining' (capítulo de clustering)",
                                    "Vídeo tutorial Khan Academy ou YouTube sobre clustering hierárquico",
                                    "Papel e caneta para desenhos manuais"
                                  ],
                                  "tips": "Comece com visualizações simples para internalizar o processo bottom-up antes de codificar.",
                                  "learningObjective": "Dominar os conceitos teóricos e métricas de distância para clustering hierárquico.",
                                  "commonMistakes": [
                                    "Confundir linkage methods (ex: single linkage leva a chaining), ignorar escalonamento de dados"
                                  ]
                                },
                                {
                                  "stepNumber": 2,
                                  "title": "Preparar Dados para Clustering Hierárquico",
                                  "subSteps": [
                                    "Carregue um dataset numérico padrão, como Iris ou um conjunto de clientes (use pandas).",
                                    "Realize pré-processamento: remova valores ausentes, escale features com StandardScaler (sklearn).",
                                    "Calcule matriz de dissimilaridade usando scipy.spatial.distance.pdist.",
                                    "Selecione linkage method apropriado baseado no dataset (ex: ward para compactos).",
                                    "Valide a preparação plotando scatter plot das features principais."
                                  ],
                                  "verification": "Execute código e confirme que a matriz de distância é simétrica e valores escalados estão entre -3 e 3.",
                                  "estimatedTime": "1 hora",
                                  "materials": [
                                    "Python com bibliotecas: pandas, numpy, scikit-learn, scipy",
                                    "Jupyter Notebook",
                                    "Dataset Iris do sklearn.datasets"
                                  ],
                                  "tips": "Sempre escale dados para evitar viés de features com escalas diferentes.",
                                  "learningObjective": "Preparar dados adequadamente para garantir resultados válidos no clustering.",
                                  "commonMistakes": [
                                    "Esquecer de escalar features, usar distâncias inadequadas para dados categóricos"
                                  ]
                                },
                                {
                                  "stepNumber": 3,
                                  "title": "Implementar e Visualizar o Clustering Hierárquico",
                                  "subSteps": [
                                    "Use AgglomerativeClustering ou linkage de scipy.cluster.hierarchy para gerar o dendrograma.",
                                    "Gere o dendrograma com dendrogram() e customize rótulos e cores.",
                                    "Ajuste parâmetros: linkage='ward', metric='euclidean'.",
                                    "Plote clusters coloridos no scatter plot original usando fcluster para labels.",
                                    "Experimente diferentes linkage methods e compare dendrogramas."
                                  ],
                                  "verification": "Gere um dendrograma plotado corretamente com pelo menos 3 clusters visíveis.",
                                  "estimatedTime": "2 horas",
                                  "materials": [
                                    "Python: scipy.cluster.hierarchy, matplotlib, seaborn",
                                    "Jupyter Notebook",
                                    "Dataset preparado do Step 2"
                                  ],
                                  "tips": "Use truncate_mode='level' em dendrogram para datasets grandes.",
                                  "learningObjective": "Implementar clustering e visualizar hierarquia via dendrograma.",
                                  "commonMistakes": [
                                    "Não rotacionar dendrograma (orientation='right'), ignorar truncamento para grandes N"
                                  ]
                                },
                                {
                                  "stepNumber": 4,
                                  "title": "Escolher Número de Clusters e Avaliar Resultados",
                                  "subSteps": [
                                    "Analise o dendrograma para identificar 'cotovelo' ou gaps naturais para corte.",
                                    "Use fcluster com criterion='maxclust' ou 'distance_threshold' para extrair clusters.",
                                    "Avalie qualidade com métricas: silhouette score, Davies-Bouldin index.",
                                    "Compare com ground truth (se disponível) via adjusted Rand index.",
                                    "Documente escolhas e gere relatório com plots e métricas."
                                  ],
                                  "verification": "Produza clusters com silhouette score > 0.5 e justifique a escolha de k via dendrograma.",
                                  "estimatedTime": "1.5 horas",
                                  "materials": [
                                    "sklearn.metrics: silhouette_score, davies_bouldin_score",
                                    "scipy.cluster.hierarchy.fcluster",
                                    "Jupyter Notebook"
                                  ],
                                  "tips": "Corte onde a distância de fusão dobra para uma escolha robusta de k.",
                                  "learningObjective": "Selecionar e validar o número ótimo de clusters.",
                                  "commonMistakes": [
                                    "Cortar arbitrariamente sem métricas, superestimar k em dados ruidosos"
                                  ]
                                },
                                {
                                  "stepNumber": 5,
                                  "title": "Aplicar e Interpretar em Contexto Real",
                                  "subSteps": [
                                    "Aplique o modelo a um dataset real (ex: segmentação de clientes).",
                                    "Interprete clusters: perfil médio por cluster (médias de features).",
                                    "Teste robustez variando linkage ou subamostrando dados.",
                                    "Salve modelo com joblib e gere relatório final.",
                                    "Discuta limitações (ex: não escala bem para N grande)."
                                  ],
                                  "verification": "Gere um relatório com interpretação de 3+ clusters e aplicação prática.",
                                  "estimatedTime": "1 hora",
                                  "materials": [
                                    "Dataset real: Wholesale customers (UCI ML Repo)",
                                    "joblib para serialização",
                                    "Jupyter Notebook"
                                  ],
                                  "tips": "Foquem em interpretabilidade: nomeie clusters baseado em insights.",
                                  "learningObjective": "Aplicar clustering hierárquico de forma prática e interpretativa.",
                                  "commonMistakes": [
                                    "Não interpretar clusters (apenas números), ignorar outliers"
                                  ]
                                }
                              ],
                              "practicalExample": "Usando o dataset Iris, aplique clustering hierárquico aglomerativo com linkage='ward'. Gere dendrograma revelando 3 clusters naturais correspondendo às espécies (setosa, versicolor, virginica). Corte em k=3 e valide com silhouette score ~0.7, segmentando flores por características físicas.",
                              "finalVerifications": [
                                "Explicar aglomerativo vs. divisivo com exemplo.",
                                "Gerar dendrograma correto de um dataset padrão.",
                                "Justificar escolha de k via análise visual e métricas.",
                                "Interpretar clusters em termos de features do dataset.",
                                "Comparar resultados com K-means no mesmo dataset.",
                                "Identificar e corrigir um erro comum em linkage choice."
                              ],
                              "assessmentCriteria": [
                                "Precisão teórica: 90%+ em quiz sobre conceitos e métricas.",
                                "Código funcional: executa sem erros e produz plots corretos.",
                                "Qualidade de visualização: dendrograma legível com labels.",
                                "Escolha de k: suportada por pelo menos 2 métricas (silhouette >0.5).",
                                "Interpretação: relatório descreve 3+ insights acionáveis.",
                                "Eficiência: tempo total <8h, código limpo e comentado.",
                                "Robustez: funciona em dataset não-visto."
                              ],
                              "crossCurricularConnections": [
                                "Estatística: medidas de distância e análise de variância (Ward).",
                                "Programação: manipulação de dados com Python/pandas e visualização matplotlib.",
                                "Biologia: árvores filogenéticas em bioinformática.",
                                "Negócios: segmentação de mercado em marketing analytics.",
                                "Matemática: grafos e teoria de clusters em combinatoria."
                              ],
                              "realWorldApplication": "Em marketing, segmentar clientes de e-commerce por padrões de compra (ex: alto valor vs. frequente) usando clustering hierárquico para campanhas personalizadas; em bioinformática, construir árvores evolutivas de espécies baseadas em similaridades genéticas."
                            },
                            "estimatedTime": "1.5 horas",
                            "difficulty": "intermediate",
                            "status": "not_started",
                            "prerequisites": [
                              "10.1.5.2.1"
                            ]
                          },
                          {
                            "id": "10.1.5.2.2.3",
                            "name": "Escolher Parâmetros em Clustering",
                            "description": "Determinar o número ótimo de clusters usando métodos como elbow ou gap statistic, considerando dados reais.",
                            "atomicExpansion": {
                              "steps": [
                                {
                                  "stepNumber": 1,
                                  "title": "Compreender os Conceitos Fundamentais dos Métodos",
                                  "subSteps": [
                                    "Estudar o conceito de número ótimo de clusters e sua importância no clustering.",
                                    "Analisar o método Elbow: como funciona a curva de inércia vs. número de clusters.",
                                    "Explorar o Gap Statistic: comparação da variância observada com variância esperada em dados uniformes.",
                                    "Comparar vantagens e desvantagens de cada método em contextos reais.",
                                    "Identificar cenários onde um método é preferível ao outro."
                                  ],
                                  "verification": "Resumir em um parágrafo as diferenças entre Elbow e Gap Statistic, com exemplos.",
                                  "estimatedTime": "1 hora",
                                  "materials": [
                                    "Documentação scikit-learn",
                                    "Artigos sobre Elbow e Gap Statistic (ex: Towards Data Science)",
                                    "Notebook Jupyter"
                                  ],
                                  "tips": "Visualize gráficos de exemplo antes de codificar para fixar conceitos.",
                                  "learningObjective": "Dominar os princípios teóricos para aplicação prática.",
                                  "commonMistakes": [
                                    "Confundir inércia com silhueta score",
                                    "Ignorar normalização de dados",
                                    "Não considerar dimensionalidade alta"
                                  ]
                                },
                                {
                                  "stepNumber": 2,
                                  "title": "Preparar o Ambiente e os Dados Reais",
                                  "subSteps": [
                                    "Instalar bibliotecas necessárias: scikit-learn, matplotlib, pandas, numpy.",
                                    "Carregar um dataset real (ex: Iris ou Wholesale Customers).",
                                    "Explorar os dados: estatísticas descritivas, visualizações e tratamento de missing values.",
                                    "Normalizar ou padronizar os dados para clustering.",
                                    "Dividir dados em treino/teste se aplicável."
                                  ],
                                  "verification": "Executar código de preparação e gerar relatório de estatísticas dos dados.",
                                  "estimatedTime": "1 hora",
                                  "materials": [
                                    "Python 3.x",
                                    "Jupyter Notebook ou Google Colab",
                                    "Datasets: sklearn.datasets.load_iris() ou Kaggle"
                                  ],
                                  "tips": "Sempre use StandardScaler para dados com escalas diferentes.",
                                  "learningObjective": "Preparar dados de forma robusta para análise de clustering.",
                                  "commonMistakes": [
                                    "Esquecer normalização, levando a clusters enviesados",
                                    "Usar dados com outliers não tratados",
                                    "Não visualizar dados antes"
                                  ]
                                },
                                {
                                  "stepNumber": 3,
                                  "title": "Implementar e Aplicar o Método Elbow",
                                  "subSteps": [
                                    "Implementar KMeans para k de 1 a 10 e calcular inércia (within-cluster sum of squares).",
                                    "Plotar a curva Elbow usando matplotlib.",
                                    "Identificar o 'cotovelo' visualmente e justificar a escolha de k.",
                                    "Testar com diferentes sementes aleatórias para robustez.",
                                    "Interpretar o gráfico considerando o contexto dos dados."
                                  ],
                                  "verification": "Gerar e interpretar gráfico Elbow com k ótimo selecionado.",
                                  "estimatedTime": "1.5 horas",
                                  "materials": [
                                    "Código Python com scikit-learn.cluster.KMeans",
                                    "Matplotlib para plots"
                                  ],
                                  "tips": "Use loop for para variar k e armazene inércias em lista.",
                                  "learningObjective": "Aplicar Elbow method para seleção prática de k.",
                                  "commonMistakes": [
                                    "Escolher k pelo menor inércia (sempre k=n_samples)",
                                    "Ignorar ruído no gráfico",
                                    "Não repetir com random_state"
                                  ]
                                },
                                {
                                  "stepNumber": 4,
                                  "title": "Implementar Gap Statistic e Realizar Análise Comparativa",
                                  "subSteps": [
                                    "Implementar Gap Statistic usando função custom ou biblioteca (ex: clustergrammer).",
                                    "Calcular Gap para k de 1 a 10 e plotar curva Gap vs. k.",
                                    "Comparar resultados do Elbow com Gap Statistic.",
                                    "Selecionar k ótimo final baseado em ambos métodos.",
                                    "Validar clusters com métricas adicionais como Silhouette Score."
                                  ],
                                  "verification": "Produzir relatório comparativo com gráficos e k final justificado.",
                                  "estimatedTime": "2 horas",
                                  "materials": [
                                    "Código Python para Gap Statistic (implementação ou gap_statistic lib)",
                                    "Datasets preparados"
                                  ],
                                  "tips": "Para Gap, gere referências nulas com uniforme em bounding box dos dados.",
                                  "learningObjective": "Integrar múltiplos métodos para decisão robusta de k.",
                                  "commonMistakes": [
                                    "Erro no cálculo de log(Wk)",
                                    "Poucas simulações de referência (use B=100)",
                                    "Não plotar bandas de erro no Gap"
                                  ]
                                }
                              ],
                              "practicalExample": "No dataset Iris (150 amostras de flores), aplique KMeans com Elbow e Gap para determinar k=3 (espécies), plotando curvas e validando clusters que separam setosas, versicolor e virginica.",
                              "finalVerifications": [
                                "Plotar corretamente curvas Elbow e Gap para dataset real.",
                                "Justificar k ótimo com evidências visuais e numéricas.",
                                "Comparar resultados de ambos métodos coherentemente.",
                                "Aplicar em dataset não-Iris com sucesso.",
                                "Interpretar limitações em dados reais (ex: ruído, não-esfericidade)."
                              ],
                              "assessmentCriteria": [
                                "Precisão na implementação dos métodos (código executável e correto).",
                                "Qualidade das visualizações (gráficos claros e anotados).",
                                "Justificativa fundamentada para escolha de k.",
                                "Tratamento adequado de dados reais (normalização, outliers).",
                                "Análise comparativa robusta entre métodos.",
                                "Uso de métricas complementares para validação."
                              ],
                              "crossCurricularConnections": [
                                "Estatística: Análise de variância e distribuições de referência.",
                                "Programação: Python com bibliotecas de ML (scikit-learn).",
                                "Visualização de Dados: Matplotlib/Seaborn para curvas e clusters.",
                                "Ciência de Dados: Validação de modelos não-supervisionados.",
                                "Matemática: Otimização e funções de custo (inércia)."
                              ],
                              "realWorldApplication": "Em marketing, segmentar clientes de e-commerce para campanhas personalizadas; em saúde, agrupar pacientes por sintomas para diagnósticos; em imagem, determinar número de regiões em segmentação de fotos."
                            },
                            "estimatedTime": "1 hora",
                            "difficulty": "intermediate",
                            "status": "not_started",
                            "prerequisites": [
                              "10.1.5.2.2.1"
                            ]
                          }
                        ]
                      },
                      {
                        "id": "10.1.5.2.3",
                        "name": "Redução de Dimensionalidade",
                        "description": "Métodos para reduzir o número de features preservando informação relevante, facilitando visualização e análise em dados de alta dimensionalidade.",
                        "specificSkills": [
                          {
                            "id": "10.1.5.2.3.1",
                            "name": "Compreender Análise de Componentes Principais (PCA)",
                            "description": "Explicar PCA como decomposição em valores singulares, projeção em componentes principais e interpretação de variância explicada.",
                            "atomicExpansion": {
                              "steps": [
                                {
                                  "stepNumber": 1,
                                  "title": "Entender os Fundamentos da Matriz de Covariância e Correlação",
                                  "subSteps": [
                                    "Calcule a matriz de covariância para um dataset simples com 2-3 variáveis.",
                                    "Explique a diferença entre covariância e correlação, normalizando os dados.",
                                    "Identifique padrões de variância nos dados originais usando scatter plots.",
                                    "Implemente o cálculo em Python com NumPy.",
                                    "Visualize a matriz de covariância como heatmap."
                                  ],
                                  "verification": "Gerar e interpretar corretamente a matriz de covariância para um dataset de teste, confirmando valores com cálculos manuais.",
                                  "estimatedTime": "1 hora",
                                  "materials": [
                                    "Python com NumPy e Matplotlib",
                                    "Dataset Iris ou similar"
                                  ],
                                  "tips": "Sempre centralize os dados (subtraia a média) antes de calcular a covariância para evitar viés.",
                                  "learningObjective": "Compreender como a matriz de covariância captura as relações lineares e a variância nos dados.",
                                  "commonMistakes": [
                                    "Esquecer de centralizar os dados",
                                    "Confundir covariância (não normalizada) com correlação",
                                    "Interpretar valores absolutos sem contexto de escala"
                                  ]
                                },
                                {
                                  "stepNumber": 2,
                                  "title": "Compreender a Decomposição em Valores Singulares (SVD)",
                                  "subSteps": [
                                    "Revise autovalores e autovetores de uma matriz simétrica.",
                                    "Aplique SVD à matriz de covariância: X = U Σ V^T.",
                                    "Identifique os componentes: U (esquerdos), Σ (valores singulares), V (direitos).",
                                    "Implemente SVD usando numpy.linalg.svd().",
                                    "Compare SVD com decomposição espectral para matrizes de covariância."
                                  ],
                                  "verification": "Decompor uma matriz de covariância via SVD e reconstruir os dados originais com erro mínimo (MSE < 0.01).",
                                  "estimatedTime": "1.5 horas",
                                  "materials": [
                                    "Python com NumPy",
                                    "Notebook Jupyter",
                                    "Documentação NumPy SVD"
                                  ],
                                  "tips": "Lembre-se que para PCA centrada, SVD da matriz de dados centrados equivale à decomposição da covariância.",
                                  "learningObjective": "Dominar SVD como base algébrica para PCA, entendendo sua relação com autovalores.",
                                  "commonMistakes": [
                                    "Confundir U e V em SVD",
                                    "Não ordenar valores singulares em ordem decrescente",
                                    "Ignorar que SVD funciona para qualquer matriz, não só quadrada"
                                  ]
                                },
                                {
                                  "stepNumber": 3,
                                  "title": "Realizar a Projeção em Componentes Principais",
                                  "subSteps": [
                                    "Extraia os autovetores principais (colunas de V) ordenados por autovalores decrescentes.",
                                    "Projete os dados originais nos primeiros k componentes: Z = X_centralizado @ V_k.",
                                    "Implemente a projeção em código Python.",
                                    "Visualize os dados projetados em 2D ou 3D.",
                                    "Reconstrua os dados originais a partir da projeção para verificar perda de informação."
                                  ],
                                  "verification": "Projetar dados em 2 componentes principais e plotar, mostrando separação de classes no dataset Iris.",
                                  "estimatedTime": "1 hora",
                                  "materials": [
                                    "Python com NumPy, Matplotlib e Scikit-learn",
                                    "Dataset Iris"
                                  ],
                                  "tips": "Use scikit-learn PCA para validação, mas implemente manualmente para compreensão profunda.",
                                  "learningObjective": "Aplicar projeção ortogonal para redução de dimensionalidade preservando variância máxima.",
                                  "commonMistakes": [
                                    "Não ordenar componentes por variância",
                                    "Projetar sem centralizar dados",
                                    "Confundir projeção com rotação simples"
                                  ]
                                },
                                {
                                  "stepNumber": 4,
                                  "title": "Interpretar a Variância Explicada e Selecionar Componentes",
                                  "subSteps": [
                                    "Calcule a variância explicada: (autovalores / soma total) * 100.",
                                    "Gere scree plot e cumulative explained variance plot.",
                                    "Decida número de componentes baseado em >80-90% variância cumulativa.",
                                    "Interprete loadings (correlações variáveis-componentes).",
                                    "Avalie impacto da redução via métricas como MSE de reconstrução."
                                  ],
                                  "verification": "Produzir scree plot e relatório explicando escolha de k componentes com >85% variância explicada.",
                                  "estimatedTime": "1 hora",
                                  "materials": [
                                    "Python com Matplotlib e Scikit-learn",
                                    "Dataset multidimensional como Wine"
                                  ],
                                  "tips": "Kaiser criterion (autovalores >1) ou elbow method para seleção automática.",
                                  "learningObjective": "Interpretar quanto da variância original é capturada e justificar escolhas de dimensionalidade.",
                                  "commonMistakes": [
                                    "Ignorar variância cumulativa, focando só no primeiro PC",
                                    "Sobreajustar k alto demais",
                                    "Não relatar em porcentagem"
                                  ]
                                }
                              ],
                              "practicalExample": "Usando o dataset Iris (4 features: comprimento/sépalas/pétalas), aplique PCA para reduzir para 2 componentes, visualize separação de espécies em scatter plot 2D e interprete que ~95% da variância é explicada, facilitando visualização sem perda significativa.",
                              "finalVerifications": [
                                "Explicar verbalmente ou por escrito a relação SVD-PCA com exemplo numérico pequeno.",
                                "Implementar PCA do zero em Python para dataset 10x3 e validar com sklearn.",
                                "Interpretar scree plot e propor k para dataset real.",
                                "Reconstruir dados e calcular MSE <5%.",
                                "Discutir loadings de um componente principal.",
                                "Comparar PCA vs. t-SNE em termos de interpretabilidade."
                              ],
                              "assessmentCriteria": [
                                "Precisão conceitual: Correta distinção SVD, projeção e variância (30%)",
                                "Implementação prática: Código funcional e eficiente (25%)",
                                "Interpretação qualitativa: Explicação clara de resultados (20%)",
                                "Visualizações adequadas: Plots informativos e legíveis (15%)",
                                "Justificativa de escolhas: Critérios para k componentes (10%)"
                              ],
                              "crossCurricularConnections": [
                                "Álgebra Linear: Autovalores e decomposições matriciais.",
                                "Estatística: Análise multivariada e testes de variância.",
                                "Ciência de Dados: Pré-processamento para ML supervisionado.",
                                "Visualização de Dados: Redução para plots exploratórios.",
                                "Computação Científica: Otimização numérica em NumPy/SciPy."
                              ],
                              "realWorldApplication": "Na genômica, PCA reduz milhares de genes em dezenas de componentes para identificar padrões de expressão gênica em câncer; em visão computacional, comprime imagens de alta dimensão para reconhecimento facial eficiente."
                            },
                            "estimatedTime": "2 horas",
                            "difficulty": "intermediate",
                            "status": "not_started",
                            "prerequisites": [
                              "10.1.5.2.1"
                            ]
                          },
                          {
                            "id": "10.1.5.2.3.2",
                            "name": "Aplicar t-SNE para Visualização",
                            "description": "Descrever t-SNE para redução não-linear, preservação de distâncias locais e uso em visualização de dados de alta dimensão.",
                            "atomicExpansion": {
                              "steps": [
                                {
                                  "stepNumber": 1,
                                  "title": "Compreender os Conceitos Fundamentais do t-SNE",
                                  "subSteps": [
                                    "Estude a necessidade de redução de dimensionalidade em dados de alta dimensão (ex.: >10 features).",
                                    "Aprenda como t-SNE calcula similaridades locais usando probabilidades condicionais baseadas em distâncias euclidianas.",
                                    "Entenda a modelagem em espaço de baixa dimensão com distribuição t-Student para preservar estruturas locais.",
                                    "Revise a minimização da divergência KL para alinhar distribuições de alta e baixa dimensão.",
                                    "Compare t-SNE com PCA: destaque a não-linearidade e foco em locais vs. globais."
                                  ],
                                  "verification": "Escreva um resumo de 200 palavras explicando os passos do algoritmo e envie para revisão.",
                                  "estimatedTime": "1.5 horas",
                                  "materials": [
                                    "Paper original 'Visualizing Data using t-SNE' de van der Maaten (2014)",
                                    "Documentação scikit-learn: TSNE",
                                    "Vídeo explicativo no YouTube sobre t-SNE (3Blue1Brown ou similar)"
                                  ],
                                  "tips": "Visualize animações do processo de embedding para melhor compreensão intuitiva.",
                                  "learningObjective": "Explicar matematicamente como t-SNE preserva distâncias locais em vez de globais.",
                                  "commonMistakes": [
                                    "Confundir t-SNE com métodos lineares como PCA",
                                    "Ignorar o trade-off entre preservação local e global",
                                    "Achar que t-SNE é determinístico (é estocástico)"
                                  ]
                                },
                                {
                                  "stepNumber": 2,
                                  "title": "Configurar o Ambiente de Computação",
                                  "subSteps": [
                                    "Instale Python 3.8+ e crie um ambiente virtual com conda ou venv.",
                                    "Instale bibliotecas essenciais: pip install scikit-learn matplotlib seaborn numpy pandas.",
                                    "Teste a instalação importando TSNE de sklearn.manifold e plotando um gráfico simples.",
                                    "Baixe um dataset de teste high-dimensional (ex.: make_blobs do sklearn com n_features=50).",
                                    "Configure Jupyter Notebook para experimentação interativa."
                                  ],
                                  "verification": "Execute um código que importe TSNE e gere um erro se falhar; compartilhe screenshot do notebook rodando.",
                                  "estimatedTime": "45 minutos",
                                  "materials": [
                                    "Anaconda ou Miniconda",
                                    "Jupyter Notebook",
                                    "Documentação pip scikit-learn"
                                  ],
                                  "tips": "Use Google Colab se não quiser instalar localmente para testes rápidos.",
                                  "learningObjective": "Preparar um ambiente reprodutível para aplicação de t-SNE.",
                                  "commonMistakes": [
                                    "Versões incompatíveis de sklearn (>1.0 pode mudar APIs)",
                                    "Esquecer de padronizar dados antes",
                                    "Não usar random_state para reprodutibilidade"
                                  ]
                                },
                                {
                                  "stepNumber": 3,
                                  "title": "Preparar e Explorar o Dataset",
                                  "subSteps": [
                                    "Carregue um dataset high-dimensional (ex.: MNIST ou Iris expandido).",
                                    "Realize EDA: verifique shape, distribuições, correlações e outliers.",
                                    "Padronize os dados com StandardScaler para mean=0 e std=1.",
                                    "Divida em train/test se aplicável e reduza ruído removendo features irrelevantes.",
                                    "Visualize dimensionalidade original com PCA 2D como baseline."
                                  ],
                                  "verification": "Gere histogramas e plot PCA inicial; confirme dados escalados com describe().",
                                  "estimatedTime": "1 hora",
                                  "materials": [
                                    "Dataset MNIST via sklearn.datasets.fetch_openml",
                                    "Pandas para EDA",
                                    "Seaborn para visualizações"
                                  ],
                                  "tips": "Sempre escale dados antes de t-SNE, pois é sensível a escalas.",
                                  "learningObjective": "Preparar dados adequadamente para evitar distorções no embedding.",
                                  "commonMistakes": [
                                    "Não escalar features",
                                    "Usar dados com missing values",
                                    "Ignorar balanceamento de classes em visualizações coloridas"
                                  ]
                                },
                                {
                                  "stepNumber": 4,
                                  "title": "Implementar e Ajustar t-SNE",
                                  "subSteps": [
                                    "Inicie TSNE com parâmetros default: n_components=2, perplexity=30, n_iter=1000.",
                                    "Ajuste hiperparâmetros: teste perplexity (5-50), learning_rate ('auto'), early_exaggeration=12.",
                                    "Execute fit_transform nos dados preparados e capture o embedding.",
                                    "Repita com múltiplas seeds (random_state=42, 0, 100) para avaliar estabilidade.",
                                    "Compare runtime e qualidade com diferentes valores de n_iter."
                                  ],
                                  "verification": "Gere 3 embeddings com seeds diferentes e calcule pairwise distances média entre eles (<0.1 de variação).",
                                  "estimatedTime": "1.5 horas",
                                  "materials": [
                                    "sklearn.manifold.TSNE",
                                    "GridSearch para tuning manual",
                                    "Timer com %timeit no Jupyter"
                                  ],
                                  "tips": "Perplexity ≈ 3 * sqrt(n_samples/3); evite > número de pontos.",
                                  "learningObjective": "Aplicar t-SNE otimizando parâmetros para melhor visualização.",
                                  "commonMistakes": [
                                    "Perplexity muito alta/baixa causando clusters ruins",
                                    "Poucas iterações levando a embedding instável",
                                    "Não usar metric='euclidean' explicitamente"
                                  ]
                                },
                                {
                                  "stepNumber": 5,
                                  "title": "Visualizar, Interpretar e Validar Resultados",
                                  "subSteps": [
                                    "Plote o embedding 2D com scatterplot colorido por labels verdadeiras.",
                                    "Analise clusters: densidades locais, separação e interpretação qualitativa.",
                                    "Calcule métricas: silhouette score, Davies-Bouldin para validar clusters.",
                                    "Compare com UMAP ou PCA para discutir trade-offs.",
                                    "Documente insights e exporte plot como PNG/PDF."
                                  ],
                                  "verification": "Interprete o plot em um relatório curto: identifique 2-3 clusters principais e sua correspondência.",
                                  "estimatedTime": "1 hora",
                                  "materials": [
                                    "Matplotlib/Seaborn para plots",
                                    "sklearn.metrics.silhouette_score",
                                    "UMAP-learn para comparação"
                                  ],
                                  "tips": "Use plotly para interatividade em hover com labels.",
                                  "learningObjective": "Interpretar embeddings t-SNE criticamente, reconhecendo limitações como não-preservação de distâncias globais.",
                                  "commonMistakes": [
                                    "Sobreinterpretar clusters como classes reais",
                                    "Ignorar variabilidade estocástica",
                                    "Confundir similaridade local com causalidade"
                                  ]
                                }
                              ],
                              "practicalExample": "Carregue o dataset MNIST (imagens 28x28 pixels achatadas para 784 dims). Aplique t-SNE com perplexity=30 e plote em 2D colorido por dígitos (0-9). Observe como dígitos similares (ex.: 4 e 9) formam clusters próximos, revelando estrutura manifold não-linear invisível em PCA.",
                              "finalVerifications": [
                                "Embedding 2D mostra preservação clara de estruturas locais (clusters bem definidos).",
                                "Silhouette score >0.3 indica boa separação de clusters.",
                                "Variação entre runs com seeds diferentes <10% em distâncias médias.",
                                "Relatório explica impacto de perplexity em 2 plots comparativos.",
                                "Comparação com PCA destaca superioridade em não-linearidade.",
                                "Código é reprodutível com random_state fixo."
                              ],
                              "assessmentCriteria": [
                                "Explicação precisa da matemática: probabilidades condicionais e KL divergence (80% acurácia).",
                                "Escolha e justificativa de hiperparâmetros (perplexity adequada ao dataset).",
                                "Preparação correta de dados (escalonamento, EDA documentado).",
                                "Interpretação crítica: reconhecimento de limitações como custo computacional O(n^2).",
                                "Visualizações claras e profissionais com legendas e títulos.",
                                "Métricas quantitativas usadas para validação além de inspeção visual."
                              ],
                              "crossCurricularConnections": [
                                "Estatística: Divergência KL e distribuições de probabilidade (t-Student vs. Gaussiana).",
                                "Programação: Manipulação vetorial com NumPy e pipelines scikit-learn.",
                                "Visualização de Dados: Técnicas de plotting com Matplotlib/Seaborn/Plotly.",
                                "Aprendizado de Máquina Supervisionado: Validação de clusters com métricas como silhouette.",
                                "Bioinformática: Aplicação em scRNA-seq para clustering de células."
                              ],
                              "realWorldApplication": "Em bioinformática, t-SNE visualiza perfis de expressão gênica em single-cell RNA sequencing (ex.: Seurat no R), permitindo descoberta de subtipos de células cancerígenas; na recomendação, embeddings de itens high-dim revelam preferências de usuários em plataformas como Netflix."
                            },
                            "estimatedTime": "1.5 horas",
                            "difficulty": "advanced",
                            "status": "not_started",
                            "prerequisites": [
                              "10.1.5.2.3.1"
                            ]
                          },
                          {
                            "id": "10.1.5.2.3.3",
                            "name": "Avaliar Impacto da Redução",
                            "description": "Medir perda de informação pós-redução usando variância explicada e comparar com dados originais em tarefas downstream.",
                            "atomicExpansion": {
                              "steps": [
                                {
                                  "stepNumber": 1,
                                  "title": "Preparar Dados e Aplicar Redução de Dimensionalidade",
                                  "subSteps": [
                                    "Carregue um dataset multidimensional (ex: Iris ou MNIST subset).",
                                    "Padronize os dados usando StandardScaler para garantir escalas comparáveis.",
                                    "Aplique PCA (Principal Component Analysis) retendo k componentes principais (ex: k=2 ou 3).",
                                    "Salve os dados originais e reduzidos para comparação posterior.",
                                    "Gere um scree plot para visualizar eigenvalues."
                                  ],
                                  "verification": "Confirme que os dados reduzidos têm a dimensionalidade desejada e o scree plot foi gerado sem erros.",
                                  "estimatedTime": "45 minutos",
                                  "materials": [
                                    "Python",
                                    "scikit-learn (PCA, StandardScaler)",
                                    "matplotlib",
                                    "pandas",
                                    "Dataset exemplo (Iris)"
                                  ],
                                  "tips": "Sempre padronize antes do PCA para evitar viés de escala; comece com k pequeno para testes rápidos.",
                                  "learningObjective": "Entender o pré-processamento necessário e aplicar redução de dimensionalidade corretamente.",
                                  "commonMistakes": [
                                    "Esquecer de padronizar dados",
                                    "Escolher k arbitrário sem análise inicial",
                                    "Usar dados não numéricos"
                                  ]
                                },
                                {
                                  "stepNumber": 2,
                                  "title": "Calcular e Analisar Variância Explicada",
                                  "subSteps": [
                                    "Extraia o explained_variance_ratio_ do modelo PCA.",
                                    "Calcule a variância cumulativa somando os ratios sequencialmente.",
                                    "Identifique o número mínimo de componentes para capturar 80-95% da variância.",
                                    "Compare a variância total explicada com a dos dados originais (idealmente próxima de 100%).",
                                    "Plote a variância cumulativa para visualização intuitiva."
                                  ],
                                  "verification": "Variância cumulativa plotada e valor total >80%; relatório com percentual de perda calculado.",
                                  "estimatedTime": "30 minutos",
                                  "materials": [
                                    "scikit-learn PCA",
                                    "matplotlib",
                                    "numpy para cálculos"
                                  ],
                                  "tips": "Use np.cumsum() para variância cumulativa; mire em 90% como threshold comum para equilíbrio.",
                                  "learningObjective": "Quantificar a perda de informação usando métricas de variância explicada.",
                                  "commonMistakes": [
                                    "Confundir explained_variance_ com ratio",
                                    "Ignorar variância cumulativa",
                                    "Não plotar para inspeção visual"
                                  ]
                                },
                                {
                                  "stepNumber": 3,
                                  "title": "Comparar Desempenho em Tarefas Downstream",
                                  "subSteps": [
                                    "Escolha uma tarefa downstream (ex: K-Means clustering ou classificação simples).",
                                    "Treine e avalie o modelo nos dados originais, registrando métricas (ex: silhouette score, accuracy).",
                                    "Repita o treinamento nos dados reduzidos e compare métricas.",
                                    "Calcule a diferença percentual nas métricas (ex: queda na accuracy).",
                                    "Teste sensibilidade variando k componentes."
                                  ],
                                  "verification": "Tabela comparativa de métricas gerada; diferença <10% considerada aceitável.",
                                  "estimatedTime": "60 minutos",
                                  "materials": [
                                    "scikit-learn (KMeans, accuracy_score)",
                                    "Dataset com labels para avaliação supervisionada opcional"
                                  ],
                                  "tips": "Use silhouette_score para clustering não-supervisionado; compare múltiplas seeds para robustez.",
                                  "learningObjective": "Avaliar impacto prático da redução em pipelines de ML.",
                                  "commonMistakes": [
                                    "Usar métrica inadequada para a tarefa",
                                    "Treinar apenas uma vez sem validação cruzada",
                                    "Ignorar overfitting em dados reduzidos"
                                  ]
                                },
                                {
                                  "stepNumber": 4,
                                  "title": "Interpretar Resultados e Decidir Threshold",
                                  "subSteps": [
                                    "Resuma perdas: variância não explicada e degradação em métricas downstream.",
                                    "Defina critérios de aceitação (ex: perda <5% em ambas).",
                                    "Recomende k ótimo baseado em trade-off dimensionalidade vs. performance.",
                                    "Documente insights em relatório (gráficos + tabelas).",
                                    "Teste com outro dataset para generalização."
                                  ],
                                  "verification": "Relatório final com recomendação clara de k e justificativa quantitativa.",
                                  "estimatedTime": "30 minutos",
                                  "materials": [
                                    "Jupyter Notebook para documentação",
                                    "Gráficos gerados anteriormente"
                                  ],
                                  "tips": "Priorize tarefas downstream sobre variância pura; use elbow method para k.",
                                  "learningObjective": "Sintetizar análises para decisões informadas em redução de dimensionalidade.",
                                  "commonMistakes": [
                                    "Focar só em variância ignorando performance prática",
                                    "Escolher k muito baixo sem justificativa",
                                    "Não documentar trade-offs"
                                  ]
                                }
                              ],
                              "practicalExample": "Usando dataset Iris: Aplique PCA para 2 componentes (95% variância). K-Means no original: silhouette=0.55; reduzido: 0.52 (queda 5%). Conclusão: Redução viável com perda mínima.",
                              "finalVerifications": [
                                "Variância explicada cumulativa ≥85%.",
                                "Degradação em métrica downstream ≤10%.",
                                "Scree plot e cumulativa plotados corretamente.",
                                "Relatório com tabela comparativa de métricas.",
                                "Recomendação de k justificada quantitativamente.",
                                "Teste sensibilidade com 2-3 valores de k."
                              ],
                              "assessmentCriteria": [
                                "Precisão no cálculo de variância explicada e cumulativa.",
                                "Seleção e aplicação correta de tarefas downstream.",
                                "Interpretação equilibrada de trade-offs (dimensionalidade vs. perda).",
                                "Visualizações claras e profissionais.",
                                "Documentação completa com conclusões acionáveis.",
                                "Tratamento de erros comuns como padronização."
                              ],
                              "crossCurricularConnections": [
                                "Estatística: Análise de variância e componentes principais.",
                                "Programação: Manipulação de dados com Python/scikit-learn.",
                                "Visualização de Dados: Plots de scree e cumulativa.",
                                "Ciência de Dados: Pipelines de ML e avaliação de modelos."
                              ],
                              "realWorldApplication": "Em genômica, reduzir 20k genes para 100 PCs sem perda em clustering de câncer; em imagens, comprimir features para detecção de objetos em tempo real."
                            },
                            "estimatedTime": "1 hora",
                            "difficulty": "intermediate",
                            "status": "not_started",
                            "prerequisites": [
                              "10.1.5.2.3.1"
                            ]
                          }
                        ]
                      }
                    ]
                  },
                  {
                    "id": "10.1.5.3",
                    "name": "Ambiente Independente e Identicamente Distribuído (i.i.d.)",
                    "description": "Assunção fundamental para modelagem em aprendizado de máquina.",
                    "individualConcepts": [
                      {
                        "id": "10.1.5.3.1",
                        "name": "Independência Estatística entre Observações",
                        "description": "Conceito fundamental que estabelece que as observações em um conjunto de dados não influenciam umas às outras, garantindo que o conhecimento de uma não afete a probabilidade das demais.",
                        "specificSkills": [
                          {
                            "id": "10.1.5.3.1.1",
                            "name": "Definir independência estatística",
                            "description": "Explicar a definição formal de independência entre variáveis aleatórias, usando probabilidades condicionais: P(X,Y) = P(X)P(Y), e ilustrar com exemplos simples como lançamento de moedas independentes.",
                            "atomicExpansion": {
                              "steps": [
                                {
                                  "stepNumber": 1,
                                  "title": "Revisar Conceitos Básicos de Probabilidade",
                                  "subSteps": [
                                    "Defina probabilidade conjunta P(X,Y) como a probabilidade de ambos os eventos ocorrerem simultaneamente.",
                                    "Explique probabilidade marginal P(X) como a soma sobre todos os valores de Y de P(X,Y).",
                                    "Calcule exemplos simples: para uma urna com 2 bolas vermelhas e 3 azuis, P(primeira vermelha) = 2/5.",
                                    "Pratique com tabela de probabilidades conjuntas para duas variáveis discretas.",
                                    "Identifique diferenças entre eventos dependentes e independentes intuitivamente."
                                  ],
                                  "verification": "Construa uma tabela de probabilidades conjuntas e marginais correta para um exemplo dado.",
                                  "estimatedTime": "20 minutos",
                                  "materials": [
                                    "Papel e caneta",
                                    "Calculadora opcional"
                                  ],
                                  "tips": "Use tabelas 2x2 para visualizar probabilidades conjuntas de forma clara.",
                                  "learningObjective": "Compreender as bases probabilísticas necessárias para independência.",
                                  "commonMistakes": [
                                    "Confundir conjunta com condicional",
                                    "Esquecer de normalizar probabilidades marginais"
                                  ]
                                },
                                {
                                  "stepNumber": 2,
                                  "title": "Definir Formalmente Independência Estatística",
                                  "subSteps": [
                                    "Estude a definição: Duas variáveis X e Y são independentes se P(X,Y) = P(X) * P(Y) para todos os valores.",
                                    "Escreva a fórmula matemática em notação LaTeX ou simbólica.",
                                    "Discuta o que isso significa: o conhecimento de uma não afeta a outra.",
                                    "Compare com dependência: P(X,Y) ≠ P(X)P(Y).",
                                    "Memorize a definição exata e para parafraseá-la em suas palavras."
                                  ],
                                  "verification": "Escreva a definição formal sem olhar para notas e dê um contraexemplo de dependência.",
                                  "estimatedTime": "15 minutos",
                                  "materials": [
                                    "Folha de anotações",
                                    "Referência de livro de probabilidade (opcional)"
                                  ],
                                  "tips": "Associe a fórmula a uma multiplicação simples para fixar na memória.",
                                  "learningObjective": "Memorizar e reproduzir a definição formal de independência.",
                                  "commonMistakes": [
                                    "Invertir a fórmula para P(X|Y)",
                                    "Pensar que independência é simetria apenas"
                                  ]
                                },
                                {
                                  "stepNumber": 3,
                                  "title": "Relacionar com Probabilidades Condicionais",
                                  "subSteps": [
                                    "Lembre que P(X|Y) = P(X,Y)/P(Y) e P(Y|X) = P(X,Y)/P(X).",
                                    "Mostre que independência implica P(X|Y) = P(X) e P(Y|X) = P(Y).",
                                    "Prove algebricamente: se P(X,Y) = P(X)P(Y), então P(X|Y) = P(X)P(Y)/P(Y) = P(X).",
                                    "Verifique o inverso: se P(X|Y)=P(X) para todo Y, então independência.",
                                    "Pratique derivando as condições equivalentes."
                                  ],
                                  "verification": "Derive corretamente a equivalência entre as definições em um quadro ou papel.",
                                  "estimatedTime": "25 minutos",
                                  "materials": [
                                    "Papel quadriculado",
                                    "Caneta"
                                  ],
                                  "tips": "Use setas para mostrar implicações: P(X,Y)=P(X)P(Y) ⇔ P(X|Y)=P(X).",
                                  "learningObjective": "Conectar independência à intuição condicional.",
                                  "commonMistakes": [
                                    "Esquecer de dividir por P(Y) na prova",
                                    "Confundir implicação com equivalência"
                                  ]
                                },
                                {
                                  "stepNumber": 4,
                                  "title": "Ilustrar com Exemplos Simples",
                                  "subSteps": [
                                    "Exemplo 1: Dois lançamentos de moeda justa. Calcule P(Cara1 e Cara2) = 1/4, P(Cara1)=1/2, P(Cara2)=1/2.",
                                    "Verifique: (1/2)*(1/2)=1/4, logo independentes.",
                                    "Exemplo 2: Duas cartas de baralho sem reposição (dependentes): P(Ace1 e Ace2) ≠ P(Ace1)P(Ace2).",
                                    "Crie seu próprio exemplo com dados ou urnas.",
                                    "Simule em tabela ou código simples para confirmar."
                                  ],
                                  "verification": "Crie e resolva um exemplo original mostrando independência ou não.",
                                  "estimatedTime": "20 minutos",
                                  "materials": [
                                    "Moeda física ou simulador online",
                                    "Papel para tabelas"
                                  ],
                                  "tips": "Sempre calcule numericamente para validar a fórmula.",
                                  "learningObjective": "Aplicar a definição em cenários concretos.",
                                  "commonMistakes": [
                                    "Ignorar reposição em exemplos",
                                    "Calcular probabilidades erradas em frações"
                                  ]
                                }
                              ],
                              "practicalExample": "Considere dois lançamentos independentes de uma moeda justa. P(primeiro Cara e segundo Cara) = 0.25. P(primeiro Cara) = 0.5 e P(segundo Cara) = 0.5. Verifique: 0.5 * 0.5 = 0.25, confirmando independência. Para dependência, retire duas bolas de uma urna sem reposição.",
                              "finalVerifications": [
                                "Define corretamente P(X,Y) = P(X)P(Y).",
                                "Explica relação com P(X|Y) = P(X).",
                                "Calcula corretamente probabilidades em exemplos simples.",
                                "Distingue independência de dependência com contraexemplos.",
                                "Aplica a definição em um novo cenário.",
                                "Prova algebricamente a equivalência das definições."
                              ],
                              "assessmentCriteria": [
                                "Precisão na definição formal (100% correto).",
                                "Correção nos cálculos probabilísticos (sem erros aritméticos).",
                                "Clareza na explicação condicional.",
                                "Criatividade e acurácia em exemplos originais.",
                                "Compreensão conceitual via parafraseamento.",
                                "Identificação de erros comuns em cenários dados."
                              ],
                              "crossCurricularConnections": [
                                "Estatística: Base para testes de independência (qui-quadrado).",
                                "Aprendizado de Máquina: Suposição i.i.d. em dados de treinamento.",
                                "Programação: Simulações em Python com numpy.random.",
                                "Física: Eventos independentes em mecânica quântica.",
                                "Economia: Modelos de risco com variáveis independentes."
                              ],
                              "realWorldApplication": "Em Ciência de Dados e Machine Learning, a independência estatística entre observações (i.i.d.) é crucial para validar modelos estatísticos, como na validação cruzada e estimação de parâmetros, evitando viés em previsões de séries temporais ou dados correlacionados."
                            },
                            "estimatedTime": "1 hora",
                            "difficulty": "beginner",
                            "status": "not_started",
                            "prerequisites": []
                          },
                          {
                            "id": "10.1.5.3.1.2",
                            "name": "Identificar exemplos de independência e dependência",
                            "description": "Diferenciar cenários de independência (ex.: medidas de temperatura em dias diferentes sem correlação) de dependência (ex.: altura e peso de pessoas), usando gráficos e testes básicos.",
                            "atomicExpansion": {
                              "steps": [
                                {
                                  "stepNumber": 1,
                                  "title": "Entender os Conceitos Básicos de Independência e Dependência Estatística",
                                  "subSteps": [
                                    "Defina independência estatística: eventos ou variáveis onde o resultado de um não afeta o outro.",
                                    "Defina dependência estatística: variáveis onde há uma relação ou correlação entre elas.",
                                    "Estude a diferença entre correlação e causalidade.",
                                    "Revise exemplos iniciais: lançamento de moedas independentes vs. altura e peso correlacionados.",
                                    "Anote definições em suas próprias palavras."
                                  ],
                                  "verification": "Crie um resumo de 100 palavras explicando os conceitos e forneça 2 exemplos corretos de cada.",
                                  "estimatedTime": "45 minutos",
                                  "materials": [
                                    "Notebook e caneta",
                                    "Vídeo introdutório sobre estatística básica (ex: Khan Academy)"
                                  ],
                                  "tips": "Use analogias cotidianas, como 'chuva hoje não afeta chuva amanhã' para independência.",
                                  "learningObjective": "Compreender definições fundamentais para diferenciar independência de dependência.",
                                  "commonMistakes": [
                                    "Confundir correlação com causalidade",
                                    "Achar que ausência de correlação implica independência total"
                                  ]
                                },
                                {
                                  "stepNumber": 2,
                                  "title": "Identificar Exemplos Cotidianos de Independência e Dependência",
                                  "subSteps": [
                                    "Liste 5 cenários independentes: ex. temperatura em dias diferentes sem padrão sazonal.",
                                    "Liste 5 cenários dependentes: ex. altura e peso de pessoas, vendas e propaganda.",
                                    "Classifique cenários ambíguos e justifique.",
                                    "Discuta com um parceiro ou em fórum por que cada exemplo se enquadra em uma categoria.",
                                    "Crie uma tabela comparativa."
                                  ],
                                  "verification": "Apresente tabela com 10 exemplos classificados corretamente, sem erros.",
                                  "estimatedTime": "30 minutos",
                                  "materials": [
                                    "Planilha Google Sheets ou papel",
                                    "Lista de cenários reais da vida diária"
                                  ],
                                  "tips": "Pergunte: 'Mudança em uma variável afeta a outra?' para testar dependência.",
                                  "learningObjective": "Reconhecer independência e dependência em contextos reais.",
                                  "commonMistakes": [
                                    "Ignorar padrões sazonais em exemplos como temperatura",
                                    "Classificar erroneamente variáveis categóricas"
                                  ]
                                },
                                {
                                  "stepNumber": 3,
                                  "title": "Visualizar Independência e Dependência com Gráficos",
                                  "subSteps": [
                                    "Gere dados simulados: 100 pontos para temperatura diária (independente) e altura/peso (dependente).",
                                    "Crie scatter plots para cada conjunto usando Python (matplotlib) ou Excel.",
                                    "Analise padrões: nuvem aleatória para independente vs. tendência linear para dependente.",
                                    "Adicione linhas de tendência e observe inclinação.",
                                    "Compare os gráficos lado a lado."
                                  ],
                                  "verification": "Produza 2 scatter plots corretos com legendas explicando padrões observados.",
                                  "estimatedTime": "60 minutos",
                                  "materials": [
                                    "Python com pandas e matplotlib ou Excel",
                                    "Datasets de exemplo (gerados ou baixados)"
                                  ],
                                  "tips": "Use cores diferentes para destacar padrões; zoom em regiões para detalhes.",
                                  "learningObjective": "Usar visualizações para diferenciar graficamente independência de dependência.",
                                  "commonMistakes": [
                                    "Escalas inadequadas que mascaram padrões",
                                    "Confundir ruído com independência"
                                  ]
                                },
                                {
                                  "stepNumber": 4,
                                  "title": "Aplicar Testes Básicos Quantitativos para Confirmar Independência/Dependência",
                                  "subSteps": [
                                    "Calcule coeficiente de correlação de Pearson para cada conjunto de dados.",
                                    "Interprete valores: próximo de 0 indica independência; próximo de ±1 indica dependência.",
                                    "Realize teste qui-quadrado simples para variáveis categóicas independentes.",
                                    "Compare resultados com visualizações anteriores.",
                                    "Documente conclusões em um relatório curto."
                                  ],
                                  "verification": "Forneça cálculos de correlação corretos e interpretações precisas para 2 datasets.",
                                  "estimatedTime": "45 minutos",
                                  "materials": [
                                    "Python com scipy.stats ou calculadora online",
                                    "Dados dos steps anteriores"
                                  ],
                                  "tips": "Lembre-se: correlação ≠ causalidade; use p-valor para significância.",
                                  "learningObjective": "Aplicar métricas quantitativas básicas para validar classificações.",
                                  "commonMistakes": [
                                    "Ignorar p-valores",
                                    "Aplicar Pearson em dados não-lineares"
                                  ]
                                }
                              ],
                              "practicalExample": "Gere dados de temperatura diária em uma cidade sem sazonalidade (independente: scatter plot aleatório, correlação ~0) vs. altura e peso de 100 alunos (dependente: scatter plot com tendência positiva, correlação ~0.7). Classifique, plote e teste.",
                              "finalVerifications": [
                                "Classifique corretamente 10 cenários mistos de independência/dependência.",
                                "Crie scatter plots que claramente diferenciem os padrões.",
                                "Calcule correlações com interpretações precisas (ex: |r| < 0.3 para independente).",
                                "Explique i.i.d. em contexto de ML com exemplos.",
                                "Identifique erros comuns em 3 gráficos fornecidos.",
                                "Relacione conceitos a aplicações reais em dados."
                              ],
                              "assessmentCriteria": [
                                "Precisão na definição e exemplos (90% correto).",
                                "Qualidade das visualizações (gráficos claros, legendas).",
                                "Correção nos cálculos de correlação e testes (erro <5%).",
                                "Profundidade na análise qualitativa e quantitativa.",
                                "Criatividade em conexões interdisciplinares.",
                                "Clareza no relatório final e verificações."
                              ],
                              "crossCurricularConnections": [
                                "Estatística: Correlação e testes de hipótese.",
                                "Ciência de Dados: Pressupostos i.i.d. em machine learning.",
                                "Biologia: Variáveis dependentes em estudos genéticos (ex: gene e traço).",
                                "Economia: Dependência entre variáveis macroeconômicas.",
                                "Física: Independência em medidas de partículas elementares."
                              ],
                              "realWorldApplication": "Em machine learning, assumir observações i.i.d. garante modelos robustos; identificar dependência em dados de sensores IoT previne viés em previsões climáticas ou análises de saúde populacional."
                            },
                            "estimatedTime": "1.5 horas",
                            "difficulty": "beginner",
                            "status": "not_started",
                            "prerequisites": []
                          },
                          {
                            "id": "10.1.5.3.1.3",
                            "name": "Calcular probabilidades em contextos independentes",
                            "description": "Resolver problemas numéricos envolvendo probabilidades de eventos independentes, como a probabilidade conjunta de múltiplos lançamentos de dados.",
                            "atomicExpansion": {
                              "steps": [
                                {
                                  "stepNumber": 1,
                                  "title": "Compreender o Conceito de Eventos Independentes",
                                  "subSteps": [
                                    "Defina eventos independentes: a ocorrência de um não afeta o outro.",
                                    "Estude exemplos clássicos, como lançamentos sucessivos de uma moeda ou dado.",
                                    "Diferencie de eventos dependentes, como cartas em um baralho sem reposição.",
                                    "Crie um diagrama de árvore para visualizar independência em dois eventos.",
                                    "Liste 3 cenários reais onde eventos são independentes (ex: chuvas em dias diferentes)."
                                  ],
                                  "verification": "Explique em suas palavras o que significa independência e dê um contraexemplo de dependência.",
                                  "estimatedTime": "20 minutos",
                                  "materials": [
                                    "Papel e caneta para diagramas",
                                    "Vídeo introdutório sobre probabilidade (Khan Academy)"
                                  ],
                                  "tips": "Use analogias cotidianas, como 'o resultado de uma moeda não muda o próximo lançamento'.",
                                  "learningObjective": "Identificar e diferenciar eventos independentes de dependentes.",
                                  "commonMistakes": [
                                    "Confundir independência com igualdade de probabilidades",
                                    "Assumir dependência em cenários claramente independentes"
                                  ]
                                },
                                {
                                  "stepNumber": 2,
                                  "title": "Calcular Probabilidades Individuais de Cada Evento",
                                  "subSteps": [
                                    "Calcule P(A) para o primeiro evento usando contagem de casos favoráveis sobre totais.",
                                    "Calcule P(B) para o segundo evento da mesma forma.",
                                    "Verifique que os eventos têm espaços amostrais fixos (ex: dado sempre 6 faces).",
                                    "Pratique com frações, decimais e porcentagens para representar probabilidades.",
                                    "Resolva 3 exercícios simples: P(cara em moeda), P(par em dado), etc."
                                  ],
                                  "verification": "Liste P(A) e P(B) corretamente para um problema dado, como dois lançamentos de dado.",
                                  "estimatedTime": "25 minutos",
                                  "materials": [
                                    "Calculadora simples",
                                    "Folha de exercícios de probabilidade básica"
                                  ],
                                  "tips": "Sempre normalize o espaço amostral para 1 (ex: 1/6 para cada face de dado).",
                                  "learningObjective": "Dominar cálculo de probabilidades marginais para eventos isolados.",
                                  "commonMistakes": [
                                    "Esquecer de dividir favoráveis por totais",
                                    "Usar decimais incorretos em frações simples"
                                  ]
                                },
                                {
                                  "stepNumber": 3,
                                  "title": "Calcular Probabilidade Conjunta para Eventos Independentes",
                                  "subSteps": [
                                    "Aplique a regra: P(A e B) = P(A) * P(B) para independentes.",
                                    "Multiplique frações: encontre denominador comum se necessário.",
                                    "Estenda para mais de dois eventos: P(A e B e C) = P(A)*P(B)*P(C).",
                                    "Pratique com 4 problemas: dois dados, moedas múltiplas, etc.",
                                    "Converta resultados para frações irredutíveis e decimais."
                                  ],
                                  "verification": "Calcule P(ambos 6 em dois dados) = (1/6)*(1/6) = 1/36 e explique o porquê.",
                                  "estimatedTime": "30 minutos",
                                  "materials": [
                                    "Calculadora",
                                    "Tabela de probabilidades de dados/moedas"
                                  ],
                                  "tips": "Lembre: independência permite multiplicar; dependência não.",
                                  "learningObjective": "Aplicar multiplicação de probabilidades para eventos independentes.",
                                  "commonMistakes": [
                                    "Multiplicar probabilidades erradas",
                                    "Adicionar em vez de multiplicar para 'e'"
                                  ]
                                },
                                {
                                  "stepNumber": 4,
                                  "title": "Resolver Problemas Completos e Validar Resultados",
                                  "subSteps": [
                                    "Leia o problema: identifique eventos independentes e o que é pedido (conjunta).",
                                    "Monte o cálculo passo a passo: P1, P2, produto.",
                                    "Verifique com simulações mentais ou contagem total de outcomes.",
                                    "Resolva 2 problemas contextualizados (ex: qualidade em fábrica com testes independentes).",
                                    "Documente solução em formato estruturado: problema, passos, resposta."
                                  ],
                                  "verification": "Resolva um problema novo corretamente e compare com solução modelo.",
                                  "estimatedTime": "35 minutos",
                                  "materials": [
                                    "Problemas de prática impressos",
                                    "Simulador online de dados/moedas"
                                  ],
                                  "tips": "Sempre pergunte: 'Os eventos se afetam?' para confirmar independência.",
                                  "learningObjective": "Integrar conceitos para resolver problemas numéricos reais.",
                                  "commonMistakes": [
                                    "Ignorar contexto de independência",
                                    "Erros aritméticos em multiplicações"
                                  ]
                                }
                              ],
                              "practicalExample": "Em uma linha de produção, cada produto é testado independentemente em duas máquinas. Probabilidade de falha na primeira é 0.05, na segunda 0.03. Calcule P(ambas falharem): 0.05 * 0.03 = 0.0015 ou 0.15%.",
                              "finalVerifications": [
                                "Calcule corretamente P(A e B) para dois eventos independentes dados.",
                                "Explique por que multiplicamos probabilidades em independentes.",
                                "Identifique se eventos em um problema são independentes.",
                                "Converta frações de probabilidade para decimal e porcentagem sem erros.",
                                "Resolva um problema com 3 eventos independentes.",
                                "Valide resultado com contagem de casos totais."
                              ],
                              "assessmentCriteria": [
                                "Precisão no cálculo de probabilidades individuais (100% correto).",
                                "Correta aplicação da regra de multiplicação para independentes.",
                                "Clareza na explicação escrita do raciocínio.",
                                "Identificação precisa de independência no contexto.",
                                "Eficiência: solução em menos de 5 minutos para problemas médios.",
                                "Uso apropriado de frações irredutíveis."
                              ],
                              "crossCurricularConnections": [
                                "Programação: Simulações Monte Carlo em Python para validar probabilidades i.i.d.",
                                "Estatística: Fundamento para distribuições i.i.d. em aprendizado de máquina.",
                                "Física: Probabilidades em colisões de partículas independentes.",
                                "Economia: Riscos independentes em portfólios de investimentos."
                              ],
                              "realWorldApplication": "Em controle de qualidade, calcular probabilidade de defeitos múltiplos em testes independentes para prever taxas de rejeição em produção; em previsão meteorológica, chance de chuvas consecutivas em dias independentes."
                            },
                            "estimatedTime": "1 hora",
                            "difficulty": "intermediate",
                            "status": "not_started",
                            "prerequisites": [
                              "10.1.5.3.1.1"
                            ]
                          }
                        ]
                      },
                      {
                        "id": "10.1.5.3.2",
                        "name": "Distribuição Identicamente Distribuída",
                        "description": "Princípio que afirma que todas as observações em um conjunto de dados são amostradas da mesma distribuição de probabilidade subjacente, assegurando homogeneidade nos dados.",
                        "specificSkills": [
                          {
                            "id": "10.1.5.3.2.1",
                            "name": "Definir distribuição idêntica",
                            "description": "Descrever o conceito de variáveis aleatórias identicamente distribuídas (i.d.), com estatísticas descritivas iguais (média, variância), e contrastar com distribuições heterogêneas.",
                            "atomicExpansion": {
                              "steps": [
                                {
                                  "stepNumber": 1,
                                  "title": "Compreender variáveis aleatórias e distribuições",
                                  "subSteps": [
                                    "Revise a definição de variável aleatória discreta e contínua.",
                                    "Identifique exemplos comuns como lançamento de moeda ou dado.",
                                    "Descreva o que é uma função de distribuição de probabilidade (PDF/CDF).",
                                    "Calcule estatísticas básicas: média (E[X]) e variância (Var(X)).",
                                    "Registre anotações sobre como distribuições definem o comportamento estatístico."
                                  ],
                                  "verification": "Resuma em um parágrafo o que é uma variável aleatória e suas estatísticas principais.",
                                  "estimatedTime": "30 minutos",
                                  "materials": "Livro de probabilidade básica, calculadora ou Python/Jupyter para cálculos.",
                                  "tips": "Use exemplos visuais como histogramas para intuitivamente entender distribuições.",
                                  "learningObjective": "Entender os fundamentos de variáveis aleatórias e suas propriedades estatísticas descritivas.",
                                  "commonMistakes": "Confundir variável aleatória com valor determinístico; ignorar que média e variância capturam o 'centro' e 'espalhamento'."
                                },
                                {
                                  "stepNumber": 2,
                                  "title": "Definir distribuição idêntica (i.d.)",
                                  "subSteps": [
                                    "Defina formalmente: Duas variáveis X e Y são identicamente distribuídas se P(X ≤ x) = P(Y ≤ x) para todo x.",
                                    "Explique que isso implica mesmas PDF/CDF e mesmas estatísticas (média, variância, etc.).",
                                    "Diferencie de independência: i.d. é sobre mesma distribuição, não sobre correlação.",
                                    "Crie um exemplo simples: dois lançamentos de uma mesma moeda justa.",
                                    "Verifique com cálculo: calcule E[X] e Var(X) para ambos e compare."
                                  ],
                                  "verification": "Escreva a definição formal e prove com um exemplo numérico que duas variáveis têm a mesma distribuição.",
                                  "estimatedTime": "45 minutos",
                                  "materials": "Papel e caneta para cálculos, software como Python (numpy.random) para simulações.",
                                  "tips": "Sempre compare CDFs graficamente para visualizar igualdade.",
                                  "learningObjective": "Capacitar o aluno a definir e reconhecer variáveis identicamente distribuídas.",
                                  "commonMistakes": "Confundir i.d. com independência; achar que i.d. implica valores iguais (não, só mesma lei probabilística)."
                                },
                                {
                                  "stepNumber": 3,
                                  "title": "Explorar estatísticas descritivas iguais",
                                  "subSteps": [
                                    "Liste estatísticas chave: média, variância, assimetria, curtose.",
                                    "Prove que se X e Y são i.d., então E[X] = E[Y] e Var(X) = Var(Y).",
                                    "Simule em código: gere amostras de duas distribuições idênticas e compare estatísticas amostrais.",
                                    "Analise impacto: por que isso é crucial para inferência estatística.",
                                    "Documente fórmulas e resultados em uma tabela comparativa."
                                  ],
                                  "verification": "Gere relatório com cálculos e simulações mostrando igualdade de estatísticas.",
                                  "estimatedTime": "40 minutos",
                                  "materials": "Python com bibliotecas numpy, matplotlib e scipy.stats.",
                                  "tips": "Use np.random.seed() para reprodutibilidade nas simulações.",
                                  "learningObjective": "Demonstrar como i.d. garante estatísticas descritivas idênticas.",
                                  "commonMistakes": "Usar amostras pequenas sem seed, levando a variações aparentes; ignorar momentos superiores."
                                },
                                {
                                  "stepNumber": 4,
                                  "title": "Contrastar com distribuições heterogêneas",
                                  "subSteps": [
                                    "Defina heterogêneas: variáveis com distribuições diferentes (ex: uma normal μ=0 σ=1, outra μ=5 σ=2).",
                                    "Compare estatísticas: calcule e mostre diferenças em média e variância.",
                                    "Crie exemplo: dado justo vs. dado viciado; simule e plote CDFs.",
                                    "Discuta implicações: viés em modelos que assumem i.i.d.",
                                    "Resuma diferenças em uma tabela: i.d. vs. heterogênea."
                                  ],
                                  "verification": "Crie um contraste escrito e gráfico entre i.d. e heterogêneo.",
                                  "estimatedTime": "35 minutos",
                                  "materials": "Python para simulações e plots, papel para tabela.",
                                  "tips": "Plote múltiplas CDFs no mesmo gráfico para contraste visual claro.",
                                  "learningObjective": "Diferenciar claramente i.d. de distribuições heterogêneas e suas consequências.",
                                  "commonMistakes": "Achar que pequenas diferenças amostrais invalidam i.d.; não contrastar estatísticas quantitativamente."
                                }
                              ],
                              "practicalExample": "Considere dois dados justos de 6 faces: X (primeiro dado) e Y (segundo dado). Ambos têm distribuição uniforme discreta P(k)=1/6 para k=1..6, média E=3.5, variância ≈2.916. São i.d. Agora, contraste com Z (dado viciado: P(6)=0.5, outros 0.1): E[Z]=4.1, Var[Z]≈2.07, heterogêneo.",
                              "finalVerifications": [
                                "Defina corretamente distribuição idêntica com notação formal.",
                                "Calcule e compare média/variância para pares i.d. e heterogêneos.",
                                "Simule e plote CDFs mostrando igualdade ou diferença.",
                                "Explique por que i.d. é assumida em ML (ex: i.i.d. para teorema do limite central).",
                                "Identifique erro em exemplo heterogêneo fornecido.",
                                "Resuma diferenças em tabela."
                              ],
                              "assessmentCriteria": [
                                "Precisão na definição formal de i.d. (mínimo 80% corretude).",
                                "Correção nos cálculos de estatísticas descritivas (erro <5%).",
                                "Qualidade dos gráficos e simulações (clareza e reprodutibilidade).",
                                "Profundidade no contraste com heterogêneas (exemplos relevantes).",
                                "Clareza na explicação de implicações práticas.",
                                "Completude da tabela comparativa."
                              ],
                              "crossCurricularConnections": [
                                "Estatística: Teorema do Limite Central requer i.i.d.",
                                "Ciência de Dados: Assunção i.i.d. em validação cruzada de ML.",
                                "Matemática Computacional: Simulações Monte Carlo com amostras i.d.",
                                "Probabilidade: Extensão para i.i.d. em processos estocásticos."
                              ],
                              "realWorldApplication": "Em Aprendizado de Máquina, assume-se que dados de treinamento e teste são i.i.d. para garantir que o modelo generalize; violação (dados heterogêneos) causa overfitting ou underfitting, como em séries temporais onde observações passadas afetam futuras."
                            },
                            "estimatedTime": "1 hora",
                            "difficulty": "beginner",
                            "status": "not_started",
                            "prerequisites": []
                          },
                          {
                            "id": "10.1.5.3.2.2",
                            "name": "Reconhecer amostragem i.d.",
                            "description": "Analisar conjuntos de dados para verificar se as amostras parecem vir da mesma distribuição, usando histogramas e testes estatísticos simples como QQ-plots.",
                            "atomicExpansion": {
                              "steps": [
                                {
                                  "stepNumber": 1,
                                  "title": "Entender o Conceito de Amostragem i.i.d. e sua Importância",
                                  "subSteps": [
                                    "Defina i.i.d. como amostras independentes e identicamente distribuídas.",
                                    "Explique independência: ausência de correlação entre amostras.",
                                    "Explique distribuição idêntica: todas as amostras vêm da mesma distribuição probabilística.",
                                    "Discuta violações comuns, como dependência temporal em séries temporais.",
                                    "Relacione com pressupostos em aprendizado de máquina."
                                  ],
                                  "verification": "Resuma em suas palavras o que significa i.i.d. e dê um exemplo de violação.",
                                  "estimatedTime": "20 minutos",
                                  "materials": [
                                    "Notebook Jupyter ou Google Colab",
                                    "Artigo introdutório sobre i.i.d. (ex: Wikipedia)"
                                  ],
                                  "tips": "Use analogias como 'lançamentos de moedas justas' para independência e identidade.",
                                  "learningObjective": "Compreender os fundamentos teóricos de amostragem i.i.d.",
                                  "commonMistakes": [
                                    "Confundir independência com identidade",
                                    "Ignorar que i.i.d. é uma suposição idealizada"
                                  ]
                                },
                                {
                                  "stepNumber": 2,
                                  "title": "Visualizar Distribuições com Histogramas",
                                  "subSteps": [
                                    "Carregue dois conjuntos de dados simulados de distribuições normais iguais e diferentes.",
                                    "Gere histogramas para cada conjunto usando matplotlib.",
                                    "Sobreponha histogramas de múltiplas amostras do mesmo conjunto.",
                                    "Compare visualmente histogramas de conjuntos supostamente i.i.d.",
                                    "Ajuste parâmetros como bins para melhor visualização."
                                  ],
                                  "verification": "Crie histogramas de dois datasets e descreva se parecem da mesma distribuição.",
                                  "estimatedTime": "30 minutos",
                                  "materials": [
                                    "Python com pandas, matplotlib, numpy",
                                    "Datasets de exemplo (gerados com np.random.normal)"
                                  ],
                                  "tips": "Use density=True para normalizar histogramas e facilitar comparações.",
                                  "learningObjective": "Interpretar histogramas para avaliar similaridade de distribuições.",
                                  "commonMistakes": [
                                    "Poucos bins levando a visualizações enganosas",
                                    "Não normalizar para comparar shapes"
                                  ]
                                },
                                {
                                  "stepNumber": 3,
                                  "title": "Aplicar QQ-Plots para Verificação Gráfica",
                                  "subSteps": [
                                    "Gere QQ-plots comparando amostras de uma distribuição teórica (normal).",
                                    "Crie QQ-plots entre duas amostras empíricas usando scipy.stats.probplot.",
                                    "Interprete alinhamento: pontos próximos à linha diagonal indicam similaridade.",
                                    "Teste com amostras i.i.d. vs. não-i.i.d. (ex: uma normal, outra exponencial).",
                                    "Quantifique desvio visual com inspeção qualitativa."
                                  ],
                                  "verification": "Produza QQ-plot de dois datasets e classifique se i.i.d. com justificativa.",
                                  "estimatedTime": "25 minutos",
                                  "materials": [
                                    "Python com scipy.stats, matplotlib",
                                    "Código de exemplo para probplot"
                                  ],
                                  "tips": "Use qqline=True para linha de referência clara.",
                                  "learningObjective": "Usar QQ-plots para detectar diferenças distributivas.",
                                  "commonMistakes": [
                                    "Interpretar desvios nas caudas como ruído",
                                    "Esquecer de comparar múltiplas amostras"
                                  ]
                                },
                                {
                                  "stepNumber": 4,
                                  "title": "Combinar Visualizações e Concluir sobre i.i.d.",
                                  "subSteps": [
                                    "Colete múltiplas amostras (ex: 3-5) de um dataset e plote histogramas/Q Q-plots em grid.",
                                    "Analise consistência visual entre amostras.",
                                    "Documente evidências de identidade (shapes similares) e independência (sem padrões).",
                                    "Teste estatístico simples: Kolmogorov-Smirnov (KS) test entre pares de amostras.",
                                    "Decida se o conjunto parece i.i.d. baseado em p-valores > 0.05."
                                  ],
                                  "verification": "Analise um dataset novo e produza relatório com plots e conclusão.",
                                  "estimatedTime": "35 minutos",
                                  "materials": [
                                    "Python com scipy.stats.ks_2samp",
                                    "Dataset real ou simulado (ex: Iris dataset splits)"
                                  ],
                                  "tips": "Sempre valide com múltiplas amostras para robustez.",
                                  "learningObjective": "Integrar ferramentas para julgamento final sobre amostragem i.i.d.",
                                  "commonMistakes": [
                                    "Confiar só em um plot",
                                    "Ignorar tamanho da amostra pequeno"
                                  ]
                                }
                              ],
                              "practicalExample": "Carregue dados de alturas de adultos de duas cidades (simulados como normais mu=170, sigma=10 vs mu=175, sigma=10). Plote histogramas e QQ-plot entre subamostras de cada cidade. Conclua se amostras intra-cidade parecem i.i.d. (sim) e inter-cidades (não).",
                              "finalVerifications": [
                                "Gere plots corretos para datasets i.i.d. e não-i.i.d.",
                                "Interprete corretamente alinhamento em QQ-plots.",
                                "Aplique KS-test e interprete p-valores.",
                                "Identifique corretamente 80% dos casos em teste cego.",
                                "Explique limitações visuais vs. testes formais.",
                                "Documente análise em relatório coerente."
                              ],
                              "assessmentCriteria": [
                                "Precisão na geração de visualizações (histogramas e QQ-plots corretos).",
                                "Qualidade da interpretação gráfica (detecta similaridades/diferenças).",
                                "Correta aplicação e interpretação de testes estatísticos simples.",
                                "Robustez: considera múltiplas amostras e tamanhos.",
                                "Clareza no relatório final com evidências.",
                                "Identificação de erros comuns e limitações."
                              ],
                              "crossCurricularConnections": [
                                "Estatística: Testes de hipóteses e distribuições probabilísticas.",
                                "Programação: Manipulação de dados com Python/pandas.",
                                "Aprendizado de Máquina: Pressupostos de i.i.d. em modelos.",
                                "Visualização de Dados: Técnicas de plotting com matplotlib."
                              ],
                              "realWorldApplication": "Em machine learning, verificar se train/validation/test sets são i.i.d. garante generalização do modelo; em controle de qualidade, amostras de produção devem ser i.i.d. para monitoramento confiável."
                            },
                            "estimatedTime": "1.5 horas",
                            "difficulty": "intermediate",
                            "status": "not_started",
                            "prerequisites": [
                              "10.1.5.3.2.1"
                            ]
                          },
                          {
                            "id": "10.1.5.3.2.3",
                            "name": "Simular dados i.d.",
                            "description": "Gerar conjuntos de dados sintéticos identicamente distribuídos usando ferramentas como Python (numpy.random), e comparar com dados não-i.d.",
                            "atomicExpansion": {
                              "steps": [
                                {
                                  "stepNumber": 1,
                                  "title": "Revisar Conceitos de i.i.d. e Configurar Ambiente Python",
                                  "subSteps": [
                                    "Defina i.i.d.: dados independentes (sem correlação) e identicamente distribuídos (mesma distribuição probabilística).",
                                    "Instale bibliotecas necessárias: numpy, matplotlib e seaborn via pip.",
                                    "Crie um Jupyter Notebook ou script Python para experimentos.",
                                    "Importe módulos: import numpy as np; import matplotlib.pyplot as plt; import seaborn as sns.",
                                    "Gere um exemplo simples de amostra aleatória para testar setup."
                                  ],
                                  "verification": "Execute imports sem erros e gere uma array numpy de 100 números aleatórios uniformes.",
                                  "estimatedTime": "15 minutos",
                                  "materials": "Python 3+, Jupyter Notebook, bibliotecas numpy, matplotlib, seaborn",
                                  "tips": "Use ambientes virtuais (venv) para isolar dependências.",
                                  "learningObjective": "Compreender os fundamentos teóricos de i.i.d. e preparar o ambiente computacional.",
                                  "commonMistakes": "Confundir independência com idêntica distribuição; esquecer de instalar pacotes."
                                },
                                {
                                  "stepNumber": 2,
                                  "title": "Gerar Conjunto de Dados Sintéticos i.i.d.",
                                  "subSteps": [
                                    "Escolha uma distribuição (ex: normal com μ=0, σ=1).",
                                    "Use np.random.normal(size=1000) para gerar dados i.i.d. (mesma seed opcional para reprodutibilidade).",
                                    "Repita geração múltiplas vezes com np.random.seed(42) para demonstrar consistência.",
                                    "Calcule estatísticas descritivas: média, variância, histograma.",
                                    "Visualize com plt.hist() ou sns.histplot()."
                                  ],
                                  "verification": "Histograma mostra distribuição estável; estatísticas similares em múltiplas runs.",
                                  "estimatedTime": "20 minutos",
                                  "materials": "Código Python com numpy e matplotlib",
                                  "tips": "Fixe a seed com np.random.seed() para resultados reproduzíveis em testes.",
                                  "learningObjective": "Dominar geração de dados i.i.d. usando numpy.random.",
                                  "commonMistakes": "Usar size incorreto levando a amostras pequenas; ignorar seed causando variabilidade excessiva."
                                },
                                {
                                  "stepNumber": 3,
                                  "title": "Gerar Conjunto de Dados Não-i.i.d.",
                                  "subSteps": [
                                    "Crie dados dependentes: ex: série temporal com tendência (adicionar ramp linear).",
                                    "Ou dados com distribuições variadas: alternar entre normal(0,1) e normal(5,2).",
                                    "Gere 1000 pontos: ex: dados = []; for i in range(1000): if i%100<50: dados.append(np.random.normal(0,1)) else: dados.append(np.random.normal(5,2)).",
                                    "Calcule estatísticas e plote histograma.",
                                    "Compare visualmente com dados i.i.d."
                                  ],
                                  "verification": "Histograma mostra múltiplos picos ou desvios; estatísticas variam significativamente.",
                                  "estimatedTime": "25 minutos",
                                  "materials": "Mesmo código Python anterior",
                                  "tips": "Use loops ou vetores para eficiência; evite loops desnecessários com np.concatenate.",
                                  "learningObjective": "Simular violações de i.i.d. para contrastar com casos ideais.",
                                  "commonMistakes": "Gerar acidentalmente i.i.d. por uniformidade; erros em loops condicionais."
                                },
                                {
                                  "stepNumber": 4,
                                  "title": "Comparar Dados i.i.d. vs Não-i.i.d.",
                                  "subSteps": [
                                    "Plote histogramas lado a lado: fig, axs = plt.subplots(1,2); sns.histplot(iid_data, ax=axs[0]); sns.histplot(non_iid_data, ax=axs[1]).",
                                    "Testes estatísticos: Kolmogorov-Smirnov (scipy.stats.ks_2samp) para similaridade.",
                                    "Compare médias, desvios e QQ-plots.",
                                    "Analise correlograma para independência (pd.plotting.autocorrelation_plot).",
                                    "Documente observações em Markdown no notebook."
                                  ],
                                  "verification": "Gráficos mostram diferenças claras; teste KS rejeita hipótese nula para não-i.i.d.",
                                  "estimatedTime": "30 minutos",
                                  "materials": "Adicionar pandas e scipy.stats",
                                  "tips": "Use subplots para comparações diretas; normalize escalas nos plots.",
                                  "learningObjective": "Aplicar métodos visuais e estatísticos para validar pressupostos i.i.d.",
                                  "commonMistakes": "Escalas diferentes nos plots distorcendo comparações; ignorar p-values em testes."
                                },
                                {
                                  "stepNumber": 5,
                                  "title": "Interpretar Resultados e Extrair Lições",
                                  "subSteps": [
                                    "Resuma diferenças: i.i.d. tem distribuição unimodal estável; não-i.i.d. mostra heterogeneidade.",
                                    "Discuta implicações para ML: violações i.i.d. causam overfitting ou poor generalization.",
                                    "Experimente tamanhos de amostra variados (100 vs 10000).",
                                    "Salve notebook e dados como CSV.",
                                    "Reflita: quando assumir i.i.d. no mundo real?"
                                  ],
                                  "verification": "Relatório escrito com conclusões claras e código funcional salvo.",
                                  "estimatedTime": "15 minutos",
                                  "materials": "Notebook Jupyter",
                                  "tips": "Use células Markdown para documentação profissional.",
                                  "learningObjective": "Conectar simulações à teoria e prática em ML.",
                                  "commonMistakes": "Superficialidade na análise; não salvar resultados."
                                }
                              ],
                              "practicalExample": "Simule alturas de 1000 pessoas i.i.d. de N(170,10) cm vs não-i.i.d. (primeiras 500 de N(170,10), últimas 500 de N(180,12) para mudança populacional); compare histogramas e testes KS para ver shift na distribuição.",
                              "finalVerifications": [
                                "Código gera dados i.i.d. com distribuições consistentes em múltiplas execuções.",
                                "Dados não-i.i.d. exibem desvios visíveis em plots e testes estatísticos.",
                                "Teste KS_2samp tem p-value >0.05 para i.i.d. subamostras e <0.05 para não-i.i.d.",
                                "Estatísticas descritivas (média, var) são estáveis para i.i.d.",
                                "Notebook documentado com plots e conclusões.",
                                "Dados salvos como CSV para reutilização."
                              ],
                              "assessmentCriteria": [
                                "Correção do código: gera dados conforme especificado sem erros.",
                                "Qualidade dos visualizações: plots claros, legendas e títulos adequados.",
                                "Uso apropriado de testes estatísticos e interpretação de resultados.",
                                "Profundidade da análise: identifica padrões e implicações corretamente.",
                                "Eficiência computacional: código limpo, vetorizado onde possível.",
                                "Documentação: explicações claras em comentários/Markdown."
                              ],
                              "crossCurricularConnections": [
                                "Estatística: testes de hipótese e distribuições probabilísticas.",
                                "Programação: manipulação de arrays e visualização de dados em Python.",
                                "Aprendizado de Máquina: pressupostos fundamentais para validação de modelos.",
                                "Matemática Computacional: simulações Monte Carlo.",
                                "Ciência de Dados: pré-processamento e análise exploratória de dados."
                              ],
                              "realWorldApplication": "Em treinamento de modelos ML, simular i.i.d. valida se dados de treino/teste seguem mesma distribuição; detectar não-i.i.d. (ex: drift em dados de produção) previne falhas em deploy, como em apps de recomendação ou detecção de fraudes onde populações mudam ao longo do tempo."
                            },
                            "estimatedTime": "2 horas",
                            "difficulty": "intermediate",
                            "status": "not_started",
                            "prerequisites": [
                              "10.1.5.3.2.1"
                            ]
                          }
                        ]
                      },
                      {
                        "id": "10.1.5.3.3",
                        "name": "Assunção i.i.d. em Aprendizado de Máquina",
                        "description": "Aplicação da assunção i.i.d. como base para a validade de modelos de ML, impactando treinamento, validação e generalização.",
                        "specificSkills": [
                          {
                            "id": "10.1.5.3.3.1",
                            "name": "Explicar a importância da assunção i.i.d.",
                            "description": "Detalhar por que a i.i.d. é crucial para teoremas como Hoeffding's Inequality e convergência de empíricos, permitindo generalização de treino para teste.",
                            "atomicExpansion": {
                              "steps": [
                                {
                                  "stepNumber": 1,
                                  "title": "Compreender a Definição de Assunção i.i.d.",
                                  "subSteps": [
                                    "Definir 'independente': eventos ou amostras onde o resultado de uma não afeta as outras.",
                                    "Definir 'identicamente distribuída': todas as amostras vêm da mesma distribuição de probabilidade.",
                                    "Combinar os conceitos: i.i.d. significa amostras independentes da mesma distribuição.",
                                    "Diferenciar de dependência: ex., séries temporais onde observações passadas influenciam futuras.",
                                    "Exemplificar com dados simples: moedas lançadas independentemente."
                                  ],
                                  "verification": "Escrever uma definição clara de i.i.d. em 3 frases e dar um exemplo correto.",
                                  "estimatedTime": "20 minutos",
                                  "materials": [
                                    "Papel e caneta",
                                    "Notebook com Python para simular dados simples"
                                  ],
                                  "tips": "Use analogias cotidianas, como sorteios de loteria independentes.",
                                  "learningObjective": "Dominar os componentes fundamentais da assunção i.i.d.",
                                  "commonMistakes": [
                                    "Confundir independência com uniformidade",
                                    "Ignorar que i.i.d. requer ambas as propriedades"
                                  ]
                                },
                                {
                                  "stepNumber": 2,
                                  "title": "Analisar a Importância da Independência",
                                  "subSteps": [
                                    "Explicar como a dependência viola a variância dos estimadores empíricos.",
                                    "Simular dados dependentes vs. independentes e comparar médias.",
                                    "Discutir impacto em confiança: sem independência, intervalos de confiança falham.",
                                    "Relacionar com teoremas: independência é pré-requisito para Lei dos Grandes Números.",
                                    "Identificar cenários reais: dados de sensores correlacionados no tempo."
                                  ],
                                  "verification": "Gerar gráfico comparando variância de médias em dados i.i.d. vs. dependentes.",
                                  "estimatedTime": "30 minutos",
                                  "materials": [
                                    "Python com NumPy e Matplotlib",
                                    "Exemplos de código prontos para simulação"
                                  ],
                                  "tips": "Comece com correlação simples entre amostras para visualizar o efeito.",
                                  "learningObjective": "Entender por que a independência garante convergência estatística.",
                                  "commonMistakes": [
                                    "Assumir que pouca correlação é suficiente",
                                    "Não quantificar o impacto na variância"
                                  ]
                                },
                                {
                                  "stepNumber": 3,
                                  "title": "Explorar a Importância da Distribuição Idêntica",
                                  "subSteps": [
                                    "Explicar o problema de 'drift' de distribuição entre treino e teste.",
                                    "Simular shift de distribuição e medir degradação de performance de modelo.",
                                    "Discutir generalização: sem idêntica distribuição, risco empírico não converge para risco verdadeiro.",
                                    "Relacionar com Hoeffding's Inequality: assume i.i.d. para bounds probabilísticos.",
                                    "Exemplificar: dados de clientes mudando ao longo do tempo."
                                  ],
                                  "verification": "Criar dois datasets com shift e treinar um modelo simples, comparando acurácia.",
                                  "estimatedTime": "40 minutos",
                                  "materials": [
                                    "Python com Scikit-learn",
                                    "Datasets sintéticos gerados via NumPy"
                                  ],
                                  "tips": "Use distribuições Gaussianas com médias diferentes para ilustrar shift.",
                                  "learningObjective": "Compreender como distribuição idêntica permite generalização.",
                                  "commonMistakes": [
                                    "Confundir com outliers",
                                    "Subestimar shifts sutis em dados reais"
                                  ]
                                },
                                {
                                  "stepNumber": 4,
                                  "title": "Conectar i.i.d. a Teoremas e Generalização em ML",
                                  "subSteps": [
                                    "Revisar Hoeffding's Inequality: bound na probabilidade de erro de generalização sob i.i.d.",
                                    "Explicar convergência de processos empíricos: média empírica → expectativa verdadeira.",
                                    "Discutir implicações em ML: treino/teste split assume i.i.d. para validação.",
                                    "Analisar violações comuns em ML: dados sequenciais, grupos amostrados.",
                                    "Propor soluções: bootstrapping ou modelar dependências explicitamente."
                                  ],
                                  "verification": "Explicar verbalmente ou por escrito como i.i.d. justifica a divisão treino/teste.",
                                  "estimatedTime": "30 minutos",
                                  "materials": [
                                    "Artigo ou vídeo sobre Hoeffding's Inequality",
                                    "Notebook Jupyter com demonstrações"
                                  ],
                                  "tips": "Memorize a intuição: i.i.d. faz amostras 'representativas' da população.",
                                  "learningObjective": "Ligar i.i.d. a fundamentos teóricos de ML para generalização.",
                                  "commonMistakes": [
                                    "Achar que i.i.d. é 'sempre verdadeira'",
                                    "Ignorar teoremas como base teórica"
                                  ]
                                }
                              ],
                              "practicalExample": "Em um modelo de classificação de spam de e-mails, assumir i.i.d. permite que uma amostra de 10.000 e-mails de treino generalize para novos e-mails de teste. Simule violando independência (e-mails em threads correlacionados) e veja o modelo falhar em estimar risco corretamente.",
                              "finalVerifications": [
                                "Definir i.i.d. corretamente com exemplos de violação.",
                                "Explicar impacto da independência na variância empírica.",
                                "Descrever como shift de distribuição afeta generalização.",
                                "Aplicar Hoeffding's Inequality intuitivamente a um cenário ML.",
                                "Identificar 3 cenários reais onde i.i.d. é violada.",
                                "Propor teste para checar i.i.d. em dados."
                              ],
                              "assessmentCriteria": [
                                "Precisão conceitual: definições exatas sem ambiguidades.",
                                "Profundidade analítica: ligação clara com teoremas e convergência.",
                                "Exemplos relevantes: uso de simulações ou casos reais concretos.",
                                "Compreensão de implicações: discussão de generalização treino-teste.",
                                "Criatividade em violações: identificação de cenários não triviais.",
                                "Clareza de comunicação: explicação acessível e estruturada."
                              ],
                              "crossCurricularConnections": [
                                "Probabilidade e Estatística: Lei dos Grandes Números e CLT.",
                                "Ciência de Dados: Validação cruzada e detecção de drift.",
                                "Aprendizado de Máquina: Teoria VC Dimension e PAC Learning.",
                                "Matemática Computacional: Análise de complexidade algorítmica.",
                                "Série Temporal: Modelos para dados não-i.i.d."
                              ],
                              "realWorldApplication": "Na indústria, como no Netflix, a assunção i.i.d. é usada para treinar recomendadores com histórico de usuários, garantindo que preferências passadas generalizem para futuras visualizações; violações (ex.: mudanças sazonais) exigem técnicas como detecção de drift para manter modelos precisos."
                            },
                            "estimatedTime": "1.5 horas",
                            "difficulty": "intermediate",
                            "status": "not_started",
                            "prerequisites": [
                              "10.1.5.3.1.1",
                              "10.1.5.3.2.1"
                            ]
                          },
                          {
                            "id": "10.1.5.3.3.2",
                            "name": "Identificar violações da assunção i.i.d.",
                            "description": "Detectar violações comuns como autocorrelação em séries temporais ou drift de distribuição, e discutir impactos como overfitting ou poor generalization.",
                            "atomicExpansion": {
                              "steps": [
                                {
                                  "stepNumber": 1,
                                  "title": "Compreender os Fundamentos da Assunção i.i.d.",
                                  "subSteps": [
                                    "Defina 'Independente' (sem dependência entre amostras) e 'Identicamente Distribuída' (mesma distribuição probabilística).",
                                    "Explique a importância da i.i.d. para validação cruzada e generalização em modelos de ML.",
                                    "Analise cenários onde a assunção i.i.d. é válida, como dados de sensores independentes.",
                                    "Compare com violações sutis usando diagramas de dependência.",
                                    "Resuma os pressupostos matemáticos em notação probabilística (P(X_i) = P(X_j) ∀ i,j)."
                                  ],
                                  "verification": "Escreva um parágrafo explicando i.i.d. e forneça um exemplo onde ela holds; revise com um colega ou ferramenta de IA.",
                                  "estimatedTime": "1.5 horas",
                                  "materials": [
                                    "Artigo 'i.i.d. in Machine Learning' (Scikit-learn docs)",
                                    "Vídeo Khan Academy sobre distribuições probabilísticas",
                                    "Notebook Jupyter vazio"
                                  ],
                                  "tips": "Use analogias cotidianas, como moedas justas lançadas independentemente, para fixar conceitos.",
                                  "learningObjective": "Dominar a definição e rationale da assunção i.i.d. em contextos de ML.",
                                  "commonMistakes": [
                                    "Confundir i.i.d. com estacionariedade",
                                    "Ignorar que i.i.d. não implica normalidade",
                                    "Achar que todos os dados reais são i.i.d."
                                  ]
                                },
                                {
                                  "stepNumber": 2,
                                  "title": "Identificar Tipos Comuns de Violações da i.i.d.",
                                  "subSteps": [
                                    "Estude autocorrelação em séries temporais (ex: dados de ações).",
                                    "Aprenda sobre drift de distribuição (concept drift e covariate shift).",
                                    "Explore violações de independência como clustering espacial ou grouping.",
                                    "Classifique violações em categorias: temporal, populacional e de amostragem.",
                                    "Crie uma tabela comparativa de violações com exemplos reais."
                                  ],
                                  "verification": "Liste 4 violações comuns com definições breves e um exemplo cada; valide contra referências.",
                                  "estimatedTime": "2 horas",
                                  "materials": [
                                    "Livro 'Elements of Statistical Learning' (Cap. 2)",
                                    "Tutoriais Statsmodels sobre autocorrelação",
                                    "Datasets Kaggle: séries temporais"
                                  ],
                                  "tips": "Priorize violações frequentes em dados reais como séries temporais antes de drifts raros.",
                                  "learningObjective": "Reconhecer e nomear violações padrão da i.i.d. com precisão.",
                                  "commonMistakes": [
                                    "Confundir covariate shift com label shift",
                                    "Subestimar violações não-temporais como em imagens agrupadas",
                                    "Ignorar prior drift em dados de produção"
                                  ]
                                },
                                {
                                  "stepNumber": 3,
                                  "title": "Detectar Violações em Dados Práticos",
                                  "subSteps": [
                                    "Implemente testes visuais: scatter plots, ACF/PACF para autocorrelação.",
                                    "Aplique testes estatísticos: Durbin-Watson para autocorrelação, KS-test para drift.",
                                    "Use métricas como Population Stability Index (PSI) para shifts de distribuição.",
                                    "Simule datasets violadores em Python (ex: ARIMA para autocorrelação).",
                                    "Compare train/test sets para detectar mismatches de distribuição."
                                  ],
                                  "verification": "Gere relatório de um dataset testado identificando pelo menos uma violação com evidência gráfica/estatística.",
                                  "estimatedTime": "2.5 horas",
                                  "materials": [
                                    "Python com pandas, matplotlib, statsmodels",
                                    "Notebook exemplo de detecção de drift (Alibi-Detect)",
                                    "Dataset UCI Air Quality (série temporal)"
                                  ],
                                  "tips": "Sempre plote dados primeiro; visualizações revelam 80% das violações antes de testes.",
                                  "learningObjective": "Aplicar ferramentas computacionais para detectar violações empiricamente.",
                                  "commonMistakes": [
                                    "Usar testes sem checar premissas (ex: normalidade)",
                                    "Interpretar p-values isolados sem contexto",
                                    "Esquecer de normalizar features antes de KS-test"
                                  ]
                                },
                                {
                                  "stepNumber": 4,
                                  "title": "Analisar Impactos e Estratégias de Mitigação",
                                  "subSteps": [
                                    "Discuta impactos: overfitting em autocorrelação, poor generalization em drifts.",
                                    "Quantifique efeitos simulando modelos com/ sem violações (métricas como accuracy drop).",
                                    "Explore mitigações: time-series CV, domain adaptation, monitoring contínuo.",
                                    "Avalie trade-offs de ignorar vs corrigir violações.",
                                    "Documente case studies de falhas reais (ex: modelo de previsão falhando por drift)."
                                  ],
                                  "verification": "Escreva um ensaio curto (300 palavras) sobre impactos de uma violação específica e sua mitigação.",
                                  "estimatedTime": "1.5 horas",
                                  "materials": [
                                    "Paper 'A Survey on Concept Drift Adaptation'",
                                    "Ferramentas MLOps como MLflow para monitoring",
                                    "Exemplos de código GitHub sobre non-i.i.d. ML"
                                  ],
                                  "tips": "Foque em impactos mensuráveis como aumento de variance em validação.",
                                  "learningObjective": "Compreender consequências e soluções para violações da i.i.d.",
                                  "commonMistakes": [
                                    "Superestimar generalização sem testar violações",
                                    "Propor mitigações sem custo computacional",
                                    "Ignorar que algumas violações são inevitáveis em produção"
                                  ]
                                }
                              ],
                              "practicalExample": "Em um dataset de séries temporais de preços de ações (Yahoo Finance), plote ACF para detectar autocorrelação (violação de independência), aplique KS-test entre train/test para drift, observe overfitting no modelo Random Forest (train acc 95%, test 70%), e mitigue com validação walk-forward.",
                              "finalVerifications": [
                                "Liste e explique 4 violações comuns da i.i.d. com exemplos.",
                                "Detecte corretamente autocorrelação em um plot ACF fornecido.",
                                "Interprete resultados de KS-test e PSI em datasets simulados.",
                                "Descreva 3 impactos quantitativos (ex: aumento de overfitting).",
                                "Proponha mitigações adequadas para 2 cenários reais.",
                                "Gere relatório completo de análise de um dataset não-i.i.d."
                              ],
                              "assessmentCriteria": [
                                "Precisão na identificação de violações (90%+ em testes simulados).",
                                "Uso correto de ferramentas de detecção (plots e testes estatísticos).",
                                "Análise profunda de impactos com evidências quantitativas.",
                                "Criatividade em mitigações práticas e viáveis.",
                                "Clareza na documentação e comunicação de achados.",
                                "Integração de conceitos em exemplos reais sem erros conceituais."
                              ],
                              "crossCurricularConnections": [
                                "Estatística: Testes de hipóteses e análise de resíduos.",
                                "Programação: Manipulação de dados com pandas e visualização com seaborn.",
                                "Série Temporal: Modelos ARIMA e forecasting.",
                                "Engenharia de Software: Monitoring de modelos em produção (MLOps).",
                                "Probabilidade: Distribuições e teoremas limite."
                              ],
                              "realWorldApplication": "Em sistemas de ML em produção, como detecção de fraude em transações bancárias, identificar drift de distribuição em dados de clientes novos previne perdas financeiras por modelos desatualizados; empresas como Google usam monitoring i.i.d. para manter precisão em buscas personalizadas."
                            },
                            "estimatedTime": "1.5 horas",
                            "difficulty": "intermediate",
                            "status": "not_started",
                            "prerequisites": [
                              "10.1.5.3.3.1"
                            ]
                          },
                          {
                            "id": "10.1.5.3.3.3",
                            "name": "Aplicar i.i.d. em validação de modelos",
                            "description": "Dividir dados em treino/teste assumindo i.i.d., usando cross-validation, e avaliar métricas como acurácia sob essa assunção.",
                            "atomicExpansion": {
                              "steps": [
                                {
                                  "stepNumber": 1,
                                  "title": "Verificar a assunção i.i.d. nos dados",
                                  "subSteps": [
                                    "Carregue um dataset exemplo (ex: Iris).",
                                    "Analise estatísticas descritivas por amostra para checar independência e distribuição idêntica.",
                                    "Plote histogramas e boxplots para variáveis para visualizar uniformidade.",
                                    "Teste estatístico simples como Kolmogorov-Smirnov para similaridade de distribuições.",
                                    "Documente se a assunção i.i.d. é razoável ou precisa de ajustes."
                                  ],
                                  "verification": "Relatório escrito confirmando se dados atendem i.i.d. com evidências visuais e testes.",
                                  "estimatedTime": "45 minutos",
                                  "materials": "Python, pandas, matplotlib/seaborn, scipy.stats, dataset Iris.",
                                  "tips": "Use amostras aleatórias para simular i.i.d.; foque em features principais primeiro.",
                                  "learningObjective": "Compreender como validar a assunção i.i.d. empiricamente antes da validação de modelo.",
                                  "commonMistakes": "Ignorar violações de i.i.d. como dependência temporal em dados sequenciais."
                                },
                                {
                                  "stepNumber": 2,
                                  "title": "Dividir dados em conjuntos de treino e teste",
                                  "subSteps": [
                                    "Importe train_test_split do scikit-learn.",
                                    "Defina proporção 80/20 ou 70/30 para treino/teste com random_state fixo.",
                                    "Aplique split estratificado se classes desbalanceadas (stratify=y).",
                                    "Verifique tamanhos e distribuições dos splits.",
                                    "Salve splits em variáveis separadas."
                                  ],
                                  "verification": "Shapes dos conjuntos treino/teste impressos e distribuições idênticas confirmadas.",
                                  "estimatedTime": "20 minutos",
                                  "materials": "Python, scikit-learn (train_test_split), dataset preparado.",
                                  "tips": "Sempre use random_state para reprodutibilidade em experimentos.",
                                  "learningObjective": "Executar divisão de dados sob i.i.d., mantendo representatividade.",
                                  "commonMistakes": "Não usar estratificação em dados desbalanceados, levando a viés."
                                },
                                {
                                  "stepNumber": 3,
                                  "title": "Implementar K-Fold Cross-Validation",
                                  "subSteps": [
                                    "Importe KFold ou cross_val_score do scikit-learn.",
                                    "Configure K=5 ou 10 folds com shuffle=True para i.i.d.",
                                    "Escolha um modelo simples (ex: LogisticRegression).",
                                    "Execute cross_val_score para acurácia média e desvio padrão.",
                                    "Visualize scores por fold em um gráfico de barras."
                                  ],
                                  "verification": "Output de cross_val_score mostrando média e std de acurácia.",
                                  "estimatedTime": "30 minutos",
                                  "materials": "Python, scikit-learn (model_selection), modelo baseline.",
                                  "tips": "Shuffle=True garante independência aproximada dos folds.",
                                  "learningObjective": "Aplicar CV para estimativa robusta de performance sob i.i.d.",
                                  "commonMistakes": "Esquecer shuffle em dados ordenados, violando i.i.d."
                                },
                                {
                                  "stepNumber": 4,
                                  "title": "Avaliar métricas no conjunto de teste",
                                  "subSteps": [
                                    "Treine modelo final no treino completo.",
                                    "Preveja no conjunto de teste.",
                                    "Calcule métricas: acurácia, precisão, recall, F1-score.",
                                    "Compare com resultados de CV.",
                                    "Interprete sob assunção i.i.d.: generalize para população."
                                  ],
                                  "verification": "Relatório de métricas com confusão matrix e interpretação.",
                                  "estimatedTime": "25 minutos",
                                  "materials": "Python, scikit-learn (metrics), modelo treinado.",
                                  "tips": "Use classification_report para visão completa das métricas.",
                                  "learningObjective": "Avaliar modelo validado e discutir validade da generalização i.i.d.",
                                  "commonMistakes": "Overfitting ao testar múltiplas vezes no mesmo teste sem CV."
                                },
                                {
                                  "stepNumber": 5,
                                  "title": "Analisar limitações e sensibilidade à i.i.d.",
                                  "subSteps": [
                                    "Simule violação i.i.d. alterando dados (ex: adicionar tendência).",
                                    "Reexecute CV e compare métricas.",
                                    "Discuta impacto em métricas como acurácia.",
                                    "Sugira alternativas (ex: TimeSeriesSplit para não-i.i.d.).",
                                    "Conclua relatório final."
                                  ],
                                  "verification": "Comparação de métricas antes/depois violação com insights.",
                                  "estimatedTime": "40 minutos",
                                  "materials": "Código anterior, dados modificados.",
                                  "tips": "Use seeds consistentes para isolar efeito da violação.",
                                  "learningObjective": "Reconhecer quando i.i.d. falha e impactos na validação.",
                                  "commonMistakes": "Assumir i.i.d. cegamente sem testes de sensibilidade."
                                }
                              ],
                              "practicalExample": "Usando dataset Iris: divida em 80/20, aplique 5-fold CV em KNN classifier, avalie acurácia média ~0.96, teste final confirma generalização sob i.i.d., simulando classificação de flores em campo.",
                              "finalVerifications": [
                                "Assunção i.i.d. validada com testes estatísticos.",
                                "Divisão treino/teste com proporções corretas e estratificada.",
                                "CV executado com scores médios e desvios reportados.",
                                "Métricas no teste alinhadas com CV.",
                                "Análise de sensibilidade à violação i.i.d. incluída.",
                                "Relatório completo com códigos e plots."
                              ],
                              "assessmentCriteria": [
                                "Correta verificação i.i.d. com evidências (20%)",
                                "Implementação precisa de split e CV (30%)",
                                "Cálculo e interpretação de métricas (20%)",
                                "Análise de limitações (15%)",
                                "Código limpo e reprodutível (10%)",
                                "Relatório claro e acionável (5%)"
                              ],
                              "crossCurricularConnections": [
                                "Estatística: Testes de hipóteses para distribuições.",
                                "Programação: Manipulação de dados com pandas/sklearn.",
                                "Matemática: Probabilidade condicional em validação.",
                                "Ciência de Dados: Pipelines de ML end-to-end."
                              ],
                              "realWorldApplication": "Em desenvolvimento de apps de ML como detecção de fraudes bancárias, onde dados transações assumem i.i.d. para validar acurácia antes de deploy em produção, evitando overfitting em cenários reais."
                            },
                            "estimatedTime": "2 horas",
                            "difficulty": "advanced",
                            "status": "not_started",
                            "prerequisites": [
                              "10.1.5.3.3.1"
                            ]
                          },
                          {
                            "id": "10.1.5.3.3.4",
                            "name": "Discutir alternativas à i.i.d.",
                            "description": "Explorar abordagens para dados não-i.i.d., como modelos de séries temporais (ARIMA) ou aprendizado online, com exemplos de bibliotecas como scikit-learn.",
                            "atomicExpansion": {
                              "steps": [
                                {
                                  "stepNumber": 1,
                                  "title": "Revisar Assunção i.i.d. e Identificar Dados Não-i.i.d.",
                                  "subSteps": [
                                    "Defina i.i.d. (independente e identicamente distribuído) e explique suas implicações em ML.",
                                    "Identifique cenários reais onde dados violam i.i.d., como séries temporais ou fluxos de dados.",
                                    "Analise um dataset simples (ex: temperaturas diárias) para demonstrar dependência temporal.",
                                    "Discuta impactos negativos de assumir i.i.d. em dados não-i.i.d., como viés em previsões.",
                                    "Crie um diagrama comparando distribuições i.i.d. vs. não-i.i.d."
                                  ],
                                  "verification": "Crie um relatório de 1 página resumindo violações de i.i.d. com exemplo de dataset.",
                                  "estimatedTime": "45 minutos",
                                  "materials": "Notebook Jupyter, dataset de séries temporais (ex: de Kaggle), documentação scikit-learn.",
                                  "tips": "Use gráficos de autocorrelação para visualizar dependências.",
                                  "learningObjective": "Compreender limitações da assunção i.i.d. em contextos reais.",
                                  "commonMistakes": "Confundir independência com estacionariedade; ignorar exemplos concretos."
                                },
                                {
                                  "stepNumber": 2,
                                  "title": "Explorar Modelos de Séries Temporais com ARIMA",
                                  "subSteps": [
                                    "Estude componentes de séries temporais: tendência, sazonalidade e ruído.",
                                    "Aprenda os parâmetros ARIMA (p, d, q) e como selecioná-los via ACF/PACF.",
                                    "Implemente ARIMA em Python usando statsmodels.",
                                    "Ajuste um modelo ARIMA a um dataset de séries temporais e valide com testes de estacionariedade.",
                                    "Compare previsões ARIMA com um modelo baseline i.i.d. (ex: média móvel)."
                                  ],
                                  "verification": "Execute código que gera previsões ARIMA e plota resíduos; verifique se resíduos são i.i.d.",
                                  "estimatedTime": "90 minutos",
                                  "materials": "Bibliotecas: statsmodels, pandas, matplotlib; dataset AirPassengers ou similar.",
                                  "tips": "Comece com ARIMA(1,1,1) para simplicidade e refine com auto_arima.",
                                  "learningObjective": "Dominar ARIMA como alternativa para dados temporalmente dependentes.",
                                  "commonMistakes": "Não diferenciar AR, I e MA; superajustar sem validação cruzada temporal."
                                },
                                {
                                  "stepNumber": 3,
                                  "title": "Introduzir Aprendizado Online para Dados Não-Estacionários",
                                  "subSteps": [
                                    "Defina aprendizado online: atualizações incrementais sem re-treinamento completo.",
                                    "Explore algoritmos como SGDRegressor ou PassiveAggressive em scikit-learn.",
                                    "Simule um fluxo de dados não-i.i.d. (ex: cliques em site ao longo do tempo).",
                                    "Implemente um modelo online e compare com batch learning.",
                                    "Discuta adaptação a mudanças de distribuição (concept drift)."
                                  ],
                                  "verification": "Desenvolva um script que atualiza um modelo online com novos dados e mede performance acumulada.",
                                  "estimatedTime": "75 minutos",
                                  "materials": "scikit-learn, stream de dados sintéticos via numpy.random.",
                                  "tips": "Use learning_rate='adaptive' para melhor convergência.",
                                  "learningObjective": "Aplicar aprendizado online para lidar com dependências sequenciais.",
                                  "commonMistakes": "Esquecer de normalizar dados em fluxo; confundir com reinforcement learning."
                                },
                                {
                                  "stepNumber": 4,
                                  "title": "Comparar Alternativas e Discutir Trade-offs",
                                  "subSteps": [
                                    "Compile uma tabela comparando i.i.d., ARIMA e aprendizado online (prós, contras, casos de uso).",
                                    "Implemente exemplos híbridos, como ARIMA + ML para features exógenas.",
                                    "Debata limitações: escalabilidade, interpretabilidade e requisitos computacionais.",
                                    "Crie um caso de estudo com dados reais (ex: previsão de vendas).",
                                    "Prepare argumentos para quando usar cada abordagem."
                                  ],
                                  "verification": "Escreva um ensaio curto (300 palavras) defendendo uma alternativa para um cenário específico.",
                                  "estimatedTime": "60 minutos",
                                  "materials": "Todos os códigos anteriores, tabela Markdown ou LaTeX.",
                                  "tips": "Foque em métricas como MAE para séries temporais vs. accuracy para i.i.d.",
                                  "learningObjective": "Avaliar criticamente alternativas à i.i.d. para seleção informada.",
                                  "commonMistakes": "Ignorar custos computacionais; generalizar demais sem evidências empíricas."
                                }
                              ],
                              "practicalExample": "Em previsão de preços de ações (dataset Yahoo Finance), um modelo i.i.d. falha devido a dependências temporais; use ARIMA para capturar autocorrelação e scikit-learn's SGDRegressor para adaptação online a notícias em tempo real, melhorando RMSE em 25%.",
                              "finalVerifications": [
                                "Explicar verbalmente diferenças entre i.i.d., ARIMA e aprendizado online.",
                                "Implementar e rodar um modelo ARIMA com validação adequada.",
                                "Demonstrar um loop de aprendizado online com dados simulados.",
                                "Comparar métricas de performance em um dataset não-i.i.d.",
                                "Identificar concept drift em um exemplo prático.",
                                "Discutir trade-offs em um whiteboard."
                              ],
                              "assessmentCriteria": [
                                "Precisão conceitual: definições corretas de i.i.d. e alternativas (30%).",
                                "Profundidade técnica: implementação funcional de ARIMA/online (25%).",
                                "Análise crítica: comparação equilibrada com evidências (20%).",
                                "Criatividade: exemplos reais e conexões interdisciplinares (15%).",
                                "Clareza de comunicação: relatórios/códigos bem documentados (10%)."
                              ],
                              "crossCurricularConnections": [
                                "Estatística: testes de estacionariedade e ACF/PACF.",
                                "Programação: manipulação de dados com pandas e modelagem em scikit-learn.",
                                "Finanças: previsão de séries temporais em mercados.",
                                "Engenharia de Software: pipelines para dados em stream."
                              ],
                              "realWorldApplication": "Em IoT para monitoramento de sensores (ex: tráfego urbano), onde dados são sequenciais e não-i.i.d., ARIMA prevê picos de congestionamento e aprendizado online adapta a eventos imprevistos como acidentes, otimizando semáforos em tempo real."
                            },
                            "estimatedTime": "1 hora",
                            "difficulty": "advanced",
                            "status": "not_started",
                            "prerequisites": [
                              "10.1.5.3.3.2"
                            ]
                          }
                        ]
                      }
                    ]
                  },
                  {
                    "id": "10.1.5.4",
                    "name": "Conceitos de Modelagem de Problemas em Aprendizado",
                    "description": "Definições de dados, informação, conhecimento e etapas iniciais de modelagem.",
                    "individualConcepts": [
                      {
                        "id": "10.1.5.4.1",
                        "name": "Definições de Dados, Informação e Conhecimento",
                        "description": "Compreensão das distinções hierárquicas entre dados brutos, informação processada e conhecimento acionável, fundamentais para a modelagem de problemas em aprendizado de máquina.",
                        "specificSkills": [
                          {
                            "id": "10.1.5.4.1.1",
                            "name": "Definir e exemplificar dados",
                            "description": "Identificar dados como fatos brutos e não estruturados, como números, textos ou imagens, e fornecer exemplos reais como registros de sensores ou logs de usuários, destacando sua falta de contexto inicial.",
                            "atomicExpansion": {
                              "steps": [
                                {
                                  "stepNumber": 1,
                                  "title": "Compreender a Definição Formal de Dados",
                                  "subSteps": [
                                    "Leia a definição: Dados são fatos brutos e não estruturados, como números, textos ou imagens, sem contexto inicial.",
                                    "Anote os elementos chave: brutos, não estruturados, falta de contexto.",
                                    "Compare com sinônimos comuns (ex: 'raw data') e registre diferenças.",
                                    "Escreva a definição em suas próprias palavras.",
                                    "Pesquise uma definição de fonte confiável (ex: Wikipedia ou livro de Ciência de Dados)."
                                  ],
                                  "verification": "Resuma a definição oralmente ou por escrito e confirme com um colega ou autoavaliação se captura 'brutos e sem contexto'.",
                                  "estimatedTime": "20 minutos",
                                  "materials": [
                                    "Definição oficial da habilidade",
                                    "Caderno ou editor de texto",
                                    "Acesso à internet para pesquisa rápida"
                                  ],
                                  "tips": "Enfatize sempre a ausência de interpretação ou estrutura nos dados.",
                                  "learningObjective": "Internalizar que dados são elementos primitivos sem significado inerente.",
                                  "commonMistakes": [
                                    "Confundir dados com informação processada",
                                    "Acreditar que dados sempre têm contexto",
                                    "Limitar dados apenas a números"
                                  ]
                                },
                                {
                                  "stepNumber": 2,
                                  "title": "Identificar Tipos e Formas de Dados Brutos",
                                  "subSteps": [
                                    "Liste tipos comuns: numéricos (ex: 25.5), textuais (ex: 'João'), imagens (ex: pixel values), categóricos.",
                                    "Classifique 5 exemplos aleatórios de entrada/saída de sensores ou logs.",
                                    "Crie uma tabela simples com colunas: Tipo | Exemplo | Por que é bruto?",
                                    "Discuta com um parceiro se um item é dado ou não.",
                                    "Expanda para formatos: CSV bruto, JSON sem schema, arquivos de imagem raw."
                                  ],
                                  "verification": "Apresente sua tabela e justifique cada classificação para um avaliador.",
                                  "estimatedTime": "25 minutos",
                                  "materials": [
                                    "Papel ou planilha (Google Sheets)",
                                    "Exemplos de logs/sensores online",
                                    "Marcador ou caneta"
                                  ],
                                  "tips": "Pergunte sempre: 'Isso tem contexto ou interpretação embutida?'",
                                  "learningObjective": "Reconhecer diversidade de dados brutos em diferentes mídias.",
                                  "commonMistakes": [
                                    "Classificar dados processados como brutos",
                                    "Ignorar dados não-numéricos",
                                    "Confundir formato com conteúdo"
                                  ]
                                },
                                {
                                  "stepNumber": 3,
                                  "title": "Analisar Exemplos Reais de Dados",
                                  "subSteps": [
                                    "Selecione exemplos reais: registros de sensores (temperatura: 23.4°C), logs de usuários (IP: 192.168.1.1, timestamp: 2023-10-01).",
                                    "Descreva cada exemplo: o que é, formato, origem.",
                                    "Identifique a falta de contexto: 'O que esse número significa sozinho?'",
                                    "Colete 3 exemplos pessoais (ex: dados de fitness app).",
                                    "Registre em um documento com screenshots se possível."
                                  ],
                                  "verification": "Compartilhe 3 exemplos analisados e explique a ausência de contexto para cada um.",
                                  "estimatedTime": "30 minutos",
                                  "materials": [
                                    "Acesso a datasets públicos (Kaggle)",
                                    "App de logs ou sensor (ex: celular)",
                                    "Document editor"
                                  ],
                                  "tips": "Use exemplos cotidianos para fixar o conceito.",
                                  "learningObjective": "Aplicar a definição a casos reais destacando bruteza.",
                                  "commonMistakes": [
                                    "Adicionar contexto artificial aos exemplos",
                                    "Escolher dados já interpretados",
                                    "Poucos exemplos variados"
                                  ]
                                },
                                {
                                  "stepNumber": 4,
                                  "title": "Exemplificar e Destacar Falta de Contexto Inicial",
                                  "subSteps": [
                                    "Crie 4 exemplos originais de dados brutos em contextos variados (sensor, log, imagem).",
                                    "Para cada um, escreva: 'Este é dado porque...' e 'Sem contexto, significa... nada'.",
                                    "Compare com informação: adicione contexto a um exemplo para transformá-lo.",
                                    "Teste compreensão: misture dados e informação, classifique 10 itens.",
                                    "Revise e refine seus exemplos para clareza."
                                  ],
                                  "verification": "Submeta um relatório com exemplos, comparações e autoavaliação de acertos.",
                                  "estimatedTime": "25 minutos",
                                  "materials": [
                                    "Caderno ou Google Docs",
                                    "Imagens ou dados de teste",
                                    "Lista de itens mistos para classificação"
                                  ],
                                  "tips": "Sempre termine com 'sem contexto inicial' para reforçar.",
                                  "learningObjective": "Gerar e validar exemplos próprios enfatizando distinção chave.",
                                  "commonMistakes": [
                                    "Exemplos vagos ou não brutos",
                                    "Não contrastar com informação",
                                    "Falta de variedade nos exemplos"
                                  ]
                                }
                              ],
                              "practicalExample": "Analise logs de um site de e-commerce: '192.168.1.1, 2023-10-01 14:30:22, /produto123, 200'. Estes são dados brutos: IPs, timestamps, URLs e códigos de status sem contexto sobre usuários, intenções ou padrões de comportamento.",
                              "finalVerifications": [
                                "Pode definir dados como fatos brutos sem contexto em 1 frase precisa?",
                                "Lista pelo menos 5 exemplos reais variados (números, textos, imagens)?",
                                "Identifica corretamente falta de contexto em dados brutos?",
                                "Diferencia dados de informação/conhecimento com exemplos?",
                                "Classifica 10 itens mistos com 90% de acerto?",
                                "Cria exemplos originais convincentes?"
                              ],
                              "assessmentCriteria": [
                                "Precisão da definição de dados (25%)",
                                "Qualidade e variedade de exemplos reais (25%)",
                                "Destaque consistente à falta de contexto (20%)",
                                "Capacidade de diferenciação de informação (15%)",
                                "Clareza e estrutura das explicações (10%)",
                                "Criatividade em exemplos originais (5%)"
                              ],
                              "crossCurricularConnections": [
                                "Ciência da Computação: Processamento de logs em programação",
                                "Estatística: Coleta de dados brutos para análise",
                                "Física: Registros de sensores em experimentos",
                                "Informática: Bancos de dados raw vs. estruturados",
                                "Biologia: Dados genômicos sequenciados sem anotação"
                              ],
                              "realWorldApplication": "Na Ciência de Dados e Aprendizado de Máquina, dados brutos de sensores IoT ou logs de apps são coletados e processados para gerar insights, como detectar fraudes em transações bancárias a partir de IPs e timestamps isolados."
                            },
                            "estimatedTime": "1 hora",
                            "difficulty": "beginner",
                            "status": "not_started",
                            "prerequisites": []
                          },
                          {
                            "id": "10.1.5.4.1.2",
                            "name": "Diferenciar dados de informação",
                            "description": "Explicar como a informação surge da organização e contextualização dos dados, usando exemplos como a agregação de vendas diárias em relatórios mensais para revelar padrões de comportamento do consumidor.",
                            "atomicExpansion": {
                              "steps": [
                                {
                                  "stepNumber": 1,
                                  "title": "Definir o conceito de dados",
                                  "subSteps": [
                                    "Pesquise definições padrão de 'dados' em fontes confiáveis como glossários de ciência de dados.",
                                    "Identifique características principais: dados são fatos brutos, sem contexto, como números, símbolos ou observações isoladas.",
                                    "Colete exemplos simples: temperatura registrada (25°C), número de cliques em um site (150).",
                                    "Registre 5 exemplos de dados crus em um caderno ou planilha.",
                                    "Discuta com um parceiro: 'Por que esses são dados e não informação?'"
                                  ],
                                  "verification": "Lista de pelo menos 5 exemplos de dados crus criados e explicados corretamente.",
                                  "estimatedTime": "20 minutos",
                                  "materials": [
                                    "Glossário online de ciência de dados",
                                    "Caderno ou Google Sheets",
                                    "Acesso à internet"
                                  ],
                                  "tips": "Foquem em dados como 'blocos de construção' isolados; evite adicionar significado prematuramente.",
                                  "learningObjective": "Compreender dados como elementos brutos e não processados.",
                                  "commonMistakes": [
                                    "Confundir dados com opiniões pessoais",
                                    "Adicionar contexto desnecessário nos exemplos"
                                  ]
                                },
                                {
                                  "stepNumber": 2,
                                  "title": "Definir o conceito de informação",
                                  "subSteps": [
                                    "Pesquise definições de 'informação': dados organizados, contextualizados e com significado.",
                                    "Note que informação responde a perguntas como 'o quê?', 'quando?' ou 'onde?'.",
                                    "Crie exemplos: 150 cliques em um site às 14h em uma página específica vira 'pico de tráfego à tarde'.",
                                    "Compare com dados do Step 1 para destacar a transformação.",
                                    "Escreva uma definição pessoal em suas palavras."
                                  ],
                                  "verification": "Definição escrita e 3 exemplos de informação derivados de dados.",
                                  "estimatedTime": "25 minutos",
                                  "materials": [
                                    "Mesmos do Step 1",
                                    "Exemplos do Step 1"
                                  ],
                                  "tips": "Pense em informação como 'dados com propósito'; o contexto é a chave.",
                                  "learningObjective": "Reconhecer informação como dados processados com relevância.",
                                  "commonMistakes": [
                                    "Tratar informação como mera cópia de dados",
                                    "Ignorar o papel do contexto"
                                  ]
                                },
                                {
                                  "stepNumber": 3,
                                  "title": "Explorar a transformação de dados em informação",
                                  "subSteps": [
                                    "Analise o processo: coleta → organização → contextualização → análise.",
                                    "Use diagrama: desenhe uma seta de 'dados crus' para 'informação útil'.",
                                    "Aplique a agregação: some vendas diárias para relatório mensal.",
                                    "Identifique padrões emergentes, como 'vendas caem aos domingos'.",
                                    "Crie um fluxograma simples da transformação."
                                  ],
                                  "verification": "Fluxograma ou diagrama mostrando pelo menos 3 etapas da transformação.",
                                  "estimatedTime": "30 minutos",
                                  "materials": [
                                    "Papel e caneta ou ferramenta como Draw.io",
                                    "Dados de exemplo de vendas"
                                  ],
                                  "tips": "Visualize o processo; agregação revela padrões que dados isolados escondem.",
                                  "learningObjective": "Entender mecanismos que convertem dados em informação acionável.",
                                  "commonMistakes": [
                                    "Pular organização",
                                    "Não contextualizar adequadamente"
                                  ]
                                },
                                {
                                  "stepNumber": 4,
                                  "title": "Praticar diferenciação com exemplos reais",
                                  "subSteps": [
                                    "Pegue dados de vendas diárias fictícias (ex: Dia1: 100un, Dia2: 120un).",
                                    "Agregue em relatório mensal e adicione contexto (comparação com meta).",
                                    "Explique: 'Dados: 100un; Informação: 20% abaixo da meta em Janeiro'.",
                                    "Debata: 'Isso revela padrões de consumidor?'.",
                                    "Teste com novo conjunto de dados."
                                  ],
                                  "verification": "Relatório completo com diferenciação clara em 2 cenários.",
                                  "estimatedTime": "35 minutos",
                                  "materials": [
                                    "Planilha Excel ou Google Sheets",
                                    "Dados fictícios de vendas"
                                  ],
                                  "tips": "Sempre pergunte: 'Isso tem significado útil?' para validar informação.",
                                  "learningObjective": "Aplicar diferenciação em cenários práticos de negócios.",
                                  "commonMistakes": [
                                    "Não quantificar padrões",
                                    "Manter dados sem agregação"
                                  ]
                                }
                              ],
                              "practicalExample": "Dados: Vendas diárias de uma loja (Seg: 50un, Ter: 60un, Qua: 45un). Informação: Relatório semanal mostra média de 52un/dia, com queda quartas-feiras, revelando possível aversão de clientes a promoções mid-week, guiando ajustes de estoque.",
                              "finalVerifications": [
                                "Explica verbalmente a diferença entre dados e informação sem hesitação.",
                                "Transforma um conjunto de dados crus em informação contextualizada.",
                                "Identifica corretamente quando algo é dado vs. informação em exemplos alheios.",
                                "Cria diagrama da transformação dados → informação.",
                                "Aplica conceito a dados reais de vendas ou tráfego web."
                              ],
                              "assessmentCriteria": [
                                "Precisão conceitual: definições corretas sem confusões.",
                                "Uso de exemplos: relevantes e ilustrativos da transformação.",
                                "Profundidade de análise: identifica contexto e padrões claramente.",
                                "Criatividade: aplica a cenários variados além do exemplo dado.",
                                "Clareza de comunicação: explicações concisas e acessíveis.",
                                "Aplicação prática: demonstra em relatório ou fluxograma funcional."
                              ],
                              "crossCurricularConnections": [
                                "Estatística: agregação e análise de tendências.",
                                "Informática: bancos de dados e queries SQL para organização.",
                                "Negócios: business intelligence e relatórios gerenciais.",
                                "Linguagem: comunicação clara de insights derivados de dados."
                              ],
                              "realWorldApplication": "Em e-commerce, dados de cliques isolados viram informação sobre preferências de usuários via relatórios analíticos, otimizando recomendações e aumentando vendas em até 15%."
                            },
                            "estimatedTime": "1 hora",
                            "difficulty": "beginner",
                            "status": "not_started",
                            "prerequisites": [
                              "10.1.5.4.1.1"
                            ]
                          },
                          {
                            "id": "10.1.5.4.1.3",
                            "name": "Caracterizar informação como conhecimento",
                            "description": "Descrever o processo de transformação da informação em conhecimento por meio de interpretação e experiência, ilustrando com casos como análises preditivas que geram estratégias de negócios acionáveis.",
                            "atomicExpansion": {
                              "steps": [
                                {
                                  "stepNumber": 1,
                                  "title": "Compreender os conceitos fundamentais: Dados, Informação e Conhecimento",
                                  "subSteps": [
                                    "Defina dados como fatos brutos sem contexto.",
                                    "Explique informação como dados processados com contexto e significado.",
                                    "Descreva conhecimento como informação interpretada e aplicada com experiência.",
                                    "Crie um diagrama hierárquico ilustrando a pirâmide DIKW (Dados-Informação-Conhecimento-Sabedoria).",
                                    "Compare exemplos cotidianos para diferenciar cada nível."
                                  ],
                                  "verification": "Criar um diagrama e explicá-lo oralmente ou por escrito, demonstrando distinções claras.",
                                  "estimatedTime": "45 minutos",
                                  "materials": "Papel e caneta ou ferramenta digital como Draw.io; artigo sobre pirâmide DIKW.",
                                  "tips": "Use analogias simples, como 'dados são ingredientes, informação é receita, conhecimento é prato cozinhado'.",
                                  "learningObjective": "Diferenciar precisamente dados, informação e conhecimento na hierarquia DIKW.",
                                  "commonMistakes": "Confundir informação com conhecimento, ignorando o papel da interpretação subjetiva."
                                },
                                {
                                  "stepNumber": 2,
                                  "title": "Analisar o processo de transformação da informação em conhecimento",
                                  "subSteps": [
                                    "Identifique etapas do processo: coleta, processamento, interpretação e aplicação.",
                                    "Descreva como contexto adiciona valor à informação.",
                                    "Explique a necessidade de filtros humanos (interpretação) para elevar informação a conhecimento.",
                                    "Mapeie fluxos de transformação com setas e exemplos.",
                                    "Discuta barreiras comuns no processo, como viés ou falta de dados."
                                  ],
                                  "verification": "Produzir um fluxograma do processo e narrar sua lógica em 2-3 minutos.",
                                  "estimatedTime": "60 minutos",
                                  "materials": "Ferramenta de fluxograma (Lucidchart ou PowerPoint); exemplos de casos reais de dados.",
                                  "tips": "Pense em um funil: informação entra larga, conhecimento sai refinado pela experiência.",
                                  "learningObjective": "Mapear e descrever o fluxo sequencial de transformação da informação em conhecimento.",
                                  "commonMistakes": "Omitir o papel iterativo da experiência, tratando o processo como linear e automático."
                                },
                                {
                                  "stepNumber": 3,
                                  "title": "Explorar o papel da interpretação e experiência na caracterização",
                                  "subSteps": [
                                    "Defina interpretação como atribuição de significado subjetivo à informação.",
                                    "Discuta como experiência pessoal ou coletiva enriquece a interpretação.",
                                    "Analise viés cognitivo e como mitigá-lo para gerar conhecimento confiável.",
                                    "Pratique reinterpretando a mesma informação sob diferentes perspectivas de experiência.",
                                    "Registre diferenças nos resultados de conhecimento gerados."
                                  ],
                                  "verification": "Reinterpretar um conjunto de dados sob duas experiências diferentes e comparar os conhecimentos resultantes.",
                                  "estimatedTime": "50 minutos",
                                  "materials": "Conjunto de dados simples (ex: tabela de vendas); caderno para anotações.",
                                  "tips": "Pergunte 'O que isso significa para mim/agora?' para ativar interpretação ativa.",
                                  "learningObjective": "Demonstrar como interpretação e experiência elevam informação a conhecimento acionável.",
                                  "commonMistakes": "Superestimar objetividade da informação, subestimando subjetividade humana."
                                },
                                {
                                  "stepNumber": 4,
                                  "title": "Ilustrar com casos práticos, como análises preditivas",
                                  "subSteps": [
                                    "Selecione um caso: dados de vendas passadas (informação).",
                                    "Aplique interpretação: padrões sazonais via modelo preditivo.",
                                    "Incorpore experiência: analista sênior ajusta modelo com histórico de mercado.",
                                    "Gere conhecimento: estratégia de estoque otimizada para maximizar lucros.",
                                    "Avalie impacto: simule resultados antes/depois da transformação."
                                  ],
                                  "verification": "Criar um relatório curto (1 página) do caso, destacando transformação e estratégia acionável.",
                                  "estimatedTime": "75 minutos",
                                  "materials": "Dataset de vendas (Kaggle ou Excel simulado); software como Python/Jupyter ou Excel.",
                                  "tips": "Comece com dados reais pequenos para evitar sobrecarga; foque em 'por quê' da estratégia.",
                                  "learningObjective": "Aplicar conceitos a um exemplo concreto de análise preditiva gerando conhecimento estratégico.",
                                  "commonMistakes": "Parar na predição (informação), sem ligar à ação estratégica (conhecimento)."
                                }
                              ],
                              "practicalExample": "Em uma empresa de varejo, dados brutos de vendas diárias (informação: 'vendas caíram 20% em dezembro') são interpretados por um analista experiente, que considera histórico sazonal e eventos econômicos, transformando em conhecimento: 'Estocar 30% menos produtos perecíveis e investir em promoções online para recuperar 15% das vendas perdidas'. Isso gera uma estratégia de negócios acionável.",
                              "finalVerifications": [
                                "Explicar verbalmente a pirâmide DIKW com exemplos precisos.",
                                "Mapear um processo de transformação de informação para conhecimento em um fluxograma.",
                                "Reinterpretar dados sob diferentes experiências e justificar mudanças no conhecimento.",
                                "Produzir um relatório de caso prático com estratégia acionável.",
                                "Identificar e mitigar um viés em uma análise hipotética.",
                                "Discutir limitações da transformação sem experiência adequada."
                              ],
                              "assessmentCriteria": [
                                "Precisão conceitual na distinção entre informação e conhecimento (80%+ acurácia).",
                                "Profundidade nos subpassos, com pelo menos 4 elementos detalhados por step.",
                                "Relevância e originalidade de exemplos práticos.",
                                "Clareza na descrição do papel da interpretação e experiência.",
                                "Capacidade de gerar outputs acionáveis e verificáveis.",
                                "Integração de dicas para evitar erros comuns."
                              ],
                              "crossCurricularConnections": [
                                "Filosofia: Epistemologia e teoria do conhecimento (Platão, Kant).",
                                "Gestão de Negócios: Tomada de decisões baseada em dados (Business Intelligence).",
                                "Psicologia Cognitiva: Processos de percepção e viés (Kahneman).",
                                "Ciência da Computação: Processamento de dados em ML e Big Data."
                              ],
                              "realWorldApplication": "Na Ciência de Dados, profissionais usam essa caracterização para transformar análises preditivas (ex: churn de clientes) em conhecimentos estratégicos, como campanhas de retenção personalizadas, impactando diretamente o ROI de empresas como Netflix ou Amazon."
                            },
                            "estimatedTime": "1.5 horas",
                            "difficulty": "beginner",
                            "status": "not_started",
                            "prerequisites": [
                              "10.1.5.4.1.2"
                            ]
                          }
                        ]
                      },
                      {
                        "id": "10.1.5.4.2",
                        "name": "Ambiente Independente e Identicamente Distribuído (IID)",
                        "description": "Assunção fundamental em aprendizado de máquina de que os dados seguem uma distribuição probabilística idêntica e independente, essencial para a validade de modelos estatísticos.",
                        "specificSkills": [
                          {
                            "id": "10.1.5.4.2.1",
                            "name": "Explicar a assunção IID",
                            "description": "Definir IID como a condição em que observações são independentes entre si e derivadas da mesma distribuição de probabilidade, com exemplos em conjuntos de dados de amostragens aleatórias.",
                            "atomicExpansion": {
                              "steps": [
                                {
                                  "stepNumber": 1,
                                  "title": "Compreender o conceito de Independência em observações",
                                  "subSteps": [
                                    "Defina independência estatística: eventos ou observações onde o resultado de um não afeta o outro.",
                                    "Discuta exemplos cotidianos, como lançamentos sucessivos de uma moeda justa.",
                                    "Explique por que independência é crucial em amostragens para evitar viés.",
                                    "Diferencie independência de dependência com um contraexemplo, como dados temporais.",
                                    "Crie um diagrama simples mostrando observações independentes."
                                  ],
                                  "verification": "Explique verbalmente ou por escrito a independência com um exemplo correto sem erros conceituais.",
                                  "estimatedTime": "20 minutos",
                                  "materials": [
                                    "Papel e caneta para diagramas",
                                    "Acesso a um editor de texto ou Jupyter Notebook"
                                  ],
                                  "tips": "Use analogias simples como 'família de dados sem parentesco' para fixar o conceito.",
                                  "learningObjective": "Identificar e explicar independência em contextos de dados.",
                                  "commonMistakes": "Confundir independência com correlação zero; assumir que todos os dados reais são independentes."
                                },
                                {
                                  "stepNumber": 2,
                                  "title": "Explorar a noção de Identicamente Distribuídos (i.d.)",
                                  "subSteps": [
                                    "Defina 'identicamente distribuídos': todas as observações vêm da mesma distribuição de probabilidade.",
                                    "Ilustre com um exemplo: alturas de pessoas aleatórias de uma população amostradas igualmente.",
                                    "Compare distribuições idênticas vs. não idênticas usando histogramas conceituais.",
                                    "Discuta implicações: média e variância consistentes em amostras grandes.",
                                    "Gere um exemplo numérico simples com números aleatórios de uma distribuição normal."
                                  ],
                                  "verification": "Desenhe ou descreva histogramas de duas amostras i.d. que se sobrepõem significativamente.",
                                  "estimatedTime": "25 minutos",
                                  "materials": [
                                    "Calculadora ou Python (numpy.random)",
                                    "Papel para histogramas manuais"
                                  ],
                                  "tips": "Pense em 'irmãos gêmeos estatísticos' – mesmos pais (distribuição), mas valores diferentes.",
                                  "learningObjective": "Reconhecer quando observações compartilham a mesma distribuição subjacente.",
                                  "commonMistakes": "Ignorar mudanças na distribuição ao longo do tempo, como em dados sazonais."
                                },
                                {
                                  "stepNumber": 3,
                                  "title": "Definir e integrar a assunção IID completa",
                                  "subSteps": [
                                    "Combine os conceitos: IID = Independente E Identicamente Distribuído.",
                                    "Escreva a definição formal: observações {X1, X2, ..., Xn} ~ IID P(X).",
                                    "Explique por que IID é uma assunção fundamental em aprendizado de máquina e estatística.",
                                    "Discuta violações comuns, como dados de séries temporais.",
                                    "Crie uma declaração pessoal resumindo IID em suas próprias palavras."
                                  ],
                                  "verification": "Forneça a definição exata de IID e identifique se um dado conjunto satisfaz a assunção.",
                                  "estimatedTime": "20 minutos",
                                  "materials": [
                                    "Livro ou artigo introdutório sobre ML (ex: 'Hands-On ML')",
                                    "Editor de texto"
                                  ],
                                  "tips": "Memorize a sigla: 'Independente e Igual' para fixação rápida.",
                                  "learningObjective": "Formular a definição precisa de IID e suas componentes.",
                                  "commonMistakes": "Omitir 'identicamente' e focar só em independência; confundir com estacionariedade."
                                },
                                {
                                  "stepNumber": 4,
                                  "title": "Aplicar IID a exemplos em conjuntos de dados de amostragens aleatórias",
                                  "subSteps": [
                                    "Selecione um dataset simples (ex: Iris ou moedas simuladas).",
                                    "Verifique independência: teste de correlação entre pares de observações.",
                                    "Verifique distribuição idêntica: compare estatísticas descritivas de subamostras.",
                                    "Simule um dataset IID vs. não-IID e compare propriedades.",
                                    "Conclua se o dataset atende à assunção IID com justificativa."
                                  ],
                                  "verification": "Analise um dataset fornecido e classifique corretamente como IID ou não, com evidências.",
                                  "estimatedTime": "30 minutos",
                                  "materials": [
                                    "Python com pandas, numpy, scipy",
                                    "Dataset Iris ou gerador de dados aleatórios"
                                  ],
                                  "tips": "Use seed para reprodutibilidade em simulações: np.random.seed(42).",
                                  "learningObjective": "Avaliar empiricamente a assunção IID em dados reais ou simulados.",
                                  "commonMistakes": "Usar testes inadequados para independência; ignorar tamanho da amostra em verificações."
                                }
                              ],
                              "practicalExample": "Em um experimento de lançamento de uma moeda justa 100 vezes, cada lançamento é independente (um não afeta o próximo) e identicamente distribuído (P(cara)=0.5 para todos), formando um dataset IID ideal para estimar a probabilidade verdadeira.",
                              "finalVerifications": [
                                "Defina IID corretamente sem omissões.",
                                "Forneça pelo menos dois exemplos válidos de dados IID.",
                                "Identifique uma violação comum de IID em dados reais.",
                                "Explique o impacto de violar IID em modelos de ML.",
                                "Crie um diagrama conceitual de observações IID.",
                                "Simule um pequeno dataset IID e verifique suas propriedades básicas."
                              ],
                              "assessmentCriteria": [
                                "Precisão na definição de independência e distribuição idêntica (30%)",
                                "Qualidade e relevância dos exemplos fornecidos (25%)",
                                "Capacidade de identificar violações de IID (20%)",
                                "Clareza na explicação escrita ou verbal (15%)",
                                "Uso correto de terminologia estatística (10%)"
                              ],
                              "crossCurricularConnections": [
                                "Estatística: Teorema Central do Limite depende de IID para convergência.",
                                "Aprendizado de Máquina: Assunção IID em validação cruzada e generalização.",
                                "Programação: Geração de dados sintéticos com random.seed em Python/R.",
                                "Ciência de Dados: Pré-processamento para aproximar dados a IID."
                              ],
                              "realWorldApplication": "Em machine learning, a assunção IID permite dividir dados em treino/teste sem viés, essencial para prever vendas futuras baseadas em histórico de amostras aleatórias de clientes, garantindo que o modelo generalize para novos dados."
                            },
                            "estimatedTime": "1 hora",
                            "difficulty": "intermediate",
                            "status": "not_started",
                            "prerequisites": [
                              "10.1.5.4.1.3"
                            ]
                          },
                          {
                            "id": "10.1.5.4.2.2",
                            "name": "Identificar violações da assunção IID",
                            "description": "Reconhecer cenários onde a independência ou identidade de distribuição é quebrada, como em séries temporais com autocorrelação, e discutir impactos na performance de modelos de ML.",
                            "atomicExpansion": {
                              "steps": [
                                {
                                  "stepNumber": 1,
                                  "title": "Compreender a Assunção IID",
                                  "subSteps": [
                                    "Defina Independência: Amostras são independentes se o valor de uma não influencia outra.",
                                    "Defina Identidade de Distribuição: Todas as amostras vêm da mesma distribuição de probabilidade.",
                                    "Explique o papel da IID em algoritmos de ML: Garante que o modelo treinado generalize para dados de teste.",
                                    "Revise teoremas fundamentais como Hoeffding's Inequality que dependem de IID.",
                                    "Diferencie IID de cenários não-IID como dados sequenciais."
                                  ],
                                  "verification": "Resuma em suas palavras o que significa IID e por que é crucial em ML.",
                                  "estimatedTime": "30 minutos",
                                  "materials": [
                                    "Documentação scikit-learn sobre premissas de ML",
                                    "Notebook Jupyter vazio"
                                  ],
                                  "tips": "Use analogias como 'lançamentos de moeda justos e independentes' para fixar o conceito.",
                                  "learningObjective": "Entender precisamente os componentes da assunção IID e sua importância em ML.",
                                  "commonMistakes": [
                                    "Confundir independência com ausência de correlação",
                                    "Ignorar que IID é uma premissa teórica, não sempre real"
                                  ]
                                },
                                {
                                  "stepNumber": 2,
                                  "title": "Identificar Violações de Independência",
                                  "subSteps": [
                                    "Estude autocorrelação: Correlação entre valores em diferentes lags temporais.",
                                    "Analise dados sequenciais: Como em séries temporais onde eventos passados afetam futuros.",
                                    "Examine dependências espaciais: Pixels em imagens ou vizinhos em grafos.",
                                    "Use testes estatísticos: Durbin-Watson para autocorrelação em resíduos.",
                                    "Visualize com gráficos: ACF (Autocorrelation Function) plots."
                                  ],
                                  "verification": "Gere um plot ACF em dados simulados com autocorrelação e interprete.",
                                  "estimatedTime": "45 minutos",
                                  "materials": [
                                    "Python com pandas, statsmodels",
                                    "Dataset de série temporal exemplo (ex: AirPassengers)"
                                  ],
                                  "tips": "Comece com dados simulados AR(1) para ver autocorrelação clara.",
                                  "learningObjective": "Reconhecer padrões onde independência é violada, especialmente em dados temporais.",
                                  "commonMistakes": [
                                    "Assumir independência sem testar correlações",
                                    "Confundir correlação fraca com independência"
                                  ]
                                },
                                {
                                  "stepNumber": 3,
                                  "title": "Identificar Violações de Identidade de Distribuição",
                                  "subSteps": [
                                    "Defina concept drift: Mudança na distribuição ao longo do tempo.",
                                    "Explore covariate shift: Mudança na distribuição de features, mas rótulos consistentes.",
                                    "Identifique prior probability shift: Mudança na distribuição de rótulos.",
                                    "Use testes como Kolmogorov-Smirnov para comparar distribuições.",
                                    "Monitore com métricas: KS statistic ou population stability index (PSI)."
                                  ],
                                  "verification": "Aplique KS test entre dois subsets de dados e interprete p-value.",
                                  "estimatedTime": "45 minutos",
                                  "materials": [
                                    "Python com scipy.stats",
                                    "Dataset com concept drift simulado (ex: eletricidade)"
                                  ],
                                  "tips": "Divida datasets em janelas temporais para simular drift.",
                                  "learningObjective": "Detectar mudanças na distribuição de dados que quebram identidade.",
                                  "commonMistakes": [
                                    "Ignorar drift em dados de produção",
                                    "Confundir shift com ruído aleatório"
                                  ]
                                },
                                {
                                  "stepNumber": 4,
                                  "title": "Avaliar Impactos e Estratégias de Detecção",
                                  "subSteps": [
                                    "Discuta impactos: Overfitting, poor generalization, biased estimates.",
                                    "Compare performance: Treine modelo IID vs não-IID e meça accuracy drop.",
                                    "Estratégias de mitigação: Windowing, robust models como RNNs.",
                                    "Integre detecção: Rotinas de monitoramento em pipelines ML.",
                                    "Documente casos: Relate violações em relatório."
                                  ],
                                  "verification": "Escreva um parágrafo discutindo impactos em um cenário hipotético.",
                                  "estimatedTime": "30 minutos",
                                  "materials": [
                                    "Exemplos de código de MLflow para monitoring",
                                    "Paper sobre non-IID learning"
                                  ],
                                  "tips": "Sempre valide assunções antes de deployar modelos.",
                                  "learningObjective": "Compreender consequências de violações IID e como detectá-las proativamente.",
                                  "commonMistakes": [
                                    "Subestimar impactos em cenários reais",
                                    "Não testar em dados out-of-distribution"
                                  ]
                                }
                              ],
                              "practicalExample": "Em um dataset de preços de ações diários (ex: AAPL), calcule a ACF e observe lags significativos >0.5, indicando autocorrelação que viola independência; treine um modelo linear e veja degradação em previsões futuras devido a tendências não capturadas.",
                              "finalVerifications": [
                                "Explique IID e dê 2 exemplos de violações.",
                                "Interprete um plot ACF com autocorrelação.",
                                "Aplique KS test e conclua sobre shift de distribuição.",
                                "Discuta 3 impactos em performance de ML.",
                                "Proponha detecção para um dataset real.",
                                "Diferencie violações de independência vs distribuição."
                              ],
                              "assessmentCriteria": [
                                "Precisão na definição e exemplos de IID (90%+ correto).",
                                "Correta identificação de violações via testes estatísticos.",
                                "Análise qualitativa/quantitativa de impactos.",
                                "Uso apropriado de visualizações e código.",
                                "Conexões com cenários reais e mitigação.",
                                "Clareza na comunicação de achados."
                              ],
                              "crossCurricularConnections": [
                                "Estatística: Testes de hipótese e distribuições.",
                                "Séries Temporais: Modelos ARIMA e forecasting.",
                                "Engenharia de ML: MLOps e monitoring de drift.",
                                "Probabilidade: Teoremas de convergência sob IID."
                              ],
                              "realWorldApplication": "Em finanças, detectar autocorrelação em retornos de ações para evitar modelos falhos em trading algorítmico; em saúde, identificar concept drift em dados de pacientes para atualizar modelos de previsão de epidemias."
                            },
                            "estimatedTime": "1.5 horas",
                            "difficulty": "intermediate",
                            "status": "not_started",
                            "prerequisites": [
                              "10.1.5.4.2.1"
                            ]
                          },
                          {
                            "id": "10.1.5.4.2.3",
                            "name": "Aplicar IID em modelagem inicial",
                            "description": "Avaliar se um conjunto de dados atende à assunção IID para prosseguir com modelagem, utilizando testes estatísticos simples como verificação de independência.",
                            "atomicExpansion": {
                              "steps": [
                                {
                                  "stepNumber": 1,
                                  "title": "Compreender a Assunção IID e Sua Importância",
                                  "subSteps": [
                                    "Defina IID: Independentemente e Identicamente Distribuídos, onde observações são independentes e seguem a mesma distribuição probabilística.",
                                    "Explique independência: ausência de correlação entre observações (ex: sem padrões temporais ou espaciais).",
                                    "Explique distribuição idêntica: todas as observações vêm da mesma população subjacente.",
                                    "Discuta implicações em ML: violação de IID invalida teoremas como Lei dos Grandes Números.",
                                    "Revise exemplos: dados aleatórios vs. séries temporais."
                                  ],
                                  "verification": "Escreva um resumo de 100 palavras explicando IID e forneça um exemplo de violação.",
                                  "estimatedTime": "30 minutos",
                                  "materials": "Documentação de ML (Scikit-learn docs), vídeo introdutório sobre IID (YouTube/Khan Academy).",
                                  "tips": "Use analogias como 'lançamentos de moeda justos' para independência.",
                                  "learningObjective": "Explicar conceitualmente IID e identificar cenários onde se aplica.",
                                  "commonMistakes": "Confundir IID com normalidade de dados; IID não requer normalidade."
                                },
                                {
                                  "stepNumber": 2,
                                  "title": "Preparar e Explorar o Conjunto de Dados",
                                  "subSteps": [
                                    "Carregue o dataset usando Python (pandas.read_csv).",
                                    "Calcule estatísticas descritivas (média, variância, histograma por subamostras).",
                                    "Crie visualizações: scatter plots para correlações, boxplots para distribuições.",
                                    "Divida dados em subamostras aleatórias (ex: 50% cada) para comparação.",
                                    "Identifique padrões óbvios de dependência (ex: ordenação temporal)."
                                  ],
                                  "verification": "Gere relatório com 3 plots e estatísticas; confirme ausência de tendências visuais.",
                                  "estimatedTime": "45 minutos",
                                  "materials": "Python/Jupyter Notebook, bibliotecas pandas, matplotlib/seaborn, dataset exemplo (ex: Iris ou Boston Housing).",
                                  "tips": "Sempre embaralhe os dados antes de plotar para simular independência.",
                                  "learningObjective": "Preparar dados para testes IID através de exploração visual e descritiva.",
                                  "commonMistakes": "Ignorar ordenação dos dados; sempre verifique índice temporal."
                                },
                                {
                                  "stepNumber": 3,
                                  "title": "Verificar Independência das Observações",
                                  "subSteps": [
                                    "Calcule matriz de correlação (corr() no pandas) e verifique valores próximos a zero.",
                                    "Aplique teste de autocorrelação (statsmodels.tsa.stattools.acf).",
                                    "Execute teste de runs (scipy.stats.runstest) ou Durbin-Watson para dependência serial.",
                                    "Interprete p-values: p > 0.05 sugere independência.",
                                    "Visualize ACF/PACF plots para padrões."
                                  ],
                                  "verification": "Registre p-values de pelo menos 2 testes; conclua se independência é atendida.",
                                  "estimatedTime": "60 minutos",
                                  "materials": "Python com scipy, statsmodels; Jupyter Notebook.",
                                  "tips": "Para dados não temporais, teste independência entre pares aleatórios.",
                                  "learningObjective": "Aplicar testes estatísticos para validar independência.",
                                  "commonMistakes": "Usar testes inadequados para dados categóricos; prefira chi-quadrado para categóricos."
                                },
                                {
                                  "stepNumber": 4,
                                  "title": "Verificar Distribuição Idêntica e Concluir Avaliação",
                                  "subSteps": [
                                    "Divida dados em 3-5 subamostras aleatórias.",
                                    "Aplique teste KS (Kolmogorov-Smirnov, scipy.stats.ks_2samp) entre subamostras.",
                                    "Use QQ-plots e histograms sobrepostos para comparação visual.",
                                    "Calcule estatísticas (média, desvios) e teste ANOVA para diferenças.",
                                    "Decida: se todos testes passam (p > 0.05), prossiga com modelagem; senão, transforme dados."
                                  ],
                                  "verification": "Produza tabela de resultados de testes e recomendação final (IID atendido ou não).",
                                  "estimatedTime": "45 minutos",
                                  "materials": "Python com scipy.stats, matplotlib; mesmo dataset.",
                                  "tips": "Teste múltiplas divisões para robustez; use bootstrap se n pequeno.",
                                  "learningObjective": "Validar homogeneidade de distribuição e integrar resultados para decisão.",
                                  "commonMistakes": "Assumir IID sem múltiplos testes; sempre combine estatísticos e visuais."
                                }
                              ],
                              "practicalExample": "Carregue o dataset Iris (sklearn.datasets.load_iris). Divida em subamostras, teste correlação entre features (deve ser baixa), aplique KS entre subamostras de 'sepal_length' (p > 0.05 confirma IID). Se falhar, note dependência entre espécies.",
                              "finalVerifications": [
                                "Lista correta de testes usados e seus p-values.",
                                "Interpretação precisa: IID atendido apenas se todos testes passam.",
                                "Relatório inclui plots e estatísticas descritivas.",
                                "Recomendação explícita para prosseguir ou ajustar modelagem.",
                                "Identificação de pelo menos uma violação potencial no exemplo.",
                                "Resumo conceitual de IID em 50 palavras."
                              ],
                              "assessmentCriteria": [
                                "Compreensão conceitual de IID: explica independência e distribuição idêntica corretamente (0-4 pontos).",
                                "Exploração de dados: plots e stats adequados, sem erros de código (0-5 pontos).",
                                "Execução de testes: escolha e aplicação correta de testes estatísticos (0-6 pontos).",
                                "Interpretação de resultados: p-values e conclusões lógicas (0-5 pontos).",
                                "Relatório final: completo, claro e acionável (0-4 pontos).",
                                "Tratamento de edge cases: menciona limitações de testes (0-3 pontos)."
                              ],
                              "crossCurricularConnections": [
                                "Estatística Inferencial: testes de hipótese e p-values.",
                                "Programação Computacional: Python para análise de dados.",
                                "Ciência de Dados: pré-processamento e validação de assumptions.",
                                "Aprendizado de Máquina: foundations teóricas de algoritmos."
                              ],
                              "realWorldApplication": "Em projetos de ML como previsão de preços de casas (dataset Boston), verificar IID evita overfitting em modelos lineares; em finanças, detecta dependências temporais em ações para escolher LSTM vs. regressão."
                            },
                            "estimatedTime": "2 horas",
                            "difficulty": "intermediate",
                            "status": "not_started",
                            "prerequisites": [
                              "10.1.5.4.2.2"
                            ]
                          }
                        ]
                      },
                      {
                        "id": "10.1.5.4.3",
                        "name": "Etapas Iniciais de Modelagem de Problemas",
                        "description": "Descrição das fases preliminares do pipeline de Ciência de Dados, incluindo coleta, integração, armazenamento, análise exploratória e limpeza, como base para modelagem em aprendizado de máquina.",
                        "specificSkills": [
                          {
                            "id": "10.1.5.4.3.1",
                            "name": "Descrever coleta, integração e armazenamento de dados",
                            "description": "Explicar as etapas de aquisição de dados de múltiplas fontes, sua fusão e armazenamento em formatos adequados como bancos relacionais ou NoSQL, com ênfase em qualidade e escalabilidade.",
                            "atomicExpansion": {
                              "steps": [
                                {
                                  "stepNumber": 1,
                                  "title": "Planejar a Coleta de Dados de Múltiplas Fontes",
                                  "subSteps": [
                                    "Identifique as fontes de dados relevantes (ex.: APIs, CSVs, bancos de dados existentes).",
                                    "Defina os requisitos de dados: volume, variedade, velocidade e veracidade (4Vs).",
                                    "Crie um diagrama de fluxo de dados para mapear fontes e formatos iniciais.",
                                    "Avalie questões éticas e legais (GDPR, privacidade).",
                                    "Estime o volume de dados e potenciais gargalos."
                                  ],
                                  "verification": "Diagrama de fluxo completado e compartilhado com justificativas por escrito.",
                                  "estimatedTime": "1 hora",
                                  "materials": "Ferramentas de diagramação (Draw.io, Lucidchart), caderno de notas.",
                                  "tips": "Priorize fontes confiáveis para evitar bias nos dados.",
                                  "learningObjective": "Compreender como planejar aquisição de dados heterogêneos.",
                                  "commonMistakes": "Ignorar formatos incompatíveis entre fontes ou subestimar volume."
                                },
                                {
                                  "stepNumber": 2,
                                  "title": "Adquirir e Coletar Dados das Fontes Identificadas",
                                  "subSteps": [
                                    "Implemente scripts para extrair dados (ex.: requests para APIs, pd.read_csv para arquivos).",
                                    "Colete amostras iniciais de cada fonte para inspeção.",
                                    "Registre metadados: origem, timestamp, schema.",
                                    "Trate falhas de coleta com retries e logging.",
                                    "Valide integridade inicial (checksums, contagens de registros)."
                                  ],
                                  "verification": "Datasets coletados salvos localmente com logs de sucesso/falha.",
                                  "estimatedTime": "2 horas",
                                  "materials": "Python (pandas, requests), Jupyter Notebook.",
                                  "tips": "Use autenticação segura para APIs e rate limiting para evitar bloqueios.",
                                  "learningObjective": "Dominar técnicas de extração de dados de fontes diversas.",
                                  "commonMistakes": "Não capturar erros de rede ou exceder limites de API."
                                },
                                {
                                  "stepNumber": 3,
                                  "title": "Integrar e Fundir os Dados Coletados",
                                  "subSteps": [
                                    "Padronize esquemas: renomeie colunas, converta tipos de dados.",
                                    "Realize junções (inner, outer) ou merges baseados em chaves comuns.",
                                    "Lide com dados faltantes: imputação, remoção ou flags.",
                                    "Detecte e resolva duplicatas e inconsistências.",
                                    "Crie um dataset unificado e valide qualidade (estatísticas descritivas)."
                                  ],
                                  "verification": "Dataset integrado gerado com relatório de qualidade (ex.: df.describe()).",
                                  "estimatedTime": "2.5 horas",
                                  "materials": "Python (pandas, numpy), bibliotecas como fuzzywuzzy para matching.",
                                  "tips": "Use chaves compostas para merges precisos em dados não estruturados.",
                                  "learningObjective": "Aprender fusão de dados heterogêneos mantendo integridade.",
                                  "commonMistakes": "Perder dados em junções inadequadas ou ignorar outliers."
                                },
                                {
                                  "stepNumber": 4,
                                  "title": "Armazenar Dados com Ênfase em Qualidade e Escalabilidade",
                                  "subSteps": [
                                    "Escolha o banco: relacional (SQL) para estruturados, NoSQL (MongoDB) para não-estruturados.",
                                    "Crie schema otimizado com índices para queries frequentes.",
                                    "Insira dados em batch para eficiência.",
                                    "Implemente validações de qualidade no armazenamento (constraints, triggers).",
                                    "Teste escalabilidade com queries simulando carga alta."
                                  ],
                                  "verification": "Dados persistidos no banco com queries de teste bem-sucedidas.",
                                  "estimatedTime": "2 horas",
                                  "materials": "PostgreSQL ou MongoDB, SQLAlchemy ou PyMongo.",
                                  "tips": "Particione tabelas por data para escalabilidade em grandes volumes.",
                                  "learningObjective": "Selecionar e implementar armazenamento escalável e de alta qualidade.",
                                  "commonMistakes": "Escolher formato inadequado (ex.: NoSQL para dados altamente relacionais)."
                                },
                                {
                                  "stepNumber": 5,
                                  "title": "Documentar e Avaliar o Processo Completo",
                                  "subSteps": [
                                    "Escreva um relatório descrevendo cada etapa, desafios e soluções.",
                                    "Meça métricas de qualidade: completude, precisão, timely.",
                                    "Simule escalabilidade com dados sintéticos maiores.",
                                    "Crie um pipeline automatizado (ex.: Airflow DAG).",
                                    "Revise e otimize com base em feedback."
                                  ],
                                  "verification": "Relatório final e pipeline executável submetidos.",
                                  "estimatedTime": "1.5 horas",
                                  "materials": "Markdown para relatório, Apache Airflow ou scripts simples.",
                                  "tips": "Use versionamento (Git) para todo o pipeline.",
                                  "learningObjective": "Sintetizar o processo em uma descrição clara e acionável.",
                                  "commonMistakes": "Omitir métricas de qualidade ou não testar escalabilidade."
                                }
                              ],
                              "practicalExample": "Colete dados de vendas de um e-commerce: API de produtos (JSON), CSV de clientes e banco SQL de transações. Integre por ID de cliente/produto, limpe duplicatas e armazene em PostgreSQL particionado por mês para análise de ML em churn prediction.",
                              "finalVerifications": [
                                "Dataset final contém dados de todas fontes sem perda significativa (>95% completude).",
                                "Queries de escalabilidade executam em <5s para 1M registros.",
                                "Relatório descreve todo pipeline com diagramas e métricas.",
                                "Qualidade validada: precisão >98%, sem duplicatas.",
                                "Pipeline automatizado roda end-to-end sem erros.",
                                "Escolha de banco justificada por caso de uso."
                              ],
                              "assessmentCriteria": [
                                "Precisão na identificação e coleta de fontes heterogêneas (30%).",
                                "Qualidade da integração: handling de missing/duplicatas (25%).",
                                "Adequação do armazenamento para escalabilidade (20%).",
                                "Clareza da descrição/documentação do processo (15%).",
                                "Métricas de qualidade e verificações implementadas (10%)."
                              ],
                              "crossCurricularConnections": [
                                "Programação: Scripts Python para ETL.",
                                "Estatística: Análise de qualidade de dados (distribuições, outliers).",
                                "Banco de Dados: Design de schema relacional/NoSQL.",
                                "Ética em TI: Privacidade e bias em dados.",
                                "Gestão de Projetos: Planejamento de pipelines escaláveis."
                              ],
                              "realWorldApplication": "Em pipelines de Machine Learning para empresas como Netflix (recomendações) ou bancos (detecção de fraude), onde dados de logs, transações e usuários são coletados, integrados e armazenados em data lakes/warehouses para modelos escaláveis."
                            },
                            "estimatedTime": "1.5 horas",
                            "difficulty": "beginner",
                            "status": "not_started",
                            "prerequisites": [
                              "10.1.5.4.2.3"
                            ]
                          },
                          {
                            "id": "10.1.5.4.3.2",
                            "name": "Realizar análise exploratória e visualização",
                            "description": "Aplicar técnicas de EDA (Exploratory Data Analysis) como histogramas, boxplots e correlogramas para identificar padrões, outliers e relações iniciais nos dados.",
                            "atomicExpansion": {
                              "steps": [
                                {
                                  "stepNumber": 1,
                                  "title": "Preparação e Inspeção Inicial dos Dados",
                                  "subSteps": [
                                    "Carregue o dataset usando pandas (pd.read_csv ou similar).",
                                    "Inspecione a estrutura com .info(), .describe() e .head() para entender tipos de dados, valores ausentes e estatísticas básicas.",
                                    "Trate valores ausentes iniciais com .isnull().sum() e métodos como fillna() ou dropna().",
                                    "Converta tipos de dados se necessário (ex: categórico para string).",
                                    "Salve uma cópia limpa dos dados."
                                  ],
                                  "verification": "Execute df.info() e df.describe(); confirme ausência de erros e dados limpos sem NaNs críticos.",
                                  "estimatedTime": "20-30 minutos",
                                  "materials": "Python com pandas instalado; dataset exemplo (ex: Iris.csv do seaborn).",
                                  "tips": "Sempre use seed para reproducibilidade com np.random.seed(42).",
                                  "learningObjective": "Entender a estrutura dos dados e prepará-los para análise visual.",
                                  "commonMistakes": "Ignorar valores ausentes ou tipos de dados incompatíveis, levando a plots falhos."
                                },
                                {
                                  "stepNumber": 2,
                                  "title": "Análise Univariada com Histogramas e Boxplots",
                                  "subSteps": [
                                    "Gere histogramas para variáveis numéricas usando plt.hist() ou sns.histplot().",
                                    "Crie boxplots com sns.boxplot() para cada feature numérica.",
                                    "Ajuste parâmetros como bins=30 para histogramas e hue para categóricas.",
                                    "Salve figuras com plt.savefig('histograma_feature.png').",
                                    "Anote distribuições (normal, skewed) e outliers visuais."
                                  ],
                                  "verification": "Visualize plots; confirme que mostram distribuição, mediana e quartis corretamente sem erros de plotting.",
                                  "estimatedTime": "30-45 minutos",
                                  "materials": "Matplotlib e Seaborn; Jupyter Notebook para visualização interativa.",
                                  "tips": "Use figure size grande (plt.figure(figsize=(10,6))) para clareza.",
                                  "learningObjective": "Identificar distribuições e outliers em variáveis individuais.",
                                  "commonMistakes": "Escala inadequada nos eixos, ocultando outliers; não rotular eixos."
                                },
                                {
                                  "stepNumber": 3,
                                  "title": "Análise Bivariada com Correlogramas e Scatterplots",
                                  "subSteps": [
                                    "Calcule matriz de correlação com df.corr() e plote heatmap com sns.heatmap().",
                                    "Gere scatterplots bivariados com sns.pairplot() ou sns.scatterplot().",
                                    "Inclua correlogramas focando em pares de features numéricas.",
                                    "Destaque correlações fortes (>0.7 ou <-0.7) e relações não lineares.",
                                    "Salve plots e anote insights em um markdown ou dict."
                                  ],
                                  "verification": "Inspecione heatmap; confirme valores de correlação corretos e padrões visíveis em pairs.",
                                  "estimatedTime": "40-50 minutos",
                                  "materials": "Seaborn para heatmaps avançados; dataset com múltiplas features numéricas.",
                                  "tips": "Máscara triângulo superior no heatmap para evitar redundância: mask=np.triu(corr).",
                                  "learningObjective": "Detectar relações entre variáveis e multicolinearidade inicial.",
                                  "commonMistakes": "Interpretar correlação como causalidade; ignorar variáveis categóricas."
                                },
                                {
                                  "stepNumber": 4,
                                  "title": "Síntese de Insights, Detecção de Outliers e Documentação",
                                  "subSteps": [
                                    "Compile outliers de boxplots usando IQR method: Q1 - 1.5*IQR.",
                                    "Liste padrões identificados (ex: bimodalidade em histograma).",
                                    "Crie um relatório com key findings em tabela ou texto.",
                                    "Gere dashboard final com subplots combinando todos os plots.",
                                    "Valide insights com estatísticas adicionais como skewness/kurtosis."
                                  ],
                                  "verification": "Revise relatório; confirme lista de 5+ insights acionáveis e código executável sem warnings.",
                                  "estimatedTime": "25-35 minutos",
                                  "materials": "Pandas para cálculos IQR; Markdown para relatório.",
                                  "tips": "Use sns.FacetGrid para dashboards multi-painel eficientes.",
                                  "learningObjective": "Sintetizar EDA em insights acionáveis para modelagem.",
                                  "commonMistakes": "Sobrecarregar plots com dados; não documentar suposições."
                                }
                              ],
                              "practicalExample": "Usando o dataset Iris (load_dataset('iris') no seaborn): 1) Inspecione features como sepal_length; 2) Plote histogramas/boxplots mostrando distribuição normal em petal_length; 3) Correlograma revela alta correlação (0.96) entre petal_length e petal_width; 4) Identifique outliers em sepal_width e insights para clustering de espécies.",
                              "finalVerifications": [
                                "Todos os plots gerados sem erros e salvos corretamente.",
                                "Matriz de correlação completa com anotações de valores >|0.5|.",
                                "Lista de pelo menos 3 outliers detectados com coordenadas.",
                                "Relatório com 5+ insights sobre padrões e relações.",
                                "Código reproduzível gera os mesmos resultados.",
                                "Nenhum valor ausente não tratado nos dados finais."
                              ],
                              "assessmentCriteria": [
                                "Clareza e legibilidade das visualizações (rótulos, títulos, escalas).",
                                "Precisão na identificação de distribuições, outliers e correlações.",
                                "Abrangência: Cobertura de univariada e bivariada para todas features relevantes.",
                                "Profundidade dos insights: Além de descrição, implicações para modelagem.",
                                "Eficiência do código: Uso de funções vetoriais, sem loops desnecessários.",
                                "Documentação: Comentários no código e relatório estruturado."
                              ],
                              "crossCurricularConnections": [
                                "Estatística: Conceitos de distribuição, IQR e correlação.",
                                "Programação: Manipulação de dados com Python/pandas.",
                                "Matemática: Álgebra linear em matrizes de correlação.",
                                "Ciência da Computação: Visualização de dados e bibliotecas gráficas."
                              ],
                              "realWorldApplication": "Em análise de dados de e-commerce, EDA revela que produtos com preços skewados (boxplot) e alta correlação entre reviews e vendas (correlograma), guiando decisões de precificação e marketing."
                            },
                            "estimatedTime": "2 horas",
                            "difficulty": "intermediate",
                            "status": "not_started",
                            "prerequisites": [
                              "10.1.5.4.3.1"
                            ]
                          },
                          {
                            "id": "10.1.5.4.3.3",
                            "name": "Executar limpeza de dados",
                            "description": "Identificar e tratar problemas como valores ausentes, duplicatas e inconsistências, utilizando estratégias como imputação, remoção ou interpolação, preparando dados para modelagem.",
                            "atomicExpansion": {
                              "steps": [
                                {
                                  "stepNumber": 1,
                                  "title": "Explorar e Identificar Problemas nos Dados",
                                  "subSteps": [
                                    "Carregue o dataset usando pandas.read_csv() ou equivalente.",
                                    "Use métodos como info(), describe() e isna().sum() para detectar missing values.",
                                    "Verifique duplicatas com duplicated().sum().",
                                    "Inspecione inconsistências como tipos de dados errados ou outliers com value_counts() e boxplots.",
                                    "Gere um relatório resumido dos problemas encontrados."
                                  ],
                                  "verification": "Confirme que um relatório lista todos os tipos de problemas (missing, duplicatas, inconsistências) com contagens precisas.",
                                  "estimatedTime": "45 minutos",
                                  "materials": [
                                    "Python com pandas e numpy instalados",
                                    "Jupyter Notebook",
                                    "Dataset de exemplo (ex: Titanic ou Housing dataset)"
                                  ],
                                  "tips": "Sempre visualize os dados com head() e tail() antes de análises profundas para entender a estrutura.",
                                  "learningObjective": "Desenvolver habilidades para diagnosticar problemas de qualidade de dados de forma sistemática.",
                                  "commonMistakes": [
                                    "Ignorar problemas sutis como strings em colunas numéricas",
                                    "Não documentar achados iniciais",
                                    "Pular visualizações exploratórias"
                                  ]
                                },
                                {
                                  "stepNumber": 2,
                                  "title": "Tratar Valores Ausentes",
                                  "subSteps": [
                                    "Decida a estratégia: remoção (dropna()), imputação média/mediana (fillna(df.mean())), ou interpolação (interpolate()).",
                                    "Aplique a estratégia escolhida por coluna, considerando o tipo de dado.",
                                    "Compare estatísticas antes e depois (describe()).",
                                    "Registre as mudanças em um log ou novo dataset.",
                                    "Valide que não há mais missing values com isna().sum().any()."
                                  ],
                                  "verification": "Execute isna().sum() e confirme zero missing values; compare shapes antes/depois.",
                                  "estimatedTime": "60 minutos",
                                  "materials": [
                                    "Pandas DataFrame limpo do Step 1",
                                    "Documentação pandas para fillna e interpolate"
                                  ],
                                  "tips": "Para dados temporais, prefira interpolação; para categóricos, use mode().",
                                  "learningObjective": "Selecionar e implementar estratégias apropriadas de imputação baseadas no contexto dos dados.",
                                  "commonMistakes": [
                                    "Imputar com média em dados skewed",
                                    "Remover linhas sem considerar impacto no tamanho do dataset",
                                    "Não backupar dados originais"
                                  ]
                                },
                                {
                                  "stepNumber": 3,
                                  "title": "Remover Duplicatas e Corrigir Inconsistências",
                                  "subSteps": [
                                    "Remova duplicatas com drop_duplicates(), resetando index.",
                                    "Padronize tipos de dados (pd.to_numeric(), astype()).",
                                    "Trate outliers usando IQR method ou z-score e decida remover/substituir.",
                                    "Corrija inconsistências textuais (str.lower(), str.replace()).",
                                    "Reverifique com duplicated().sum() e value_counts()."
                                  ],
                                  "verification": "Confirme zero duplicatas e tipos de dados consistentes via info() e sample(10).",
                                  "estimatedTime": "50 minutos",
                                  "materials": [
                                    "DataFrame do Step 2",
                                    "Bibliotecas matplotlib/seaborn para plots de outliers"
                                  ],
                                  "tips": "Use keep='first' em drop_duplicates para preservar ordem temporal.",
                                  "learningObjective": "Aplicar técnicas para eliminar redundâncias e uniformizar dados.",
                                  "commonMistakes": [
                                    "Remover duplicatas sem considerar chaves primárias",
                                    "Ignorar case sensitivity em strings",
                                    "Remover outliers sem justificativa estatística"
                                  ]
                                },
                                {
                                  "stepNumber": 4,
                                  "title": "Verificar Qualidade Final e Preparar para Modelagem",
                                  "subSteps": [
                                    "Execute um pipeline completo de verificações (missing, duplicatas, estatísticas).",
                                    "Salve o dataset limpo como CSV ou pickle.",
                                    "Gere um relatório final com métricas de qualidade (shape changes, % dados removidos).",
                                    "Teste compatibilidade com uma modelagem simples (ex: train_test_split).",
                                    "Documente todas as decisões em um README ou notebook."
                                  ],
                                  "verification": "Dataset final passa em todos os checks iniciais e roda sem erros em split de dados.",
                                  "estimatedTime": "30 minutos",
                                  "materials": [
                                    "DataFrame final dos steps anteriores",
                                    "Scikit-learn para teste rápido"
                                  ],
                                  "tips": "Automatize verificações em uma função reutilizável para eficiência.",
                                  "learningObjective": "Garantir que dados limpos estejam prontos para análise avançada.",
                                  "commonMistakes": [
                                    "Não documentar transformações",
                                    "Salvar sem reset_index()",
                                    "Pular validação final"
                                  ]
                                }
                              ],
                              "practicalExample": "Em um dataset de vendas de imóveis (ex: housing.csv), identifique 15% de preços missing (impute com mediana por bairro), remova 50 duplicatas de propriedades idênticas, corrija 'sqft' como string para float, e clip outliers acima de 3 desvios padrão, resultando em um dataset pronto para regressão linear.",
                              "finalVerifications": [
                                "Nenhum valor ausente em qualquer coluna (isna().sum().sum() == 0)",
                                "Zero duplicatas (duplicated().sum() == 0)",
                                "Tipos de dados consistentes e apropriados (info() sem erros)",
                                "Estatísticas resumidas estáveis pós-limpeza (describe() comparado ao inicial)",
                                "Sem outliers extremos não tratados (boxplots limpos)",
                                "Dataset salva e legível (pd.read_csv() sem warnings)"
                              ],
                              "assessmentCriteria": [
                                "Precisão na identificação de problemas (cobertura >95%)",
                                "Escolha adequada de estratégias de tratamento contextualizadas",
                                "Eficiência computacional (tempo de execução <5min para 10k rows)",
                                "Documentação clara de todas as decisões e mudanças",
                                "Robustez: dataset final roda em modelagem sem erros",
                                "Minimização de perda de dados (<20% removido sem justificativa)"
                              ],
                              "crossCurricularConnections": [
                                "Estatística: Uso de medidas como média, mediana e IQR para decisões",
                                "Programação: Manipulação de estruturas de dados em Python/pandas",
                                "Matemática: Interpolação linear e detecção de anomalias",
                                "Ética em Dados: Considerações sobre bias introduzido por imputação"
                              ],
                              "realWorldApplication": "Em empresas como bancos ou e-commerces, limpar dados de transações garante precisão em modelos de ML para previsão de churn ou recomendação de produtos, evitando perdas financeiras por decisões baseadas em dados ruins."
                            },
                            "estimatedTime": "2 horas",
                            "difficulty": "intermediate",
                            "status": "not_started",
                            "prerequisites": [
                              "10.1.5.4.3.2"
                            ]
                          },
                          {
                            "id": "10.1.5.4.3.4",
                            "name": "Planejar ajuste e avaliação inicial de modelos",
                            "description": "Definir métricas básicas de avaliação como acurácia e erro quadrático médio, e esboçar o processo de treinamento e validação cruzada para modelos iniciais.",
                            "atomicExpansion": {
                              "steps": [
                                {
                                  "stepNumber": 1,
                                  "title": "Definir e Compreender Métricas Básicas de Avaliação",
                                  "subSteps": [
                                    "Estude a definição de acurácia: proporção de previsões corretas sobre o total de previsões.",
                                    "Aprenda o Erro Quadrático Médio (MSE): média dos quadrados das diferenças entre valores previstos e reais.",
                                    "Compare acurácia (para classificação) e MSE (para regressão) com exemplos numéricos simples.",
                                    "Liste vantagens e limitações de cada métrica, como acurácia sensível a desbalanceamento de classes.",
                                    "Pratique calculando manualmente em um dataset pequeno de 10 amostras."
                                  ],
                                  "verification": "Explique verbalmente ou por escrito as fórmulas de acurácia e MSE com um exemplo calculado corretamente.",
                                  "estimatedTime": "45 minutos",
                                  "materials": "Notebook Jupyter ou papel/caneta, dataset simples como Iris (classificação) ou housing prices (regressão).",
                                  "tips": "Use fórmulas matemáticas para fixar: Accuracy = (TP + TN) / Total; MSE = (1/n) * Σ(y - ŷ)^2.",
                                  "learningObjective": "Compreender e calcular métricas básicas de avaliação de modelos de ML.",
                                  "commonMistakes": "Confundir acurácia com precisão/recall; esquecer de quadrado no MSE."
                                },
                                {
                                  "stepNumber": 2,
                                  "title": "Esboçar o Processo de Treinamento de Modelos Iniciais",
                                  "subSteps": [
                                    "Divida o dataset em treino (70-80%) e teste (20-30%) usando split aleatório estratificado.",
                                    "Selecione um modelo inicial simples: regressão linear para regressão ou árvore de decisão para classificação.",
                                    "Implemente o treinamento: ajuste hiperparâmetros básicos como learning rate padrão.",
                                    "Treine o modelo e registre métricas iniciais no conjunto de treino.",
                                    "Visualize curvas de aprendizado para detectar overfitting/underfitting básico."
                                  ],
                                  "verification": "Execute código que treina um modelo e imprime acurácia/MSE no treino, sem erros.",
                                  "estimatedTime": "60 minutos",
                                  "materials": "Python com scikit-learn, pandas, matplotlib; dataset Iris ou Boston Housing.",
                                  "tips": "Sempre use train_test_split com random_state=42 para reprodutibilidade.",
                                  "learningObjective": "Planejar e executar treinamento inicial de um modelo com métricas básicas.",
                                  "commonMistakes": "Não estratificar split para datasets desbalanceados; ignorar seed para resultados inconsistentes."
                                },
                                {
                                  "stepNumber": 3,
                                  "title": "Planejar e Implementar Validação Cruzada",
                                  "subSteps": [
                                    "Entenda K-Fold CV: divida dados em K folds, treine K vezes usando 1 fold para validação.",
                                    "Escolha K=5 ou 10; implemente usando cross_val_score do scikit-learn.",
                                    "Calcule média e desvio padrão das métricas (acurácia/MSE) sobre os folds.",
                                    "Compare CV scores com split simples para validar robustez.",
                                    "Ajuste um parâmetro simples (ex: max_depth em árvore) baseado em CV scores."
                                  ],
                                  "verification": "Gere relatório com CV scores médios e intervalos de confiança para pelo menos 2 métricas.",
                                  "estimatedTime": "75 minutos",
                                  "materials": "Scikit-learn (cross_val_score, KFold), mesmo dataset anterior.",
                                  "tips": "Use cv=5 para equilíbrio entre viés e variância; plote boxplot dos scores por fold.",
                                  "learningObjective": "Aplicar validação cruzada para avaliação confiável de modelos iniciais.",
                                  "commonMistakes": "Usar o mesmo fold para treino e teste; ignorar variância alta nos scores."
                                },
                                {
                                  "stepNumber": 4,
                                  "title": "Integrar Plano de Ajuste e Avaliação Inicial Completo",
                                  "subSteps": [
                                    "Crie um pipeline: split dados → treinar baseline → CV → selecionar melhor modelo inicial.",
                                    "Defina thresholds para aceitação: ex. acurácia > 70%, MSE < threshold domínio-específico.",
                                    "Documente plano em pseudocódigo ou fluxograma.",
                                    "Teste end-to-end em novo dataset e compare métricas.",
                                    "Planeje próximos passos: tuning avançado se baseline fraco."
                                  ],
                                  "verification": "Apresente fluxograma/plano escrito e resultados de pipeline executado com sucesso.",
                                  "estimatedTime": "50 minutos",
                                  "materials": "Ferramentas de diagramação (Draw.io ou papel), código Python integrado.",
                                  "tips": "Estruture como checklist para reutilização em projetos futuros.",
                                  "learningObjective": "Elaborar plano holístico de ajuste e avaliação inicial de modelos.",
                                  "commonMistakes": "Pular CV e confiar só em train/test split; não documentar decisões."
                                }
                              ],
                              "practicalExample": "Usando dataset Iris (classificação): defina acurácia, treine KNN baseline (k=5), aplique 5-fold CV obtendo média 0.96 ± 0.02, compare com regressão linear em dataset Diabetes para MSE médio de 3000 via CV.",
                              "finalVerifications": [
                                "Calcula corretamente acurácia e MSE em dataset de teste.",
                                "Implementa 5-fold CV com scores médios e desvios reportados.",
                                "Identifica overfitting via gap entre train e CV scores.",
                                "Esboça fluxograma completo do processo de modelagem inicial.",
                                "Seleciona modelo baseline com justificativa baseada em métricas.",
                                "Documenta limitações das métricas usadas."
                              ],
                              "assessmentCriteria": [
                                "Precisão nas definições e cálculos de métricas (30%)",
                                "Correta implementação de treinamento e CV (30%)",
                                "Qualidade do plano integrado e fluxograma (20%)",
                                "Análise de resultados com interpretação (10%)",
                                "Uso adequado de ferramentas e reprodutibilidade (10%)"
                              ],
                              "crossCurricularConnections": [
                                "Estatística: Compreensão de variância e médias em CV scores.",
                                "Programação: Manipulação de dados com pandas e scikit-learn.",
                                "Matemática: Álgebra linear em fórmulas de MSE e otimização.",
                                "Ética em Dados: Discussão de viés em métricas como acurácia."
                              ],
                              "realWorldApplication": "Em projetos de data science, como prever churn de clientes em bancos: use baseline com acurácia via CV para validar modelo inicial antes de deploy em produção, garantindo performance robusta."
                            },
                            "estimatedTime": "1.5 horas",
                            "difficulty": "intermediate",
                            "status": "not_started",
                            "prerequisites": [
                              "10.1.5.4.3.3"
                            ]
                          }
                        ]
                      }
                    ]
                  },
                  {
                    "id": "10.1.5.5",
                    "name": "Ética no Uso e Manipulação de Dados",
                    "description": "Princípios éticos aplicados ao aprendizado de máquina e ciência de dados.",
                    "individualConcepts": [
                      {
                        "id": "10.1.5.5.1",
                        "name": "Privacidade e Proteção de Dados",
                        "description": "Princípios éticos para garantir a privacidade dos indivíduos durante a coleta, armazenamento e uso de dados em projetos de ciência de dados e aprendizado de máquina, incluindo anonimização e conformidade com regulamentações.",
                        "specificSkills": [
                          {
                            "id": "10.1.5.5.1.1",
                            "name": "Identificar riscos de privacidade em conjuntos de dados",
                            "description": "Analisar conjuntos de dados para identificar informações sensíveis pessoais (PII) e potenciais violações de privacidade, como reidentificação de dados anonimizados.",
                            "atomicExpansion": {
                              "steps": [
                                {
                                  "stepNumber": 1,
                                  "title": "Compreender Conceitos Fundamentais de PII e Privacidade",
                                  "subSteps": [
                                    "Estude definições de PII (Personally Identifiable Information) conforme regulamentações como GDPR e LGPD.",
                                    "Classifique tipos de PII: direta (nome, CPF) e indireta (endereço, data de nascimento).",
                                    "Revise princípios de privacidade diferencial e anonimização (k-anonimato, l-diversidade).",
                                    "Analise exemplos de violações históricas, como o caso Netflix Prize.",
                                    "Crie um glossário pessoal com 10 termos chave de privacidade."
                                  ],
                                  "verification": "Glossário completo criado e conceitos explicados em um resumo de 1 página.",
                                  "estimatedTime": "1 hora",
                                  "materials": [
                                    "Documentação GDPR/LGPD (PDFs oficiais)",
                                    "Artigos sobre violações de privacidade (ex: EFF.org)",
                                    "Notebook para anotações"
                                  ],
                                  "tips": "Use mnemônicos para lembrar tipos de PII, como 'Nomes, Endereços, Telefones, Identificadores'.",
                                  "learningObjective": "Dominar definições e classificações de PII para análise inicial.",
                                  "commonMistakes": [
                                    "Confundir PII direta com indireta",
                                    "Ignorar contextos culturais em regulamentações",
                                    "Subestimar dados quasi-identificadores"
                                  ]
                                },
                                {
                                  "stepNumber": 2,
                                  "title": "Identificar PII em Conjuntos de Dados Estruturados e Não Estruturados",
                                  "subSteps": [
                                    "Carregue um dataset de exemplo (ex: CSV de pacientes) usando Pandas.",
                                    "Inspecione colunas e valores únicos para detectar PII direta (regex para emails, CPFs).",
                                    "Examine dados não estruturados (textos) com ferramentas como spaCy para entidades nomeadas.",
                                    "Mapeie todos os campos potenciais de PII em uma tabela de inventário.",
                                    "Teste amostras aleatórias para padrões sensíveis (ex: CEP + idade)."
                                  ],
                                  "verification": "Tabela de inventário preenchida com pelo menos 5 tipos de PII identificados.",
                                  "estimatedTime": "2 horas",
                                  "materials": [
                                    "Python com Pandas e spaCy",
                                    "Dataset de exemplo: Kaggle 'Adult Census' ou 'Medical Dataset'",
                                    "Jupyter Notebook"
                                  ],
                                  "tips": "Use funções regex prontas como re.match(r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b', text) para emails.",
                                  "learningObjective": "Desenvolver habilidades para escanear datasets e catalogar PII.",
                                  "commonMistakes": [
                                    "Focar só em colunas óbvias",
                                    "Não tratar dados faltantes como risco",
                                    "Ignorar metadados do dataset"
                                  ]
                                },
                                {
                                  "stepNumber": 3,
                                  "title": "Avaliar Riscos de Reidentificação e Violações de Privacidade",
                                  "subSteps": [
                                    "Aplique testes de reidentificação: combine quasi-identificadores (ex: CEP, gênero, data nascimento).",
                                    "Calcule unicidade: verifique se combinações únicas identificam indivíduos (threshold <5%).",
                                    "Simule ataques de linkage com datasets públicos (ex: cruzar com listas eleitorais).",
                                    "Avalie falhas em anonimização: cheque k-anonimato usando groupby no Pandas.",
                                    "Classifique riscos: baixo/médio/alto baseado em probabilidade de reidentificação."
                                  ],
                                  "verification": "Relatório de riscos gerado com classificações para 3 cenários simulados.",
                                  "estimatedTime": "2.5 horas",
                                  "materials": [
                                    "Python Pandas/NumPy",
                                    "Datasets auxiliares: Kaggle 'US Census' e 'Voter Records'",
                                    "Ferramenta ARX para anonimização (opcional)"
                                  ],
                                  "tips": "Comece com queries simples: df.groupby(['zip', 'birth_year']).size() para detectar unicidade.",
                                  "learningObjective": "Identificar vulnerabilidades de reidentificação em dados anonimizados.",
                                  "commonMistakes": [
                                    "Assumir anonimização perfeita sem testes",
                                    "Não considerar ataques de inferência",
                                    "Subestimar combinações de poucos campos"
                                  ]
                                },
                                {
                                  "stepNumber": 4,
                                  "title": "Documentar Achados e Propor Medidas de Mitigação",
                                  "subSteps": [
                                    "Compile relatório: liste PII, riscos e evidências com screenshots/queries.",
                                    "Recomende anonimização: supressão, generalização, perturbação.",
                                    "Crie checklist de conformidade para futuras análises.",
                                    "Teste mitigações em uma cópia do dataset e reverifique riscos.",
                                    "Discuta implicações éticas em um parágrafo reflexivo."
                                  ],
                                  "verification": "Relatório final completo com checklist e testes de mitigações validados.",
                                  "estimatedTime": "1.5 horas",
                                  "materials": [
                                    "Template de relatório (Google Docs/Word)",
                                    "Dataset original modificado",
                                    "Ferramentas de visualização: Matplotlib"
                                  ],
                                  "tips": "Use templates padronizados para relatórios para agilizar o processo.",
                                  "learningObjective": "Transformar análise em ações práticas e documentadas.",
                                  "commonMistakes": [
                                    "Relatórios vagos sem evidências",
                                    "Sugestões de mitigação inviáveis",
                                    "Omitir considerações éticas/custo"
                                  ]
                                }
                              ],
                              "practicalExample": "Analise o dataset 'Adult Income' do UCI ML Repository: identifique PII como 'education' + 'occupation' + 'marital-status' que permitem reidentificação cruzando com LinkedIn; proponha generalizar 'occupation' em categorias amplas.",
                              "finalVerifications": [
                                "Todos os PII diretos e indiretos foram catalogados corretamente.",
                                "Testes de reidentificação mostram unicidade <5% pós-mitigação.",
                                "Relatório inclui evidências quantitativas (ex: queries Pandas).",
                                "Checklist de conformidade cobre 80% dos riscos identificados.",
                                "Reflexão ética discute impactos em indivíduos/grupos vulneráveis.",
                                "Mitigações foram testadas e validadas em dataset simulado."
                              ],
                              "assessmentCriteria": [
                                "Precisão na identificação de PII (90%+ recall em testes).",
                                "Profundidade na análise de reidentificação (múltiplos cenários).",
                                "Qualidade do relatório: claro, evidenciado e acionável.",
                                "Criatividade nas mitigações, alinhadas a melhores práticas.",
                                "Compreensão interdisciplinar (ética, legal, técnica).",
                                "Eficiência temporal: completado dentro do estimado."
                              ],
                              "crossCurricularConnections": [
                                "Ética em IA: discussões sobre consentimento e dano.",
                                "Direito Digital: conformidade com LGPD/GDPR/HIPAA.",
                                "Estatística: medidas de unicidade e linkage attacks.",
                                "Programação: manipulação de dados com Python/SQL.",
                                "Cibersegurança: riscos de exposição em pipelines de dados."
                              ],
                              "realWorldApplication": "Em empresas como bancos ou healthtechs, profissionais usam isso para auditar datasets antes de ML models, evitando multas milionárias por vazamentos (ex: Cambridge Analytica) e garantindo confiança em produtos de dados."
                            },
                            "estimatedTime": "2 horas",
                            "difficulty": "beginner",
                            "status": "not_started",
                            "prerequisites": []
                          },
                          {
                            "id": "10.1.5.5.1.2",
                            "name": "Aplicar técnicas de anonimização de dados",
                            "description": "Implementar métodos como supressão, generalização, máscara e k-anonimato para proteger a identidade dos indivíduos em datasets usados em treinamento de modelos de ML.",
                            "atomicExpansion": {
                              "steps": [
                                {
                                  "stepNumber": 1,
                                  "title": "Compreender Conceitos Fundamentais de Anonimização",
                                  "subSteps": [
                                    "Estude definições de supressão (remover dados), generalização (reduzir precisão), máscara (substituir por símbolos) e k-anonimato (garantir pelo menos k registros indistinguíveis).",
                                    "Identifique atributos quasi-identificadores (QI) como idade, CEP, gênero que podem reidentificar indivíduos.",
                                    "Analise riscos de reidentificação em datasets de ML, como linkage attacks.",
                                    "Revise exemplos reais de vazamentos de privacidade (e.g., Netflix Prize dataset).",
                                    "Crie um glossário pessoal com termos chave."
                                  ],
                                  "verification": "Resuma cada técnica em uma tabela comparativa e explique com palavras próprias para um colega.",
                                  "estimatedTime": "1 hora",
                                  "materials": [
                                    "Notebook para anotações",
                                    "Artigos introdutórios sobre privacidade diferencial (links: Towards Data Science, Kaggle tutorials)",
                                    "Vídeos Khan Academy ou YouTube sobre anonimização"
                                  ],
                                  "tips": "Use analogias: supressão é como apagar nomes, generalização é arredondar idades para faixas.",
                                  "learningObjective": "Dominar terminologia e identificar atributos sensíveis em datasets.",
                                  "commonMistakes": [
                                    "Confundir QI com identificadores diretos",
                                    "Subestimar combinações de atributos não sensíveis",
                                    "Ignorar contexto de ML onde features são usadas para predição"
                                  ]
                                },
                                {
                                  "stepNumber": 2,
                                  "title": "Preparar e Explorar o Dataset",
                                  "subSteps": [
                                    "Carregue um dataset exemplo (e.g., Adult UCI ou dataset de pacientes sintético) usando Pandas.",
                                    "Explore estrutura: df.info(), df.describe(), visualize distribuições com histograms.",
                                    "Identifique colunas sensíveis e QI: marque idade, profissão, CEP.",
                                    "Crie cópias seguras do dataset original para experimentos.",
                                    "Calcule estatísticas basais de utilidade (e.g., entropia de informação)."
                                  ],
                                  "verification": "Gere um relatório Jupyter com exploração e lista de atributos QI anotados.",
                                  "estimatedTime": "1.5 horas",
                                  "materials": [
                                    "Python 3.x",
                                    "Bibliotecas: pandas, numpy, matplotlib",
                                    "Dataset: Adult UCI de Kaggle",
                                    "Jupyter Notebook"
                                  ],
                                  "tips": "Sempre trabalhe em cópias: df_copy = df.copy() para evitar perda de dados originais.",
                                  "learningObjective": "Preparar dataset realista para anonimização preservando utilidade para ML.",
                                  "commonMistakes": [
                                    "Não fazer backup do original",
                                    "Ignorar missing values que afetam anonimização",
                                    "Explorar só numericamente, esquecer categóricos"
                                  ]
                                },
                                {
                                  "stepNumber": 3,
                                  "title": "Implementar Técnicas Básicas de Anonimização",
                                  "subSteps": [
                                    "Aplique supressão: df.drop(columns=['nome']) ou df['idade'].fillna('suprimido').",
                                    "Generalize: pd.cut(df['idade'], bins=[0,18,35,50,100], labels=['<18','18-35','36-50','>50']).",
                                    "Mascare: df['CEP'] = df['CEP'].astype(str).str[:-3] + '***' para últimos dígitos.",
                                    "Combine técnicas em pipeline: crie função anonymize_basic(df).",
                                    "Compare dataset antes/depois com df_original.equals(df_anonymized) (deve falhar)."
                                  ],
                                  "verification": "Execute pipeline em dataset e exporte CSV anonimizado comparando tamanhos e amostras.",
                                  "estimatedTime": "2 horas",
                                  "materials": [
                                    "Python com pandas",
                                    "Jupyter Notebook",
                                    "Dataset preparado do Step 2"
                                  ],
                                  "tips": "Teste em subsets pequenos primeiro: df.sample(100) para depuração rápida.",
                                  "learningObjective": "Codificar e aplicar supressão, generalização e máscara de forma programática.",
                                  "commonMistakes": [
                                    "Generalizar demais perdendo utilidade para ML",
                                    "Mascar inconsistente (e.g., strings vs ints)",
                                    "Não validar tipos de dados pós-transformação"
                                  ]
                                },
                                {
                                  "stepNumber": 4,
                                  "title": "Implementar e Aplicar K-Anonimato",
                                  "subSteps": [
                                    "Instale/inclua biblioteca como sdcMicro ou implemente manualmente com groupby.",
                                    "Defina QI set: ['idade_grupo', 'CEP_mascara', 'genero'].",
                                    "Agrupe por QI e verifique min(size) >= k (e.g., k=5): df.groupby(QI).size().min().",
                                    "Ajuste generalizações iterativamente até satisfazer k-anonimato.",
                                    "Treine um modelo ML simples (e.g., LogisticRegression) antes/depois para medir perda de utility."
                                  ],
                                  "verification": "Prove k-anonimato com print(df.groupby(QI).size().min() >= k) e accuracy ML > threshold.",
                                  "estimatedTime": "2.5 horas",
                                  "materials": [
                                    "Python: pandas, scikit-learn",
                                    "Biblioteca opcional: sdcMicro (R/Python wrapper)",
                                    "Jupyter Notebook"
                                  ],
                                  "tips": "Comece com k baixo (2-5) para testar, aumente gradualmente; use loops para automação.",
                                  "learningObjective": "Garantir anonimato forte via k-anonimato em contexto de ML.",
                                  "commonMistakes": [
                                    "Escolher QI inadequados levando a grupos muito grandes",
                                    "Ignorar utility drop excessivo",
                                    "Não tratar outliers que quebram grupos"
                                  ]
                                },
                                {
                                  "stepNumber": 5,
                                  "title": "Avaliar Efetividade da Anonimização",
                                  "subSteps": [
                                    "Meça risco: calcule % de registros únicos em QI (deve ser <1/k).",
                                    "Avalie utility: treine ML model (accuracy, F1-score) pré/pós-anonimização.",
                                    "Teste reidentificação simulada: tente matching com dataset auxiliar.",
                                    "Gere relatório: tabelas de métricas, gráficos de perda utility vs risco.",
                                    "Itere melhorias baseadas em thresholds (e.g., utility drop <20%)."
                                  ],
                                  "verification": "Produza dashboard com métricas: risco <5%, utility >80% original.",
                                  "estimatedTime": "1.5 horas",
                                  "materials": [
                                    "scikit-learn para models",
                                    "matplotlib/seaborn para plots",
                                    "Datasets anonimizados anteriores"
                                  ],
                                  "tips": "Use cross-validation para utility robusta; defina métricas claras upfront.",
                                  "learningObjective": "Balancear privacidade e utilidade quantitativamente.",
                                  "commonMistakes": [
                                    "Focar só privacidade ignorando ML performance",
                                    "Métricas subjetivas sem baselines",
                                    "Não considerar ataques avançados como homogeneity"
                                  ]
                                }
                              ],
                              "practicalExample": "Em um dataset de pacientes hospitalares com idade, CEP, gênero e diagnóstico, aplique: generalizar idade para faixas, mascarar CEP últimos 3 dígitos, suprimir nomes; garanta k=10-anonimato nos QI restantes. Treine um classificador de risco cardíaco antes/depois, visando accuracy >85%.",
                              "finalVerifications": [
                                "Implemente pipeline completo em novo dataset sem erros.",
                                "Demonstre k-anonimato com groupby.size() >=k em todos QI.",
                                "Compare utility ML: perda <25% em accuracy/F1.",
                                "Explique trade-offs em relatório escrito.",
                                "Responda quiz: identifique falhas em dataset anonimizado simulado.",
                                "Aplique em dataset real-world baixado (e.g., Kaggle health data)."
                              ],
                              "assessmentCriteria": [
                                "Precisão técnica: todas métodos implementados corretamente (80%).",
                                "Balanceamento privacidade-utility: métricas quantificadas e otimizadas (15%).",
                                "Código limpo: comentado, modular, reproduzível (5%).",
                                "Relatório claro: visualizações, conclusões acionáveis.",
                                "Criatividade: adaptações para cenários específicos.",
                                "Conhecimento teórico: explica limitações (e.g., não protege contra background knowledge)."
                              ],
                              "crossCurricularConnections": [
                                "Ética e Direito: LGPD/GDPR conformidade em datasets.",
                                "Programação: Python avançado com data manipulation.",
                                "Estatística: Medidas de risco e utility (entropia, chi-quadrado).",
                                "Inteligência Artificial: Impacto em feature engineering para ML.",
                                "Ciências Sociais: Implicações de privacidade em dados sensíveis."
                              ],
                              "realWorldApplication": "Em empresas como bancos (anonimizar transações para detecção de fraude ML), saúde (HIPs-compliant datasets para pesquisa), ou governos (censo dados para políticas públicas sem expor identidades), reduzindo riscos de multas por vazamentos e permitindo inovação em IA ética."
                            },
                            "estimatedTime": "3 horas",
                            "difficulty": "intermediate",
                            "status": "not_started",
                            "prerequisites": []
                          },
                          {
                            "id": "10.1.5.5.1.3",
                            "name": "Compreender regulamentações de proteção de dados",
                            "description": "Estudar e aplicar conceitos da LGPD (Lei Geral de Proteção de Dados) e GDPR no contexto de ciência de dados, incluindo consentimento e direitos dos titulares de dados.",
                            "atomicExpansion": {
                              "steps": [
                                {
                                  "stepNumber": 1,
                                  "title": "Estudar os fundamentos da LGPD (Lei Geral de Proteção de Dados)",
                                  "subSteps": [
                                    "Acessar o texto integral da Lei nº 13.709/2018 no site oficial do Planalto",
                                    "Identificar os princípios fundamentais (finalidade, adequação, necessidade, etc.)",
                                    "Analisar o âmbito de aplicação e as obrigações do controlador e operador",
                                    "Estudar as penalidades por descumprimento",
                                    "Mapear exemplos de dados pessoais sensíveis no contexto de ciência de dados"
                                  ],
                                  "verification": "Resumir em um documento os 10 princípios da LGPD e listar 3 obrigações principais",
                                  "estimatedTime": "3 horas",
                                  "materials": [
                                    "Texto da LGPD (planalto.gov.br)",
                                    "Guia ANPD para Iniciantes (anpd.gov.br)"
                                  ],
                                  "tips": "Use destaques coloridos para princípios chave e anote exemplos reais de violações",
                                  "learningObjective": "Compreender a estrutura legal e princípios da LGPD aplicados a dados",
                                  "commonMistakes": "Confundir dados pessoais com dados anonimizados; ignorar diferenças entre controlador e operador"
                                },
                                {
                                  "stepNumber": 2,
                                  "title": "Explorar os fundamentos da GDPR (Regulamento Geral de Proteção de Dados)",
                                  "subSteps": [
                                    "Ler o Regulamento (UE) 2016/679 no site oficial da União Europeia",
                                    "Identificar os princípios semelhantes à LGPD (transparência, minimização de dados, etc.)",
                                    "Analisar direitos dos titulares (acesso, retificação, esquecimento)",
                                    "Estudar requisitos para transferências internacionais de dados",
                                    "Comparar territorialidade da GDPR com a LGPD"
                                  ],
                                  "verification": "Criar uma tabela comparativa inicial de princípios GDPR vs LGPD",
                                  "estimatedTime": "3 horas",
                                  "materials": [
                                    "Texto GDPR (eur-lex.europa.eu)",
                                    "Guia EDPB para Leigos"
                                  ],
                                  "tips": "Foquem em artigos chave como 5 (princípios), 15-22 (direitos) e 44-50 (transferências)",
                                  "learningObjective": "Dominar os pilares da GDPR e suas semelhanças/diferenças com legislações nacionais",
                                  "commonMistakes": "Subestimar o escopo extraterritorial da GDPR para empresas não-europeias"
                                },
                                {
                                  "stepNumber": 3,
                                  "title": "Analisar conceitos chave: Consentimento e Direitos dos Titulares",
                                  "subSteps": [
                                    "Definir consentimento válido segundo LGPD (art. 7º) e GDPR (art. 4º e 7º)",
                                    "Listar e descrever os 8 direitos ARCO+ (acesso, retificação, cancelamento, oposição, etc.)",
                                    "Estudar requisitos para obtenção de consentimento (livre, informado, inequívoco)",
                                    "Explorar exceções ao consentimento (legítimo interesse, obrigação legal)",
                                    "Simular cenários de violação de direitos em datasets de ML"
                                  ],
                                  "verification": "Elaborar um fluxograma de processo de consentimento para coleta de dados",
                                  "estimatedTime": "2 horas",
                                  "materials": [
                                    "Materiais ANPD sobre Consentimento",
                                    "GDPR Info Consent Guide"
                                  ],
                                  "tips": "Sempre valide consentimento com dupla opt-in em exemplos práticos",
                                  "learningObjective": "Aplicar regras de consentimento e direitos em contextos de dados",
                                  "commonMistakes": "Aceitar consentimento implícito ou pré-marcado como válido"
                                },
                                {
                                  "stepNumber": 4,
                                  "title": "Aplicar regulamentações no contexto de Ciência de Dados",
                                  "subSteps": [
                                    "Identificar riscos de privacidade em pipelines de ML (anonimização, pseudonimização)",
                                    "Desenvolver DPO (Encarregado de Proteção) fictício e plano de conformidade",
                                    "Simular DPIA (Avaliação de Impacto à Proteção de Dados) para um projeto de dados",
                                    "Estudar técnicas como differential privacy compatíveis com LGPD/GDPR",
                                    "Criar checklist de compliance para uso de dados em treinamento de modelos"
                                  ],
                                  "verification": "Produzir um relatório de 1 página com checklist de conformidade para um caso hipotético",
                                  "estimatedTime": "4 horas",
                                  "materials": [
                                    "Guia ANPD para Ciência de Dados",
                                    "GDPR em AI - ENISA Report"
                                  ],
                                  "tips": "Integre privacy by design desde o início do projeto de dados",
                                  "learningObjective": "Integrar LGPD/GDPR em fluxos de trabalho de ciência de dados",
                                  "commonMistakes": "Ignorar re-identificação em dados supostamente anonimizados"
                                }
                              ],
                              "practicalExample": "Em um projeto de ML para prever churn de clientes, obtenha consentimento explícito para usar dados pessoais, realize DPIA identificando riscos de vazamento, aplique pseudonimização nos features e permita direito de acesso/oposição via API, garantindo conformidade LGPD/GDPR.",
                              "finalVerifications": [
                                "Explicar verbalmente 5 princípios comuns LGPD/GDPR",
                                "Listar e descrever 6 direitos dos titulares de dados",
                                "Demonstrar um fluxograma de consentimento válido",
                                "Aplicar checklist de compliance a um dataset real",
                                "Comparar sanções da LGPD vs GDPR em um caso hipotético",
                                "Identificar 3 técnicas de privacidade em ML compatíveis"
                              ],
                              "assessmentCriteria": [
                                "Precisão na identificação de princípios e artigos legais (90%+ correto)",
                                "Profundidade na análise de consentimento e direitos (exemplos contextualizados)",
                                "Criatividade e relevância em aplicações de ciência de dados",
                                "Completude do checklist e fluxogramas (todos elementos presentes)",
                                "Capacidade de comparação crítica LGPD vs GDPR",
                                "Uso adequado de terminologia técnica e legal"
                              ],
                              "crossCurricularConnections": [
                                "Direito Digital: Análise de jurisprudência ANPD",
                                "Ética em IA: Princípios éticos de privacidade",
                                "Segurança da Informação: Criptografia e anonimização",
                                "Governança de Dados: Políticas de DPO e compliance"
                              ],
                              "realWorldApplication": "Em empresas como bancos ou e-commerces, garantir compliance LGPD/GDPR evita multas milionárias (ex: multas à Meta pelo GDPR), permite inovação ética em ML e constrói confiança com usuários via transparência em modelos preditivos."
                            },
                            "estimatedTime": "2 horas",
                            "difficulty": "beginner",
                            "status": "not_started",
                            "prerequisites": []
                          }
                        ]
                      },
                      {
                        "id": "10.1.5.5.2",
                        "name": "Viés e Fairness em Modelos",
                        "description": "Reconhecimento e mitigação de vieses nos dados e algoritmos de aprendizado de máquina para evitar discriminação e promover equidade nas decisões automatizadas.",
                        "specificSkills": [
                          {
                            "id": "10.1.5.5.2.1",
                            "name": "Detectar viés em conjuntos de dados e modelos",
                            "description": "Utilizar métricas como disparate impact e statistical parity para identificar vieses demográficos (gênero, raça, idade) em dados de treinamento e previsões de modelos.",
                            "atomicExpansion": {
                              "steps": [
                                {
                                  "stepNumber": 1,
                                  "title": "Compreender as métricas de viés: Disparate Impact e Statistical Parity",
                                  "subSteps": [
                                    "Estude a definição de Disparate Impact: razão entre taxas positivas para grupos protegidos e não protegidos.",
                                    "Aprenda Statistical Parity: diferença absoluta nas probabilidades de predição positiva entre grupos.",
                                    "Identifique grupos protegidos: gênero, raça, idade.",
                                    "Revise fórmulas matemáticas: DI = P(positivo|protegido) / P(positivo|não protegido); SP = |P(pred=1|grupoA) - P(pred=1|grupoB)|.",
                                    "Examine exemplos numéricos simples com tabelas de contingência."
                                  ],
                                  "verification": "Explique verbalmente ou por escrito as fórmulas e forneça um exemplo calculado manualmente.",
                                  "estimatedTime": "45 minutos",
                                  "materials": [
                                    "Documentação AIF360 ou Fairlearn",
                                    "Notebook Jupyter",
                                    "Artigos introdutórios sobre fairness em ML"
                                  ],
                                  "tips": "Use diagramas de Venn para visualizar diferenças entre grupos.",
                                  "learningObjective": "Dominar as definições e fórmulas das métricas principais de viés demográfico.",
                                  "commonMistakes": "Confundir Disparate Impact com Statistical Parity; ignorar normalização por tamanho de grupo."
                                },
                                {
                                  "stepNumber": 2,
                                  "title": "Preparar o conjunto de dados de treinamento com atributos demográficos",
                                  "subSteps": [
                                    "Carregue o dataset usando pandas (ex: Adult UCI ou dataset sintético).",
                                    "Identifique e extraia colunas demográficas: gênero, raça, idade (binarizar idade se necessário).",
                                    "Verifique valores ausentes e balanceamento inicial dos grupos.",
                                    "Crie subconjuntos por grupo protegido usando groupby.",
                                    "Salve uma cópia limpa do dataset preparado."
                                  ],
                                  "verification": "Execute df.describe() por grupo e confirme ausência de NaNs e distribuição básica.",
                                  "estimatedTime": "30 minutos",
                                  "materials": [
                                    "Python com pandas e numpy",
                                    "Dataset exemplo: Adult UCI do UCI ML Repository"
                                  ],
                                  "tips": "Use value_counts() para checar distribuição demográfica rapidamente.",
                                  "learningObjective": "Preparar dados rotulados demograficamente para análise de viés.",
                                  "commonMistakes": "Não tratar categorias desbalanceadas; assumir dados limpos sem verificação."
                                },
                                {
                                  "stepNumber": 3,
                                  "title": "Calcular métricas de viés nos dados de treinamento",
                                  "subSteps": [
                                    "Implemente função para calcular Disparate Impact por grupo.",
                                    "Calcule Statistical Parity para pares de grupos (ex: masculino vs feminino).",
                                    "Use bibliotecas como AIF360 para métricas automatizadas.",
                                    "Registre resultados em uma tabela: métrica, grupo, valor.",
                                    "Defina thresholds: DI < 0.8 ou > 1.25 indica viés; SP > 0.1 indica viés."
                                  ],
                                  "verification": "Gere um relatório tabular com valores calculados e compare com thresholds.",
                                  "estimatedTime": "40 minutos",
                                  "materials": [
                                    "Biblioteca AIF360 ou fairlearn",
                                    "Jupyter Notebook"
                                  ],
                                  "tips": "Teste com dados sintéticos primeiro para validar funções.",
                                  "learningObjective": "Aplicar métricas diretamente nos dados de entrada para detectar viés inerente.",
                                  "commonMistakes": "Calcular sem estratificar corretamente por grupo; ignorar múltiplos atributos simultaneamente."
                                },
                                {
                                  "stepNumber": 4,
                                  "title": "Calcular métricas de viés nas previsões do modelo",
                                  "subSteps": [
                                    "Treine um modelo baseline (ex: Logistic Regression) no dataset preparado.",
                                    "Gere previsões probabilísticas ou binárias no conjunto de teste.",
                                    "Aplique as mesmas funções de métricas nas previsões, condicionadas aos grupos demográficos.",
                                    "Compare viés pré e pós-modelo.",
                                    "Visualize com gráficos de barras ou heatmaps por grupo e métrica."
                                  ],
                                  "verification": "Crie um gráfico comparativo e confirme se viés aumentou ou diminuiu.",
                                  "estimatedTime": "50 minutos",
                                  "materials": [
                                    "Scikit-learn para modelo",
                                    "Matplotlib/Seaborn para visualização",
                                    "AIF360"
                                  ],
                                  "tips": "Use cross-validation para previsões robustas.",
                                  "learningObjective": "Detectar viés amplificado pelo modelo nas saídas preditivas.",
                                  "commonMistakes": "Usar probabilidades em vez de labels binários onde requerido; não separar train/test adequadamente."
                                },
                                {
                                  "stepNumber": 5,
                                  "title": "Interpretar resultados e documentar achados de viés",
                                  "subSteps": [
                                    "Analise se DI/SP excedem thresholds e identifique grupos mais afetados.",
                                    "Discuta causas potenciais: viés histórico nos dados ou no modelo.",
                                    "Proponha próximos passos: reamostragem, reweighting.",
                                    "Gere relatório final com tabelas, gráficos e recomendações.",
                                    "Teste sensibilidade variando thresholds."
                                  ],
                                  "verification": "Escreva um parágrafo resumindo vieses detectados e implicações éticas.",
                                  "estimatedTime": "35 minutos",
                                  "materials": [
                                    "Jupyter para relatório",
                                    "Templates de relatório em Markdown"
                                  ],
                                  "tips": "Priorize grupos minoritários em interpretações.",
                                  "learningObjective": "Interpretar quantitativamente vieses e comunicar riscos éticos.",
                                  "commonMistakes": "Interpretar valores próximos a 1 como 'sem viés' sem contexto; omitir discussão ética."
                                }
                              ],
                              "practicalExample": "Em um dataset de aprovação de empréstimos bancários (como German Credit), calcule Disparate Impact por gênero: se mulheres têm taxa de aprovação 20% menor, DI=0.8 indica viés; visualize em gráfico de barras as taxas por grupo.",
                              "finalVerifications": [
                                "Cálculo correto de Disparate Impact e Statistical Parity em dados e previsões.",
                                "Identificação precisa de grupos com viés (ex: DI <0.8 para mulheres).",
                                "Gráficos gerados mostrando diferenças demográficas.",
                                "Relatório com thresholds aplicados e interpretação ética.",
                                "Código reproduzível sem erros em execução.",
                                "Comparação pré/pós-modelo documentada."
                              ],
                              "assessmentCriteria": [
                                "Precisão matemática nas métricas (erro <1%).",
                                "Cobertura completa de atributos demográficos (gênero, raça, idade).",
                                "Uso correto de bibliotecas e visualizações claras.",
                                "Interpretação contextualizada com implicações reais.",
                                "Código limpo, comentado e modular.",
                                "Identificação de pelo menos um viés simulado ou real."
                              ],
                              "crossCurricularConnections": [
                                "Estatística: Testes de hipótese e distribuições condicionais.",
                                "Ética e Filosofia: Discussões sobre justiça e discriminação algorítmica.",
                                "Programação: Manipulação de dados com Python/pandas.",
                                "Sociologia: Impacto demográfico em decisões automatizadas.",
                                "Matemática: Probabilidade condicional e razões."
                              ],
                              "realWorldApplication": "Em sistemas de RH para triagem de currículos, detectando viés racial que favorece nomes 'brancos'; ou em modelos de crédito, evitando discriminação por gênero, garantindo compliance com leis anti-discriminação como LGPD no Brasil."
                            },
                            "estimatedTime": "3 horas",
                            "difficulty": "intermediate",
                            "status": "not_started",
                            "prerequisites": []
                          },
                          {
                            "id": "10.1.5.5.2.2",
                            "name": "Aplicar técnicas de mitigação de viés",
                            "description": "Implementar reamostragem, reponderação de classes e algoritmos fair-ML como fairlearn para equilibrar datasets e reduzir discriminação em modelos supervisionados.",
                            "atomicExpansion": {
                              "steps": [
                                {
                                  "stepNumber": 1,
                                  "title": "Identificar e Quantificar Viés no Dataset",
                                  "subSteps": [
                                    "Carregue o dataset usando Pandas e explore suas características demográficas.",
                                    "Defina grupos protegidos (ex: gênero, raça) com base no contexto do problema.",
                                    "Calcule métricas de viés como demographic parity, equalized odds e disparate impact usando fairlearn.metrics.",
                                    "Visualize distribuições de classes e predições por grupo protegido com Seaborn ou Matplotlib.",
                                    "Gere um relatório resumindo níveis de viés detectados."
                                  ],
                                  "verification": "Relatório de métricas de viés gerado e salvo como PDF ou Jupyter notebook, mostrando valores > 0.8 para disparate impact como indicativo de viés.",
                                  "estimatedTime": "1-1.5 horas",
                                  "materials": [
                                    "Python 3.8+",
                                    "Pandas",
                                    "Fairlearn",
                                    "Scikit-learn",
                                    "Matplotlib/Seaborn",
                                    "Jupyter Notebook"
                                  ],
                                  "tips": "Comece com métricas simples como proporção de amostras por grupo para uma visão rápida.",
                                  "learningObjective": "Quantificar viés em datasets para priorizar técnicas de mitigação.",
                                  "commonMistakes": [
                                    "Não definir grupos protegidos relevantes ao contexto",
                                    "Ignorar viés interseccional entre múltiplos atributos",
                                    "Usar métricas sem normalização"
                                  ]
                                },
                                {
                                  "stepNumber": 2,
                                  "title": "Implementar Técnicas de Reamostragem",
                                  "subSteps": [
                                    "Separe o dataset em train/test e identifique classes desbalanceadas por grupo protegido.",
                                    "Aplique undersampling na classe majoritária usando RandomUnderSampler do imbalanced-learn.",
                                    "Implemente oversampling na classe minoritária com SMOTE, estratificado por grupo protegido.",
                                    "Combine técnicas híbridas (ex: SMOTE + Tomek links) para otimizar equilíbrio.",
                                    "Valide o dataset reamostrado calculando novas distribuições."
                                  ],
                                  "verification": "Dataset reamostrado salva com distribuições equilibradas (diferença < 5% entre grupos).",
                                  "estimatedTime": "1.5-2 horas",
                                  "materials": [
                                    "Imbalanced-learn",
                                    "Scikit-learn",
                                    "Fairlearn",
                                    "Pandas"
                                  ],
                                  "tips": "Use random_state para reprodutibilidade e evite vazamento de dados do test set.",
                                  "learningObjective": "Equilibrar datasets desbalanceados preservando representatividade de grupos protegidos.",
                                  "commonMistakes": [
                                    "Aplicar reamostragem no dataset completo incluindo test set",
                                    "Oversampling excessivo levando a overfitting",
                                    "Ignorar ruído introduzido por SMOTE sintético"
                                  ]
                                },
                                {
                                  "stepNumber": 3,
                                  "title": "Aplicar Reponderação de Classes e Algoritmos Fair-ML",
                                  "subSteps": [
                                    "Calcule pesos de amostras baseados em proporções de grupos protegidos usando compute_weights do fairlearn.",
                                    "Implemente reponderação no treinamento do modelo com sample_weight no fit() do Scikit-learn.",
                                    "Instale e use fairlearn para mitigators como GridSearch ou ThresholdOptimizer em um classificador base (ex: LogisticRegression).",
                                    "Treine múltiplos modelos com diferentes configurações de fairness constraints.",
                                    "Compare predições pós-mitigação com métricas de fairlearn."
                                  ],
                                  "verification": "Modelos treinados com pesos aplicados e relatório de métricas de fairness gerado.",
                                  "estimatedTime": "2-2.5 horas",
                                  "materials": [
                                    "Fairlearn",
                                    "Scikit-learn",
                                    "Pandas",
                                    "NumPy"
                                  ],
                                  "tips": "Comece com mitigators simples como reponderação antes de otimização avançada.",
                                  "learningObjective": "Ajustar pesos e usar fair-ML para reduzir discriminação algorítmica.",
                                  "commonMistakes": [
                                    "Pesos incorretos invertendo o viés",
                                    "Não calibrar thresholds para preservar accuracy",
                                    "Ignorar trade-offs entre fairness e performance"
                                  ]
                                },
                                {
                                  "stepNumber": 4,
                                  "title": "Avaliar e Comparar Modelos Mitigados",
                                  "subSteps": [
                                    "Treine um baseline sem mitigação e compare com modelos mitigados usando cross-validation.",
                                    "Calcule métricas compostas: accuracy, F1-score, e fairness (demographic parity difference < 0.1).",
                                    "Gere gráficos de ROC curvas e confusion matrices estratificadas por grupo.",
                                    "Realize teste de sensibilidade variando hiperparâmetros de mitigação.",
                                    "Documente trade-offs e selecione o melhor modelo."
                                  ],
                                  "verification": "Tabela comparativa de métricas antes/depois mitigação, com melhoria em fairness > 20%.",
                                  "estimatedTime": "1-1.5 horas",
                                  "materials": [
                                    "Scikit-learn",
                                    "Fairlearn",
                                    "Matplotlib",
                                    "Seaborn"
                                  ],
                                  "tips": "Use fairlearn.datasets para datasets de benchmark com viés conhecido.",
                                  "learningObjective": "Avaliar efetividade de mitigação equilibrando performance e fairness.",
                                  "commonMistakes": [
                                    "Focar só em accuracy ignorando fairness",
                                    "Não usar CV estratificado por grupos",
                                    "Concluir mitigação sem testes estatísticos"
                                  ]
                                }
                              ],
                              "practicalExample": "No dataset Adult UCI (previsão de renda >50k), há viés de gênero: homens são aprovados 2x mais. Aplique reamostragem SMOTE estratificada por gênero na classe minoritária, reponderação com fairlearn para equilibrar pesos, e ThresholdOptimizer em um RandomForestClassifier. Compare baseline (disparity=0.25) vs mitigado (disparity=0.08), mantendo F1-score >0.75.",
                              "finalVerifications": [
                                "Métricas de viés reduzidas em pelo menos 20% (ex: demographic parity difference <0.1).",
                                "Distribuições de classes equilibradas por grupo protegido (desvio <5%).",
                                "Performance do modelo mantida (F1-score varia <10%).",
                                "Relatório Jupyter com visualizações e código reproduzível.",
                                "Testes em hold-out set confirmam generalização.",
                                "Documentação de trade-offs fairness vs accuracy."
                              ],
                              "assessmentCriteria": [
                                "Redução efetiva em múltiplas métricas de fairness (demographic parity, equalized odds).",
                                "Manutenção de utility do modelo (accuracy, precision, recall balanceados).",
                                "Código limpo, modular e comentado com random_state para reprodutibilidade.",
                                "Análise de trade-offs quantitativa e qualitativa.",
                                "Uso correto de bibliotecas fair-ML sem erros de implementação.",
                                "Sensibilidade a diferentes datasets/grupos protegidos demonstrada."
                              ],
                              "crossCurricularConnections": [
                                "Ética e Filosofia: Discussão de justiça distributiva em algoritmos.",
                                "Estatística: Análise de desbalanceamento e testes de hipóteses para viés.",
                                "Programação: Manipulação avançada de dados com Python e bibliotecas ML.",
                                "Direito e Sociedade: Regulamentações como GDPR e AI Act sobre discriminação.",
                                "Matemática Computacional: Otimização com constraints de fairness."
                              ],
                              "realWorldApplication": "Em sistemas de concessão de empréstimos bancários, mitigar viés racial/gênero para cumprir regulamentações anti-discriminação, reduzindo recusas injustas em 15-30% enquanto mantém precisão de risco de crédito."
                            },
                            "estimatedTime": "4 horas",
                            "difficulty": "intermediate",
                            "status": "not_started",
                            "prerequisites": []
                          },
                          {
                            "id": "10.1.5.5.2.3",
                            "name": "Avaliar fairness em cenários reais",
                            "description": "Analisar estudos de caso, como recrutamento automatizado, para quantificar e discutir impactos éticos de vieses em aplicações de ML.",
                            "atomicExpansion": {
                              "steps": [
                                {
                                  "stepNumber": 1,
                                  "title": "Selecionar e Compreender o Estudo de Caso",
                                  "subSteps": [
                                    "Escolha um estudo de caso real, como o sistema de recrutamento automatizado da Amazon (2018), que discriminava candidatas mulheres.",
                                    "Leia relatórios ou artigos sobre o caso, identificando o contexto do modelo de ML usado (ex: regressão logística para scoring de currículos).",
                                    "Mapeie os grupos protegidos envolvidos (ex: gênero, raça) e os dados de treinamento originais.",
                                    "Resuma o problema em 3-5 pontos chave: origem do viés, sintomas observados e consequências iniciais.",
                                    "Documente fontes confiáveis usadas (ex: artigos da Reuters, papers acadêmicos)."
                                  ],
                                  "verification": "Criar um resumo de 1 página do caso com mapeamento de grupos e fontes citadas.",
                                  "estimatedTime": "45 minutos",
                                  "materials": "Artigos online (Reuters sobre Amazon), acesso à internet, bloco de notas ou Google Docs.",
                                  "tips": "Priorize casos recentes e bem documentados para relevância; use timelines para visualizar eventos.",
                                  "learningObjective": "Compreender o contexto real de um viés em ML para análise posterior.",
                                  "commonMistakes": "Ignorar fontes primárias ou confundir correlação com causalidade no viés."
                                },
                                {
                                  "stepNumber": 2,
                                  "title": "Identificar e Definir Métricas de Fairness",
                                  "subSteps": [
                                    "Liste métricas comuns: Demographic Parity, Equalized Odds, Equal Opportunity.",
                                    "Explique cada métrica em termos simples e sua fórmula matemática (ex: P(Ŷ=1|A=0) ≈ P(Ŷ=1|A=1) para Demographic Parity).",
                                    "Selecione 2-3 métricas adequadas ao caso (ex: Equalized Odds para recrutamento, onde precisão importa).",
                                    "Discuta trade-offs entre métricas e accuracy do modelo.",
                                    "Crie uma tabela comparativa das métricas com exemplos numéricos hipotéticos."
                                  ],
                                  "verification": "Tabela completa com definições, fórmulas e seleção justificada para o caso.",
                                  "estimatedTime": "60 minutos",
                                  "materials": "Notebook Jupyter ou papel, referências como 'Fairness and Machine Learning' de Barocas et al. (disponível online).",
                                  "tips": "Use diagramas de Venn para visualizar sobreposições entre métricas; teste com dados fictícios pequenos.",
                                  "learningObjective": "Dominar métricas padrão de fairness para quantificação objetiva.",
                                  "commonMistakes": "Confundir group fairness com individual fairness ou ignorar dependências condicionais."
                                },
                                {
                                  "stepNumber": 3,
                                  "title": "Quantificar Vieses no Estudo de Caso",
                                  "subSteps": [
                                    "Colete ou simule dados do caso (ex: dataset público como Adult UCI ou simulação de currículos).",
                                    "Implemente as métricas selecionadas em código (Python com scikit-learn-fairlearn).",
                                    "Calcule valores numéricos para cada grupo protegido e compare com thresholds ideais (ex: diferença < 0.1).",
                                    "Gere gráficos (ex: bar charts de predições por grupo) para visualização.",
                                    "Interprete resultados: magnitude do viés e se viola critérios de fairness."
                                  ],
                                  "verification": "Relatório com cálculos, código executável e gráficos mostrando quantificação.",
                                  "estimatedTime": "90 minutos",
                                  "materials": "Python (fairlearn, pandas, matplotlib), datasets públicos (Kaggle: COMPAS ou Adult).",
                                  "tips": "Comece com dados simulados se reais não estiverem disponíveis; valide código com testes unitários.",
                                  "learningObjective": "Aplicar métricas para medir viés de forma quantitativa e reprodutível.",
                                  "commonMistakes": "Usar dados não representativos ou erros de implementação em probabilidades condicionais."
                                },
                                {
                                  "stepNumber": 4,
                                  "title": "Discutir Impactos Éticos e Propor Mitigações",
                                  "subSteps": [
                                    "Analise impactos: discriminação sistêmica, perda de confiança pública, violações legais (ex: EEOC nos EUA).",
                                    "Discuta dilemas éticos: utilitarismo vs. justiça (ex: modelo mais preciso mas enviesado).",
                                    "Proponha 3-5 mitigações: reamostragem, regularização fairness-aware, auditorias humanas.",
                                    "Avalie viabilidade das mitigações no contexto do caso (custo, efetividade).",
                                    "Conclua com recomendações para deployment ético de ML."
                                  ],
                                  "verification": "Ensaio de 500 palavras com análise ética, mitigações e referências.",
                                  "estimatedTime": "75 minutos",
                                  "materials": "Framework ético como EU AI Act guidelines, papel para brainstorming.",
                                  "tips": "Use framework RADAR (Re-identification, Attribution, Demographic, Algorithmic) para estruturar discussão.",
                                  "learningObjective": "Integrar análise quantitativa com raciocínio ético para avaliação holística.",
                                  "commonMistakes": "Focar só em tech sem considerar impactos sociais ou subestimar custos de mitigações."
                                }
                              ],
                              "practicalExample": "No caso Amazon (2018), o modelo treinou em currículos históricos dominados por homens, resultando em scores mais baixos para termos como 'mulheres'. Usando Equalized Odds, calculou-se uma diferença de 0.25 na taxa de falsos positivos entre gêneros, levando à exclusão do sistema.",
                              "finalVerifications": [
                                "Pode quantificar viés com pelo menos duas métricas e interpretar resultados corretamente?",
                                "Identifica impactos éticos específicos ao caso, com exemplos reais?",
                                "Propõe mitigações viáveis com justificativa baseada em evidências?",
                                "Explica trade-offs entre fairness e performance do modelo?",
                                "Documenta todo processo de forma reprodutível (código + relatório)?",
                                "Conecta análise a regulamentações reais como GDPR ou AI Act?"
                              ],
                              "assessmentCriteria": [
                                "Precisão na seleção e cálculo de métricas de fairness (80%+ acurácia nos valores).",
                                "Profundidade na análise ética, com referências a princípios (ex: Rawlsian fairness).",
                                "Criatividade e viabilidade nas mitigações propostas.",
                                "Qualidade visual e clareza nos gráficos e tabelas.",
                                "Completude da documentação (fontes, código, resumo).",
                                "Capacidade de discutir trade-offs quantitativamente."
                              ],
                              "crossCurricularConnections": [
                                "Ética Filosófica: Aplicação de teorias como utilitarismo vs. deontologia em decisões algorítmicas.",
                                "Direito e Políticas Públicas: Conformidade com leis anti-discriminação (ex: Title VII nos EUA).",
                                "Sociologia: Entender vieses sociais refletidos em dados e impactos em desigualdades.",
                                "Estatística: Uso de testes de hipótese para validar significância de vieses.",
                                "Programação: Implementação prática de bibliotecas de fairness em ML."
                              ],
                              "realWorldApplication": "Em recrutamento (ex: LinkedIn, Google), empréstimos bancários (ex: Apple Card controversies) e justiça criminal (ex: COMPAS), avaliar fairness previne ações judiciais, melhora diversidade e constrói confiança pública em sistemas de IA."
                            },
                            "estimatedTime": "2 horas",
                            "difficulty": "beginner",
                            "status": "not_started",
                            "prerequisites": []
                          }
                        ]
                      },
                      {
                        "id": "10.1.5.5.3",
                        "name": "Transparência e Accountability",
                        "description": "Garantir que processos de ciência de dados e modelos de ML sejam auditáveis, explicáveis e responsáveis, promovendo confiança e responsabilidade ética.",
                        "specificSkills": [
                          {
                            "id": "10.1.5.5.3.1",
                            "name": "Documentar processos éticos em projetos de dados",
                            "description": "Criar relatórios e logs detalhados das etapas de coleta, limpeza e modelagem, destacando decisões éticas e potenciais impactos sociais.",
                            "atomicExpansion": {
                              "steps": [
                                {
                                  "stepNumber": 1,
                                  "title": "Planejar a Estrutura da Documentação Ética",
                                  "subSteps": [
                                    "Identifique as etapas principais do projeto de dados: coleta, limpeza e modelagem.",
                                    "Crie um template de relatório com seções para logs técnicos, decisões éticas e impactos sociais.",
                                    "Defina métricas para rastrear transparência, como timestamps e responsáveis por decisões.",
                                    "Liste potenciais riscos éticos iniciais, como viés em dados ou privacidade.",
                                    "Estabeleça um formato padronizado (ex: Markdown, Jupyter Notebook) para logs acessíveis."
                                  ],
                                  "verification": "Template preenchido com outline completo e aprovado por revisão inicial.",
                                  "estimatedTime": "1 hora",
                                  "materials": [
                                    "Ferramentas de edição: Google Docs, Jupyter Notebook, Markdown editor",
                                    "Exemplos de relatórios éticos de projetos open-source"
                                  ],
                                  "tips": "Use version control como Git para rastrear mudanças no template.",
                                  "learningObjective": "Entender como estruturar documentação para garantir transparência desde o início.",
                                  "commonMistakes": [
                                    "Ignorar impactos sociais no planejamento",
                                    "Criar templates muito genéricos sem foco ético"
                                  ]
                                },
                                {
                                  "stepNumber": 2,
                                  "title": "Documentar a Etapa de Coleta de Dados",
                                  "subSteps": [
                                    "Registre fontes de dados, incluindo URLs, datas de acesso e licenças.",
                                    "Descreva critérios de seleção de dados e quaisquer exclusões feitas.",
                                    "Anote consentimentos obtidos e medidas de anonimização aplicadas.",
                                    "Destaque decisões éticas, como rejeitar dados de fontes duvidosas por viés.",
                                    "Inclua logs de volume de dados coletados e amostras iniciais."
                                  ],
                                  "verification": "Log de coleta completo com evidências (screenshots de fontes, hashes de arquivos).",
                                  "estimatedTime": "2 horas",
                                  "materials": [
                                    "Ferramentas de coleta: Pandas, APIs como Kaggle",
                                    "Guia GDPR ou FAIR principles"
                                  ],
                                  "tips": "Sempre inclua 'por quê' atrás de cada decisão para accountability.",
                                  "learningObjective": "Capturar detalhes da coleta para permitir auditoria ética.",
                                  "commonMistakes": [
                                    "Omitir origens de dados proprietários",
                                    "Não registrar recusas éticas de dados"
                                  ]
                                },
                                {
                                  "stepNumber": 3,
                                  "title": "Documentar Limpeza e Modelagem de Dados",
                                  "subSteps": [
                                    "Liste transformações aplicadas: remoção de outliers, normalização, feature engineering.",
                                    "Registre algoritmos de modelagem usados e hiperparâmetros escolhidos.",
                                    "Anote testes de viés (ex: fairness metrics) e ajustes realizados.",
                                    "Descreva iterações do modelo e razões para mudanças.",
                                    "Inclua visualizações de dados antes/depois para transparência."
                                  ],
                                  "verification": "Seções de limpeza/modelagem preenchidas com código snippets e métricas.",
                                  "estimatedTime": "3 horas",
                                  "materials": [
                                    "Bibliotecas: Scikit-learn, Pandas",
                                    "Ferramentas de visualização: Matplotlib, Seaborn"
                                  ],
                                  "tips": "Embed código executável em notebooks para reprodutibilidade.",
                                  "learningObjective": "Garantir que processos técnicos sejam rastreáveis e éticos.",
                                  "commonMistakes": [
                                    "Não documentar remoções de dados que afetam grupos minoritários",
                                    "Esquecer hiperparâmetros em logs"
                                  ]
                                },
                                {
                                  "stepNumber": 4,
                                  "title": "Integrar Análises Éticas e Finalizar Relatório",
                                  "subSteps": [
                                    "Compile impactos sociais potenciais: discriminação, privacidade, equidade.",
                                    "Descreva mitigações éticas implementadas e trade-offs decididos.",
                                    "Crie sumário executivo destacando decisões chave e lições aprendidas.",
                                    "Revise o relatório com checklist de completude ética.",
                                    "Compartilhe draft para feedback de pares ou stakeholders."
                                  ],
                                  "verification": "Relatório final versionado e aprovado, com seção ética proeminente.",
                                  "estimatedTime": "2 horas",
                                  "materials": [
                                    "Checklist ético (ex: Google's PAIR framework)",
                                    "Ferramentas de revisão colaborativa: GitHub, Overleaf"
                                  ],
                                  "tips": "Use linguagem clara e não técnica para seções de impacto social.",
                                  "learningObjective": "Conectar aspectos técnicos a implicações éticas amplas.",
                                  "commonMistakes": [
                                    "Enterrar análises éticas no final",
                                    "Ignorar feedback externo"
                                  ]
                                }
                              ],
                              "practicalExample": "Em um projeto de ML para prever aprovação de empréstimos usando dados de crédito público, documente: coleta de dataset Kaggle (fonte, licença), limpeza removendo features sensíveis como gênero (decisão ética contra viés), modelagem com Random Forest (hiperparâmetros testados), e impactos como risco de discriminação contra baixa renda, mitigado por fairness checks.",
                              "finalVerifications": [
                                "Relatório permite reproduzir o projeto inteiro de forma ética.",
                                "Todas decisões éticas estão explicitadas com justificativas.",
                                "Logs incluem timestamps, responsáveis e evidências.",
                                "Impactos sociais são avaliados realisticamente.",
                                "Documento é acessível e legível para não-especialistas.",
                                "Checklist de transparência (FAIR) é atendido."
                              ],
                              "assessmentCriteria": [
                                "Completude: Todas etapas cobertas com detalhes acionáveis (80%+).",
                                "Clareza ética: Decisões e impactos destacados proeminentemente.",
                                "Reprodutibilidade: Logs permitem recriar processo (teste bem-sucedido).",
                                "Profundidade: Sub-passos detalhados sem ambiguidades.",
                                "Profissionalismo: Formato padronizado, sem erros gramaticais.",
                                "Inovação: Inclusão de mitigações criativas para riscos éticos."
                              ],
                              "crossCurricularConnections": [
                                "Ética Filosófica: Debates sobre responsabilidade moral em IA.",
                                "Comunicação Técnica: Redação de relatórios profissionais.",
                                "Direito e Privacidade: Conformidade com LGPD/GDPR.",
                                "Gestão de Projetos: Versionamento e accountability em equipes."
                              ],
                              "realWorldApplication": "Em empresas como Google ou bancos, relatórios éticos são mandatórios para auditorias regulatórias (ex: EU AI Act), evitando multas e construindo confiança pública em modelos de dados."
                            },
                            "estimatedTime": "2 horas",
                            "difficulty": "beginner",
                            "status": "not_started",
                            "prerequisites": []
                          },
                          {
                            "id": "10.1.5.5.3.2",
                            "name": "Utilizar ferramentas de explicabilidade de modelos",
                            "description": "Aplicar técnicas como LIME, SHAP e feature importance para interpretar decisões de modelos black-box em aprendizado de máquina.",
                            "atomicExpansion": {
                              "steps": [
                                {
                                  "stepNumber": 1,
                                  "title": "Configuração do Ambiente e Preparação do Dataset",
                                  "subSteps": [
                                    "Instalar bibliotecas necessárias via pip: scikit-learn, lime, shap, matplotlib e pandas.",
                                    "Carregar um dataset de classificação black-box, como 'breast_cancer' do sklearn.",
                                    "Dividir o dataset em treino e teste (80/20).",
                                    "Treinar um modelo opaco, como RandomForestClassifier.",
                                    "Realizar predições em 5-10 instâncias de teste específicas."
                                  ],
                                  "verification": "Verificar se o modelo treina sem erros, gera predições e o ambiente Jupyter executa células sem falhas.",
                                  "estimatedTime": "30 minutos",
                                  "materials": "Python 3.8+, Jupyter Notebook, comandos: pip install scikit-learn lime shap matplotlib pandas",
                                  "tips": "Use um dataset pequeno inicialmente para agilizar testes e evitar sobrecarga computacional.",
                                  "learningObjective": "Preparar um ambiente funcional com modelo black-box treinado e pronto para análise de explicabilidade.",
                                  "commonMistakes": "Esquecer de importar bibliotecas ou usar datasets com features categóricas não tratadas."
                                },
                                {
                                  "stepNumber": 2,
                                  "title": "Implementação de Feature Importance",
                                  "subSteps": [
                                    "Extrair feature importance do modelo treinado usando model.feature_importances_.",
                                    "Criar um gráfico de barras ordenado das top 10 features mais importantes.",
                                    "Mapear nomes das features aos valores de importância.",
                                    "Analisar qualitativamente as features dominantes para uma predição específica.",
                                    "Comparar importâncias entre predições positivas e negativas."
                                  ],
                                  "verification": "Gerar e exibir plot de feature importance com valores somando aproximadamente 1.0.",
                                  "estimatedTime": "45 minutos",
                                  "materials": "Código Python com sklearn.inspection.plot_importance ou matplotlib para visualização.",
                                  "tips": "Normalize os valores para facilitar comparação e foque em features com >5% de importância.",
                                  "learningObjective": "Compreender e visualizar a contribuição global de features em decisões do modelo.",
                                  "commonMistakes": "Interpretar importância como causalidade em vez de correlação, ou ignorar features de baixa importância."
                                },
                                {
                                  "stepNumber": 3,
                                  "title": "Aplicação da Técnica LIME",
                                  "subSteps": [
                                    "Instanciar o explicador LIME para o modelo e dataset.",
                                    "Gerar explicação local para uma instância específica usando lime.explain_instance.",
                                    "Visualizar o plot de explicação LIME destacando superpixels/superfeatures.",
                                    "Interpretar contribuições positivas e negativas para a predição.",
                                    "Testar em múltiplas instâncias e comparar com feature importance global."
                                  ],
                                  "verification": "Produzir plots LIME com contribuições numéricas e qualitativas corretas para pelo menos 3 instâncias.",
                                  "estimatedTime": "60 minutos",
                                  "materials": "Biblioteca lime (from lime import lime_tabular), modelo e dados preparados.",
                                  "tips": "Ajuste num_samples=5000 para precisão, mas reduza para testes rápidos.",
                                  "learningObjective": "Aplicar LIME para explicações locais aproximadas de modelos black-box.",
                                  "commonMistakes": "Não discretizar features contínuas adequadamente ou usar scaler incompatível."
                                },
                                {
                                  "stepNumber": 4,
                                  "title": "Aplicação da Técnica SHAP",
                                  "subSteps": [
                                    "Criar um explicador SHAP (KernelExplainer ou TreeExplainer para árvores).",
                                    "Calcular valores SHAP para uma instância usando explainer.shap_values.",
                                    "Gerar summary plot e force plot para visualização.",
                                    "Analisar forças de push/pull de features na predição final.",
                                    "Comparar resultados SHAP com LIME e feature importance."
                                  ],
                                  "verification": "Exibir force plot e summary plot SHAP com valores SHAP coerentes (soma zero para base value).",
                                  "estimatedTime": "75 minutos",
                                  "materials": "Biblioteca shap (import shap), modelo tree-based preferencialmente.",
                                  "tips": "Use TreeExplainer para modelos de árvores para eficiência; amostre background dataset pequeno.",
                                  "learningObjective": "Dominar SHAP para explicações locais e globais baseadas em teoria de jogos.",
                                  "commonMistakes": "Confundir SHAP local com global ou ignorar o background dataset."
                                },
                                {
                                  "stepNumber": 5,
                                  "title": "Interpretação, Comparação e Relatório",
                                  "subSteps": [
                                    "Compilar resultados de feature importance, LIME e SHAP em um dashboard unificado.",
                                    "Identificar consistências e discrepâncias entre técnicas.",
                                    "Escrever um relatório curto explicando uma predição controversa.",
                                    "Discutir implicações éticas (ex: bias em features sensíveis).",
                                    "Testar robustez variando instâncias e hiperparâmetros."
                                  ],
                                  "verification": "Produzir relatório com plots comparativos e conclusões claras sobre interpretabilidade.",
                                  "estimatedTime": "45 minutos",
                                  "materials": "Jupyter Notebook para consolidação, bibliotecas de plotagem.",
                                  "tips": "Use subplots para comparações visuais lado a lado.",
                                  "learningObjective": "Integrar múltiplas técnicas de explicabilidade para uma visão holística do modelo.",
                                  "commonMistakes": "Superestimar precisão das aproximações ou negligenciar limitações das ferramentas."
                                }
                              ],
                              "practicalExample": "No dataset breast_cancer do sklearn, treine um RandomForest para prever malignidade. Para uma instância predita como 'maligno' (probabilidade 0.85), use SHAP para mostrar que 'mean texture' contribui +0.3 (push para maligno), enquanto 'worst smoothness' contribui -0.1 (pull para benigno), LIME confirma com pesos locais similares, e feature importance global destaca 'worst radius' como top feature, justificando a decisão opaca.",
                              "finalVerifications": [
                                "Geração correta de plots de feature importance com ordenação decrescente.",
                                "Explicações LIME com contribuições superpixel corretas para 3+ instâncias.",
                                "SHAP force plots e summary plots exibindo valores coerentes.",
                                "Relatório comparativo identificando pelo menos 2 consistências entre técnicas.",
                                "Interpretação ética de pelo menos uma feature sensível.",
                                "Robustez testada em 5 instâncias variadas."
                              ],
                              "assessmentCriteria": [
                                "Precisão técnica: Códigos executam sem erros e plots corretos (30%).",
                                "Profundidade de interpretação: Análise qualitativa e quantitativa clara (25%).",
                                "Comparação entre métodos: Identificação de forças/limitações (20%).",
                                "Aplicação ética: Discussão de transparência e accountability (15%).",
                                "Relatório estruturado: Clareza, visualizações e conclusões (10%)."
                              ],
                              "crossCurricularConnections": [
                                "Ética em IA: Promove transparência e accountability em decisões automatizadas.",
                                "Programação Python: Uso avançado de bibliotecas ML e visualização.",
                                "Estatística: Interpretação de contribuições de variáveis e incertezas.",
                                "Ciência de Dados: Integração em pipelines de ML responsáveis.",
                                "Matemática Computacional: Teoria de jogos em SHAP e aproximações lineares em LIME."
                              ],
                              "realWorldApplication": "Em sistemas de saúde para justificar predições de diagnósticos (ex: detecção de câncer), permitindo que médicos entendam e confiem em modelos black-box; em finanças para auditoria de aprovações de crédito, reduzindo vieses e atendendo regulamentações como GDPR; ou em recrutamento para explicar rejeições de candidatos, promovendo fairness."
                            },
                            "estimatedTime": "3 horas",
                            "difficulty": "intermediate",
                            "status": "not_started",
                            "prerequisites": []
                          },
                          {
                            "id": "10.1.5.5.3.3",
                            "name": "Definir responsabilidades em equipes de ML",
                            "description": "Estabelecer papéis e protocolos para accountability, incluindo auditorias éticas e mecanismos de contestação de decisões automatizadas.",
                            "atomicExpansion": {
                              "steps": [
                                {
                                  "stepNumber": 1,
                                  "title": "Identificar papéis essenciais em equipes de ML",
                                  "subSteps": [
                                    "Listar funções chave como Cientista de Dados, Engenheiro de ML, Especialista em Ética e Gerente de Produto.",
                                    "Analisar responsabilidades típicas de cada papel em projetos de ML.",
                                    "Mapear interdependências entre papéis para evitar sobreposições.",
                                    "Consultar frameworks existentes como os do Google PAIR ou Microsoft Responsible AI.",
                                    "Priorizar papéis com base no escopo do projeto específico."
                                  ],
                                  "verification": "Criar uma tabela com papéis, descrições e interdependências preenchida.",
                                  "estimatedTime": "1 hora",
                                  "materials": "Artigos sobre estruturas de equipes de ML (ex: Towards Data Science), planilha Google Sheets ou Excel.",
                                  "tips": "Inclua um papel dedicado a ética desde o início para evitar retrofits.",
                                  "learningObjective": "Compreender a composição ideal de equipes de ML para transparência.",
                                  "commonMistakes": "Ignorar papéis de suporte como compliance, levando a lacunas éticas."
                                },
                                {
                                  "stepNumber": 2,
                                  "title": "Definir responsabilidades e protocolos de accountability",
                                  "subSteps": [
                                    "Atribuir tarefas específicas a cada papel, como 'Cientista de Dados valida dados de entrada'.",
                                    "Estabelecer métricas de performance e relatórios periódicos para cada um.",
                                    "Criar protocolos de escalonamento para falhas, como notificações automáticas.",
                                    "Definir assinaturas digitais ou aprovações obrigatórias para etapas críticas.",
                                    "Simular cenários de falha para testar accountability."
                                  ],
                                  "verification": "Documentar um RACI matrix (Responsible, Accountable, Consulted, Informed) para o time.",
                                  "estimatedTime": "1.5 horas",
                                  "materials": "Template RACI matrix, exemplos de protocolos de equipes open-source como TensorFlow.",
                                  "tips": "Use linguagem clara e mensurável nas responsabilidades para facilitar auditorias.",
                                  "learningObjective": "Estabelecer protocolos claros para rastrear responsabilidade individual.",
                                  "commonMistakes": "Definir responsabilidades vagas, como 'gerenciar ética', sem métricas específicas."
                                },
                                {
                                  "stepNumber": 3,
                                  "title": "Projetar auditorias éticas regulares",
                                  "subSteps": [
                                    "Definir frequência de auditorias (ex: mensal ou por milestone).",
                                    "Criar checklist para bias, fairness e conformidade regulatória (GDPR, LGPD).",
                                    "Designar auditor independente ou rodízio de papéis.",
                                    "Integrar ferramentas automatizadas como AIF360 para detecção de bias.",
                                    "Documentar relatórios de auditoria com ações corretivas."
                                  ],
                                  "verification": "Produzir um template de relatório de auditoria com checklist preenchido em um exemplo.",
                                  "estimatedTime": "1 hora",
                                  "materials": "Ferramentas como IBM AIF360, guidelines da UE AI Act, template de checklist.",
                                  "tips": "Automatize partes da auditoria para escalabilidade em projetos grandes.",
                                  "learningObjective": "Implementar mecanismos proativos para monitorar ética em ML.",
                                  "commonMistakes": "Auditorias muito infrequentes, permitindo acumulação de riscos éticos."
                                },
                                {
                                  "stepNumber": 4,
                                  "title": "Estabelecer mecanismos de contestação de decisões automatizadas",
                                  "subSteps": [
                                    "Definir canais de apelo para usuários afetados (ex: formulário online).",
                                    "Criar processo de revisão humana com prazos (ex: 48h).",
                                    "Integrar explainability tools como SHAP ou LIME nos modelos.",
                                    "Treinar equipe para lidar com contestações e atualizar modelos conforme feedback.",
                                    "Registrar todas contestações para análise contínua."
                                  ],
                                  "verification": "Desenhar um fluxograma do processo de contestação com prazos e responsáveis.",
                                  "estimatedTime": "1 hora",
                                  "materials": "Bibliotecas SHAP/LIME, exemplos de políticas de contestação (ex: Amazon Rekognition).",
                                  "tips": "Garanta anonimato nos canais de contestação para encorajar uso.",
                                  "learningObjective": "Criar caminhos acessíveis para humanos questionarem IA.",
                                  "commonMistakes": "Processos complexos que desencorajam contestações reais."
                                },
                                {
                                  "stepNumber": 5,
                                  "title": "Documentar, comunicar e validar o framework",
                                  "subSteps": [
                                    "Compilar todos os elementos em um documento central (ex: wiki ou PDF).",
                                    "Realizar workshop com a equipe para feedback e alinhamento.",
                                    "Testar o framework em um piloto de projeto pequeno.",
                                    "Estabelecer plano de revisão anual do framework.",
                                    "Treinar novos membros com simulações."
                                  ],
                                  "verification": "Obter assinaturas ou aprovações da equipe no documento final.",
                                  "estimatedTime": "1 hora",
                                  "materials": "Ferramentas como Notion ou Confluence para documentação, agenda para workshop.",
                                  "tips": "Mantenha o documento vivo com versionamento para evoluções.",
                                  "learningObjective": "Garantir adoção e sustentabilidade do framework de responsabilidades.",
                                  "commonMistakes": "Não comunicar adequadamente, levando a não adesão."
                                }
                              ],
                              "practicalExample": "Em um projeto de ML para aprovação de empréstimos em um banco, defina: Cientista de Dados (valida dados sem bias), Engenheiro de ML (deploy com explainability), Especialista em Ética (auditoria mensal de fairness), com protocolo RACI, auditorias usando AIF360 e formulário de contestação online com revisão humana em 24h.",
                              "finalVerifications": [
                                "Lista de papéis com RACI matrix completa e aprovada.",
                                "Protocolos de auditoria ética com checklist e frequência definida.",
                                "Mecanismos de contestação com fluxograma e integração de tools.",
                                "Documento centralizado com treinamento planejado.",
                                "Simulação de piloto validada sem gaps de accountability.",
                                "Feedback da equipe incorporado (pelo menos 80% aprovação)."
                              ],
                              "assessmentCriteria": [
                                "Completude: Todos papéis, protocolos e mecanismos cobertos (100%).",
                                "Clareza: Linguagem precisa e mensurável em todas definições.",
                                "Praticidade: Tempos, materiais e verificações realistas.",
                                "Abrangência ética: Inclusão de bias, fairness e conformidade legal.",
                                "Escalabilidade: Framework adaptável a projetos de diferentes tamanhos.",
                                "Inovação: Uso de tools modernas como SHAP e AIF360."
                              ],
                              "crossCurricularConnections": [
                                "Gestão de Projetos: Uso de RACI e milestones.",
                                "Ética Filosófica: Princípios de justiça e responsabilidade moral.",
                                "Direito Digital: Conformidade com LGPD e AI Act.",
                                "Liderança e Trabalho em Equipe: Definição de papéis colaborativos."
                              ],
                              "realWorldApplication": "Empresas como Google (com People + AI Research) e Microsoft (Responsible AI Standard) aplicam frameworks semelhantes para garantir accountability em sistemas de IA, reduzindo riscos legais e melhorando confiança pública em decisões automatizadas."
                            },
                            "estimatedTime": "2 horas",
                            "difficulty": "beginner",
                            "status": "not_started",
                            "prerequisites": []
                          }
                        ]
                      }
                    ]
                  }
                ]
              },
              {
                "id": "10.1.6",
                "name": "Ética no Uso e Manipulação de Dados",
                "description": "Princípios éticos envolvidos no uso e manipulação de dados.",
                "totalSkills": 52,
                "atomicTopics": [
                  {
                    "id": "10.1.6.1",
                    "name": "Privacidade e Proteção de Dados",
                    "description": "Princípios e técnicas para proteger dados pessoais durante coleta, armazenamento e manipulação, incluindo anonimização e conformidade com regulamentações.",
                    "individualConcepts": [
                      {
                        "id": "10.1.6.1.1",
                        "name": "Princípios Fundamentais de Privacidade de Dados",
                        "description": "Conceitos básicos que orientam a proteção de dados pessoais, incluindo minimização de dados, consentimento informado e transparência no processamento.",
                        "specificSkills": [
                          {
                            "id": "10.1.6.1.1.1",
                            "name": "Identificar princípios de privacidade",
                            "description": "Reconhecer e explicar os princípios chave como minimização de dados, limitação de propósito, integridade e confidencialidade, aplicados à coleta e uso de dados em projetos de ciência de dados.",
                            "atomicExpansion": {
                              "steps": [
                                {
                                  "stepNumber": 1,
                                  "title": "Familiarizar-se com os princípios fundamentais de privacidade",
                                  "subSteps": [
                                    "Pesquise e liste os quatro princípios chave: minimização de dados, limitação de propósito, integridade e confidencialidade.",
                                    "Leia definições oficiais de fontes como GDPR ou LGPD.",
                                    "Crie um mapa mental conectando cada princípio ao contexto de dados.",
                                    "Compare com princípios semelhantes em outras regulamentações.",
                                    "Anote exemplos iniciais para cada um."
                                  ],
                                  "verification": "Mapa mental completo com definições e exemplos para os quatro princípios.",
                                  "estimatedTime": "45 minutos",
                                  "materials": [
                                    "Documento GDPR/LGPD online",
                                    "Ferramenta de mapa mental (ex: MindMeister ou papel/caneta)",
                                    "Notebook para anotações"
                                  ],
                                  "tips": "Use cores diferentes para cada princípio para facilitar a memorização visual.",
                                  "learningObjective": "Reconhecer e definir os quatro princípios fundamentais de privacidade de dados.",
                                  "commonMistakes": [
                                    "Confundir minimização com anonimização",
                                    "Ignorar o contexto regulatório específico",
                                    "Listar princípios incompletos"
                                  ]
                                },
                                {
                                  "stepNumber": 2,
                                  "title": "Explorar minimização de dados e limitação de propósito",
                                  "subSteps": [
                                    "Defina minimização: coletar apenas dados necessários para o objetivo.",
                                    "Estude limitação de propósito: usar dados só para fins declarados.",
                                    "Analise casos onde violação ocorre, como coletar dados extras desnecessários.",
                                    "Crie exemplos em ciência de dados, como um modelo de ML que não precisa de dados sensíveis.",
                                    "Discuta trade-offs entre utilidade dos dados e privacidade."
                                  ],
                                  "verification": "Dois exemplos personalizados para cada princípio, com justificativa de aplicação.",
                                  "estimatedTime": "60 minutos",
                                  "materials": [
                                    "Artigos sobre GDPR Artigo 5",
                                    "Casos de estudo de violações (ex: Cambridge Analytica)",
                                    "Planilha para exemplos"
                                  ],
                                  "tips": "Pergunte sempre: 'Este dado é estritamente necessário?' para minimização.",
                                  "learningObjective": "Explicar e diferenciar minimização de dados e limitação de propósito com exemplos.",
                                  "commonMistakes": [
                                    "Achar que minimização significa zero dados",
                                    "Confundir propósito com reutilização futura sem consentimento"
                                  ]
                                },
                                {
                                  "stepNumber": 3,
                                  "title": "Analisar integridade e confidencialidade",
                                  "subSteps": [
                                    "Defina integridade: manter dados precisos, atualizados e consistentes.",
                                    "Defina confidencialidade: proteger contra acessos não autorizados.",
                                    "Explore técnicas como criptografia para confidencialidade e validação para integridade.",
                                    "Identifique riscos em pipelines de dados de ciência de dados.",
                                    "Crie fluxogramas mostrando como aplicar esses princípios em um dataset."
                                  ],
                                  "verification": "Fluxograma ilustrando aplicação dos dois princípios em um pipeline de dados.",
                                  "estimatedTime": "50 minutos",
                                  "materials": [
                                    "Documentação de criptografia básica (ex: AES)",
                                    "Ferramenta de fluxograma (ex: Draw.io)",
                                    "Exemplos de breaches de dados"
                                  ],
                                  "tips": "Lembre-se: integridade é sobre 'qualidade', confidencialidade sobre 'segurança'.",
                                  "learningObjective": "Entender e exemplificar integridade e confidencialidade em contextos de dados.",
                                  "commonMistakes": [
                                    "Confundir integridade com backup",
                                    "Subestimar ameaças internas para confidencialidade"
                                  ]
                                },
                                {
                                  "stepNumber": 4,
                                  "title": "Aplicar princípios em cenários de ciência de dados",
                                  "subSteps": [
                                    "Selecione um dataset fictício de ciência de dados (ex: dados de saúde).",
                                    "Identifique violações potenciais de cada princípio.",
                                    "Proponha correções para conformidade.",
                                    "Simule uma revisão de projeto aplicando todos os princípios.",
                                    "Documente um checklist de privacidade para projetos futuros."
                                  ],
                                  "verification": "Checklist completo com identificação e correções para um cenário dado.",
                                  "estimatedTime": "75 minutos",
                                  "materials": [
                                    "Dataset de exemplo (Kaggle: saúde ou e-commerce)",
                                    "Modelo de checklist em Google Docs"
                                  ],
                                  "tips": "Comece pela coleta de dados e avance para uso/análise.",
                                  "learningObjective": "Identificar e aplicar os princípios em projetos reais de ciência de dados.",
                                  "commonMistakes": [
                                    "Focar só em um princípio, ignorando os outros",
                                    "Não considerar o ciclo completo de dados"
                                  ]
                                }
                              ],
                              "practicalExample": "Em um projeto de predição de churn de clientes para uma empresa de telecom, aplique minimização coletando apenas histórico de chamadas e uso de dados (não localização exata); limitação de propósito usando só para predição de churn (não marketing); integridade validando dados de uso mensalmente; confidencialidade criptografando PII em trânsito e repouso.",
                              "finalVerifications": [
                                "Liste e defina corretamente os quatro princípios sem consulta.",
                                "Aplique cada princípio a um cenário de coleta de dados fornecido.",
                                "Identifique violações em um caso de uso de dados de ML descrito.",
                                "Crie um checklist de 5 itens para revisão de privacidade em projetos.",
                                "Explique trade-offs entre privacidade e performance de modelo.",
                                "Discuta exemplos reais como GDPR multas por violações."
                              ],
                              "assessmentCriteria": [
                                "Precisão e completude das definições dos princípios (80%+ acurácia).",
                                "Qualidade e relevância dos exemplos em contextos de ciência de dados.",
                                "Capacidade de identificar violações em cenários hipotéticos.",
                                "Profundidade na aplicação integrada dos quatro princípios.",
                                "Criatividade e praticidade do checklist ou fluxograma criado.",
                                "Uso correto de terminologia regulatória (ex: GDPR Art. 5)."
                              ],
                              "crossCurricularConnections": [
                                "Ética em Computação: Discussões sobre dilemas morais em dados.",
                                "Direito Digital: Estudo de leis como LGPD e GDPR.",
                                "Segurança da Informação: Técnicas de criptografia e controle de acesso.",
                                "Estatística: Differential Privacy como extensão prática.",
                                "Gestão de Projetos: Integração de privacidade em ciclos ágeis."
                              ],
                              "realWorldApplication": "Em empresas de tech como Google ou Nubank, esses princípios garantem conformidade com regulamentações como GDPR/LGPD, evitando multas milionárias (ex: British Airways multada €20M por falha em confidencialidade), e constroem confiança com usuários em apps de análise de dados sensíveis."
                            },
                            "estimatedTime": "1 hora",
                            "difficulty": "beginner",
                            "status": "not_started",
                            "prerequisites": []
                          },
                          {
                            "id": "10.1.6.1.1.2",
                            "name": "Aplicar consentimento informado",
                            "description": "Desenvolver mecanismos para obter e registrar consentimento explícito dos titulares de dados antes da coleta, garantindo revogabilidade e granularidade.",
                            "atomicExpansion": {
                              "steps": [
                                {
                                  "stepNumber": 1,
                                  "title": "Compreender os Princípios Fundamentais do Consentimento Informado",
                                  "subSteps": [
                                    "Estude a definição de consentimento explícito conforme regulamentações como LGPD e GDPR.",
                                    "Identifique os elementos essenciais: informação clara, voluntariedade, especificidade e revogabilidade.",
                                    "Analise exemplos de consentimento granular (ex: optar por dados específicos como email vs. dados biométricos).",
                                    "Discuta cenários onde consentimento implícito é inválido.",
                                    "Crie um glossário pessoal com termos chave: granularidade, revogação, titular de dados."
                                  ],
                                  "verification": "Você completou quando puder explicar os 4 pilares do consentimento informado em suas próprias palavras e citar exemplos de violações comuns.",
                                  "estimatedTime": "30 minutos",
                                  "materials": [
                                    "Documentação LGPD/GDPR (PDFs online)",
                                    "Vídeos educativos sobre privacidade (YouTube/Khan Academy)",
                                    "Caderno para anotações"
                                  ],
                                  "tips": "Use analogias cotidianas, como consentimento médico, para fixar conceitos.",
                                  "learningObjective": "Ao final deste passo, você será capaz de definir e diferenciar consentimento informado de outras formas de autorização.",
                                  "commonMistakes": [
                                    "Confundir consentimento explícito com implícito (ex: checkboxes pré-marcados).",
                                    "Ignorar a necessidade de linguagem acessível e não técnica."
                                  ]
                                },
                                {
                                  "stepNumber": 2,
                                  "title": "Projetar um Formulário de Consentimento Claro e Granular",
                                  "subSteps": [
                                    "Esboce a estrutura do formulário: introdução, descrição de dados coletados, finalidades e opções granulares.",
                                    "Inclua checkboxes independentes para cada tipo de dado (ex: nome, email, localização).",
                                    "Adicione seção explícita para revogação (link ou instruções claras).",
                                    "Teste a legibilidade com linguagem simples (nível 8º ano).",
                                    "Incorpore botão de 'Aceitar' proeminente e 'Rejeitar' igualmente acessível."
                                  ],
                                  "verification": "Você completou quando tiver um protótipo de formulário em HTML ou papel que cubra todos os elementos granulares e passe em uma revisão de pares.",
                                  "estimatedTime": "45 minutos",
                                  "materials": [
                                    "Editor de texto/HTML (VS Code)",
                                    "Templates de formulários de privacidade (ex: GitHub)",
                                    "Ferramenta de design como Figma (gratuito)"
                                  ],
                                  "tips": "Priorize opt-in sobre opt-out e evite 'walls of text'; use bullet points.",
                                  "learningObjective": "Ao final, você projetará formulários que garantem consentimento voluntário e granular.",
                                  "commonMistakes": [
                                    "Sobrecarregar o usuário com jargão jurídico.",
                                    "Fazer checkboxes pré-selecionados ou agrupar dados não relacionados."
                                  ]
                                },
                                {
                                  "stepNumber": 3,
                                  "title": "Implementar Registro e Armazenamento Seguro do Consentimento",
                                  "subSteps": [
                                    "Crie um banco de dados simples (ex: JSON ou SQLite) para registrar: ID usuário, data/hora, dados consentidos, hash da assinatura.",
                                    "Desenvolva lógica para capturar e validar o consentimento via código (ex: JavaScript).",
                                    "Implemente timestamps para rastrear revogações futuras.",
                                    "Garanta criptografia no armazenamento (ex: hash do consentimento).",
                                    "Adicione logs de auditoria para todas interações de consentimento."
                                  ],
                                  "verification": "Você completou quando o sistema registrar corretamente um consentimento simulado e permitir consulta via ID.",
                                  "estimatedTime": "60 minutos",
                                  "materials": [
                                    "Node.js ou Python",
                                    "SQLite ou arquivo JSON",
                                    "Bibliotecas como crypto-js para hash"
                                  ],
                                  "tips": "Sempre valide no backend, não confie apenas no frontend.",
                                  "learningObjective": "Ao final, você implementará um sistema robusto para registrar consentimentos de forma auditável.",
                                  "commonMistakes": [
                                    "Armazenar dados sensíveis sem hash ou criptografia.",
                                    "Não incluir timestamps precisos para rastreabilidade."
                                  ]
                                },
                                {
                                  "stepNumber": 4,
                                  "title": "Desenvolver Mecanismos de Revogação e Verificação Contínua",
                                  "subSteps": [
                                    "Crie uma interface de usuário para revogação (ex: página 'Gerenciar Consentimento').",
                                    "Implemente API para atualizar status de consentimento no banco (marcar como revogado).",
                                    "Adicione verificações periódicas: bloquear acesso a dados revogados.",
                                    "Teste cenários de revogação parcial (ex: revogar só localização).",
                                    "Documente processo de revogação para o usuário final."
                                  ],
                                  "verification": "Você completou quando simular uma revogação e confirmar que o sistema bloqueia acesso aos dados afetados.",
                                  "estimatedTime": "45 minutos",
                                  "materials": [
                                    "Código do passo anterior",
                                    "Ferramentas de teste como Postman",
                                    "Frontend framework básico (HTML/JS)"
                                  ],
                                  "tips": "Torne a revogação tão fácil quanto o consentimento inicial.",
                                  "learningObjective": "Ao final, você garantirá que consentimentos sejam verdadeiramente revogáveis em tempo real.",
                                  "commonMistakes": [
                                    "Tornar revogação oculta ou complicada.",
                                    "Não propagar revogação para todos os sistemas conectados."
                                  ]
                                }
                              ],
                              "practicalExample": "Em um app de pesquisa de saúde, crie um popup inicial explicando coleta de dados de passos diários via smartwatch. Ofereça checkboxes granulares: 'Consentir passos', 'Consentir localização', com botão 'Revogar a qualquer momento' linkando para dashboard. Registre em banco com hash e permita revogação que apaga dados específicos.",
                              "finalVerifications": [
                                "Explicar verbalmente os 4 pilares do consentimento.",
                                "Demonstrar um formulário funcional com granularidade.",
                                "Mostrar registro e revogação em um protótipo rodando.",
                                "Identificar falhas em exemplos reais de sites (ex: Facebook).",
                                "Auditar logs de consentimento simulado.",
                                "Confirmar conformidade com LGPD em checklist."
                              ],
                              "assessmentCriteria": [
                                "Clareza e granularidade do formulário (nota 1-10).",
                                "Segurança no registro (ausência de dados plain-text).",
                                "Facilidade de revogação (tempo < 30s).",
                                "Cobertura de todos princípios (explícito, revogável).",
                                "Testes de edge cases (revogação parcial).",
                                "Documentação de auditoria completa."
                              ],
                              "crossCurricularConnections": [
                                "Direito: Conformidade com LGPD/GDPR e direitos humanos.",
                                "UI/UX Design: Princípios de acessibilidade e usabilidade.",
                                "Programação: Bancos de dados, criptografia e APIs.",
                                "Ética: Discussões filosóficas sobre autonomia e privacidade.",
                                "Negócios: Impacto em confiança do cliente e multas regulatórias."
                              ],
                              "realWorldApplication": "Em empresas como Google ou apps de saúde (ex: Fitbit), implementar consentimento granular evita multas milionárias por violações de privacidade e constrói confiança, permitindo coleta ética de dados para IA e análises preditivas."
                            },
                            "estimatedTime": "1.5 horas",
                            "difficulty": "beginner",
                            "status": "not_started",
                            "prerequisites": []
                          },
                          {
                            "id": "10.1.6.1.1.3",
                            "name": "Garantir transparência no processamento",
                            "description": "Criar políticas e notificações claras sobre como os dados serão coletados, armazenados e manipulados, incluindo direitos de acesso e correção dos titulares.",
                            "atomicExpansion": {
                              "steps": [
                                {
                                  "stepNumber": 1,
                                  "title": "Compreender os princípios fundamentais de transparência em privacidade de dados",
                                  "subSteps": [
                                    "Estude a LGPD (Lei Geral de Proteção de Dados) e princípios como transparência no art. 6º.",
                                    "Analise exemplos de políticas de privacidade de empresas como Google e Facebook.",
                                    "Identifique obrigações: informar sobre coleta, armazenamento, uso e direitos dos titulares.",
                                    "Revise conceitos de consentimento explícito e direito de acesso (art. 18 da LGPD)."
                                  ],
                                  "verification": "Resuma em um documento os 5 princípios chave da LGPD relacionados à transparência.",
                                  "estimatedTime": "2 horas",
                                  "materials": [
                                    "Texto da LGPD (disponível no Planalto.gov.br)",
                                    "Exemplos de políticas de privacidade de sites reais"
                                  ],
                                  "tips": "Use destaques para marcar obrigações específicas de transparência.",
                                  "learningObjective": "Dominar os requisitos legais para transparência no processamento de dados.",
                                  "commonMistakes": "Ignorar diferenças entre LGPD e GDPR, focando apenas em leis estrangeiras."
                                },
                                {
                                  "stepNumber": 2,
                                  "title": "Mapear o fluxo de dados do projeto ou sistema",
                                  "subSteps": [
                                    "Liste todos os dados coletados (ex: nome, email, localização).",
                                    "Descreva como os dados são armazenados (banco de dados, nuvem).",
                                    "Detalhe manipulações (análise, compartilhamento com terceiros).",
                                    "Inclua fluxograma visual do ciclo de vida dos dados."
                                  ],
                                  "verification": "Crie um fluxograma simples no Draw.io ou papel confirmando todos os pontos de processamento.",
                                  "estimatedTime": "3 horas",
                                  "materials": [
                                    "Ferramenta de fluxograma como Draw.io ou Lucidchart",
                                    "Templates de mapeamento de dados da ANPD"
                                  ],
                                  "tips": "Inclua cenários de falha, como vazamentos, para antecipar riscos.",
                                  "learningObjective": "Visualizar completamente o processamento para garantir cobertura total na política.",
                                  "commonMistakes": "Esquecer dados indiretos, como logs de IP ou cookies."
                                },
                                {
                                  "stepNumber": 3,
                                  "title": "Redigir políticas e notificações claras",
                                  "subSteps": [
                                    "Escreva a política de privacidade cobrindo coleta, armazenamento, uso e direitos.",
                                    "Crie banners/pop-ups de notificação com linguagem simples e acessível.",
                                    "Inclua seções sobre direitos: acesso, correção, anonimização e portabilidade.",
                                    "Use bullet points e FAQs para facilitar leitura."
                                  ],
                                  "verification": "Peça feedback de 2 colegas simulando titulares de dados.",
                                  "estimatedTime": "4 horas",
                                  "materials": [
                                    "Templates de política de privacidade (ANPD ou sites como Privacidade.com.br)",
                                    "Ferramentas de edição como Google Docs"
                                  ],
                                  "tips": "Mantenha linguagem em nível 8º ano escolar para acessibilidade.",
                                  "learningObjective": "Produzir documentos legíveis e completos que informem efetivamente os usuários.",
                                  "commonMistakes": "Usar jargões técnicos sem explicação, violando o princípio de clareza."
                                },
                                {
                                  "stepNumber": 4,
                                  "title": "Implementar, testar e auditar as medidas de transparência",
                                  "subSteps": [
                                    "Integre notificações no sistema (ex: modal de consentimento no app).",
                                    "Teste usabilidade com usuários reais (5 testes).",
                                    "Realize auditoria interna verificando conformidade com LGPD.",
                                    "Documente atualizações futuras na política."
                                  ],
                                  "verification": "Registre logs de testes mostrando 100% de cobertura dos fluxos de dados.",
                                  "estimatedTime": "3 horas",
                                  "materials": [
                                    "Ferramentas de teste como UserTesting ou Google Forms para feedback",
                                    "Checklist de conformidade ANPD"
                                  ],
                                  "tips": "Automatize notificações para mudanças em políticas.",
                                  "learningObjective": "Garantir que a transparência seja prática e verificável no ambiente real.",
                                  "commonMistakes": "Implementar sem testar, resultando em notificações confusas."
                                }
                              ],
                              "practicalExample": "Em um app de e-commerce, crie uma política explicando que emails são coletados para newsletters (com opt-out), armazenados no AWS com criptografia, e usuários podem solicitar correção via suporte@email.com, com banner no login.",
                              "finalVerifications": [
                                "Política cobre todos os fluxos de dados mapeados.",
                                "Notificações são visíveis e legíveis em todos dispositivos.",
                                "Usuários entendem direitos em testes (nota >8/10).",
                                "Documentos atualizáveis e versionados.",
                                "Conformidade com LGPD auditada sem gaps.",
                                "Feedback de titulares confirma clareza."
                              ],
                              "assessmentCriteria": [
                                "Clareza e simplicidade da linguagem (sem jargões).",
                                "Completude: todos aspectos do processamento cobertos.",
                                "Acessibilidade: formato legível e multilíngue se aplicável.",
                                "Eficácia: testes mostram compreensão >90%.",
                                "Conformidade legal: alinhado à LGPD art. 9 e 18."
                              ],
                              "crossCurricularConnections": [
                                "Ética em Ciência de Dados: Princípios morais de honestidade.",
                                "Direito Digital: Legislação e compliance.",
                                "Design de UX/UI: Criação de interfaces intuitivas.",
                                "Programação: Integração de pop-ups e logs em código."
                              ],
                              "realWorldApplication": "Empresas como Nubank usam políticas transparentes em apps para construir confiança, evitando multas da ANPD (ex: R$50mi) e aumentando retenção de usuários em 20% com consentimento claro."
                            },
                            "estimatedTime": "1 hora",
                            "difficulty": "intermediate",
                            "status": "not_started",
                            "prerequisites": []
                          }
                        ]
                      },
                      {
                        "id": "10.1.6.1.2",
                        "name": "Técnicas de Anonimização e Pseudonimização",
                        "description": "Métodos para remover ou mascarar identificadores pessoais, tornando os dados não rastreáveis a indivíduos específicos durante análise e modelagem.",
                        "specificSkills": [
                          {
                            "id": "10.1.6.1.2.1",
                            "name": "Implementar anonimização por supressão",
                            "description": "Aplicar técnicas de remoção de atributos identificadores diretos (como nome, CPF) e indiretos (como combinações de idade e CEP) em conjuntos de dados.",
                            "atomicExpansion": {
                              "steps": [
                                {
                                  "stepNumber": 1,
                                  "title": "Preparar o Ambiente e Carregar o Dataset",
                                  "subSteps": [
                                    "Instalar bibliotecas necessárias como Pandas e NumPy em um ambiente Jupyter Notebook.",
                                    "Carregar o dataset de exemplo em um DataFrame do Pandas.",
                                    "Explorar a estrutura do dataset com métodos como head(), info() e describe().",
                                    "Identificar colunas iniciais e tipos de dados.",
                                    "Salvar uma cópia original do dataset para comparação posterior."
                                  ],
                                  "verification": "DataFrame carregado corretamente e estrutura visualizada sem erros.",
                                  "estimatedTime": "20 minutos",
                                  "materials": [
                                    "Jupyter Notebook ou Google Colab",
                                    "Biblioteca Pandas",
                                    "Dataset exemplo em CSV (ex: dados_ficticios_clientes.csv)"
                                  ],
                                  "tips": "Use pd.read_csv() com encoding='utf-8' para evitar problemas com acentos.",
                                  "learningObjective": "Configurar ambiente de análise de dados e manipular datasets iniciais.",
                                  "commonMistakes": [
                                    "Não fazer backup do dataset original",
                                    "Ignorar tipos de dados mistos",
                                    "Esquecer de instalar dependências"
                                  ]
                                },
                                {
                                  "stepNumber": 2,
                                  "title": "Identificar Identificadores Diretos e Indiretos",
                                  "subSteps": [
                                    "Listar atributos diretos: nome, CPF, RG, email, telefone.",
                                    "Analisar atributos indiretos: combinações como idade + CEP, salário + cidade.",
                                    "Calcular frequências de valores únicos em colunas suspeitas com value_counts().",
                                    "Aplicar critério de quasi-identificadores (ex: grupos com <5 registros únicos).",
                                    "Documentar uma lista priorizada de atributos a suprimir."
                                  ],
                                  "verification": "Lista documentada de identificadores diretos e indiretos com justificativas.",
                                  "estimatedTime": "40 minutos",
                                  "materials": [
                                    "Notebook Jupyter",
                                    "Dataset carregado",
                                    "Documentação de referência (ex: LGPD ou GDPR guidelines)"
                                  ],
                                  "tips": "Visualize distribuições com gráficos de barras para spotting de combinações raras.",
                                  "learningObjective": "Reconhecer Personally Identifiable Information (PII) e quasi-identificadores.",
                                  "commonMistakes": [
                                    "Focar só em diretos e ignorar indiretos",
                                    "Não considerar interações entre colunas",
                                    "Subestimar combinações contextuais"
                                  ]
                                },
                                {
                                  "stepNumber": 3,
                                  "title": "Aplicar Supressão de Atributos",
                                  "subSteps": [
                                    "Selecionar colunas para supressão baseadas na lista do passo anterior.",
                                    "Remover colunas diretas com del ou drop().",
                                    "Para indiretos, suprimir valores sensíveis (ex: substituir CEP por categoria ampla).",
                                    "Criar novo DataFrame anonimizado e salvar como CSV.",
                                    "Comparar tamanhos e estatísticas antes/depois para medir perda de informação."
                                  ],
                                  "verification": "Novo dataset sem colunas suprimidas e estatísticas comparadas.",
                                  "estimatedTime": "30 minutos",
                                  "materials": [
                                    "Pandas DataFrame",
                                    "Notebook Jupyter"
                                  ],
                                  "tips": "Use drop(columns=['nome', 'CPF']) para remoção eficiente.",
                                  "learningObjective": "Executar operações de supressão em datasets usando Python.",
                                  "commonMistakes": [
                                    "Remover colunas úteis acidentalmente",
                                    "Não registrar mudanças",
                                    "Suprimir demais e perder utility"
                                  ]
                                },
                                {
                                  "stepNumber": 4,
                                  "title": "Verificar e Validar a Anonimização",
                                  "subSteps": [
                                    "Buscar resquícios de PII com buscas por strings sensíveis (ex: regex para CPF).",
                                    "Testar re-identificação: agrupar por quasi-identificadores e contar únicos.",
                                    "Avaliar utility: rodar análises básicas (médias, correlações) no dataset anonimizado.",
                                    "Documentar riscos residuais e sugestões de melhorias.",
                                    "Compartilhar dataset anonimizado para revisão simulada."
                                  ],
                                  "verification": "Relatório de validação confirmando ausência de re-identificação >1%.",
                                  "estimatedTime": "35 minutos",
                                  "materials": [
                                    "Dataset anonimizado",
                                    "Regex para validação",
                                    "Ferramentas de groupby no Pandas"
                                  ],
                                  "tips": "Use nunique() após groupby para checar anonimato k=5.",
                                  "learningObjective": "Validar efetividade da anonimização e balancear privacidade/utility.",
                                  "commonMistakes": [
                                    "Não testar combinações restantes",
                                    "Assumir supressão total sem validação",
                                    "Ignorar perda de qualidade de dados"
                                  ]
                                }
                              ],
                              "practicalExample": "Em um dataset de pacientes hospitalares com colunas: nome, CPF, idade, CEP, diagnóstico, tratamento. Remova 'nome' e 'CPF' (diretos); suprima 'CEP' quando combinado com 'idade' <30 ou >70 (indireto), substituindo por 'regiao_norte/sul/etc.'. O dataset resultante permite análise agregada de tratamentos por região sem riscos de re-identificação.",
                              "finalVerifications": [
                                "Ausência total de identificadores diretos no dataset final.",
                                "Nenhum grupo de quasi-identificadores com menos de 5 registros.",
                                "Busca regex não encontra padrões de CPF ou email.",
                                "Análises estatísticas básicas funcionam sem erros.",
                                "Relatório documentado com mudanças e riscos residuais.",
                                "Dataset salvo e comparável com original."
                              ],
                              "assessmentCriteria": [
                                "Identificação precisa e completa de PII (90%+ cobertura).",
                                "Implementação correta de supressão sem perda acidental de dados úteis.",
                                "Validação robusta com testes de re-identificação.",
                                "Preservação de pelo menos 80% da utility original (ex: correlações mantidas).",
                                "Documentação clara e profissional.",
                                "Tempo respeitado e código limpo/comentado."
                              ],
                              "crossCurricularConnections": [
                                "Ética e Filosofia: Discussão sobre privacidade como direito fundamental.",
                                "Direito: Aplicação da LGPD e GDPR em proteção de dados.",
                                "Estatística: Medição de perda de informação e k-anonimato.",
                                "Programação: Manipulação de dados com Python/Pandas.",
                                "Ciências Sociais: Impacto da anonimização em pesquisas populacionais."
                              ],
                              "realWorldApplication": "Em instituições de saúde para compartilhar datasets de pacientes com pesquisadores, removendo nome/CPF e suprimindo CEP+idade para análises epidemiológicas sem violar privacidade, conforme exigido por leis como LGPD."
                            },
                            "estimatedTime": "2 horas",
                            "difficulty": "intermediate",
                            "status": "not_started",
                            "prerequisites": []
                          },
                          {
                            "id": "10.1.6.1.2.2",
                            "name": "Utilizar k-anonimato",
                            "description": "Garantir que cada registro em um dataset seja indistinguível de pelo menos k-1 outros registros, usando generalização e supressão para proteger privacidade em análises exploratórias.",
                            "atomicExpansion": {
                              "steps": [
                                {
                                  "stepNumber": 1,
                                  "title": "Compreender os Conceitos Fundamentais de k-Anonimato",
                                  "subSteps": [
                                    "Estude a definição de k-anonimato: cada registro deve ser indistinguível de pelo menos k-1 outros em um grupo de equivalência.",
                                    "Aprenda sobre quasi-identifiers (QI): atributos que, combinados, podem identificar indivíduos (ex: idade, gênero, CEP).",
                                    "Diferencie generalização (agrupar valores, ex: idade exata para faixa etária) de supressão (remover ou mascarar valores).",
                                    "Explore exemplos simples de datasets que violam e cumprem k-anonimato.",
                                    "Revise métricas como tamanho mínimo de grupo (k) e perda de informação."
                                  ],
                                  "verification": "Resuma em suas palavras o que é k-anonimato e identifique QIs em um dataset de exemplo.",
                                  "estimatedTime": "1 hora",
                                  "materials": [
                                    "Documentação sobre privacidade diferencial (ex: artigos de Sweeney)",
                                    "Vídeos tutoriais sobre k-anonimato (YouTube/Khan Academy)",
                                    "Notebook Jupyter para anotações"
                                  ],
                                  "tips": "Use diagramas para visualizar grupos de equivalência; comece com k=2 para simplicidade.",
                                  "learningObjective": "Dominar a teoria por trás do k-anonimato e identificar componentes chave.",
                                  "commonMistakes": [
                                    "Confundir k-anonimato com anonimato total",
                                    "Ignorar combinações de QIs",
                                    "Subestimar impacto da supressão em utility dos dados"
                                  ]
                                },
                                {
                                  "stepNumber": 2,
                                  "title": "Preparar e Analisar o Dataset",
                                  "subSteps": [
                                    "Carregue um dataset de exemplo com atributos sensíveis e potenciais QIs (ex: tabela de pacientes com idade, gênero, doença).",
                                    "Identifique QIs manualmente e valide com testes de re-identificação simulados.",
                                    "Defina o valor de k desejado (ex: k=5) baseado no contexto de privacidade.",
                                    "Particione o dataset em grupos iniciais baseados em QIs exatos.",
                                    "Calcule estatísticas iniciais: número de registros, distribuição de QIs e grupos únicos."
                                  ],
                                  "verification": "Gere um relatório listando QIs identificados e partições iniciais com tamanhos de grupo < k.",
                                  "estimatedTime": "1.5 horas",
                                  "materials": [
                                    "Python com pandas e numpy",
                                    "Dataset de exemplo (ex: Adult UCI ou sintético de saúde)",
                                    "Jupyter Notebook"
                                  ],
                                  "tips": "Comece com datasets pequenos (100-500 linhas) para depuração rápida; visualize com histograms.",
                                  "learningObjective": "Preparar dados adequadamente e diagnosticar riscos de privacidade.",
                                  "commonMistakes": [
                                    "Escolher QIs irrelevantes",
                                    "Definir k muito baixo sem justificativa",
                                    "Não documentar suposições sobre o dataset"
                                  ]
                                },
                                {
                                  "stepNumber": 3,
                                  "title": "Aplicar Generalização e Supressão",
                                  "subSteps": [
                                    "Implemente hierarquias de generalização para QIs numéricos (ex: idade: exata > década > faixa ampla) e categóricos (ex: CEP: 5 dígitos > 3 dígitos).",
                                    "Aplique algoritmos iterativos: generalize o QI que minimiza perda de informação até todos os grupos terem tamanho >= k.",
                                    "Use supressão seletiva para outliers ou grupos pequenos resistentes.",
                                    "Automatize com bibliotecas como ARX ou código customizado em Python.",
                                    "Monitore perda de informação (ex: métrica de entropia ou precisão)."
                                  ],
                                  "verification": "Execute o processo e confirme que 100% dos registros estão em grupos de tamanho >= k.",
                                  "estimatedTime": "2 horas",
                                  "materials": [
                                    "Biblioteca ARX (Java) ou sdcMicro (R)/Python custom",
                                    "Hierarquias de generalização pré-definidas",
                                    "Jupyter Notebook para tracking"
                                  ],
                                  "tips": "Priorize generalizações top-down para eficiência; teste com k variados para trade-offs.",
                                  "learningObjective": "Executar transformações práticas para alcançar k-anonimato.",
                                  "commonMistakes": [
                                    "Generalizar excessivamente levando a perda total de utility",
                                    "Ignorar hierarquias inadequadas",
                                    "Não iterar o suficiente"
                                  ]
                                },
                                {
                                  "stepNumber": 4,
                                  "title": "Verificar e Avaliar o Anonimato",
                                  "subSteps": [
                                    "Verifique conformidade: conte grupos de equivalência e confirme min(k).",
                                    "Teste ataques simulados: tente re-identificar com background knowledge.",
                                    "Meça utility: compare estatísticas pré/pós-anonimização (ex: médias, correlações).",
                                    "Gere relatório com métricas de privacidade e utility.",
                                    "Ajuste parâmetros se necessário e reexecute."
                                  ],
                                  "verification": "Produza um dataset final anonimizado e relatório comprovando k-anonimato com utility preservada.",
                                  "estimatedTime": "1 hora",
                                  "materials": [
                                    "Ferramentas de validação (ARX risk analyzer)",
                                    "Datasets de background knowledge simulados",
                                    "Python para métricas estatísticas"
                                  ],
                                  "tips": "Use visualizações side-by-side dos datasets original e anonimizado; defina thresholds para utility.",
                                  "learningObjective": "Avaliar robustez do anonimato e balancear privacidade/utility.",
                                  "commonMistakes": [
                                    "Apenas verificar k sem testar ataques",
                                    "Aceitar utility baixa sem iterações",
                                    "Esquecer de documentar trade-offs"
                                  ]
                                }
                              ],
                              "practicalExample": "Em um dataset hospitalar com 1000 registros (idade, gênero, CEP, diagnóstico), identifique QIs (idade, gênero, CEP). Aplique k=5: generalize idade para faixas de 10 anos, CEP para prefixo de 3 dígitos, suprima gênero em grupos pequenos. Resultado: todos registros em grupos >=5, permitindo análise de prevalência de doenças sem risco de identificação.",
                              "finalVerifications": [
                                "Todos os grupos de equivalência têm tamanho >= k.",
                                "Nenhum registro único ou pequeno grupo persiste após transformações.",
                                "Testes de re-identificação simulados falham em >95% dos casos.",
                                "Relatório inclui métricas de perda de informação <20%.",
                                "Dataset mantém utility para análises exploratórias (ex: médias preservadas).",
                                "Documentação completa de QIs, hierarquias e decisões."
                              ],
                              "assessmentCriteria": [
                                "Precisão na identificação de QIs e definição de k (80%+ acurácia).",
                                "Implementação correta de generalização/supressão alcançando k-anonimato.",
                                "Balanceamento de privacidade e utility medido quantitativamente.",
                                "Relatório claro com verificações e visualizações.",
                                "Tratamento de edge cases (ex: outliers).",
                                "Código limpo, reproduzível e comentado."
                              ],
                              "crossCurricularConnections": [
                                "Ética e Direito: Conformidade com LGPD/GDPR em proteção de dados.",
                                "Programação: Uso de Python/pandas para manipulação de dados.",
                                "Estatística: Métricas de perda de informação e análise exploratória.",
                                "Matemática: Particionamento e otimização hierárquica.",
                                "Ciência da Computação: Algoritmos de privacidade diferencial."
                              ],
                              "realWorldApplication": "Hospitais usam k-anonimato para compartilhar datasets de pacientes em pesquisas epidemiológicas, bancos financeiros anonimizaram transações para detecção de fraudes, e governos publicam censos agregados sem expor indivíduos."
                            },
                            "estimatedTime": "2.5 horas",
                            "difficulty": "intermediate",
                            "status": "not_started",
                            "prerequisites": []
                          },
                          {
                            "id": "10.1.6.1.2.3",
                            "name": "Aplicar pseudonimização",
                            "description": "Substituir identificadores por pseudônimos reversíveis com chaves seguras, mantendo utilidade para aprendizado de máquina enquanto protege identidade durante armazenamento.",
                            "atomicExpansion": {
                              "steps": [
                                {
                                  "stepNumber": 1,
                                  "title": "Compreender Conceitos de Pseudonimização e Preparar Ambiente",
                                  "subSteps": [
                                    "Estude a definição: pseudonimização substitui identificadores por pseudônimos reversíveis usando chaves seguras, diferentemente da anonimização irreversível.",
                                    "Identifique tipos de identificadores diretos (ex: nome, CPF, email) e indiretos em um dataset exemplo.",
                                    "Instale e importe bibliotecas Python: pandas para manipulação de dados, hashlib para hashing reversível com salt.",
                                    "Carregue um dataset de teste com dados sensíveis simulados (ex: nomes e IDs de pacientes).",
                                    "Crie uma chave segura (salt) usando secrets.token_hex() para reversibilidade controlada."
                                  ],
                                  "verification": "Ambiente Python rodando com dataset carregado e chave gerada; responda quiz interno sobre diferenças com anonimização.",
                                  "estimatedTime": "25 minutos",
                                  "materials": [
                                    "Python 3.x",
                                    "Bibliotecas: pandas, hashlib, secrets",
                                    "Dataset CSV de exemplo com 100 registros"
                                  ],
                                  "tips": "Use Jupyter Notebook para experimentação interativa e visualização rápida.",
                                  "learningObjective": "Dominar conceitos fundamentais e configurar ferramentas para implementação segura.",
                                  "commonMistakes": [
                                    "Confundir pseudonimização com criptografia simples.",
                                    "Usar salts previsíveis ou vazados.",
                                    "Ignorar identificadores indiretos como combinações de idade + cidade."
                                  ]
                                },
                                {
                                  "stepNumber": 2,
                                  "title": "Selecionar e Mapear Identificadores para Pseudonimos",
                                  "subSteps": [
                                    "Analise o dataset com df.describe() e df.info() para listar colunas sensíveis.",
                                    "Crie um dicionário de mapeamento: identificador original -> pseudônimo (ex: hash(identificador + salt)).",
                                    "Implemente função de hashing reversível: def pseudonomize(id, salt): return hashlib.sha256((id + salt).encode()).hexdigest()",
                                    "Aplique mapeamento inicial em uma cópia do dataset, preservando colunas não sensíveis.",
                                    "Salve o dicionário de mapeamento em arquivo criptografado para uso futuro."
                                  ],
                                  "verification": "Dataset copiado com pseudônimos aplicados em colunas selecionadas; verifique unicidade com df.duplicated().",
                                  "estimatedTime": "30 minutos",
                                  "materials": [
                                    "Jupyter Notebook",
                                    "Dataset preparado do Step 1",
                                    "Arquivo para salvar mapeamento (JSON criptografado)"
                                  ],
                                  "tips": "Mantenha o salt secreto e versionado para evitar reutilização em contextos diferentes.",
                                  "learningObjective": "Selecionar corretamente dados sensíveis e criar mapeamentos reversíveis seguros.",
                                  "commonMistakes": [
                                    "Aplicar hash em todo dataset sem distinção.",
                                    "Perder referências originais sem backup.",
                                    "Usar hash MD5 fraco em vez de SHA-256."
                                  ]
                                },
                                {
                                  "stepNumber": 3,
                                  "title": "Implementar Pseudonimização Completa e Testar Reversibilidade",
                                  "subSteps": [
                                    "Substitua todas as colunas identificadoras no dataset principal usando a função de pseudonimização.",
                                    "Crie função de despseudonimização: def despseudonimize(pseudo, mapping): return mapping.get(pseudo, 'Unknown')",
                                    "Teste reversibilidade: pseudonimize e despseudonimize 10 amostras aleatórias e compare.",
                                    "Avalie utilidade para ML: rode uma regressão simples (ex: sklearn.LinearRegression) antes/depois e compare scores.",
                                    "Armazene dataset pseudonimizado em CSV e mapeamento em local seguro."
                                  ],
                                  "verification": "Testes de reversibilidade 100% precisos e score de ML varia <5%; dataset salvo sem dados originais expostos.",
                                  "estimatedTime": "40 minutos",
                                  "materials": [
                                    "Scikit-learn para testes ML",
                                    "Funções criadas nos steps anteriores"
                                  ],
                                  "tips": "Use vectorized operations no Pandas (apply com axis=1) para eficiência em grandes datasets.",
                                  "learningObjective": "Implementar pseudonimização prática mantendo utilidade analítica e controlando reversibilidade.",
                                  "commonMistakes": [
                                    "Expor mapeamento sem criptografia.",
                                    "Não testar em subconjuntos representativos.",
                                    "Ignorar colunas derivadas que re-identificam (ex: hash de email + telefone)."
                                  ]
                                },
                                {
                                  "stepNumber": 4,
                                  "title": "Validar Proteção e Integrar em Pipeline de ML",
                                  "subSteps": [
                                    "Execute ataques simulados de re-identificação (ex: dicionário rainbow table) sem salt para falhar.",
                                    "Treine um modelo ML completo (ex: classificação) no dataset pseudonimizado e compare performance com original.",
                                    "Documente processo: crie relatório com chaves usadas, métricas de utilidade e riscos residuais.",
                                    "Integre em pipeline: crie script automatizado para pseudonimizar novos dados.",
                                    "Discuta conformidade com regulamentações como LGPD/GDPR."
                                  ],
                                  "verification": "Modelo ML atinge >90% da performance original; ataques falham sem chave; script roda sem erros.",
                                  "estimatedTime": "35 minutos",
                                  "materials": [
                                    "Scikit-learn completo",
                                    "Script de pipeline (Python file)"
                                  ],
                                  "tips": "Monitore entropia dos pseudônimos para garantir aleatoriedade (use scipy.stats.entropy).",
                                  "learningObjective": "Garantir proteção efetiva e integração prática em fluxos de dados reais.",
                                  "commonMistakes": [
                                    "Subestimar riscos de linkage attacks.",
                                    "Não documentar para auditoria.",
                                    "Treinar ML só em dados pseudonimizados sem baseline."
                                  ]
                                }
                              ],
                              "practicalExample": "Em um dataset médico com 1.000 registros de pacientes (nomes, CPFs, diagnósticos), aplique pseudonimização nos CPFs usando SHA-256 + salt único. Treine um modelo de predição de diabetes: performance cai de 0.92 para 0.90 AUC, mas identidades estão protegidas para compartilhamento com pesquisadores.",
                              "finalVerifications": [
                                "Pseudônimos são únicos e consistentes no dataset.",
                                "Reversibilidade só possível com chave/salt correta.",
                                "Utilidade para ML preservada (métricas variam <10%).",
                                "Nenhum identificador original legível no dataset final.",
                                "Mapeamento salvo de forma segura e auditável.",
                                "Resistência comprovada a ataques básicos de re-identificação."
                              ],
                              "assessmentCriteria": [
                                "Implementação correta de hashing reversível com salt.",
                                "Preservação de correlações estatísticas para ML.",
                                "Eficiência computacional (tempo <1s por 1k registros).",
                                "Documentação clara de processo e riscos.",
                                "Conformidade com princípios de privacidade (LGPD).",
                                "Testes abrangentes de reversibilidade e proteção."
                              ],
                              "crossCurricularConnections": [
                                "Criptografia: Hashing e funções de derivação de chaves.",
                                "Programação Python: Manipulação de dados com Pandas e Scikit-learn.",
                                "Ética e Direito Digital: Regulamentações de privacidade como LGPD/GDPR.",
                                "Estatística e ML: Avaliação de impacto na utilidade de dados.",
                                "Segurança da Informação: Gerenciamento de chaves e ataques de re-identificação."
                              ],
                              "realWorldApplication": "Hospitais e bancos usam pseudonimização para compartilhar datasets de pacientes/clientes em pesquisas de ML, cumprindo LGPD/GDPR, permitindo análises preditivas (ex: detecção de fraudes) sem expor identidades individuais durante armazenamento e processamento."
                            },
                            "estimatedTime": "1.5 horas",
                            "difficulty": "intermediate",
                            "status": "not_started",
                            "prerequisites": []
                          },
                          {
                            "id": "10.1.6.1.2.4",
                            "name": "Avaliar riscos de re-identificação",
                            "description": "Realizar testes para detectar vulnerabilidades em dados anonimizados, como linkage attacks, e mitigar com técnicas adicionais de perturbação.",
                            "atomicExpansion": {
                              "steps": [
                                {
                                  "stepNumber": 1,
                                  "title": "Preparar o ambiente de teste e o conjunto de dados anonimizado",
                                  "subSteps": [
                                    "Selecionar um dataset representativo anonimizado, como dados demográficos ou de saúde.",
                                    "Carregar o dataset em Python usando pandas e explorar sua estrutura.",
                                    "Preparar um dataset auxiliar público (ex: dados do IBGE ou census) para simular ataques.",
                                    "Documentar o método de anonimização original (k-anonimato, supressão etc.).",
                                    "Verificar integridade dos dados e ausência de identificadores diretos."
                                  ],
                                  "verification": "Dataset carregado sem erros, estrutura explorada e documentada em notebook Jupyter.",
                                  "estimatedTime": "30 minutos",
                                  "materials": "Python com pandas, numpy; Jupyter Notebook; datasets anonimizado e público auxiliar.",
                                  "tips": "Inicie com datasets pequenos (n<1000) para agilizar testes iniciais.",
                                  "learningObjective": "Configurar ambiente seguro para testes de privacidade em dados.",
                                  "commonMistakes": "Usar datasets incompatíveis entre si ou ignorar metadados de anonimização."
                                },
                                {
                                  "stepNumber": 2,
                                  "title": "Identificar atributos quasi-identificadores (QI)",
                                  "subSteps": [
                                    "Listar todos os atributos e avaliar sua cardinalidade e distribuição.",
                                    "Testar unicidade de combinações (ex: idade + CEP + gênero) calculando k-anonimato.",
                                    "Usar scripts para medir frequência mínima em grupos de atributos.",
                                    "Classificar atributos como QI de alto, médio ou baixo risco.",
                                    "Visualizar riscos com histogramas ou heatmaps."
                                  ],
                                  "verification": "Lista documentada de QI com valores de k-anonimato <10 indicando alto risco.",
                                  "estimatedTime": "45 minutos",
                                  "materials": "Python com pandas, scipy; ferramenta ARX (opcional); Jupyter Notebook.",
                                  "tips": "Priorize combinações de 3-5 atributos comuns em linkage attacks.",
                                  "learningObjective": "Reconhecer atributos que permitem re-identificação indireta.",
                                  "commonMistakes": "Superestimar anonimato ignorando combinações de atributos raros."
                                },
                                {
                                  "stepNumber": 3,
                                  "title": "Executar testes de linkage attack",
                                  "subSteps": [
                                    "Selecionar QI identificados e dataset auxiliar.",
                                    "Implementar linkage exato via merge em pandas ou fuzzy matching.",
                                    "Executar ataques simulados contando matches bem-sucedidos.",
                                    "Variar parâmetros (ex: tolerância de 2 anos em idade).",
                                    "Registrar taxa de re-identificação e falsos positivos."
                                  ],
                                  "verification": "Relatório com taxa de sucesso >5% confirmando vulnerabilidade.",
                                  "estimatedTime": "60 minutos",
                                  "materials": "Biblioteca recordlinkage ou pandas; datasets preparados.",
                                  "tips": "Use amostras aleatórias para evitar overfitting no teste.",
                                  "learningObjective": "Simular ataques reais de re-identificação em dados anonimizados.",
                                  "commonMistakes": "Não considerar matching fuzzy, subestimando riscos reais."
                                },
                                {
                                  "stepNumber": 4,
                                  "title": "Quantificar e avaliar os riscos",
                                  "subSteps": [
                                    "Calcular métricas: taxa de re-identificação, precisão, recall e risco populacional.",
                                    "Comparar com thresholds (ex: re-id <1% aceitável).",
                                    "Avaliar sensibilidade dos dados re-identificados.",
                                    "Priorizar riscos por atributo ou grupo.",
                                    "Gerar relatório visual com gráficos de risco."
                                  ],
                                  "verification": "Relatório quantificado com métricas e recomendações de priorização.",
                                  "estimatedTime": "30 minutos",
                                  "materials": "Matplotlib/Seaborn para visualizações; Jupyter Notebook.",
                                  "tips": "Use privacidade diferencial como benchmark para métricas avançadas.",
                                  "learningObjective": "Medir e interpretar riscos de privacidade de forma quantitativa.",
                                  "commonMistakes": "Ignorar contexto sensível dos dados ao avaliar impacto."
                                },
                                {
                                  "stepNumber": 5,
                                  "title": "Mitigar riscos com técnicas de perturbação e reavaliar",
                                  "subSteps": [
                                    "Aplicar perturbação: ruído gaussiano em numéricos, generalização em categóricos.",
                                    "Aumentar anonimização (ex: k-anonimato k>20) ou supressão.",
                                    "Repetir linkage attacks no dataset mitigado.",
                                    "Comparar métricas antes/depois.",
                                    "Documentar técnicas e residual risks."
                                  ],
                                  "verification": "Redução da taxa de re-identificação para <1% confirmada.",
                                  "estimatedTime": "45 minutos",
                                  "materials": "Ferramentas ARX, sdcMicro ou scripts Python personalizados.",
                                  "tips": "Teste múltiplas perturbações e equilibre utilidade vs. privacidade.",
                                  "learningObjective": "Implementar e validar mitigações efetivas de re-identificação.",
                                  "commonMistakes": "Aplicar perturbação excessiva, destruindo utilidade dos dados."
                                }
                              ],
                              "practicalExample": "Em um dataset anonimizado de atendimentos hospitalares (idade, gênero, CEP, diagnóstico), realize linkage com dados públicos do IBGE via CEP+idade. Detecte 18% de re-identificações. Aplique ruído na idade (+/-3 anos) e generalize CEP para estado, reduzindo para 0.8%, mantendo utilidade para análises agregadas.",
                              "finalVerifications": [
                                "Ambiente e dados preparados corretamente sem erros.",
                                "Quasi-identificadores identificados com k-anonimato calculado.",
                                "Testes de linkage executados com taxa de re-identificação reportada.",
                                "Riscos quantificados com métricas precisas e thresholds.",
                                "Mitigações aplicadas e reavaliação confirma redução efetiva.",
                                "Documentação completa incluindo código e relatórios.",
                                "Compreensão ética de impactos demonstrada."
                              ],
                              "assessmentCriteria": [
                                "Identificação precisa de QI com cobertura >80%.",
                                "Execução correta de linkage attacks com métricas válidas.",
                                "Quantificação de riscos alinhada a padrões (ex: ARX).",
                                "Seleção e aplicação eficaz de mitigações de perturbação.",
                                "Relatórios claros, visuais e acionáveis.",
                                "Equilíbrio demonstrado entre privacidade e utilidade dos dados.",
                                "Adesão a princípios éticos e regulatórios (LGPD)."
                              ],
                              "crossCurricularConnections": [
                                "Estatística: Probabilidades, distribuições e métricas de risco.",
                                "Programação: Manipulação de dados e algoritmos de matching.",
                                "Ética e Direito: Privacidade, LGPD/GDPR e responsabilidade em dados.",
                                "Ciência de Dados: Anonimização e privacidade diferencial."
                              ],
                              "realWorldApplication": "Em organizações de saúde, finanças e governo para validar anonimização de datasets antes de publicação, garantindo conformidade com LGPD/GDPR, evitando re-identificações que levam a multas milionárias e violações de privacidade em análises compartilhadas."
                            },
                            "estimatedTime": "2 horas",
                            "difficulty": "advanced",
                            "status": "not_started",
                            "prerequisites": []
                          }
                        ]
                      },
                      {
                        "id": "10.1.6.1.3",
                        "name": "Conformidade com Regulamentações de Proteção de Dados",
                        "description": "Normas legais como LGPD e GDPR que impõem obrigações para coleta, armazenamento e manipulação de dados pessoais em contextos de ciência de dados.",
                        "specificSkills": [
                          {
                            "id": "10.1.6.1.3.1",
                            "name": "Compreender a LGPD",
                            "description": "Explicar os artigos principais da Lei Geral de Proteção de Dados (LGPD) no Brasil, incluindo bases legais para tratamento de dados e obrigações do controlador e operador.",
                            "atomicExpansion": {
                              "steps": [
                                {
                                  "stepNumber": 1,
                                  "title": "Entender os Fundamentos e Estrutura da LGPD",
                                  "subSteps": [
                                    "Ler o preâmbulo e Artigo 1º para compreender o objeto e âmbito de aplicação da lei.",
                                    "Identificar os conceitos fundamentais (Art. 5º): dados pessoais, sensíveis, titular, controlador, operador.",
                                    "Estudar os princípios do tratamento de dados (Art. 6º): finalidade, adequação, necessidade, etc.",
                                    "Analisar a estrutura geral da lei: capítulos sobre direitos, deveres e enforcement.",
                                    "Mapear a vigência e entrada em vigor (Lei 13.709/2018, com atualizações)."
                                  ],
                                  "verification": "Criar um mapa mental resumindo conceitos chave e princípios, com pelo menos 10 itens corretos.",
                                  "estimatedTime": "1-2 horas",
                                  "materials": [
                                    "Texto integral da LGPD (site oficial Planalto.gov.br)",
                                    "Guia introdutório da ANPD (Autoridade Nacional de Proteção de Dados)"
                                  ],
                                  "tips": "Use cores no mapa mental para diferenciar conceitos, princípios e papéis.",
                                  "learningObjective": "Dominar os conceitos básicos e princípios orientadores da LGPD.",
                                  "commonMistakes": [
                                    "Confundir dados pessoais com dados públicos",
                                    "Ignorar dados sensíveis como categoria especial",
                                    "Não associar princípios a exemplos reais"
                                  ]
                                },
                                {
                                  "stepNumber": 2,
                                  "title": "Estudar os Artigos Principais e Direitos dos Titulares",
                                  "subSteps": [
                                    "Analisar Artigos 7º a 10º: direitos dos titulares (acesso, correção, anonimização, portabilidade).",
                                    "Estudar Artigos 17 a 22: exercício dos direitos e prazos de resposta (15 dias).",
                                    "Revisar Artigo 9º: agente de tratamento e transparência.",
                                    "Mapear sanções por descumprimento (Capítulo V, Artigos 42 a 49).",
                                    "Comparar com GDPR para contextualizar similaridades."
                                  ],
                                  "verification": "Listar e explicar 5 direitos dos titulares com exemplos hipotéticos de exercício.",
                                  "estimatedTime": "2 horas",
                                  "materials": [
                                    "Texto da LGPD destacando artigos 7-22",
                                    "Infográficos da ANPD sobre direitos dos titulares"
                                  ],
                                  "tips": "Crie flashcards com artigo X direito Y para revisão rápida.",
                                  "learningObjective": "Explicar os principais artigos relacionados a direitos e transparência.",
                                  "commonMistakes": [
                                    "Confundir prazos de resposta (15 dias úteis)",
                                    "Subestimar o direito de petição gratuita",
                                    "Não diferenciar titular de representante"
                                  ]
                                },
                                {
                                  "stepNumber": 3,
                                  "title": "Compreender as Bases Legais para Tratamento de Dados",
                                  "subSteps": [
                                    "Memorizar as 10 bases legais do Artigo 7º (consentimento, obrigação legal, execução de contrato, etc.).",
                                    "Estudar bases para dados sensíveis (Artigo 11): consentimento específico, tutela saúde, etc.",
                                    "Analisar exceções e hipóteses de dispensa de consentimento (Artigo 7º, II a X).",
                                    "Praticar identificação de bases em cenários: marketing, RH, saúde.",
                                    "Revisar compartilhamento internacional (Artigo 33)."
                                  ],
                                  "verification": "Classificar 5 cenários comuns de tratamento de dados com a base legal correta.",
                                  "estimatedTime": "1.5 horas",
                                  "materials": [
                                    "Tabela comparativa de bases legais (criar ou baixar da ANPD)",
                                    "Casos de estudo da ANPD"
                                  ],
                                  "tips": "Use mnemônicos para lembrar as 10 bases: COEPLSDPEF (Consentimento, Obrigação, Execução, etc.).",
                                  "learningObjective": "Identificar e justificar a base legal adequada para cada tratamento de dados.",
                                  "commonMistakes": [
                                    "Usar consentimento como base padrão quando há obrigação legal",
                                    "Ignorar bases para sensíveis",
                                    "Confundir proteção legal com legítimo interesse"
                                  ]
                                },
                                {
                                  "stepNumber": 4,
                                  "title": "Analisar Obrigações do Controlador e Operador",
                                  "subSteps": [
                                    "Definir papéis: Controlador (decide sobre tratamento), Operador (executa).",
                                    "Estudar deveres do controlador (Art. 6º, 9º, 48-51): governança, relatório de impacto, notificação de incidentes em 2 dias.",
                                    "Obrigações do operador (Art. 42): contrato específico, assistência ao controlador.",
                                    "Explorar ANPD: fiscalização, multas até 2% do faturamento.",
                                    "Simular elaboração de cláusula contratual entre controlador e operador."
                                  ],
                                  "verification": "Redigir um resumo comparativo das obrigações de cada papel em tabela.",
                                  "estimatedTime": "2 horas",
                                  "materials": [
                                    "Modelo de contrato controlador-operador (ANPD)",
                                    "Decisões normativas da ANPD"
                                  ],
                                  "tips": "Sempre pergunte: quem decide (controlador) vs. quem processa (operador).",
                                  "learningObjective": "Diferenciar e listar obrigações específicas de controlador e operador.",
                                  "commonMistakes": [
                                    "Confundir controlador com operador em cloud services",
                                    "Subestimar prazo de notificação de incidentes (razão 2 dias)",
                                    "Ignorar governança de dados como obrigação"
                                  ]
                                }
                              ],
                              "practicalExample": "Em uma startup de e-commerce brasileira, analise o cadastro de clientes: identifique dados pessoais/sensíveis, aplique base legal (execução de contrato para cadastro, consentimento para marketing), liste direitos dos titulares acionáveis e elabore obrigações do controlador (relatório de impacto) e operador (provedor de pagamento).",
                              "finalVerifications": [
                                "Explicar verbalmente os 5 princípios do Art. 6º com exemplos.",
                                "Listar e justificar 3 bases legais para cenários reais.",
                                "Diferenciar obrigações de controlador vs. operador em um fluxograma.",
                                "Simular resposta a um titular exercendo direito de acesso.",
                                "Identificar 3 artigos principais e suas implicações práticas.",
                                "Mapear sanções para violações comuns."
                              ],
                              "assessmentCriteria": [
                                "Precisão na citação de artigos e bases legais (sem erros factuais).",
                                "Capacidade de aplicar conceitos a cenários concretos.",
                                "Completude na distinção de papéis e obrigações.",
                                "Clareza na explicação de princípios e direitos.",
                                "Uso correto de terminologia da LGPD.",
                                "Profundidade nas justificativas e exemplos."
                              ],
                              "crossCurricularConnections": [
                                "Direito Civil: Contratos e responsabilidade civil por danos.",
                                "Ética em Computação: Princípios de privacidade by design.",
                                "Gestão de TI: Governança de dados e segurança da informação.",
                                "Ciência de Dados: Anonimização e pseudonimização em datasets.",
                                "Empreendedorismo: Compliance em startups e multas evitáveis."
                              ],
                              "realWorldApplication": "Em empresas de tecnologia como bancos digitais ou apps de delivery, implementar políticas de LGPD para evitar multas milionárias da ANPD, garantindo tratamento ético de dados de milhões de usuários e competitividade no mercado."
                            },
                            "estimatedTime": "1.5 horas",
                            "difficulty": "beginner",
                            "status": "not_started",
                            "prerequisites": []
                          },
                          {
                            "id": "10.1.6.1.3.2",
                            "name": "Mapear conformidade com GDPR",
                            "description": "Identificar equivalências entre LGPD e GDPR, focando em direitos dos titulares (como portabilidade e esquecimento) e requisitos para transferências internacionais de dados.",
                            "atomicExpansion": {
                              "steps": [
                                {
                                  "stepNumber": 1,
                                  "title": "Revisar Fundamentos da LGPD e GDPR",
                                  "subSteps": [
                                    "Ler os artigos principais da LGPD (Lei 13.709/2018) sobre princípios e direitos dos titulares.",
                                    "Estudar o Regulamento Geral de Proteção de Dados (GDPR, UE 2016/679), focando nos Capítulos II e III.",
                                    "Identificar princípios comuns como finalidade, adequação e transparência.",
                                    "Criar uma tabela comparativa inicial de escopo e aplicabilidade.",
                                    "Anotar definições chave como 'titular de dados' e 'controlador' em ambos."
                                  ],
                                  "verification": "Tabela comparativa preenchida com pelo menos 5 princípios equivalentes listados.",
                                  "estimatedTime": "2 horas",
                                  "materials": [
                                    "Texto integral da LGPD",
                                    "Texto do GDPR (disponível em eur-lex.europa.eu)",
                                    "Planilha ou documento comparativo"
                                  ],
                                  "tips": "Use cores para destacar similaridades (verde) e diferenças (vermelho).",
                                  "learningObjective": "Compreender as bases legais para identificar equivalências iniciais.",
                                  "commonMistakes": "Confundir LGPD com GDPR sem considerar contextos nacionais vs. supranacional."
                                },
                                {
                                  "stepNumber": 2,
                                  "title": "Mapear Direitos dos Titulares de Dados",
                                  "subSteps": [
                                    "Comparar direito de acesso (Art. 18 LGPD vs. Art. 15 GDPR).",
                                    "Analisar portabilidade de dados (Art. 18, VII LGPD vs. Art. 20 GDPR).",
                                    "Mapear direito ao esquecimento/deleção (Art. 18, V LGPD vs. Art. 17 GDPR).",
                                    "Listar outros direitos como retificação e oposição em ambos.",
                                    "Documentar prazos de resposta e exceções aplicáveis."
                                  ],
                                  "verification": "Matriz de direitos com colunas para LGPD, GDPR e equivalências/nuances.",
                                  "estimatedTime": "3 horas",
                                  "materials": [
                                    "Artigos específicos das leis",
                                    "Ferramentas de tabela como Google Sheets ou Excel"
                                  ],
                                  "tips": "Priorize direitos focados: portabilidade e esquecimento para alinhamento rápido.",
                                  "learningObjective": "Identificar correspondências exatas e adaptações nos direitos dos titulares.",
                                  "commonMistakes": "Ignorar exceções como interesse legítimo que diferem ligeiramente."
                                },
                                {
                                  "stepNumber": 3,
                                  "title": "Analisar Requisitos para Transferências Internacionais",
                                  "subSteps": [
                                    "Estudar Art. 33 da LGPD (transferências para países com proteção adequada).",
                                    "Comparar com Capítulo V GDPR (Arts. 44-50), incluindo cláusulas contratuais padrão.",
                                    "Mapear garantias como BCRs (Binding Corporate Rules) e decisões de adequação.",
                                    "Identificar diferenças em listas de países adequados (ex: UE vs. ANPD).",
                                    "Criar fluxograma de decisão para transferências."
                                  ],
                                  "verification": "Fluxograma completo com caminhos para conformidade em transferências.",
                                  "estimatedTime": "2.5 horas",
                                  "materials": [
                                    "Guias da ANPD e EDPB sobre transferências",
                                    "Ferramenta de fluxograma como Lucidchart ou Draw.io"
                                  ],
                                  "tips": "Verifique atualizações recentes na lista de adequação da ANPD.",
                                  "learningObjective": "Mapear mecanismos legais para fluxos internacionais de dados.",
                                  "commonMistakes": "Assumir equivalência total sem considerar Schrems II e impactos pós-2020."
                                },
                                {
                                  "stepNumber": 4,
                                  "title": "Sintetizar Mapa de Conformidade e Identificar Lacunas",
                                  "subSteps": [
                                    "Compilar todos os mapas em um documento unificado.",
                                    "Destacar lacunas (ex: ausência de DPO obrigatório na LGPD).",
                                    "Propor ações de conformidade para alinhar LGPD com GDPR.",
                                    "Revisar com checklist de equivalências.",
                                    "Preparar relatório executivo com recomendações."
                                  ],
                                  "verification": "Mapa final de conformidade com pelo menos 80% de cobertura nos tópicos focais.",
                                  "estimatedTime": "2 horas",
                                  "materials": [
                                    "Documentos dos steps anteriores",
                                    "Template de relatório de conformidade"
                                  ],
                                  "tips": "Use bullet points para lacunas e soluções acionáveis.",
                                  "learningObjective": "Gerar um artefato prático para auditorias de conformidade.",
                                  "commonMistakes": "Superestimar equivalências sem evidências regulatórias."
                                }
                              ],
                              "practicalExample": "Para uma startup brasileira de e-commerce processando dados de clientes europeus, mapear que o direito à portabilidade (LGPD Art. 18 VII) equivale ao GDPR Art. 20, mas adicionar cláusulas contratuais padrão para transferências para garantir conformidade em envios de dados para servidores no Brasil.",
                              "finalVerifications": [
                                "Matriz de direitos dos titulares completa com equivalências confirmadas.",
                                "Fluxograma de transferências internacionais funcional e atualizado.",
                                "Lista de lacunas identificadas com pelo menos 3 ações corretivas.",
                                "Relatório executivo revisado por pares.",
                                "Checklist de conformidade assinado com data."
                              ],
                              "assessmentCriteria": [
                                "Precisão nas equivalências (90%+ de acurácia com textos legais).",
                                "Profundidade nos direitos focais (portabilidade e esquecimento detalhados).",
                                "Completude do fluxograma de transferências (todos cenários cobertos).",
                                "Clareza e usabilidade do mapa final.",
                                "Identificação de pelo menos 2 lacunas reais com soluções viáveis.",
                                "Referências a fontes oficiais em todos os itens."
                              ],
                              "crossCurricularConnections": [
                                "Ética em Ciência de Dados: Princípios de privacidade by design.",
                                "Direito Digital: Análise comparativa de legislações.",
                                "Gestão de Projetos: Planejamento de compliance em fluxos de dados.",
                                "Segurança da Informação: Integração com DPIAs (Data Protection Impact Assessments)."
                              ],
                              "realWorldApplication": "Em empresas multinacionais como Google ou Nubank, esse mapeamento é essencial para evitar multas milionárias da ANPD ou autoridades europeias, facilitando expansão global com conformidade dupla LGPD-GDPR em operações de dados sensíveis."
                            },
                            "estimatedTime": "1.5 horas",
                            "difficulty": "intermediate",
                            "status": "not_started",
                            "prerequisites": []
                          },
                          {
                            "id": "10.1.6.1.3.3",
                            "name": "Realizar DPIA (Data Protection Impact Assessment)",
                            "description": "Elaborar avaliações de impacto à proteção de dados para projetos de ciência de dados de alto risco, identificando medidas de mitigação antes da coleta e processamento.",
                            "atomicExpansion": {
                              "steps": [
                                {
                                  "stepNumber": 1,
                                  "title": "Determinar a necessidade de DPIA",
                                  "subSteps": [
                                    "Analisar o tipo de processamento de dados: verifique se envolve dados sensíveis, grande escala ou tecnologias novas como IA.",
                                    "Consultar listas de processamento de alto risco da ANPD ou GDPR (ex.: perfis automáticos, vigilância).",
                                    "Preencher questionário de screening oficial para confirmar obrigatoriedade.",
                                    "Documentar justificativa da decisão (obrigatório ou voluntário).",
                                    "Envolver stakeholders iniciais (DPO, equipe de projeto)."
                                  ],
                                  "verification": "Questionário de screening preenchido e decisão documentada com assinatura do responsável.",
                                  "estimatedTime": "1 hora",
                                  "materials": [
                                    "Questionário de screening DPIA (ANPD/GDPR)",
                                    "Lista de processamentos de alto risco",
                                    "Guia LGPD/ANPD"
                                  ],
                                  "tips": "Use checklists oficiais para evitar subestimação de riscos; envolva jurídico cedo.",
                                  "learningObjective": "Identificar corretamente quando um DPIA é requerido conforme regulamentações.",
                                  "commonMistakes": [
                                    "Ignorar processamentos em larga escala",
                                    "Não consultar DPO",
                                    "Decidir sem documentação"
                                  ]
                                },
                                {
                                  "stepNumber": 2,
                                  "title": "Descrever o processamento de dados",
                                  "subSteps": [
                                    "Mapear fluxo de dados: coleta, armazenamento, processamento, compartilhamento e descarte.",
                                    "Identificar dados pessoais envolvidos (categorias, volume, fontes).",
                                    "Descrever propósitos, destinatários e transferências internacionais.",
                                    "Listar responsáveis (controlador, operador) e bases legais (LGPD art. 7).",
                                    "Incluir contexto do projeto de ciência de dados (modelos ML, etc.)."
                                  ],
                                  "verification": "Diagrama de fluxo de dados completo e tabela de dados pessoais aprovada pela equipe.",
                                  "estimatedTime": "2 horas",
                                  "materials": [
                                    "Templates DPIA (ANPD)",
                                    "Ferramentas de mapeamento como Draw.io ou Lucidchart",
                                    "Registro de atividades de tratamento (LGPD)"
                                  ],
                                  "tips": "Crie diagramas visuais para clareza; revise com equipe técnica.",
                                  "learningObjective": "Documentar de forma precisa e exaustiva o ciclo de vida dos dados.",
                                  "commonMistakes": [
                                    "Descrições vagas",
                                    "Omitir transferências",
                                    "Ignorar dados derivados de ML"
                                  ]
                                },
                                {
                                  "stepNumber": 3,
                                  "title": "Identificar e avaliar riscos",
                                  "subSteps": [
                                    "Listar potenciais violações de direitos (confidencialidade, integridade, disponibilidade).",
                                    "Avaliar gravidade e probabilidade de riscos (matriz risco: baixa/média/alta).",
                                    "Considerar vulnerabilidades específicas de ciência de dados (reattack, bias, re-identificação).",
                                    "Consultar fontes de ameaça (ameaças cibernéticas, insiders).",
                                    "Priorizar riscos com pontuação quantitativa (ex.: 1-5 para probabilidade x impacto)."
                                  ],
                                  "verification": "Matriz de riscos preenchida com pelo menos 5 riscos identificados e pontuados.",
                                  "estimatedTime": "3 horas",
                                  "materials": [
                                    "Matriz de risco DPIA template",
                                    "Guia de ameaças ENISA/ANPD",
                                    "Ferramenta Excel para matriz"
                                  ],
                                  "tips": "Use escalas padronizadas; envolva especialistas em segurança.",
                                  "learningObjective": "Avaliar riscos de forma sistemática e baseada em evidências.",
                                  "commonMistakes": [
                                    "Foco só em riscos técnicos",
                                    "Subestimar riscos não intencionais",
                                    "Sem priorização"
                                  ]
                                },
                                {
                                  "stepNumber": 4,
                                  "title": "Propor medidas de mitigação e avaliar riscos residuais",
                                  "subSteps": [
                                    "Para cada risco alto, propor controles (técnicos: anonimização; organizacionais: treinamentos).",
                                    "Verificar efetividade das medidas (redução de probabilidade/impacto).",
                                    "Calcular riscos residuais e justificar aceitabilidade.",
                                    "Incluir plano de monitoramento contínuo.",
                                    "Documentar custos e responsáveis pelas medidas."
                                  ],
                                  "verification": "Tabela de mitigação com riscos residuais todos em nível aceitável (média ou baixa).",
                                  "estimatedTime": "2 horas",
                                  "materials": [
                                    "Lista de controles Privacy by Design",
                                    "Templates de plano de ação",
                                    "Guia ANPD mitigação"
                                  ],
                                  "tips": "Priorize Privacy by Design; teste medidas em piloto se possível.",
                                  "learningObjective": "Desenvolver medidas eficazes que minimizem riscos residuais.",
                                  "commonMistakes": [
                                    "Medidas genéricas",
                                    "Não avaliar residuais",
                                    "Ignorar custos"
                                  ]
                                },
                                {
                                  "stepNumber": 5,
                                  "title": "Concluir, assinar e consultar autoridades",
                                  "subSteps": [
                                    "Resumir achados: riscos principais, mitigados e residuais.",
                                    "Obter assinaturas de responsáveis (DPO, controlador).",
                                    "Decidir sobre consulta à ANPD (riscos residuais altos).",
                                    "Arquivar DPIA e integrar ao projeto.",
                                    "Planejar revisão periódica (ex.: anual ou mudança de processamento)."
                                  ],
                                  "verification": "DPIA final assinado, arquivado e plano de revisão definido.",
                                  "estimatedTime": "1 hora",
                                  "materials": [
                                    "Modelo de relatório DPIA final",
                                    "Portal ANPD para submissão",
                                    "Ferramenta de versionamento como Google Docs"
                                  ],
                                  "tips": "Mantenha confidencialidade do documento; prepare para auditorias.",
                                  "learningObjective": "Finalizar DPIA de forma compliant e acionável.",
                                  "commonMistakes": [
                                    "Sem assinaturas",
                                    "Não planejar revisões",
                                    "Atrasar consulta à ANPD"
                                  ]
                                }
                              ],
                              "practicalExample": "Em um projeto de ciência de dados para prever churn de clientes em um banco, usando dados sensíveis como histórico financeiro e biométricos: 1) Screening confirma alto risco por profiling; 2) Mapa revela coleta via app; 3) Riscos: vazamento e bias discriminatório (alta prob.); 4) Mitigações: pseudonimização, auditoria de bias, criptografia; 5) DPIA assinado, riscos residuais médios, sem consulta ANPD.",
                              "finalVerifications": [
                                "DPIA completo com todas seções preenchidas e assinaturas.",
                                "Todos riscos altos mitigados para nível aceitável.",
                                "Fluxo de dados mapeado e documentado.",
                                "Plano de monitoramento e revisão definido.",
                                "Integração confirmada com plano do projeto.",
                                "Conformidade com LGPD/ANPD verificada."
                              ],
                              "assessmentCriteria": [
                                "Completude: Todas seções presentes e detalhadas.",
                                "Precisão: Riscos identificados corretamente com evidências.",
                                "Efetividade: Mitigações reduzem riscos adequadamente.",
                                "Documentação: Clara, visual e auditável.",
                                "Conformidade: Alinhada a guidelines ANPD/GDPR.",
                                "Praticidade: Medidas viáveis e com responsáveis."
                              ],
                              "crossCurricularConnections": [
                                "Ética em IA: Integração com princípios de fairness e accountability.",
                                "Segurança da Informação: Aplicação de controles CISPs.",
                                "Gestão de Projetos: Incorporação em ciclos ágeis.",
                                "Direito Digital: Conhecimento de LGPD e GDPR.",
                                "Matemática Computacional: Mitigação de riscos em modelos ML."
                              ],
                              "realWorldApplication": "Em empresas de dados como bancos ou healthtechs, DPIA é obrigatório antes de projetos de ML com dados pessoais, evitando multas da ANPD (até 2% faturamento) e garantindo confiança de usuários, como no caso do Google com GDPR fines por falta de DPIA."
                            },
                            "estimatedTime": "2 horas",
                            "difficulty": "advanced",
                            "status": "not_started",
                            "prerequisites": []
                          }
                        ]
                      },
                      {
                        "id": "10.1.6.1.4",
                        "name": "Proteção de Dados nas Etapas da Ciência de Dados",
                        "description": "Estratégias específicas para salvaguardar dados durante coleta, armazenamento, limpeza e modelagem, integrando privacidade by design.",
                        "specificSkills": [
                          {
                            "id": "10.1.6.1.4.1",
                            "name": "Proteger dados na coleta",
                            "description": "Implementar criptografia em trânsito (TLS) e validação de fontes para prevenir vazamentos durante aquisição de dados de APIs ou formulários.",
                            "atomicExpansion": {
                              "steps": [
                                {
                                  "stepNumber": 1,
                                  "title": "Compreender Fundamentos de TLS e Validação de Fontes",
                                  "subSteps": [
                                    "Estude o que é TLS/SSL e como funciona a criptografia em trânsito.",
                                    "Aprenda sobre certificados digitais e autoridades certificadoras (CAs).",
                                    "Entenda validação de fontes: CORS, pinning de certificados e verificação de domínios.",
                                    "Revise ataques comuns como MITM (Man-in-the-Middle) e injeção de fontes maliciosas.",
                                    "Analise exemplos de vazamentos de dados em APIs públicas sem proteção."
                                  ],
                                  "verification": "Resuma em um diagrama ou parágrafo os conceitos chave e identifique 3 riscos prevenidos por TLS.",
                                  "estimatedTime": "45 minutos",
                                  "materials": [
                                    "Documentação Mozilla TLS: https://developer.mozilla.org/en-US/docs/Web/Security/Transport_Layer_Security",
                                    "Vídeo Khan Academy sobre criptografia básica",
                                    "Notebook para anotações"
                                  ],
                                  "tips": "Use analogias como 'carteiro confiável' para TLS para fixar conceitos.",
                                  "learningObjective": "Explicar como TLS protege dados em trânsito e por que validar fontes previne vazamentos.",
                                  "commonMistakes": "Confundir TLS com criptografia em repouso; ignorar que HTTP é inseguro por padrão."
                                },
                                {
                                  "stepNumber": 2,
                                  "title": "Configurar Criptografia TLS em um Endpoint de Coleta",
                                  "subSteps": [
                                    "Instale um servidor local com suporte HTTPS (ex: Node.js com Express e https module).",
                                    "Gere ou obtenha certificados autoassinados usando OpenSSL para testes.",
                                    "Configure o servidor para forçar TLS: defina portas HTTPS e redirecione HTTP.",
                                    "Integre em um formulário ou API endpoint para coleta de dados (ex: POST /collect-data).",
                                    "Teste conexão básica com curl ou browser para confirmar HTTPS ativo."
                                  ],
                                  "verification": "Acesse o endpoint via HTTPS sem erros de certificado e confirme handshake TLS bem-sucedido via dev tools.",
                                  "estimatedTime": "1 hora",
                                  "materials": [
                                    "Node.js instalado",
                                    "OpenSSL: openssl req -newkey rsa:2048 -nodes -keyout key.pem -x509 -days 365 -out certificate.pem",
                                    "Código template Express HTTPS: https://expressjs.com/en/starter/https.html"
                                  ],
                                  "tips": "Use mkcert para certificados locais confiáveis pelo browser sem warnings.",
                                  "learningObjective": "Implementar TLS em um servidor para criptografar dados de APIs e formulários.",
                                  "commonMistakes": "Usar certificados expirados ou não configurar HSTS para forçar HTTPS."
                                },
                                {
                                  "stepNumber": 3,
                                  "title": "Implementar Validação de Fontes e Controles de Acesso",
                                  "subSteps": [
                                    "Configure CORS no servidor para permitir apenas domínios confiáveis.",
                                    "Adicione verificação de certificados no cliente (ex: rejectUnauthorized em Node.js requests).",
                                    "Implemente token-based auth ou API keys para validar fontes de requisições.",
                                    "Adicione headers de segurança como Content-Security-Policy (CSP) contra injeções.",
                                    "Logue tentativas de acesso inválido para monitoramento."
                                  ],
                                  "verification": "Tente requisições de domínios não permitidos e confirme rejeição (erro 403/ CORS).",
                                  "estimatedTime": "45 minutos",
                                  "materials": [
                                    "Documentação CORS MDN",
                                    "Biblioteca helmet.js para Node.js: npm install helmet",
                                    "Postman para testar requisições cross-origin"
                                  ],
                                  "tips": "Comece com CORS '*' para testes, mas restrinja imediatamente para produção.",
                                  "learningObjective": "Validar origens de dados para prevenir vazamentos de fontes não autorizadas.",
                                  "commonMistakes": "Deixar CORS muito permissivo, permitindo qualquer origem acessar dados sensíveis."
                                },
                                {
                                  "stepNumber": 4,
                                  "title": "Testar e Validar a Proteção Contra Vazamentos",
                                  "subSteps": [
                                    "Use ferramentas como Wireshark para capturar tráfego e confirmar criptografia.",
                                    "Simule ataques MITM com ferramentas como mitmproxy e verifique falha.",
                                    "Teste formulários/APIs com dados sensíveis e inspecione network tab no browser.",
                                    "Execute scans de segurança com OWASP ZAP ou similar para vulnerabilidades TLS.",
                                    "Documente resultados em um relatório de teste."
                                  ],
                                  "verification": "Nenhum dado plano visível em tráfego de rede; todas validações passam sem erros.",
                                  "estimatedTime": "1 hora",
                                  "materials": [
                                    "Wireshark ou Fiddler",
                                    "mitmproxy: pip install mitmproxy",
                                    "OWASP ZAP gratuito"
                                  ],
                                  "tips": "Filtre pacotes por porta 443 no Wireshark para focar em TLS.",
                                  "learningObjective": "Verificar empiricamente que dados estão protegidos durante coleta.",
                                  "commonMistakes": "Ignorar warnings de certificado autoassinado como 'falha real'."
                                }
                              ],
                              "practicalExample": "Crie um formulário web para coletar dados de usuários (nome, email). Configure servidor Node.js com TLS e CORS restrito a seu domínio. Teste envio de formulário via HTTPS de um site autorizado (sucesso) e de outro (bloqueado), confirmando no Wireshark que dados são criptografados.",
                              "finalVerifications": [
                                "Tráfego de rede mostra apenas dados criptografados (TLS 1.2+).",
                                "Requisições de origens inválidas são rejeitadas com erro CORS/403.",
                                "Certificados são validados sem erros no cliente.",
                                "Logs mostram bloqueio de fontes suspeitas.",
                                "Scan de segurança OWASP não reporta vulnerabilidades de coleta.",
                                "Dados coletados permanecem confidenciais em testes simulados."
                              ],
                              "assessmentCriteria": [
                                "Implementação correta de TLS com handshake bem-sucedido (80% peso).",
                                "Validação de fontes eficaz contra cross-origin malicioso (15% peso).",
                                "Testes abrangentes cobrindo MITM e vazamentos (5% peso).",
                                "Código limpo com headers de segurança e logging.",
                                "Relatório de verificação completo e preciso.",
                                "Tempo de resposta da API sob carga segura."
                              ],
                              "crossCurricularConnections": [
                                "Ética em Dados: Alinha com princípios GDPR para privacidade by design.",
                                "Matemática: Aplicação de algoritmos assimétricos (RSA/ECDH) em criptografia.",
                                "Programação: Integração com linguagens como Python (requests com verify=True) ou JS.",
                                "Segurança Cibernética: Conceitos de threat modeling em coleta de dados."
                              ],
                              "realWorldApplication": "Em apps de e-commerce como Shopify, TLS protege dados de cartões em formulários de checkout; em APIs de saúde como Fitbit, validação de fontes previne roubo de dados biométricos durante sync com wearables."
                            },
                            "estimatedTime": "1.5 horas",
                            "difficulty": "intermediate",
                            "status": "not_started",
                            "prerequisites": []
                          },
                          {
                            "id": "10.1.6.1.4.2",
                            "name": "Garantir segurança no armazenamento",
                            "description": "Usar criptografia em repouso (AES), controle de acesso (RBAC) e auditoria de logs em bancos de dados para armazenamento seguro de dados sensíveis.",
                            "atomicExpansion": {
                              "steps": [
                                {
                                  "stepNumber": 1,
                                  "title": "Implementar Criptografia em Repouso com AES",
                                  "subSteps": [
                                    "Estude os conceitos básicos de criptografia AES-256 e seu uso em dados em repouso.",
                                    "Instale uma biblioteca de criptografia compatível, como pgcrypto no PostgreSQL ou cryptography no Python.",
                                    "Crie uma tabela de banco de dados com coluna para dados sensíveis.",
                                    "Aplique criptografia AES nas inserções e decrifração nas consultas usando chaves gerenciadas.",
                                    "Teste a criptografia armazenando e recuperando um dado sensível."
                                  ],
                                  "verification": "Verifique se os dados armazenados na tabela aparecem criptografados (não legíveis) ao consultar diretamente o banco.",
                                  "estimatedTime": "45 minutos",
                                  "materials": "PostgreSQL ou SQLite, biblioteca cryptography (Python), chave de criptografia gerada.",
                                  "tips": "Use chaves derivadas de senhas fortes com PBKDF2 para maior segurança.",
                                  "learningObjective": "Compreender e aplicar criptografia AES para proteger dados sensíveis em repouso.",
                                  "commonMistakes": "Armazenar chaves junto aos dados criptografados; esquecer de gerenciar rotação de chaves."
                                },
                                {
                                  "stepNumber": 2,
                                  "title": "Configurar Controle de Acesso com RBAC",
                                  "subSteps": [
                                    "Defina roles como 'admin', 'user' e 'auditor' com permissões específicas (ler, escrever, auditar).",
                                    "Crie usuários e atribua roles usando GRANT/REVOKE em SQL.",
                                    "Configure políticas de acesso baseadas em roles para tabelas sensíveis.",
                                    "Teste acessos com diferentes usuários para validar restrições.",
                                    "Documente a matriz de permissões RBAC."
                                  ],
                                  "verification": "Tente acessar dados com um usuário sem permissão e confirme que recebe erro de acesso negado.",
                                  "estimatedTime": "40 minutos",
                                  "materials": "Banco de dados PostgreSQL ou MySQL com suporte a RBAC, ferramenta de gerenciamento como pgAdmin.",
                                  "tips": "Princípio do menor privilégio: conceda apenas permissões necessárias.",
                                  "learningObjective": "Implementar RBAC para controlar granularmente o acesso a dados sensíveis.",
                                  "commonMistakes": "Conceder permissões excessivas a roles; não testar cenários de falha de autenticação."
                                },
                                {
                                  "stepNumber": 3,
                                  "title": "Configurar Auditoria de Logs para Acessos",
                                  "subSteps": [
                                    "Habilite logging de consultas no banco de dados (ex: log_statement no PostgreSQL).",
                                    "Crie triggers ou extensões para logar acessos a tabelas sensíveis com timestamp, usuário e ação.",
                                    "Configure retenção de logs por 90 dias e rotação automática.",
                                    "Simule acessos e verifique se logs capturam eventos corretamente.",
                                    "Integre logs com uma ferramenta de visualização como ELK Stack (opcional)."
                                  ],
                                  "verification": "Execute consultas como diferentes usuários e confirme que logs registram usuário, ação, timestamp e dados acessados.",
                                  "estimatedTime": "50 minutos",
                                  "materials": "PostgreSQL com extensões pgaudit, arquivo de configuração postgresql.conf.",
                                  "tips": "Mascare dados sensíveis nos logs para evitar vazamentos secundários.",
                                  "learningObjective": "Estabelecer auditoria contínua para rastrear e investigar acessos a dados.",
                                  "commonMistakes": "Desabilitar logs em produção por performance sem otimização; ignorar logs de falhas de autenticação."
                                },
                                {
                                  "stepNumber": 4,
                                  "title": "Integrar e Testar o Sistema Completo de Armazenamento Seguro",
                                  "subSteps": [
                                    "Combine criptografia, RBAC e logs em um banco de dados único.",
                                    "Crie scripts de teste para cenários de uso normal e ataques simulados.",
                                    "Monitore performance e ajuste configurações se necessário.",
                                    "Realize uma auditoria simulada revisando logs.",
                                    "Documente procedimentos de manutenção e recuperação."
                                  ],
                                  "verification": "Sistema resiste a acessos não autorizados, dados permanecem criptografados e todos acessos são logados.",
                                  "estimatedTime": "60 minutos",
                                  "materials": "Ambiente de teste com banco de dados completo, scripts Python/SQL para testes automatizados.",
                                  "tips": "Use ferramentas como Docker para isolar o ambiente de teste.",
                                  "learningObjective": "Integrar mecanismos de segurança para um armazenamento robusto e verificável.",
                                  "commonMistakes": "Não testar integrações; subestimar impacto na performance de criptografia."
                                }
                              ],
                              "practicalExample": "Em um sistema de saúde, armazene dados de pacientes (nome, CPF, histórico médico) em uma tabela PostgreSQL: criptografe o histórico com AES, restrinja acesso via RBAC (médicos leem/escrevem, recepcionistas só leem nome), e audite todos acessos para conformidade com LGPD.",
                              "finalVerifications": [
                                "Dados sensíveis estão criptografados em repouso e indecifráveis sem chave.",
                                "Usuários sem role apropriada não acessam dados.",
                                "Logs capturam 100% dos acessos com detalhes completos.",
                                "Sistema mantém performance aceitável (<10% overhead).",
                                "Recuperação de chaves e rotação funcionam corretamente.",
                                "Conformidade com regulamentações como LGPD/GDPR é demonstrada."
                              ],
                              "assessmentCriteria": [
                                "Precisão na implementação de AES sem vazamentos de chaves.",
                                "RBAC impede acessos indevidos em todos cenários testados.",
                                "Logs são completos, imutáveis e pesquisáveis.",
                                "Integração holística sem conflitos entre mecanismos.",
                                "Documentação clara de configuração e manutenção.",
                                "Testes cobrem casos edge e ataques simulados."
                              ],
                              "crossCurricularConnections": [
                                "Matemática: Criptografia AES baseia-se em álgebra linear e teoria dos números.",
                                "Ética e Direito: Alinhamento com LGPD e princípios de privacidade por design.",
                                "Programação: SQL avançado, triggers e bibliotecas de criptografia.",
                                "Gestão de TI: Políticas de segurança e conformidade em DevSecOps."
                              ],
                              "realWorldApplication": "Em bancos de dados de fintech para proteger saldos e transações; em saúde para históricos médicos sob HIPAA/LGPD; em e-commerce para dados de cartões de crédito, prevenindo breaches como o Equifax."
                            },
                            "estimatedTime": "2 horas",
                            "difficulty": "intermediate",
                            "status": "not_started",
                            "prerequisites": []
                          },
                          {
                            "id": "10.1.6.1.4.3",
                            "name": "Aplicar privacidade na manipulação e análise",
                            "description": "Integrar técnicas de differential privacy em análises exploratórias e treinamento de modelos para evitar inferências sobre indivíduos específicos.",
                            "atomicExpansion": {
                              "steps": [
                                {
                                  "stepNumber": 1,
                                  "title": "Compreender os Conceitos Fundamentais de Privacidade Diferencial",
                                  "subSteps": [
                                    "Estudar a definição de privacidade diferencial (DP): mecanismo que adiciona ruído aos dados para limitar inferências sobre indivíduos.",
                                    "Aprender parâmetros chave: ε (epsilon) para privacidade, δ (delta) para falha probabilística.",
                                    "Explorar mecanismos básicos: Laplace (para contagens), Gaussian (para médias).",
                                    "Analisar propriedades: pure DP vs. approximate DP.",
                                    "Resolver exercícios teóricos sobre composição de consultas."
                                  ],
                                  "verification": "Explicar em palavras próprias os parâmetros ε e δ e calcular o ruído necessário para uma consulta simples.",
                                  "estimatedTime": "2 horas",
                                  "materials": [
                                    "Documentação oficial do OpenDP",
                                    "Artigo 'Deep Learning with Differential Privacy' de Abadi et al.",
                                    "Notebook Jupyter introdutório de DP"
                                  ],
                                  "tips": "Comece com exemplos visuais de adição de ruído para intuitar o conceito.",
                                  "learningObjective": "Dominar os princípios teóricos da privacidade diferencial para aplicação prática.",
                                  "commonMistakes": [
                                    "Confundir ε com precisão dos dados",
                                    "Ignorar o trade-off entre privacidade e utilidade",
                                    "Não considerar composição de múltiplas consultas"
                                  ]
                                },
                                {
                                  "stepNumber": 2,
                                  "title": "Preparar o Ambiente e o Conjunto de Dados",
                                  "subSteps": [
                                    "Instalar bibliotecas: Opacus (PyTorch), TensorFlow Privacy ou diffprivlib.",
                                    "Carregar um dataset sensível simulado (ex: Adult UCI para renda).",
                                    "Pré-processar dados: anonimizar IDs, normalizar features.",
                                    "Definir níveis de privacidade (ε=1.0, δ=1e-5).",
                                    "Testar carregamento com verificação de integridade."
                                  ],
                                  "verification": "Executar script de setup sem erros e gerar relatório de shape e estatísticas básicas do dataset.",
                                  "estimatedTime": "1 hora",
                                  "materials": [
                                    "Python 3.9+",
                                    "Bibliotecas: diffprivlib, pandas, numpy",
                                    "Dataset Adult UCI do UCI ML Repository"
                                  ],
                                  "tips": "Use ambientes virtuais (conda) para isolar dependências de privacidade.",
                                  "learningObjective": "Configurar um ambiente pronto para experimentos com DP.",
                                  "commonMistakes": [
                                    "Não calibrar δ adequadamente",
                                    "Usar datasets reais sem consentimento simulado",
                                    "Ignorar overhead computacional"
                                  ]
                                },
                                {
                                  "stepNumber": 3,
                                  "title": "Implementar Privacidade Diferencial em Análises Exploratórias",
                                  "subSteps": [
                                    "Aplicar mecanismo Laplace a contagens e médias (ex: histograma de idades com ruído).",
                                    "Usar diffprivlib para queries privadas: mean, histogram, count.",
                                    "Visualizar comparações: dados originais vs. privatizados.",
                                    "Avaliar utilidade: medir erro médio quadrático (MSE) entre versões.",
                                    "Iterar com diferentes ε para observar trade-offs."
                                  ],
                                  "verification": "Gerar gráficos de EDA privada e calcular utilidade > 80% em relação ao original.",
                                  "estimatedTime": "3 horas",
                                  "materials": [
                                    "diffprivlib",
                                    "matplotlib, seaborn",
                                    "Notebook Jupyter para EDA"
                                  ],
                                  "tips": "Sempre plote antes/depois para validar visualmente a preservação de padrões.",
                                  "learningObjective": "Executar EDA sem comprometer privacidade individual.",
                                  "commonMistakes": [
                                    "Adicionar ruído fixo sem calibração",
                                    "Sobreprivatizar (ε muito baixo)",
                                    "Não testar sensibilidade das queries"
                                  ]
                                },
                                {
                                  "stepNumber": 4,
                                  "title": "Integrar Privacidade Diferencial no Treinamento de Modelos",
                                  "subSteps": [
                                    "Configurar DP-SGD (Stochastic Gradient Descent) com Opacus ou TF Privacy.",
                                    "Treinar modelo simples (logistic regression ou NN) em dataset preparado.",
                                    "Monitorar perda de privacidade durante epochs via accountant.",
                                    "Comparar accuracy: modelo privado vs. não privado.",
                                    "Ajustar clipping e noise_multiplier para otimizar."
                                  ],
                                  "verification": "Treinar modelo com ε<5.0 e accuracy >70% do baseline não privado.",
                                  "estimatedTime": "4 horas",
                                  "materials": [
                                    "Opacus (PyTorch)",
                                    "Dataset preparado do Step 2",
                                    "GPU recomendada para aceleração"
                                  ],
                                  "tips": "Comece com batch_size pequeno para depuração rápida.",
                                  "learningObjective": "Treinar modelos ML com garantias de privacidade.",
                                  "commonMistakes": [
                                    "Exceder orçamento de privacidade por epochs excessivas",
                                    "Não clipar gradients corretamente",
                                    "Ignorar sampling rate na composição"
                                  ]
                                }
                              ],
                              "practicalExample": "Em um dataset de registros hospitalares (ex: idades e diagnósticos de pacientes), aplicar DP-Laplace para calcular a média de idades por diagnóstico, adicionando ruído calibrado (ε=1.0), permitindo análise agregada sem risco de identificar pacientes individuais.",
                              "finalVerifications": [
                                "Explicar e calcular ruído para uma query específica.",
                                "Implementar EDA privada em dataset novo.",
                                "Treinar modelo DP-SGD com orçamento controlado.",
                                "Comparar utilidade vs. privacidade quantitativamente.",
                                "Identificar violações potenciais em cenários reais.",
                                "Documentar trade-offs em relatório."
                              ],
                              "assessmentCriteria": [
                                "Correta calibração de parâmetros ε/δ (erro <10%).",
                                "Utilidade preservada (>75% do baseline).",
                                "Código reproduzível e comentado.",
                                "Análise de trade-offs explícita.",
                                "Tratamento de composição de queries.",
                                "Verificação empírica de privacidade (auditoria)."
                              ],
                              "crossCurricularConnections": [
                                "Ética em IA: Princípios de privacidade por design.",
                                "Direito Digital: Conformidade com LGPD/GDPR.",
                                "Estatística: Inferência e ruído probabilístico.",
                                "Criptografia: Mecanismos homomórficos complementares.",
                                "Gestão de Dados: Pipelines seguros em big data."
                              ],
                              "realWorldApplication": "Empresas como Google (RAPPOR) e Apple usam DP para análises agregadas em telemetria de usuários, liberando estatísticas de uso de apps sem expor dados pessoais, atendendo regulamentações como GDPR."
                            },
                            "estimatedTime": "2.5 horas",
                            "difficulty": "advanced",
                            "status": "not_started",
                            "prerequisites": []
                          }
                        ]
                      }
                    ]
                  },
                  {
                    "id": "10.1.6.2",
                    "name": "Consentimento Informado",
                    "description": "Requisitos éticos para obtenção de consentimento explícito e informado dos indivíduos antes do uso de seus dados.",
                    "individualConcepts": [
                      {
                        "id": "10.1.6.2.1",
                        "name": "Definição e Princípios do Consentimento Informado",
                        "description": "Compreensão fundamental do que constitui consentimento informado, incluindo seus princípios éticos basilares no contexto da manipulação de dados pessoais.",
                        "specificSkills": [
                          {
                            "id": "10.1.6.2.1.1",
                            "name": "Definir Consentimento Informado",
                            "description": "Explicar o consentimento informado como a aprovação voluntária, específica e inequívoca de um indivíduo, após ser adequadamente informado sobre o uso de seus dados pessoais, destacando sua relevância na ética da ciência de dados.",
                            "atomicExpansion": {
                              "steps": [
                                {
                                  "stepNumber": 1,
                                  "title": "Compreender a Definição Básica de Consentimento Informado",
                                  "subSteps": [
                                    "Leia a definição oficial: 'aprovação voluntária, específica e inequívoca de um indivíduo, após ser adequadamente informado sobre o uso de seus dados pessoais'.",
                                    "Identifique e anote os quatro elementos principais: voluntária, específica, inequívoca e informada.",
                                    "Compare com consentimento genérico (ex.: aceite de termos de uso sem detalhes).",
                                    "Escreva a definição com suas próprias palavras.",
                                    "Pesquise origens legais, como LGPD ou GDPR."
                                  ],
                                  "verification": "Recitar a definição completa e listar os quatro elementos sem consulta.",
                                  "estimatedTime": "25 minutos",
                                  "materials": [
                                    "Texto da definição da LGPD (artigo 8º)",
                                    "Dicionário jurídico online",
                                    "Bloco de notas"
                                  ],
                                  "tips": "Use cores para destacar cada elemento chave na definição.",
                                  "learningObjective": "Memorizar e internalizar a definição fundamental do consentimento informado.",
                                  "commonMistakes": [
                                    "Confundir com 'autorização automática'",
                                    "Ignorar o aspecto 'inequívoco'",
                                    "Omitir a necessidade de informação adequada"
                                  ]
                                },
                                {
                                  "stepNumber": 2,
                                  "title": "Explorar os Princípios Chave do Consentimento",
                                  "subSteps": [
                                    "Analise 'voluntária': discuta ausência de coação ou pressão.",
                                    "Estude 'específica': consentimento deve ser para propósitos definidos, não amplos.",
                                    "Examine 'inequívoca': deve ser clara, sem ambiguidades (ex.: checkbox obrigatório).",
                                    "Detalhe 'informada': indivíduo deve receber informações claras sobre dados, uso e riscos.",
                                    "Crie um fluxograma ilustrando os quatro princípios."
                                  ],
                                  "verification": "Explicar cada princípio com um exemplo pessoal.",
                                  "estimatedTime": "30 minutos",
                                  "materials": [
                                    "Infográfico sobre LGPD",
                                    "Vídeo explicativo de 5 minutos sobre GDPR",
                                    "Ferramenta de fluxograma como Draw.io"
                                  ],
                                  "tips": "Associe cada princípio a uma situação cotidiana para fixar.",
                                  "learningObjective": "Diferenciar e aplicar os quatro princípios essenciais.",
                                  "commonMistakes": [
                                    "Achar que 'voluntária' significa só 'clique sim'",
                                    "Confundir específica com genérica",
                                    "Subestimar riscos na informação"
                                  ]
                                },
                                {
                                  "stepNumber": 3,
                                  "title": "Entender o Processo de Obtenção e Revogação",
                                  "subSteps": [
                                    "Descreva o processo: fornecer informação clara → confirmar compreensão → obter consentimento explícito.",
                                    "Discuta meios válidos: escrito, eletrônico com registro, verbal em contextos limitados.",
                                    "Aprenda sobre revogação: deve ser tão fácil quanto conceder, a qualquer momento.",
                                    "Simule um formulário de consentimento com campos obrigatórios.",
                                    "Identifique violações comuns em sites reais."
                                  ],
                                  "verification": "Criar um modelo simples de formulário de consentimento válido.",
                                  "estimatedTime": "35 minutos",
                                  "materials": [
                                    "Modelo de formulário LGPD",
                                    "Exemplos de pop-ups de cookies",
                                    "Editor de texto como Google Docs"
                                  ],
                                  "tips": "Teste o formulário com um colega para validar clareza.",
                                  "learningObjective": "Dominar os passos práticos para obter consentimento ético.",
                                  "commonMistakes": [
                                    "Omitir opção de revogação",
                                    "Usar jargão técnico no texto",
                                    "Fazer consentimento 'por omissão'"
                                  ]
                                },
                                {
                                  "stepNumber": 4,
                                  "title": "Relacionar com Ética na Ciência de Dados",
                                  "subSteps": [
                                    "Explique relevância: protege privacidade em coleta, processamento e compartilhamento de dados.",
                                    "Conecte a casos: análise de dados médicos ou marketing comportamental.",
                                    "Discuta impactos éticos: confiança pública, avoidance de multas (ex.: R$ 50 mi pela LGPD).",
                                    "Debata dilemas: anonimização vs. consentimento granular.",
                                    "Redija um parágrafo sobre por que é essencial na ciência de dados."
                                  ],
                                  "verification": "Escrever um ensaio curto (100 palavras) ligando à ética.",
                                  "estimatedTime": "30 minutos",
                                  "materials": [
                                    "Artigo sobre escândalos como Cambridge Analytica",
                                    "Guia ético de dados da ABNT",
                                    "Notícias recentes sobre multas LGPD"
                                  ],
                                  "tips": "Use exemplos reais para tornar a conexão mais impactante.",
                                  "learningObjective": "Compreender o papel do consentimento na ética de dados.",
                                  "commonMistakes": [
                                    "Isolar de contextos reais",
                                    "Subestimar sanções legais",
                                    "Confundir com conformidade técnica apenas"
                                  ]
                                }
                              ],
                              "practicalExample": "Em um app de saúde como o Google Fit, o usuário recebe uma tela detalhando que seus dados de passos e batimentos serão usados para treinar modelos de IA preditiva, com opções de aceitar/rejeitar/revogar, checkboxes específicos para cada uso, sem pré-marcação.",
                              "finalVerifications": [
                                "Definir corretamente os quatro elementos sem erros.",
                                "Explicar processo de obtenção em 5 passos.",
                                "Identificar violações em um formulário real.",
                                "Relacionar a um caso ético de ciência de dados.",
                                "Diferenciar consentimento de outras autorizações.",
                                "Listar 3 consequências de não obter consentimento válido."
                              ],
                              "assessmentCriteria": [
                                "Precisão na definição (100% dos elementos corretos).",
                                "Profundidade nos princípios (exemplos relevantes).",
                                "Clareza no processo de obtenção/revogação.",
                                "Conexão ética com ciência de dados (relevância demonstrada).",
                                "Criatividade em exemplos práticos.",
                                "Ausência de equívocos comuns."
                              ],
                              "crossCurricularConnections": [
                                "Direito Digital (LGPD e regulamentações de privacidade).",
                                "Psicologia (compreensão e tomada de decisão informada).",
                                "Educação Cívica (direitos digitais e cidadania).",
                                "Tecnologia da Informação (implementação em sistemas).",
                                "Filosofia (ética utilitária vs. deontológica em dados)."
                              ],
                              "realWorldApplication": "Na ciência de dados para empresas como bancos ou e-commerces, garante conformidade com LGPD ao coletar dados de clientes para personalização, evitando multas milionárias e construindo confiança, como no caso do WhatsApp multado em R$ 100 mi por compartilhamento sem consentimento granular."
                            },
                            "estimatedTime": "1 hora",
                            "difficulty": "beginner",
                            "status": "not_started",
                            "prerequisites": []
                          },
                          {
                            "id": "10.1.6.2.1.2",
                            "name": "Identificar Princípios Essenciais",
                            "description": "Listar e descrever os princípios chave: voluntariedade (sem coação), informação clara (propósito, duração, riscos), especificidade (para fins definidos) e revogabilidade (direito de retirar a qualquer momento).",
                            "atomicExpansion": {
                              "steps": [
                                {
                                  "stepNumber": 1,
                                  "title": "Introdução aos Princípios do Consentimento Informado",
                                  "subSteps": [
                                    "Pesquise a definição geral de consentimento informado no contexto ético de dados.",
                                    "Identifique os quatro princípios essenciais mencionados: voluntariedade, informação clara, especificidade e revogabilidade.",
                                    "Crie um mapa mental ligando cada princípio ao consentimento.",
                                    "Leia exemplos reais de consentimento em sites como GDPR ou LGPD.",
                                    "Anote por que esses princípios são cruciais na ciência de dados."
                                  ],
                                  "verification": "Você consegue listar e nomear corretamente os quatro princípios com uma frase curta para cada.",
                                  "estimatedTime": "15 minutos",
                                  "materials": [
                                    "Acesso à internet",
                                    "Papel e caneta ou ferramenta digital como MindMeister"
                                  ],
                                  "tips": "Comece com fontes confiáveis como sites oficiais de ética em dados para evitar informações erradas.",
                                  "learningObjective": "Reconhecer os quatro princípios fundamentais do consentimento informado.",
                                  "commonMistakes": [
                                    "Confundir consentimento com mera permissão",
                                    "Ignorar o contexto ético específico de dados pessoais"
                                  ]
                                },
                                {
                                  "stepNumber": 2,
                                  "title": "Explorar o Princípio da Voluntariedade",
                                  "subSteps": [
                                    "Defina voluntariedade como ausência de coação ou pressão.",
                                    "Analise exemplos: consentimento livre vs. sob ameaça de perda de serviço.",
                                    "Crie dois cenários: um voluntário e um coercitivo em coleta de dados.",
                                    "Discuta com um colega ou anote diferenças chave.",
                                    "Resuma em uma frase: 'Voluntariedade significa...'"
                                  ],
                                  "verification": "Explique com um exemplo pessoal por que um consentimento sob pressão não é válido.",
                                  "estimatedTime": "20 minutos",
                                  "materials": [
                                    "Exemplos de casos reais (vídeos ou artigos sobre ética em apps)"
                                  ],
                                  "tips": "Pense em situações como redes sociais pedindo dados sem alternativas.",
                                  "learningObjective": "Compreender e aplicar o conceito de voluntariedade sem coação.",
                                  "commonMistakes": [
                                    "Subestimar coações sutis como 'default opt-in'",
                                    "Achar que recompensas invalidam voluntariedade"
                                  ]
                                },
                                {
                                  "stepNumber": 3,
                                  "title": "Analisar o Princípio da Informação Clara",
                                  "subSteps": [
                                    "Liste os elementos: propósito, duração e riscos devem ser claros.",
                                    "Compare um formulário de consentimento claro vs. vago (ex: termos de uso longos).",
                                    "Escreva um exemplo de texto claro para um projeto de dados.",
                                    "Identifique jargões comuns e simplifique-os.",
                                    "Verifique se sua descrição cobre todos os três elementos."
                                  ],
                                  "verification": "Redija um parágrafo de informação clara para um cenário hipotético de coleta de dados.",
                                  "estimatedTime": "25 minutos",
                                  "materials": [
                                    "Modelos de formulários de consentimento online"
                                  ],
                                  "tips": "Use linguagem simples, evite termos técnicos sem explicação.",
                                  "learningObjective": "Desenvolver habilidades para criar informações transparentes sobre dados.",
                                  "commonMistakes": [
                                    "Omitir riscos potenciais",
                                    "Ser muito genérico sobre propósito"
                                  ]
                                },
                                {
                                  "stepNumber": 4,
                                  "title": "Examinar o Princípio da Especificidade",
                                  "subSteps": [
                                    "Defina especificidade como consentimento para fins definidos, não genéricos.",
                                    "Diferencie: 'usar dados para pesquisa' vs. 'vender para marketing'.",
                                    "Crie um fluxograma de como especificar usos em um dataset.",
                                    "Analise um caso real de violação (ex: Cambridge Analytica).",
                                    "Reescreva um consentimento genérico para torná-lo específico."
                                  ],
                                  "verification": "Transforme uma permissão ampla em uma específica com 3 fins detalhados.",
                                  "estimatedTime": "20 minutos",
                                  "materials": [
                                    "Casos de estudo éticos em ciência de dados"
                                  ],
                                  "tips": "Sempre pergunte: 'Para quê exatamente esses dados serão usados?'",
                                  "learningObjective": "Garantir que consentimentos sejam limitados a propósitos precisos.",
                                  "commonMistakes": [
                                    "Permitir 'qualquer uso futuro'",
                                    "Não listar fins exatos"
                                  ]
                                },
                                {
                                  "stepNumber": 5,
                                  "title": "Compreender o Princípio da Revogabilidade",
                                  "subSteps": [
                                    "Explique revogabilidade como direito de retirar consentimento a qualquer momento.",
                                    "Descreva mecanismos: botão 'retirar', e-mail, etc.",
                                    "Simule um processo de revogação em um app fictício.",
                                    "Discuta consequências: exclusão de dados ao revogar.",
                                    "Integre com os outros princípios em um resumo final."
                                  ],
                                  "verification": "Desenhe um diagrama simples de como implementar revogação em um sistema de dados.",
                                  "estimatedTime": "20 minutos",
                                  "materials": [
                                    "Ferramentas de design como Draw.io"
                                  ],
                                  "tips": "Torne o processo de revogação tão fácil quanto o de consentir.",
                                  "learningObjective": "Implementar e valorizar o direito de revogação no manejo de dados.",
                                  "commonMistakes": [
                                    "Fazer revogação difícil ou oculta",
                                    "Ignorar obrigações pós-revogação"
                                  ]
                                }
                              ],
                              "practicalExample": "Em um projeto de análise de dados de saúde usando machine learning, ao coletar dados de usuários: explique o propósito (treinar modelo de predição de doenças), duração (2 anos), riscos (breach de privacidade), obtenha sem coação, especifique só para esse modelo, e permita revogação via app com deleção imediata dos dados.",
                              "finalVerifications": [
                                "Liste os quatro princípios com definições precisas.",
                                "Forneça um exemplo e contraexemplo para cada princípio.",
                                "Crie um formulário de consentimento completo incorporando todos os princípios.",
                                "Explique como violar um princípio afeta a ética de um projeto de dados.",
                                "Identifique princípios em um formulário real de consentimento online.",
                                "Resuma os princípios em um infográfico simples."
                              ],
                              "assessmentCriteria": [
                                "Precisão e completude das descrições dos quatro princípios (30%)",
                                "Qualidade e relevância dos exemplos práticos (25%)",
                                "Capacidade de integrar princípios em cenários reais (20%)",
                                "Clareza na comunicação e ausência de jargões desnecessários (15%)",
                                "Evidência de compreensão de interconexões entre princípios (10%)"
                              ],
                              "crossCurricularConnections": [
                                "Direito: Relação com LGPD/GDPR e direitos fundamentais.",
                                "Psicologia: Influências cognitivas e vieses em decisões de consentimento.",
                                "Filosofia: Ética kantiana e autonomia individual.",
                                "Ciência da Computação: Implementação técnica de consentimento em sistemas.",
                                "Comunicação: Redação clara de políticas de privacidade."
                              ],
                              "realWorldApplication": "Na ciência de dados, aplicar esses princípios garante conformidade legal (ex: LGPD), previne escândalos como vazamentos éticos, constrói confiança pública em IA e permite datasets robustos e éticos para modelos de ML."
                            },
                            "estimatedTime": "1 hora",
                            "difficulty": "beginner",
                            "status": "not_started",
                            "prerequisites": []
                          },
                          {
                            "id": "10.1.6.2.1.3",
                            "name": "Diferenciar Tipos de Consentimento",
                            "description": "Comparar consentimento explícito (opt-in ativo) versus implícito (opt-out), justificando por que o explícito é preferido em ciência de dados para conformidade ética e legal.",
                            "atomicExpansion": {
                              "steps": [
                                {
                                  "stepNumber": 1,
                                  "title": "Definir e Compreender Consentimento Explícito (Opt-in Ativo)",
                                  "subSteps": [
                                    "Pesquise a definição oficial de consentimento explícito de fontes como GDPR ou LGPD.",
                                    "Identifique características chave: ação afirmativa, voluntária, informada e específica.",
                                    "Analise exemplos reais, como checkboxes obrigatórios para newsletters.",
                                    "Registre diferenças de contextos onde é obrigatório.",
                                    "Crie um diagrama visual representando o fluxo de opt-in."
                                  ],
                                  "verification": "Crie um resumo de 100 palavras definindo opt-in e liste 3 exemplos; revise se cobre todos os elementos chave.",
                                  "estimatedTime": "45 minutos",
                                  "materials": [
                                    "Acesso à internet para GDPR/LGPD docs",
                                    "Papel e caneta ou ferramenta de diagrama como Draw.io"
                                  ],
                                  "tips": "Use analogias cotidianas, como assinar um contrato, para fixar o conceito.",
                                  "learningObjective": "Compreender os pilares do consentimento explícito e sua aplicação prática.",
                                  "commonMistakes": [
                                    "Confundir com pré-selecionado (pré-opt-in)",
                                    "Ignorar requisito de revocabilidade"
                                  ]
                                },
                                {
                                  "stepNumber": 2,
                                  "title": "Definir e Compreender Consentimento Implícito (Opt-out)",
                                  "subSteps": [
                                    "Pesquise definições de consentimento implícito em regulamentações éticas.",
                                    "Liste traços: ausência de objeção, contexto presumido, menos rigoroso.",
                                    "Examine cenários como cookies de sessão em sites.",
                                    "Compare riscos éticos iniciais com opt-in.",
                                    "Desenhe um fluxograma mostrando como o usuário 'aceita por inação'."
                                  ],
                                  "verification": "Escreva um parágrafo explicando opt-out com 2 exemplos; verifique se destaca passividade.",
                                  "estimatedTime": "40 minutos",
                                  "materials": [
                                    "Documentos regulatórios online",
                                    "Ferramenta de fluxograma"
                                  ],
                                  "tips": "Pense em situações onde 'não dizer não' é aceito, mas questione ética.",
                                  "learningObjective": "Identificar quando e por que o opt-out é usado, apesar de limitações.",
                                  "commonMistakes": [
                                    "Assumir que implícito é sempre inválido",
                                    "Não notar contextos onde é permitido"
                                  ]
                                },
                                {
                                  "stepNumber": 3,
                                  "title": "Comparar Consentimento Explícito versus Implícito",
                                  "subSteps": [
                                    "Crie uma tabela comparativa: colunas para definição, requisitos, riscos, exemplos.",
                                    "Discuta prós e contras de cada tipo em termos de usabilidade e conformidade.",
                                    "Simule cenários: um para opt-in e um para opt-out em app de dados.",
                                    "Avalie impactos em privacidade do usuário.",
                                    "Debata em voz alta diferenças chave."
                                  ],
                                  "verification": "Produza tabela comparativa completa; confirme se tem pelo menos 5 linhas de comparação.",
                                  "estimatedTime": "50 minutos",
                                  "materials": [
                                    "Planilha Google Sheets ou Excel",
                                    "Exemplos de sites reais"
                                  ],
                                  "tips": "Use cores na tabela para destacar forças (verde) e fraquezas (vermelho).",
                                  "learningObjective": "Desenvolver habilidade de análise comparativa crítica.",
                                  "commonMistakes": [
                                    "Focar só em definição sem riscos",
                                    "Ignorar nuances contextuais"
                                  ]
                                },
                                {
                                  "stepNumber": 4,
                                  "title": "Justificar Preferência pelo Explícito em Ciência de Dados",
                                  "subSteps": [
                                    "Revise leis como GDPR Artigo 7 (consentimento deve ser granular e opt-in).",
                                    "Analise casos reais de multas por opt-out inadequado (ex: Google fines).",
                                    "Discuta ética: autonomia do usuário, minimização de dados, accountability.",
                                    "Elabore argumentos para cenários de dados sensíveis em ML.",
                                    "Escreva uma justificativa de 200 palavras."
                                  ],
                                  "verification": "Redija justificativa e liste 3 leis/casos suportando; autoavalie clareza.",
                                  "estimatedTime": "55 minutos",
                                  "materials": [
                                    "Casos de estudo de violações de privacidade",
                                    "Editor de texto"
                                  ],
                                  "tips": "Ligue a princípios éticos como 'do no harm' em dados.",
                                  "learningObjective": "Argumentar racionalmente a superioridade ética/legal do opt-in.",
                                  "commonMistakes": [
                                    "Generalizar sem evidências legais",
                                    "Subestimar usabilidade como contra-argumento"
                                  ]
                                }
                              ],
                              "practicalExample": "Em um projeto de análise de dados de saúde, implemente um formulário onde usuários devem clicar 'Aceito compartilhar meus dados para pesquisa' (opt-in explícito) em vez de ter um botão 'Não compartilhar' pré-selecionado (opt-out implícito), garantindo conformidade GDPR e rastreabilidade.",
                              "finalVerifications": [
                                "Defina corretamente opt-in e opt-out com exemplos precisos.",
                                "Explique 3 diferenças chave entre os tipos.",
                                "Justifique com pelo menos 2 leis por que opt-in é preferido em dados.",
                                "Identifique riscos éticos do opt-out em cenários de ciência de dados.",
                                "Crie uma tabela comparativa sem erros factuais.",
                                "Aplique conceitos a um caso real de privacidade de dados."
                              ],
                              "assessmentCriteria": [
                                "Precisão conceitual (definições corretas: 30%)",
                                "Profundidade comparativa (análise equilibrada: 25%)",
                                "Justificativa ética/legal robusta (evidências: 20%)",
                                "Clareza e estrutura (tabelas/diagramas: 15%)",
                                "Aplicação prática (exemplos relevantes: 10%)"
                              ],
                              "crossCurricularConnections": [
                                "Direito: Conformidade com GDPR/LGPD e direitos fundamentais.",
                                "Psicologia: Comportamento do usuário e vieses de inação.",
                                "Tecnologia da Informação: Implementação de UI/UX para consentimento.",
                                "Filosofia Ética: Autonomia e princípio de não-maleficência."
                              ],
                              "realWorldApplication": "Em empresas como Meta ou Google, usar opt-in explícito para tracking de usuários evita multas milionárias (ex: €1.2B GDPR fine em 2023), promovendo confiança e reutilização ética de dados em IA."
                            },
                            "estimatedTime": "0.5 hora",
                            "difficulty": "intermediate",
                            "status": "not_started",
                            "prerequisites": []
                          }
                        ]
                      },
                      {
                        "id": "10.1.6.2.2",
                        "name": "Requisitos Éticos e Legais",
                        "description": "Análise dos padrões éticos e regulamentações que regem a obtenção de consentimento informado, com foco em frameworks aplicáveis à ciência de dados.",
                        "specificSkills": [
                          {
                            "id": "10.1.6.2.2.1",
                            "name": "Descrever Requisitos da LGPD e GDPR",
                            "description": "Explicar os artigos relevantes da Lei Geral de Proteção de Dados (LGPD) no Brasil e do GDPR na Europa, como a necessidade de consentimento granular, livre e informado para processamento de dados sensíveis.",
                            "atomicExpansion": {
                              "steps": [
                                {
                                  "stepNumber": 1,
                                  "title": "Compreender os Fundamentos e Princípios da LGPD",
                                  "subSteps": [
                                    "Acessar o texto integral da Lei nº 13.709/2018 (LGPD) no site da ANPD",
                                    "Identificar o objeto e escopo: proteção de dados pessoais de pessoas naturais no Brasil",
                                    "Listar e definir os 10 princípios fundamentais (finalidade, adequação, necessidade, transparência, etc.)",
                                    "Estudar definições chave: dado pessoal, dado sensível, controlador, operador e ANPD",
                                    "Explicar a base legal para tratamento de dados (Art. 7º)"
                                  ],
                                  "verification": "Escrever um resumo de 200 palavras cobrindo escopo, princípios e definições, com citação de artigos",
                                  "estimatedTime": "1.5 horas",
                                  "materials": [
                                    "Texto oficial da LGPD (ANPD.gov.br)",
                                    "Guia introdutório ANPD PDF",
                                    "Vídeo explicativo ANPD no YouTube"
                                  ],
                                  "tips": "Comece pelos princípios, pois eles fundamentam todos os requisitos de consentimento",
                                  "learningObjective": "Dominar a estrutura legal e princípios da LGPD como base para requisitos de proteção de dados",
                                  "commonMistakes": [
                                    "Confundir dados pessoais com dados anonimizados",
                                    "Ignorar o papel da ANPD como autoridade fiscalizadora"
                                  ]
                                },
                                {
                                  "stepNumber": 2,
                                  "title": "Analisar os Fundamentos e Princípios do GDPR",
                                  "subSteps": [
                                    "Acessar o Regulamento (UE) 2016/679 no site oficial da UE",
                                    "Identificar escopo: dados pessoais de residentes na UE, aplicável globalmente para targeting",
                                    "Listar princípios chave (legalidade, imparcialidade, transparência, limitação de propósito, minimização, etc.) - Art. 5º",
                                    "Estudar definições: dado pessoal, especial (sensível), controlador, processador e DPA",
                                    "Explicar bases legais para processamento (Art. 6º) e processamento de dados especiais (Art. 9º)"
                                  ],
                                  "verification": "Criar uma tabela comparativa inicial de princípios LGPD vs. GDPR (5 colunas: princípio, LGPD, GDPR, similaridades, diferenças)",
                                  "estimatedTime": "1.5 horas",
                                  "materials": [
                                    "Texto oficial GDPR (eur-lex.europa.eu)",
                                    "Guia EDPB em inglês/português",
                                    "Infográfico GDPR vs LGPD"
                                  ],
                                  "tips": "Use traduções oficiais para evitar erros de interpretação de termos jurídicos",
                                  "learningObjective": "Entender a estrutura e princípios do GDPR, preparando para comparações com LGPD",
                                  "commonMistakes": [
                                    "Achar que GDPR só vale na Europa",
                                    "Não diferenciar 'dados especiais' de 'sensíveis'"
                                  ]
                                },
                                {
                                  "stepNumber": 3,
                                  "title": "Explorar Requisitos de Consentimento para Dados Sensíveis na LGPD e GDPR",
                                  "subSteps": [
                                    "Na LGPD: Estudar Art. 7º (bases), Art. 8º (consentimento livre, informado, inequívoco, granular) e Art. 11 (sensíveis)",
                                    "No GDPR: Analisar Art. 4(11) (definição consentimento), Art. 7 (condições), Art. 9 (sensíveis: explícito, granular)",
                                    "Identificar elementos comuns: granularidade (opt-in específico), livre (sem coação), informado (detalhes claros)",
                                    "Mapear proibições e exceções para dados sensíveis (saúde, biometria, origem racial, etc.)",
                                    "Revisar sanções: multas até 2% faturamento LGPD, 4% GDPR"
                                  ],
                                  "verification": "Redigir um fluxograma de consentimento para dados sensíveis, citando artigos de ambas as leis",
                                  "estimatedTime": "2 horas",
                                  "materials": [
                                    "Artigos destacados LGPD/GDPR PDFs",
                                    "Casos ANPD/EDPB",
                                    "Template de fluxograma (Draw.io)"
                                  ],
                                  "tips": "Enfatize 'granular': consentimento deve ser por propósito específico, não genérico",
                                  "learningObjective": "Descrever com precisão os requisitos de consentimento granular para dados sensíveis em ambas as leis",
                                  "commonMistakes": [
                                    "Aceitar consentimento implícito para sensíveis",
                                    "Não citar necessidade de prova de consentimento"
                                  ]
                                },
                                {
                                  "stepNumber": 4,
                                  "title": "Comparar Requisitos e Sintetizar Descrições Integradas",
                                  "subSteps": [
                                    "Criar tabela comparativa detalhada: consentimento (LGPD Art.8 vs GDPR Art.7/4.11), sensíveis (Art.11 vs Art.9)",
                                    "Destacar similaridades (granular, livre, informado) e diferenças (explícito no GDPR para sensíveis)",
                                    "Simular cenários: consentimento para app de saúde processando dados brasileiros/europeus",
                                    "Desenvolver modelo de política de privacidade com cláusulas de consentimento",
                                    "Revisar atualizações recentes (ex: Regulamento ANPD 2023)"
                                  ],
                                  "verification": "Apresentar oralmente ou por escrito uma descrição completa dos requisitos, respondendo a 5 perguntas hipotéticas",
                                  "estimatedTime": "1 hora",
                                  "materials": [
                                    "Tabelas comparativas prontas (sites jurídicos)",
                                    "Modelos de consentimento GitHub",
                                    "Gravador de áudio para simulação"
                                  ],
                                  "tips": "Use exemplos reais de multas (ex: WhatsApp LGPD) para fixar importância",
                                  "learningObjective": "Sintetizar e comparar requisitos da LGPD e GDPR para aplicação prática",
                                  "commonMistakes": [
                                    "Superestimar similaridades, ignorando nuances como 'inequívoco' vs 'explícito'"
                                  ]
                                }
                              ],
                              "practicalExample": "Desenvolva um formulário de consentimento granular para um aplicativo de telemedicina que coleta dados de saúde (sensíveis): inclua checkboxes separados para 'armazenamento', 'compartilhamento com terceiros' e 'análise de IA', com links para política de privacidade citando LGPD Art. 8 e GDPR Art. 7/9, garantindo linguagem clara e opção de revogação fácil.",
                              "finalVerifications": [
                                "Listar corretamente 5 artigos chave da LGPD e 5 do GDPR sobre consentimento e sensíveis",
                                "Explicar granularidade com exemplo prático",
                                "Diferenciar consentimento 'livre e informado' (LGPD) de 'explícito' (GDPR para sensíveis)",
                                "Identificar 3 exceções para dados sensíveis em cada lei",
                                "Descrever sanções por não conformidade",
                                "Criar um modelo básico de aviso de consentimento válido"
                              ],
                              "assessmentCriteria": [
                                "Precisão na citação e interpretação de artigos (sem erros factuais)",
                                "Clareza e estrutura na descrição de requisitos (linguagem acessível)",
                                "Profundidade na explicação de consentimento granular/livre/informado",
                                "Capacidade de comparação entre LGPD e GDPR com exemplos",
                                "Uso de exemplos práticos e conexões com dados sensíveis",
                                "Completude: cobertura de princípios, definições e sanções"
                              ],
                              "crossCurricularConnections": [
                                "Direito Constitucional e Civil (direitos fundamentais à privacidade)",
                                "Ética em Tecnologia e Ciência de Dados (responsabilidade no processamento)",
                                "Gestão de Projetos de Software (implementação de compliance)",
                                "Administração Empresarial (políticas de governança de dados)",
                                "Cibersegurança (proteção técnica de dados sensíveis)"
                              ],
                              "realWorldApplication": "Em empresas como bancos digitais ou e-commerces (ex: Magazine Luiza multada por LGPD), usar esses requisitos para criar banners de consentimento cookie-compliant com GDPR para usuários europeus e formulários LGPD para brasileiros, evitando multas de até R$50mi (LGPD) ou €20mi (GDPR) e construindo confiança do usuário."
                            },
                            "estimatedTime": "1.5 horas",
                            "difficulty": "intermediate",
                            "status": "not_started",
                            "prerequisites": []
                          },
                          {
                            "id": "10.1.6.2.2.2",
                            "name": "Avaliar Capacidade do Indivíduo",
                            "description": "Identificar critérios para validar a capacidade do titular dos dados (maioridade, compreensão mental), incluindo exemplos de vulnerabilidades como crianças ou pessoas com deficiências.",
                            "atomicExpansion": {
                              "steps": [
                                {
                                  "stepNumber": 1,
                                  "title": "Compreender Critérios Legais de Capacidade",
                                  "subSteps": [
                                    "Pesquisar idade mínima para consentimento informado (ex: 18 anos no Brasil pela LGPD).",
                                    "Estudar definições legais de maioridade e emancipação civil.",
                                    "Analisar legislações como LGPD, GDPR e Marco Civil da Internet sobre capacidade.",
                                    "Identificar exceções legais, como consentimento por representantes.",
                                    "Comparar critérios em diferentes jurisdições (Brasil vs. UE)."
                                  ],
                                  "verification": "Elaborar uma tabela com 3 critérios legais e suas fontes.",
                                  "estimatedTime": "45 minutos",
                                  "materials": [
                                    "Texto da LGPD (site oficial ANPD)",
                                    "GDPR Artigo 8",
                                    "Artigos jurídicos sobre consentimento"
                                  ],
                                  "tips": "Priorize fontes oficiais para evitar informações desatualizadas.",
                                  "learningObjective": "Dominar os fundamentos legais de capacidade para consentimento.",
                                  "commonMistakes": [
                                    "Confundir maioridade com capacidade mental.",
                                    "Ignorar variações por país.",
                                    "Não considerar emancipação de menores."
                                  ]
                                },
                                {
                                  "stepNumber": 2,
                                  "title": "Identificar Grupos Vulneráveis",
                                  "subSteps": [
                                    "Listar vulnerabilidades clássicas: crianças, adolescentes e idosos.",
                                    "Estudar deficiências mentais (ex: demência, autismo grave).",
                                    "Analisar exemplos: incapazes relativos (alcoolismo, drogadição).",
                                    "Pesquisar indicadores de vulnerabilidade (testes cognitivos simples).",
                                    "Documentar exemplos reais de casos judiciais envolvendo vulneráveis."
                                  ],
                                  "verification": "Criar uma lista com 5 grupos vulneráveis e justificativas.",
                                  "estimatedTime": "30 minutos",
                                  "materials": [
                                    "LGPD Artigo 5º (incapazes)",
                                    "Manuais de bioética",
                                    "Casos do STJ sobre consentimento"
                                  ],
                                  "tips": "Use exemplos concretos para fixar conceitos abstratos.",
                                  "learningObjective": "Reconhecer perfis de indivíduos com capacidade reduzida.",
                                  "commonMistakes": [
                                    "Limitar a crianças apenas.",
                                    "Não diferenciar incapazes absolutos de relativos.",
                                    "Subestimar vulnerabilidades em adultos saudáveis."
                                  ]
                                },
                                {
                                  "stepNumber": 3,
                                  "title": "Desenvolver Métodos de Avaliação de Compreensão Mental",
                                  "subSteps": [
                                    "Aprender perguntas de teste: 'Explique os riscos em suas palavras?'.",
                                    "Estudar ferramentas: Teach-back method e questionários de capacidade.",
                                    "Praticar avaliação de compreensão: simular diálogos com 'pacientes'.",
                                    "Integrar verificações visuais: confusão, hesitação ou inconsistências.",
                                    "Documentar protocolos éticos para avaliação (confidencialidade)."
                                  ],
                                  "verification": "Realizar uma simulação gravada avaliando compreensão fictícia.",
                                  "estimatedTime": "60 minutos",
                                  "materials": [
                                    "Vídeos tutoriais de teach-back",
                                    "Modelos de questionários (OMS)",
                                    "Gravador de áudio"
                                  ],
                                  "tips": "Registre respostas para autoavaliação posterior.",
                                  "learningObjective": "Aplicar técnicas práticas para validar compreensão mental.",
                                  "commonMistakes": [
                                    "Aceitar 'sim' sem verificação profunda.",
                                    "Ignorar barreiras linguísticas.",
                                    "Não registrar evidências da avaliação."
                                  ]
                                },
                                {
                                  "stepNumber": 4,
                                  "title": "Praticar Avaliação Integrada e Documentação",
                                  "subSteps": [
                                    "Simular cenários completos: formulário de dados com perfil vulnerável.",
                                    "Combinar critérios legais, vulnerabilidades e testes mentais.",
                                    "Elaborar relatório de avaliação com decisão (apto/incapaz).",
                                    "Revisar e refinar protocolo pessoal de avaliação.",
                                    "Discutir dilemas éticos em grupo ou fórum."
                                  ],
                                  "verification": "Produzir relatório de 1 cenário com decisão justificada.",
                                  "estimatedTime": "45 minutos",
                                  "materials": [
                                    "Cenários simulados impressos",
                                    "Modelo de relatório LGPD",
                                    "Fórum online ético"
                                  ],
                                  "tips": "Sempre priorize o 'não consentimento' em dúvida.",
                                  "learningObjective": "Integrar todos os elementos em uma avaliação ética.",
                                  "commonMistakes": [
                                    "Documentar insuficientemente.",
                                    "Ignorar contexto cultural.",
                                    "Apressar a decisão sem evidências."
                                  ]
                                }
                              ],
                              "practicalExample": "Em uma pesquisa de dados de saúde via app, ao receber dados de um usuário de 16 anos com histórico de autismo, aplicar teste de compreensão: perguntar 'O que acontece se você compartilhar seus dados?' e verificar maioridade via documento; se falhar, buscar responsável legal.",
                              "finalVerifications": [
                                "Listar corretamente 4 critérios legais de capacidade.",
                                "Identificar 5 exemplos de vulnerabilidades com justificativa.",
                                "Demonstrar teach-back em simulação gravada.",
                                "Produzir relatório completo de avaliação simulada.",
                                "Explicar 3 diferenças entre capacidade legal e mental.",
                                "Discutir impacto de erro em multa LGPD."
                              ],
                              "assessmentCriteria": [
                                "Precisão na identificação de critérios legais (80% acerto).",
                                "Profundidade na descrição de vulnerabilidades (mínimo 4 exemplos).",
                                "Qualidade da simulação de avaliação (clareza e evidências).",
                                "Completude do relatório (todos campos preenchidos).",
                                "Integração ética e conexões interdisciplinares.",
                                "Criatividade em exemplos práticos."
                              ],
                              "crossCurricularConnections": [
                                "Direito Civil: Incapazes e representação legal.",
                                "Psicologia: Avaliação cognitiva e vulnerabilidades mentais.",
                                "Bioética: Princípios de autonomia e beneficência.",
                                "Educação: Alfabetização digital para consentimento.",
                                "Saúde Pública: Consentimento em pesquisas clínicas."
                              ],
                              "realWorldApplication": "Em empresas de tecnologia (ex: apps de saúde ou redes sociais), implementar checklists de capacidade para evitar violações da LGPD, multas de até 2% do faturamento e processos judiciais por coleta indevida de dados de vulneráveis."
                            },
                            "estimatedTime": "1 hora",
                            "difficulty": "intermediate",
                            "status": "not_started",
                            "prerequisites": []
                          },
                          {
                            "id": "10.1.6.2.2.3",
                            "name": "Analisar Provas de Consentimento",
                            "description": "Discutir métodos para registrar e provar o consentimento, como logs digitais, assinaturas eletrônicas e auditorias, garantindo rastreabilidade em projetos de dados.",
                            "atomicExpansion": {
                              "steps": [
                                {
                                  "stepNumber": 1,
                                  "title": "Compreender Conceitos Fundamentais de Provas de Consentimento",
                                  "subSteps": [
                                    "Definir consentimento informado e suas características essenciais (voluntário, específico, informado e revogável).",
                                    "Explicar a importância de provas rastreáveis para compliance legal e ético em projetos de dados.",
                                    "Identificar requisitos legais chave, como LGPD no Brasil e GDPR na Europa.",
                                    "Discutir riscos de ausência de provas, como multas e perda de confiança.",
                                    "Mapear componentes de uma prova válida: timestamp, identidade do usuário e ação explícita."
                                  ],
                                  "verification": "Escrever um resumo de 200 palavras explicando os conceitos e requisitos, com exemplos.",
                                  "estimatedTime": "1 hora",
                                  "materials": [
                                    "Documentos da LGPD/GDPR (PDFs oficiais)",
                                    "Vídeos introdutórios sobre ética em dados (YouTube/Khan Academy)"
                                  ],
                                  "tips": [
                                    "Use analogias cotidianas, como assinaturas em contratos físicos, para fixar conceitos."
                                  ],
                                  "learningObjective": "Dominar os fundamentos teóricos de provas de consentimento e sua relevância ética/legal.",
                                  "commonMistakes": [
                                    "Confundir consentimento com mera aceitação de termos genéricos.",
                                    "Ignorar a revogabilidade do consentimento."
                                  ]
                                },
                                {
                                  "stepNumber": 2,
                                  "title": "Explorar Métodos de Registro Digital de Consentimento",
                                  "subSteps": [
                                    "Estudar logs digitais: estrutura (timestamp, IP, ação, hash de confirmação).",
                                    "Analisar assinaturas eletrônicas: ferramentas como DocuSign ou Adobe Sign e certificados digitais.",
                                    "Investigar formulários web com checkboxes duplos e gravações de vídeo/áudio de consentimento.",
                                    "Comparar métodos: prós e contras em termos de usabilidade e segurança.",
                                    "Praticar criação de um log simples usando ferramentas como Google Forms com timestamps."
                                  ],
                                  "verification": "Criar um exemplo de log digital para um cenário fictício e documentar sua estrutura.",
                                  "estimatedTime": "1.5 horas",
                                  "materials": [
                                    "Ferramentas gratuitas: Google Forms, Timestamp online generators",
                                    "Tutoriais sobre assinaturas eletrônicas (sites oficiais)"
                                  ],
                                  "tips": [
                                    "Sempre inclua hashes criptográficos para imutabilidade dos logs."
                                  ],
                                  "learningObjective": "Identificar e aplicar métodos práticos para registrar consentimento de forma digital.",
                                  "commonMistakes": [
                                    "Usar métodos não auditáveis como screenshots simples.",
                                    "Esquecer de registrar tentativas de revogação."
                                  ]
                                },
                                {
                                  "stepNumber": 3,
                                  "title": "Estudar Auditorias e Rastreabilidade de Provas",
                                  "subSteps": [
                                    "Definir auditoria de consentimento: processos de verificação periódica e sob demanda.",
                                    "Aprender técnicas de rastreabilidade: chains de blocos, blockchain simples para logs imutáveis.",
                                    "Simular uma auditoria: verificar integridade, autenticidade e temporalidade das provas.",
                                    "Explorar ferramentas de auditoria como ELK Stack (Elasticsearch, Logstash, Kibana) para análise de logs.",
                                    "Analisar falhas comuns em rastreabilidade, como edição posterior de logs."
                                  ],
                                  "verification": "Realizar uma auditoria simulada em um log criado no step anterior e relatar achados.",
                                  "estimatedTime": "2 horas",
                                  "materials": [
                                    "Ferramentas: Blockchain explorers gratuitos, ELK Stack demo online",
                                    "Casos de estudo de breaches de dados (Equifax, Cambridge Analytica)"
                                  ],
                                  "tips": [
                                    "Teste a imutabilidade alterando um log e verificando detecção de mudanças."
                                  ],
                                  "learningObjective": "Compreender e aplicar técnicas de auditoria para garantir rastreabilidade.",
                                  "commonMistakes": [
                                    "Confiar apenas em metadados sem verificação criptográfica.",
                                    "Não considerar ataques de replay em logs."
                                  ]
                                },
                                {
                                  "stepNumber": 4,
                                  "title": "Analisar Provas de Consentimento em Cenários Práticos",
                                  "subSteps": [
                                    "Escolher um cenário real: projeto de app de saúde coletando dados sensíveis.",
                                    "Coletar e analisar provas existentes: identificar gaps em logs e assinaturas.",
                                    "Propor melhorias: integrar auditorias automáticas e alertas de revogação.",
                                    "Avaliar conformidade com leis: checklist LGPD/GDPR aplicado.",
                                    "Documentar relatório de análise com recomendações acionáveis."
                                  ],
                                  "verification": "Produzir um relatório de 1 página analisando um caso e propondo soluções.",
                                  "estimatedTime": "1.5 horas",
                                  "materials": [
                                    "Cenários fictícios ou casos reais anonimizados",
                                    "Templates de relatórios de compliance"
                                  ],
                                  "tips": [
                                    "Priorize cenários de alto risco, como dados biométricos."
                                  ],
                                  "learningObjective": "Aplicar análise crítica de provas em contextos reais de projetos de dados.",
                                  "commonMistakes": [
                                    "Focar só em conformidade técnica, ignorando usabilidade para o usuário.",
                                    "Não quantificar riscos em termos de multas potenciais."
                                  ]
                                }
                              ],
                              "practicalExample": "Em um projeto de aplicativo de telemedicina, analise os logs de consentimento de 100 usuários: verifique timestamps, assinaturas eletrônicas via DocuSign e rastreabilidade via blockchain. Identifique que 20% dos logs faltam hashes, proponha auditoria com ELK Stack e implemente revogação automática.",
                              "finalVerifications": [
                                "Listar e descrever 5 métodos válidos de prova de consentimento com exemplos.",
                                "Simular uma auditoria completa em um log digital fornecido.",
                                "Explicar como garantir rastreabilidade em um projeto de dados escalável.",
                                "Aplicar checklist LGPD/GDPR a um cenário real e identificar gaps.",
                                "Propor soluções para falhas comuns em provas de consentimento.",
                                "Demonstrar criação de um log imutável usando ferramentas simples."
                              ],
                              "assessmentCriteria": [
                                "Precisão e completude na identificação de métodos de registro (80% cobertura).",
                                "Profundidade na análise de rastreabilidade e auditorias (incluindo criptografia).",
                                "Relevância e viabilidade das recomendações práticas.",
                                "Integração correta de aspectos legais (LGPD/GDPR) na análise.",
                                "Clareza e estrutura do relatório final.",
                                "Criatividade em conexões com ferramentas reais do mercado."
                              ],
                              "crossCurricularConnections": [
                                "Direito Digital: Conformidade com LGPD e GDPR em contextos legais.",
                                "Segurança da Informação: Criptografia e imutabilidade de logs.",
                                "Ética Filosófica: Princípios de autonomia e privacidade do usuário.",
                                "Desenvolvimento de Software: Integração de APIs de consentimento em apps."
                              ],
                              "realWorldApplication": "Em empresas de big data como Nubank ou iFood, analisar provas de consentimento garante compliance regulatório, evita multas milionárias da ANPD e constrói confiança com usuários ao demonstrar transparência em manipulação de dados pessoais."
                            },
                            "estimatedTime": "0.5 hora",
                            "difficulty": "intermediate",
                            "status": "not_started",
                            "prerequisites": []
                          }
                        ]
                      },
                      {
                        "id": "10.1.6.2.3",
                        "name": "Implementação na Ciência de Dados",
                        "description": "Aplicação prática do consentimento informado em ciclos de vida de projetos de ciência de dados, desde coleta até modelagem.",
                        "specificSkills": [
                          {
                            "id": "10.1.6.2.3.1",
                            "name": "Elaborar Formulários de Consentimento",
                            "description": "Criar modelos de formulários ou pop-ups que incluam linguagem clara, checkboxes separados por propósito e links para políticas de privacidade, adaptados a cenários de coleta de dados.",
                            "atomicExpansion": {
                              "steps": [
                                {
                                  "stepNumber": 1,
                                  "title": "Pesquisar Requisitos Legais e Éticos para Consentimento",
                                  "subSteps": [
                                    "Identificar leis aplicáveis como LGPD, GDPR ou CCPA relevantes ao contexto de coleta de dados.",
                                    "Analisar exemplos de formulários de consentimento de sites reais (ex: Google, Facebook).",
                                    "Listar elementos obrigatórios: linguagem clara, opções granulares, revogação fácil.",
                                    "Documentar princípios éticos: transparência, voluntariedade e minimização de dados.",
                                    "Criar um checklist inicial de conformidade."
                                  ],
                                  "verification": "Checklist preenchido com pelo menos 10 itens e fontes citadas.",
                                  "estimatedTime": "45 minutos",
                                  "materials": [
                                    "Acesso à internet",
                                    "Documentos legais (LGPD/GDPR)",
                                    "Bloco de notas ou editor de texto"
                                  ],
                                  "tips": "Priorize leis do país-alvo; use fontes oficiais como sites governamentais.",
                                  "learningObjective": "Compreender o arcabouço legal e ético que fundamenta formulários de consentimento.",
                                  "commonMistakes": [
                                    "Ignorar diferenças regionais de leis",
                                    "Copiar formulários sem entender o porquê",
                                    "Subestimar a necessidade de revogação"
                                  ]
                                },
                                {
                                  "stepNumber": 2,
                                  "title": "Definir Propósitos e Cenários de Coleta de Dados",
                                  "subSteps": [
                                    "Descrever o cenário específico (ex: app de saúde coletando dados biométricos).",
                                    "Listar propósitos granulares (ex: análise estatística, marketing personalizado).",
                                    "Mapear dados coletados por propósito para checkboxes separados.",
                                    "Garantir que cada checkbox seja independente e opcional.",
                                    "Esboçar fluxograma de como o consentimento afeta o fluxo de dados."
                                  ],
                                  "verification": "Mapa de propósitos com pelo menos 3 checkboxes distintos e fluxograma simples.",
                                  "estimatedTime": "30 minutos",
                                  "materials": [
                                    "Papel ou ferramenta de diagrama (Draw.io, Miro)",
                                    "Descrição do cenário hipotético"
                                  ],
                                  "tips": "Use verbos ativos nos propósitos para clareza; evite bundling de consentimentos.",
                                  "learningObjective": "Mapear coleta de dados de forma granular e transparente.",
                                  "commonMistakes": [
                                    "Consentimento único para múltiplos propósitos",
                                    "Propósitos vagos como 'melhorar serviços'"
                                  ]
                                },
                                {
                                  "stepNumber": 3,
                                  "title": "Projetar Estrutura e Layout do Formulário",
                                  "subSteps": [
                                    "Escolher formato: formulário web, pop-up ou PDF.",
                                    "Criar título claro e introdução explicativa.",
                                    "Posicionar checkboxes separados com labels descritivos.",
                                    "Incluir links clicáveis para políticas de privacidade e termos.",
                                    "Adicionar botão de confirmação e opção de revogação."
                                  ],
                                  "verification": "Wireframe ou mockup visual do formulário com todos elementos marcados.",
                                  "estimatedTime": "40 minutos",
                                  "materials": [
                                    "Ferramentas de design (Figma, Canva, HTML/CSS básico)",
                                    "Exemplos de formulários reais"
                                  ],
                                  "tips": "Garanta acessibilidade: contraste alto, suporte a leitores de tela; teste em mobile.",
                                  "learningObjective": "Desenvolver uma interface intuitiva e legalmente compliant.",
                                  "commonMistakes": [
                                    "Checkboxes pré-marcados",
                                    "Links para políticas escondidos",
                                    "Layout sobrecarregado"
                                  ]
                                },
                                {
                                  "stepNumber": 4,
                                  "title": "Escrever Linguagem Clara e Implementar o Formulário",
                                  "subSteps": [
                                    "Redigir textos em linguagem simples, sem jargões (nível 8º ano).",
                                    "Validar checkboxes com descrições curtas e específicas.",
                                    "Integrar links para políticas externas ou anexos.",
                                    "Implementar em código se aplicável (HTML para protótipo).",
                                    "Adicionar data/hora e IP para auditoria."
                                  ],
                                  "verification": "Formulário protótipo funcional com textos revisados por clareza.",
                                  "estimatedTime": "50 minutos",
                                  "materials": [
                                    "Editor de código (VS Code)",
                                    "Ferramentas de validação de texto (Hemingway App)"
                                  ],
                                  "tips": "Leia em voz alta para testar fluidez; use bullet points para complexidade.",
                                  "learningObjective": "Produzir comunicação ética e compreensível para usuários leigos.",
                                  "commonMistakes": [
                                    "Termos longos e complexos",
                                    "Ausência de confirmação de leitura",
                                    "Links quebrados"
                                  ]
                                },
                                {
                                  "stepNumber": 5,
                                  "title": "Testar, Refinar e Validar Conformidade",
                                  "subSteps": [
                                    "Realizar teste de usabilidade com 3-5 pessoas simuladas.",
                                    "Verificar se todos checkboxes funcionam independentemente.",
                                    "Simular cenários de revogação e auditoria.",
                                    "Comparar com checklist inicial e ajustar.",
                                    "Documentar versão final com changelog."
                                  ],
                                  "verification": "Relatório de testes com feedback incorporado e formulário finalizado.",
                                  "estimatedTime": "35 minutos",
                                  "materials": [
                                    "Protótipo do formulário",
                                    "Grupo de teste (amigos ou auto-teste)",
                                    "Checklist do step 1"
                                  ],
                                  "tips": "Registre tempo de compreensão; mire em <2 minutos para leitura e consentimento.",
                                  "learningObjective": "Garantir eficácia e conformidade através de iterações.",
                                  "commonMistakes": [
                                    "Pular testes reais",
                                    "Ignorar feedback de não-especialistas",
                                    "Não atualizar políticas linkadas"
                                  ]
                                }
                              ],
                              "practicalExample": "Para um app de fitness coletando dados de passos e peso: formulário com checkboxes 'Permitir análise agregada de passos para rankings' (link para política de anonimato), 'Compartilhar peso com nutricionista parceiro' (link para termos de compartilhamento), e 'Receber e-mails promocionais' (opt-in separado). Inclui 'Li e entendo' com data e botão 'Confirmar'.",
                              "finalVerifications": [
                                "Formulário possui checkboxes granulares e independentes.",
                                "Linguagem é clara, com nível de leitura acessível.",
                                "Links para políticas funcionam e são proeminentes.",
                                "Opção de revogação é explícita e fácil.",
                                "Checklist de conformidade legal está 100% atendido.",
                                "Protótipo foi testado com usuários simulados."
                              ],
                              "assessmentCriteria": [
                                "Clareza e granularidade dos consentimentos (30%)",
                                "Conformidade com leis citadas (25%)",
                                "Usabilidade e design intuitivo (20%)",
                                "Documentação completa de processo (15%)",
                                "Inovação em adaptação ao cenário (10%)"
                              ],
                              "crossCurricularConnections": [
                                "Direito Digital: Aplicação prática de LGPD/GDPR.",
                                "Design de UX/UI: Princípios de interfaces acessíveis.",
                                "Programação Web: Implementação em HTML/JS.",
                                "Ética Filosófica: Análise de autonomia e transparência.",
                                "Comunicação: Redação técnica clara."
                              ],
                              "realWorldApplication": "Em plataformas como Google Forms ou sites e-commerce, esses formulários garantem conformidade regulatória, evitam multas milionárias (ex: Meta multada em €1.2B por GDPR) e constroem confiança do usuário, essencial em ciência de dados para retenção de dados éticos."
                            },
                            "estimatedTime": "1 hora",
                            "difficulty": "intermediate",
                            "status": "not_started",
                            "prerequisites": []
                          },
                          {
                            "id": "10.1.6.2.3.2",
                            "name": "Integrar em Etapas de Coleta de Dados",
                            "description": "Demonstrar como incorporar verificação de consentimento nas etapas iniciais de um pipeline de ciência de dados, usando ferramentas como formulários web ou APIs com autenticação.",
                            "atomicExpansion": {
                              "steps": [
                                {
                                  "stepNumber": 1,
                                  "title": "Identificar e Mapear Pontos de Coleta de Dados no Pipeline",
                                  "subSteps": [
                                    "Revise o pipeline de ciência de dados existente desde a coleta inicial.",
                                    "Liste todos os métodos de coleta: formulários web, APIs, scraping ou bancos de dados.",
                                    "Identifique pontos obrigatórios para verificação de consentimento, priorizando dados pessoais.",
                                    "Classifique o tipo de consentimento necessário (explícito para sensíveis, implícito para anônimos).",
                                    "Crie um diagrama de fluxo destacando inserções de verificação."
                                  ],
                                  "verification": "Diagrama ou tabela documentada aprovada, listando todos os pontos de coleta com consentimento mapeado.",
                                  "estimatedTime": "45 minutos",
                                  "materials": [
                                    "Ferramentas de diagramação (Draw.io, Lucidchart)",
                                    "Documentação do pipeline atual"
                                  ],
                                  "tips": "Comece pelo início do pipeline para evitar retrabalho posterior.",
                                  "learningObjective": "Mapear precisamente onde o consentimento impacta a coleta de dados.",
                                  "commonMistakes": [
                                    "Omitir coletas indiretas como cookies ou trackers.",
                                    "Confundir dados anônimos com dados pseudonimizados."
                                  ]
                                },
                                {
                                  "stepNumber": 2,
                                  "title": "Projetar o Mecanismo de Verificação de Consentimento",
                                  "subSteps": [
                                    "Selecione a ferramenta baseada no método de coleta: formulários web (HTML/JS) ou autenticação API (OAuth/JWT).",
                                    "Desenhe interface clara com checkbox de consentimento, texto explicativo e data/hora.",
                                    "Defina armazenamento seguro da prova de consentimento (hash, timestamp, ID do usuário).",
                                    "Inclua opção de revogação e link para política de privacidade.",
                                    "Prototipe e teste usabilidade com usuários simulados."
                                  ],
                                  "verification": "Protótipo funcional testado, com captura e exibição de consentimento.",
                                  "estimatedTime": "1 hora",
                                  "materials": [
                                    "HTML/CSS/JS para formulários",
                                    "Postman para testar APIs",
                                    "Google Forms como alternativa rápida"
                                  ],
                                  "tips": "Use linguagem acessível, evitando termos técnicos para garantir consentimento válido.",
                                  "learningObjective": "Criar mecanismos de consentimento user-friendly e legalmente robustos.",
                                  "commonMistakes": [
                                    "Checkbox pré-marcado (inválido legalmente).",
                                    "Texto vago que não explica uso dos dados."
                                  ]
                                },
                                {
                                  "stepNumber": 3,
                                  "title": "Implementar Verificação nas Etapas Iniciais do Pipeline",
                                  "subSteps": [
                                    "Integre o check de consentimento no código de coleta (ex: condicional antes de requests).",
                                    "Para formulários web: valide submission só com consentimento confirmado.",
                                    "Para APIs: verifique token/autenticação e armazene metadados com os dados coletados.",
                                    "Adicione logging de eventos de consentimento e rejeições.",
                                    "Atualize o pipeline para bloquear processamento sem consentimento válido."
                                  ],
                                  "verification": "Código executável que rejeita coleta sem consentimento e armazena prova corretamente.",
                                  "estimatedTime": "1.5 horas",
                                  "materials": [
                                    "Python (requests, Flask)",
                                    "Jupyter Notebook para protótipo",
                                    "Bibliotecas como PyJWT para autenticação"
                                  ],
                                  "tips": "Use variáveis de ambiente para chaves sensíveis de autenticação.",
                                  "learningObjective": "Incorporar verificações de consentimento de forma programática e segura.",
                                  "commonMistakes": [
                                    "Não validar consentimento em todos os fluxos paralelos.",
                                    "Armazenar consentimento em claro sem criptografia."
                                  ]
                                },
                                {
                                  "stepNumber": 4,
                                  "title": "Testar, Auditar e Documentar a Integração",
                                  "subSteps": [
                                    "Execute testes unitários: cenários com/sem consentimento.",
                                    "Teste integração end-to-end no pipeline completo.",
                                    "Audite logs e armazenamento para conformidade (ex: GDPR/LGPD).",
                                    "Crie documentação: como verificar/revogar consentimento e manutenção.",
                                    "Revise com pares ou stakeholders éticos."
                                  ],
                                  "verification": "Relatório de testes 100% passado e documentação completa no repositório.",
                                  "estimatedTime": "45 minutos",
                                  "materials": [
                                    "pytest ou unittest para Python",
                                    "Ferramentas de logging (ELK Stack)",
                                    "Markdown para docs"
                                  ],
                                  "tips": "Automatize testes de consentimento em CI/CD para validação contínua.",
                                  "learningObjective": "Garantir robustez e rastreabilidade da implementação ética.",
                                  "commonMistakes": [
                                    "Testar só casos positivos, ignorando falhas.",
                                    "Documentação incompleta que dificulta auditorias futuras."
                                  ]
                                }
                              ],
                              "practicalExample": "Em um pipeline de análise de sentimentos em redes sociais, antes de coletar tweets via API do Twitter, implemente OAuth 2.0: o usuário loga, consente explicitamente via checkbox ('Permitir uso de meus tweets para análise agregada?'), token é validado e armazenado com timestamp/hash junto aos dados coletados.",
                              "finalVerifications": [
                                "Pipeline rejeita 100% das coletas sem consentimento válido.",
                                "Prova de consentimento (timestamp, hash) associada a cada conjunto de dados.",
                                "Opção de revogação funciona e remove dados associados.",
                                "Logs auditáveis mostram todas as verificações de consentimento.",
                                "Conformidade com LGPD/GDPR verificada em amostra de dados.",
                                "Documentação acessível cobre setup e troubleshooting."
                              ],
                              "assessmentCriteria": [
                                "Mapeamento completo de pontos de coleta (sem omissões).",
                                "Implementação segura e sem vazamentos de dados não consentidos.",
                                "Interface de consentimento clara e validável legalmente.",
                                "Integração seamless, sem impactar performance do pipeline.",
                                "Testes abrangentes cobrindo edge cases éticos.",
                                "Documentação detalhada e atualizável."
                              ],
                              "crossCurricularConnections": [
                                "Ética e Filosofia: Princípios de autonomia e não-maleficência.",
                                "Direito Digital: Conformidade com LGPD, GDPR e direitos fundamentais.",
                                "Desenvolvimento Web: Autenticação, formulários e UX design.",
                                "Gestão de Projetos: Integração de requisitos não-funcionais éticos."
                              ],
                              "realWorldApplication": "Em plataformas como Meta ou Google Analytics, pipelines integram consentimento via cookies banners e OAuth para coleta ética de dados de usuários, evitando multas bilionárias por violações de privacidade e fomentando confiança para modelos de IA treinados em dados consentidos."
                            },
                            "estimatedTime": "1.5 horas",
                            "difficulty": "advanced",
                            "status": "not_started",
                            "prerequisites": []
                          },
                          {
                            "id": "10.1.6.2.3.3",
                            "name": "Gerenciar Revogação de Consentimento",
                            "description": "Planejar processos para honrar revogações, incluindo exclusão de dados, anonimização e notificações em sistemas de ML, com exemplos de impactos em modelos treinados.",
                            "atomicExpansion": {
                              "steps": [
                                {
                                  "stepNumber": 1,
                                  "title": "Compreender Conceitos e Requisitos Legais de Revogação de Consentimento",
                                  "subSteps": [
                                    "Estude definições de consentimento e revogação conforme GDPR e LGPD.",
                                    "Identifique gatilhos para revogação (e-mails, APIs, formulários).",
                                    "Analise impactos legais de não cumprir revogações (multas, ações judiciais).",
                                    "Revise exemplos de casos reais de violações de privacidade.",
                                    "Documente requisitos específicos para ciência de dados e ML."
                                  ],
                                  "verification": "Criar um mapa mental ou documento resumindo conceitos chave e requisitos legais.",
                                  "estimatedTime": "2 horas",
                                  "materials": [
                                    "Documentação GDPR/LGPD (PDFs oficiais)",
                                    "Artigos sobre casos de privacidade (ex: Cambridge Analytica)",
                                    "Ferramenta de mind mapping (ex: XMind)"
                                  ],
                                  "tips": "Foquem em analogias: revogação é como 'deletar histórico de navegação' mas em escala enterprise.",
                                  "learningObjective": "Dominar fundamentos legais e éticos da revogação para planejar conformidade.",
                                  "commonMistakes": [
                                    "Confundir revogação com opt-out simples",
                                    "Ignorar diferenças entre jurisdições",
                                    "Subestimar prazos legais (ex: 30 dias para exclusão)"
                                  ]
                                },
                                {
                                  "stepNumber": 2,
                                  "title": "Projetar Fluxo de Processamento de Revogações",
                                  "subSteps": [
                                    "Mapeie o journey do usuário: detecção → validação → processamento.",
                                    "Defina métodos de detecção (webhooks, queues como Kafka).",
                                    "Crie fluxogramas para validação de identidade do usuário.",
                                    "Planeje notificações automáticas de confirmação.",
                                    "Integre logging para auditoria completa."
                                  ],
                                  "verification": "Desenhar e validar um fluxograma usando ferramentas como Lucidchart.",
                                  "estimatedTime": "3 horas",
                                  "materials": [
                                    "Ferramentas de diagramação (Draw.io, Lucidchart)",
                                    "Templates de fluxos de privacidade",
                                    "Exemplos de código para queues (RabbitMQ docs)"
                                  ],
                                  "tips": "Use BPMN para fluxos complexos; teste cenários edge como revogações em massa.",
                                  "learningObjective": "Desenvolver um blueprint acionável para gerenciar revogações eficientemente.",
                                  "commonMistakes": [
                                    "Fluxos lineares sem branches para erros",
                                    "Falta de autenticação forte",
                                    "Não considerar escalabilidade para alto volume"
                                  ]
                                },
                                {
                                  "stepNumber": 3,
                                  "title": "Implementar Exclusão e Anonimização de Dados",
                                  "subSteps": [
                                    "Identifique dados pessoais em bancos (SQL/NoSQL queries).",
                                    "Implemente scripts para exclusão lógica/física (DELETE, soft-delete).",
                                    "Aplique técnicas de anonimização (k-anonymity, hashing).",
                                    "Atualize backups e caches para refletir mudanças.",
                                    "Teste queries para confirmar remoção completa."
                                  ],
                                  "verification": "Executar script de teste em dataset mock e verificar logs de remoção.",
                                  "estimatedTime": "4 horas",
                                  "materials": [
                                    "Banco de dados mock (PostgreSQL Docker)",
                                    "Bibliotecas Python (pandas, faker para mocks)",
                                    "Ferramentas de anonimização (ARX toolbox)"
                                  ],
                                  "tips": "Sempre use transações ACID para evitar perda parcial de dados.",
                                  "learningObjective": "Executar remoção segura de dados pessoais preservando integridade do sistema.",
                                  "commonMistakes": [
                                    "Excluir apenas primary keys sem propagar",
                                    "Anonimização fraca reversível",
                                    "Esquecer logs ou shadows tables"
                                  ]
                                },
                                {
                                  "stepNumber": 4,
                                  "title": "Gerenciar Impactos em Modelos de Machine Learning",
                                  "subSteps": [
                                    "Avalie dependência do modelo nos dados revogados (feature importance).",
                                    "Planeje re-treinamento incremental ou full (usando pipelines como MLflow).",
                                    "Implemente detecção de drift pós-remoção.",
                                    "Versione modelos e datasets (DVC para tracking).",
                                    "Simule impactos em métricas (accuracy drop)."
                                  ],
                                  "verification": "Treinar modelo antes/depois da revogação e comparar métricas.",
                                  "estimatedTime": "5 horas",
                                  "materials": [
                                    "Frameworks ML (scikit-learn, TensorFlow)",
                                    "Datasets mock (UCI ML repo)",
                                    "MLflow ou DVC para versionamento"
                                  ],
                                  "tips": "Priorize unlearning techniques para eficiência em grandes modelos.",
                                  "learningObjective": "Mitigar efeitos de revogações em performance de modelos de ML.",
                                  "commonMistakes": [
                                    "Ignorar bias introduzido por remoções seletivas",
                                    "Re-treinar sem baselines",
                                    "Não versionar artefatos ML"
                                  ]
                                },
                                {
                                  "stepNumber": 5,
                                  "title": "Testar, Auditar e Documentar o Processo Completo",
                                  "subSteps": [
                                    "Crie cenários de teste end-to-end (unit, integration, E2E).",
                                    "Realize auditoria simulada com checklists de conformidade.",
                                    "Gere relatórios de impacto e notificações.",
                                    "Documente SOPs (Standard Operating Procedures).",
                                    "Planeje monitoramento contínuo (alertas em dashboards)."
                                  ],
                                  "verification": "Passar todos testes automatizados e gerar relatório de auditoria.",
                                  "estimatedTime": "3 horas",
                                  "materials": [
                                    "Ferramentas de teste (Pytest, Postman)",
                                    "Dashboards (Grafana)",
                                    "Templates de SOP GDPR"
                                  ],
                                  "tips": "Automatize testes com CI/CD para revogações recorrentes.",
                                  "learningObjective": "Garantir robustez e conformidade através de testes e auditorias.",
                                  "commonMistakes": [
                                    "Testes superficiais sem cenários reais",
                                    "Documentação incompleta",
                                    "Falta de monitoramento pós-deploy"
                                  ]
                                }
                              ],
                              "practicalExample": "Em um app de recomendação de saúde, um usuário revoga consentimento para dados de fitness. O sistema detecta via API, remove registros do banco, anonimiza histórico, re-treina o modelo de recomendação removendo features derivadas e notifica o usuário com confirmação, evitando drift no modelo.",
                              "finalVerifications": [
                                "Dados revogados foram excluídos/aninimozados em todos os storage (DB, cache, backups).",
                                "Modelo de ML re-treinado ou ajustado sem degradação >5% em métricas chave.",
                                "Logs de auditoria completos e imutáveis.",
                                "Notificação de confirmação enviada ao usuário em <24h.",
                                "Testes end-to-end passam 100% em cenários simulados.",
                                "Checklist de conformidade GDPR/LGPD assinado."
                              ],
                              "assessmentCriteria": [
                                "Precisão e completude do fluxo de revogação (90%+ cobertura).",
                                "Eficiência temporal: processamento <30 dias, ideal <24h.",
                                "Manutenção de performance ML pós-revogação (métricas estáveis).",
                                "Conformidade legal demonstrada via auditoria.",
                                "Documentação clara e acionável para equipe.",
                                "Capacidade de escalar para 1000+ revogações/dia."
                              ],
                              "crossCurricularConnections": [
                                "Direito Digital: Aplicação prática de GDPR/LGPD.",
                                "Programação: Integração de APIs, queues e bancos de dados.",
                                "Machine Learning: Unlearning e model versioning.",
                                "Ética e Governança: Princípios de privacidade by design.",
                                "Gestão de Projetos: Auditoria e compliance workflows."
                              ],
                              "realWorldApplication": "Em plataformas como Google ou Meta, gerenciar revogações em bilhões de dados garante conformidade GDPR, evita multas de €20M+, e mantém confiança do usuário em features de IA personalizadas."
                            },
                            "estimatedTime": "1 hora",
                            "difficulty": "advanced",
                            "status": "not_started",
                            "prerequisites": []
                          },
                          {
                            "id": "10.1.6.2.3.4",
                            "name": "Avaliar Riscos Éticos",
                            "description": "Realizar análise de riscos relacionados à falta de consentimento, como multas regulatórias e perda de confiança, usando estudos de caso de violações em ciência de dados.",
                            "atomicExpansion": {
                              "steps": [
                                {
                                  "stepNumber": 1,
                                  "title": "Compreender Conceitos Fundamentais de Consentimento e Riscos Éticos",
                                  "subSteps": [
                                    "Defina consentimento informado e seus componentes essenciais (voluntariedade, informação completa, compreensão e capacidade).",
                                    "Liste riscos éticos comuns na ciência de dados relacionados à falta de consentimento, como violações de privacidade e discriminação algorítmica.",
                                    "Estude regulamentações chave como GDPR, LGPD e CCPA, focando em penalidades por não conformidade.",
                                    "Crie um mapa mental conectando falta de consentimento a impactos como multas e perda de confiança.",
                                    "Discuta exemplos iniciais de violações éticas em datasets públicos."
                                  ],
                                  "verification": "Mapa mental completo com pelo menos 10 conexões validadas por auto-revisão ou peer review.",
                                  "estimatedTime": "1 hora",
                                  "materials": [
                                    "Artigos sobre GDPR/LGPD",
                                    "Vídeos introdutórios sobre ética em IA (ex: Coursera Ethics of AI)",
                                    "Ferramenta de mind mapping como MindMeister"
                                  ],
                                  "tips": "Use analogias cotidianas, como consentimento médico, para fixar conceitos.",
                                  "learningObjective": "Identificar e explicar os pilares do consentimento informado e riscos associados.",
                                  "commonMistakes": [
                                    "Confundir consentimento implícito com explícito",
                                    "Ignorar contextos culturais em regulamentações globais"
                                  ]
                                },
                                {
                                  "stepNumber": 2,
                                  "title": "Identificar e Classificar Riscos Específicos",
                                  "subSteps": [
                                    "Categorize riscos em financeiros (multas), reputacionais (perda de confiança), legais (processos) e operacionais (bloqueio de dados).",
                                    "Quantifique exemplos: multas GDPR até 4% da receita global.",
                                    "Crie uma matriz de riscos com probabilidade e impacto para cenários sem consentimento.",
                                    "Analise como falta de consentimento afeta stakeholders (usuários, empresas, reguladores).",
                                    "Priorize riscos usando escala de 1-5 para gravidade."
                                  ],
                                  "verification": "Matriz de riscos preenchida com 5+ cenários e priorização lógica.",
                                  "estimatedTime": "1.5 horas",
                                  "materials": [
                                    "Templates de matriz de riscos (Excel/Google Sheets)",
                                    "Casos reais de multas: Equifax breach report"
                                  ],
                                  "tips": "Comece com riscos mais tangíveis (financeiros) para construir momentum.",
                                  "learningObjective": "Classificar e priorizar riscos éticos quantitativa e qualitativamente.",
                                  "commonMistakes": [
                                    "Subestimar riscos reputacionais de longo prazo",
                                    "Não considerar cadeias de impacto em stakeholders"
                                  ]
                                },
                                {
                                  "stepNumber": 3,
                                  "title": "Analisar Estudos de Caso de Violações",
                                  "subSteps": [
                                    "Selecione 2-3 casos reais: Cambridge Analytica, Clearview AI e um breach brasileiro (ex: Serasa).",
                                    "Para cada caso, extraia falhas de consentimento, riscos materializados e lições aprendidas.",
                                    "Aplique a matriz de riscos do Step 2 aos casos.",
                                    "Compare outcomes: multas pagas, confiança perdida (queda em ações/usuários).",
                                    "Documente takeaways em formato de relatório curto."
                                  ],
                                  "verification": "Relatório de 1 página por caso com análise estruturada.",
                                  "estimatedTime": "2 horas",
                                  "materials": [
                                    "Relatórios de casos: FTC Cambridge Analytica",
                                    "Artigos da EFF sobre Clearview AI",
                                    "Notícias sobre LGPD violações"
                                  ],
                                  "tips": "Busque fontes primárias como relatórios regulatórios para precisão.",
                                  "learningObjective": "Aplicar framework de riscos a casos reais para extrair insights acionáveis.",
                                  "commonMistakes": [
                                    "Focar só em fatos superficiais sem análise de causa-raiz",
                                    "Ignorar perspectivas das vítimas"
                                  ]
                                },
                                {
                                  "stepNumber": 4,
                                  "title": "Realizar Análise Própria e Formular Recomendações",
                                  "subSteps": [
                                    "Escolha um cenário hipotético: uso de dados de redes sociais sem consentimento para ML.",
                                    "Aplique análise completa: identifique riscos, use matriz e compare com casos reais.",
                                    "Desenvolva recomendações mitigadoras: opt-in explícito, auditorias éticas.",
                                    "Simule impacto: calcule potenciais multas baseadas em receita fictícia.",
                                    "Crie um checklist para avaliação ética futura."
                                  ],
                                  "verification": "Checklist e relatório de análise com recomendações testadas em peer review.",
                                  "estimatedTime": "1.5 horas",
                                  "materials": [
                                    "Ferramenta de simulação: Google Sheets para cálculos de multas",
                                    "Templates de checklist ético"
                                  ],
                                  "tips": "Torne o cenário pessoal para maior engajamento.",
                                  "learningObjective": "Sintetizar aprendizado em análise independente e plano de mitigação.",
                                  "commonMistakes": [
                                    "Recomendações genéricas sem ligação a riscos identificados",
                                    "Omitir métricas quantificáveis"
                                  ]
                                }
                              ],
                              "practicalExample": "Analise o caso Cambridge Analytica: dados de 87 milhões de usuários do Facebook coletados sem consentimento explícito via quiz. Identifique riscos (multa de US$5bi ao Facebook, perda de confiança global), aplique matriz de riscos e recomende consentimento granular para apps de terceiros.",
                              "finalVerifications": [
                                "Pode listar 5+ riscos éticos da falta de consentimento com exemplos reais.",
                                "Constrói matriz de riscos precisa para um cenário dado.",
                                "Analisa estudos de caso identificando causas e impactos corretamente.",
                                "Formula recomendações específicas e mensuráveis.",
                                "Explica conexões entre regulamentações e penalidades.",
                                "Aplica checklist ético a novo caso com 90% de cobertura."
                              ],
                              "assessmentCriteria": [
                                "Precisão na identificação de riscos (80% cobertura de tipos principais).",
                                "Profundidade na análise de casos (inclui causa-raiz e lições).",
                                "Qualidade da matriz de riscos (lógica, quantificação).",
                                "Relevância e acionabilidade das recomendações.",
                                "Uso correto de evidências regulatórias e quantitativas.",
                                "Clareza e estrutura no relatório final."
                              ],
                              "crossCurricularConnections": [
                                "Direito: Aplicação de LGPD/GDPR em contratos de dados.",
                                "Psicologia: Impacto comportamental da perda de confiança em usuários.",
                                "Gestão Empresarial: Análise de risco em estratégias corporativas.",
                                "Filosofia: Debates éticos sobre autonomia e privacidade.",
                                "Tecnologia: Integração com segurança cibernética e criptografia."
                              ],
                              "realWorldApplication": "Em uma empresa de ciência de dados, use essa análise para auditar pipelines de dados antes de deployment, evitando multas milionárias e construindo confiança com clientes, como visto em conformidade pós-Cambridge Analytica."
                            },
                            "estimatedTime": "0.5 hora",
                            "difficulty": "intermediate",
                            "status": "not_started",
                            "prerequisites": []
                          }
                        ]
                      }
                    ]
                  },
                  {
                    "id": "10.1.6.3",
                    "name": "Viés e Equidade em Dados",
                    "description": "Identificação, análise e mitigação de vieses nos dados e modelos para garantir decisões justas e imparciais.",
                    "individualConcepts": [
                      {
                        "id": "10.1.6.3.1",
                        "name": "Tipos e Fontes de Viés em Dados",
                        "description": "Compreender os principais tipos de viés que afetam conjuntos de dados e modelos de aprendizado de máquina, incluindo suas origens em processos de coleta, preparação e modelagem de dados.",
                        "specificSkills": [
                          {
                            "id": "10.1.6.3.1.1",
                            "name": "Identificar viés de seleção e amostragem",
                            "description": "Reconhecer como a seleção não representativa de dados ou amostras desbalanceadas introduzem viés, levando a modelos que generalizam mal para subgrupos populacionais minoritários.",
                            "atomicExpansion": {
                              "steps": [
                                {
                                  "stepNumber": 1,
                                  "title": "Compreender Definições e Conceitos Básicos de Viés de Seleção e Amostragem",
                                  "subSteps": [
                                    "Leia definições precisas: viés de seleção ocorre quando o método de seleção exclui partes da população; viés de amostragem quando a amostra é desbalanceada.",
                                    "Estude exemplos clássicos, como a pesquisa de Literary Digest de 1936 que subestimou Roosevelt devido a amostra de telefonemas (ricos).",
                                    "Compare com viés não relacionado, como viés de confirmação, para diferenciar.",
                                    "Anote diferenças chave em um quadro comparativo.",
                                    "Resuma em suas palavras o impacto em generalização de modelos."
                                  ],
                                  "verification": "Criar um quadro comparativo com definições e exemplos corretos, sem erros conceituais.",
                                  "estimatedTime": "45 minutos",
                                  "materials": [
                                    "Artigo 'Selection Bias' da Wikipedia",
                                    "Vídeo Khan Academy sobre amostragem",
                                    "Notebook para anotações"
                                  ],
                                  "tips": [
                                    "Use analogias cotidianas, como provar sopa com colher furada para viés de seleção.",
                                    "Foque em causas raiz antes de sintomas."
                                  ],
                                  "learningObjective": "Definir e diferenciar viés de seleção e amostragem com precisão.",
                                  "commonMistakes": [
                                    "Confundir com viés de sobrevivência.",
                                    "Ignorar contexto populacional alvo.",
                                    "Achar que todo desbalanceamento é viés."
                                  ]
                                },
                                {
                                  "stepNumber": 2,
                                  "title": "Identificar Fontes Comuns de Viés em Processos de Coleta de Dados",
                                  "subSteps": [
                                    "Liste fontes: voluntários auto-selecionados, conveniência (ex: dados de redes sociais), exclusão não intencional de minorias.",
                                    "Analise fluxogramas de coleta de dados para pontos de viés (ex: filtros de idioma).",
                                    "Estude casos: dados médicos de hospitais urbanos ignorando rurais.",
                                    "Crie um checklist de 10 perguntas para detectar fontes potenciais.",
                                    "Aplique checklist a um cenário hipotético simples."
                                  ],
                                  "verification": "Checklist completo e aplicado corretamente a um cenário, identificando pelo menos 2 fontes.",
                                  "estimatedTime": "60 minutos",
                                  "materials": [
                                    "Infográfico sobre tipos de viés (Google Images)",
                                    "Exemplos de papers arXiv sobre bias em datasets",
                                    "Ferramenta Draw.io para fluxogramas"
                                  ],
                                  "tips": [
                                    "Pense em 'quem foi convidado para a festa?' para visualizar seleção.",
                                    "Priorize fontes upstream na coleta."
                                  ],
                                  "learningObjective": "Reconhecer fontes específicas de viés de seleção e amostragem em pipelines de dados.",
                                  "commonMistakes": [
                                    "Focar só em tamanho da amostra, ignorando representatividade.",
                                    "Subestimar viés em dados 'big data'.",
                                    "Não considerar não-resposta como viés."
                                  ]
                                },
                                {
                                  "stepNumber": 3,
                                  "title": "Analisar Conjuntos de Dados Práticos para Detectar Viés",
                                  "subSteps": [
                                    "Carregue um dataset desbalanceado (ex: Adult UCI Income dataset).",
                                    "Calcule estatísticas descritivas: proporções por gênero/raça/idade.",
                                    "Visualize distribuições com histogramas e boxplots.",
                                    "Compare com população real (ex: censos oficiais).",
                                    "Documente evidências de viés com métricas (ex: KL-divergence simples)."
                                  ],
                                  "verification": "Relatório com visualizações e conclusão sobre presença de viés, apoiado por números.",
                                  "estimatedTime": "90 minutos",
                                  "materials": [
                                    "Python/Jupyter Notebook",
                                    "Dataset Adult UCI de Kaggle",
                                    "Bibliotecas Pandas, Matplotlib, Seaborn"
                                  ],
                                  "tips": [
                                    "Use groupby() para breakdowns rápidos.",
                                    "Sempre benchmark contra população alvo."
                                  ],
                                  "learningObjective": "Aplicar análise quantitativa e visual para detectar viés em dados reais.",
                                  "commonMistakes": [
                                    "Interpretar correlação como causalidade no viés.",
                                    "Esquecer normalização por população.",
                                    "Ignorar outliers como sinal de viés."
                                  ]
                                },
                                {
                                  "stepNumber": 4,
                                  "title": "Avaliar Impacto em Modelos e Propor Mitigações",
                                  "subSteps": [
                                    "Treine um modelo simples (ex: regressão logística) em dataset viesado.",
                                    "Meça performance por subgrupos (ex: accuracy por raça).",
                                    "Calcule métricas de fairness (ex: disparate impact).",
                                    "Proponha mitigações: reamostragem, oversampling minorias, quotas.",
                                    "Simule correção e re-treine para comparar."
                                  ],
                                  "verification": "Relatório com métricas antes/depois, mostrando melhoria em equidade.",
                                  "estimatedTime": "75 minutos",
                                  "materials": [
                                    "Python com Scikit-learn",
                                    "Mesmo dataset do step 3",
                                    "Fairlearn ou AIF360 library"
                                  ],
                                  "tips": [
                                    "Comece com baseline para quantificar impacto.",
                                    "Teste mitigações uma por vez."
                                  ],
                                  "learningObjective": "Quantificar impacto de viés em ML e aplicar correções básicas.",
                                  "commonMistakes": [
                                    "Achar que balanceamento resolve tudo.",
                                    "Não validar em holdout set.",
                                    "Ignorar trade-offs com accuracy geral."
                                  ]
                                }
                              ],
                              "practicalExample": "Em um algoritmo de empréstimos bancários treinado com dados históricos onde 90% das aprovações são para homens brancos (devido a históricos passados), o modelo falha em generalizar para mulheres e minorias, negando empréstimos injustamente. Detectar isso via análise de distribuição demográfica vs. população geral.",
                              "finalVerifications": [
                                "Explicar viés de seleção vs. amostragem com exemplo próprio.",
                                "Identificar viés em um novo dataset fornecido.",
                                "Calcular disparate impact em um modelo simulado.",
                                "Propor 3 mitigações viáveis para um caso dado.",
                                "Discutir impacto em subgrupos minoritários.",
                                "Criar visualização de desbalanceamento."
                              ],
                              "assessmentCriteria": [
                                "Precisão conceitual (definições corretas: 25%)",
                                "Detecção em dados (análises quantitativas: 25%)",
                                "Análise de impacto (métricas de fairness: 20%)",
                                "Propostas de mitigação (praticáveis e justificadas: 15%)",
                                "Clareza e exemplos (comunicação: 10%)",
                                "Conexão com generalização pobre: 5%"
                              ],
                              "crossCurricularConnections": [
                                "Estatística: Técnicas de amostragem probabilística vs. não-probabilística.",
                                "Ética e Filosofia: Justiça distributiva e discriminação algorítmica.",
                                "Programação/Informática: Manipulação de dados com Pandas e visualização.",
                                "Sociologia: Desigualdades estruturais e representatividade populacional.",
                                "Matemática: Distribuições e divergências probabilísticas."
                              ],
                              "realWorldApplication": "Em sistemas de IA para justiça criminal (ex: COMPAS), viés de seleção em dados de prisioneiros leva a previsões recidivistas enviesadas contra minorias, perpetuando desigualdades; identificação permite auditorias e correções para equidade judicial."
                            },
                            "estimatedTime": "2 horas",
                            "difficulty": "intermediate",
                            "status": "not_started",
                            "prerequisites": []
                          },
                          {
                            "id": "10.1.6.3.1.2",
                            "name": "Reconhecer viés de confirmação e histórico",
                            "description": "Analisar como dados históricos enviesados por práticas discriminatórias passadas perpetuam desigualdades em modelos preditivos, como em sistemas de recrutamento ou concessão de crédito.",
                            "atomicExpansion": {
                              "steps": [
                                {
                                  "stepNumber": 1,
                                  "title": "Entender os Conceitos Básicos de Viés de Confirmação e Viés Histórico",
                                  "subSteps": [
                                    "Defina viés de confirmação como a tendência de buscar ou interpretar informações que confirmam crenças pré-existentes.",
                                    "Explique viés histórico como dados coletados de práticas passadas discriminatórias que refletem desigualdades sociais.",
                                    "Diferencie os dois: viés de confirmação é cognitivo, viés histórico é inerente aos dados.",
                                    "Leia exemplos iniciais de literatura acadêmica sobre viés em IA.",
                                    "Anote definições em suas próprias palavras."
                                  ],
                                  "verification": "Crie um glossário pessoal com definições claras e exemplos simples; revise com um colega ou ferramenta de IA para precisão.",
                                  "estimatedTime": "45 minutos",
                                  "materials": [
                                    "Artigos introdutórios sobre viés em IA (ex: 'Fairness and Machine Learning' de Barocas et al.), caderno ou app de notas"
                                  ],
                                  "tips": "Use analogias cotidianas, como 'viés histórico é como herdar uma casa com rachaduras antigas'.",
                                  "learningObjective": "Dominar definições precisas e distinções entre viés de confirmação e histórico.",
                                  "commonMistakes": "Confundir viés histórico com viés de confirmação; ignorar o aspecto temporal do histórico."
                                },
                                {
                                  "stepNumber": 2,
                                  "title": "Identificar Fontes de Viés Histórico em Conjuntos de Dados",
                                  "subSteps": [
                                    "Analise conjuntos de dados históricos para padrões discriminatórios (ex: gênero, raça em dados de crédito).",
                                    "Investigue origens dos dados: como foram coletados em épocas de segregação ou discriminação.",
                                    "Use ferramentas como pandas para visualizar distribuições demográficas em datasets públicos.",
                                    "Documente evidências de sub-representação ou super-representação de grupos.",
                                    "Compare com dados atuais para ver perpetuação."
                                  ],
                                  "verification": "Gere um relatório curto (1 página) listando 3 fontes de viés em um dataset exemplo.",
                                  "estimatedTime": "60 minutos",
                                  "materials": [
                                    "Datasets públicos (ex: UCI Credit Approval, Kaggle HR Analytics), Python/Jupyter Notebook, pandas library"
                                  ],
                                  "tips": "Comece com gráficos de barras para distribuições; foque em métricas como % de aprovação por grupo demográfico.",
                                  "learningObjective": "Reconhecer padrões de viés em dados históricos através de análise exploratória.",
                                  "commonMistakes": "Assumir neutralidade de dados antigos sem checar contexto histórico; ignorar dados ausentes como viés."
                                },
                                {
                                  "stepNumber": 3,
                                  "title": "Analisar o Impacto em Modelos Preditivos",
                                  "subSteps": [
                                    "Construa um modelo simples de ML (ex: regressão logística) com dados enviesados.",
                                    "Avalie métricas de performance por subgrupos (ex: precisão para homens vs. mulheres).",
                                    "Simule predições e observe perpetuação de desigualdades.",
                                    "Aplique métricas de equidade como disparate impact ratio.",
                                    "Discuta como o modelo 'aprende' o viés histórico."
                                  ],
                                  "verification": "Execute o modelo e produza um gráfico mostrando diferenças de performance entre grupos.",
                                  "estimatedTime": "90 minutos",
                                  "materials": [
                                    "Python, scikit-learn, datasets enviesados, Jupyter Notebook"
                                  ],
                                  "tips": "Use train-test split estratificado por grupo para evitar overfitting em subgrupos.",
                                  "learningObjective": "Compreender como viés histórico se propaga para predições desiguais.",
                                  "commonMistakes": "Treinar modelo sem métricas de fairness; interpretar alta acurácia geral como ausência de viés."
                                },
                                {
                                  "stepNumber": 4,
                                  "title": "Aplicar a Exemplos Reais e Refletir sobre Mitigação",
                                  "subSteps": [
                                    "Estude casos: Amazon hiring tool (viés de gênero histórico) e sistemas de crédito (viés racial).",
                                    "Mapeie cadeia: práticas passadas → dados enviesados → modelo discriminatório.",
                                    "Brainstorm estratégias de mitigação: reamostragem, reweighting.",
                                    "Debata prós/contras de ignorar viés histórico vs. corrigi-lo.",
                                    "Escreva um parágrafo resumindo lições aprendidas."
                                  ],
                                  "verification": "Crie uma tabela comparativa de 2 casos reais com colunas: fonte de viés, impacto, lições.",
                                  "estimatedTime": "45 minutos",
                                  "materials": [
                                    "Casos de estudo (ex: ProPublica COMPAS, artigos sobre Amazon), acesso à internet"
                                  ],
                                  "tips": "Busque fontes primárias como relatórios de auditoria para credibilidade.",
                                  "learningObjective": "Conectar teoria a prática real e reconhecer necessidade de intervenções éticas.",
                                  "commonMistakes": "Generalizar um caso para todos; subestimar complexidade da mitigação."
                                }
                              ],
                              "practicalExample": "Em um sistema de recrutamento da Amazon (2018), dados históricos de currículos (dominados por homens em tech) treinaram um modelo que penalizava palavras como 'mulheres' em CVs femininos, perpetuando viés de gênero e rejeitando candidatas qualificadas.",
                              "finalVerifications": [
                                "Explique em 2 minutos a diferença entre viés de confirmação e histórico para um colega.",
                                "Identifique viés histórico em um dataset fornecido sem dicas.",
                                "Preveja impactos de um modelo treinado em dados enviesados.",
                                "Liste 3 estratégias para detectar viés histórico em novos projetos.",
                                "Discuta um caso real com evidências de perpetuação de desigualdades.",
                                "Avalie um modelo simples quanto a equidade por subgrupos."
                              ],
                              "assessmentCriteria": [
                                "Precisão conceitual: definições corretas sem confusões (30%)",
                                "Análise de dados: identificação correta de padrões enviesados (25%)",
                                "Compreensão de impacto: explicação clara da propagação em ML (20%)",
                                "Aplicação prática: uso relevante de exemplos reais (15%)",
                                "Reflexão crítica: discussão de mitigação e limitações (10%)"
                              ],
                              "crossCurricularConnections": [
                                "História/Sociologia: Contexto de discriminação passada (ex: leis segregacionistas).",
                                "Ética/Filosofia: Debates sobre justiça algorítmica e responsabilidade.",
                                "Direito: Regulamentações como GDPR e AI Act sobre viés.",
                                "Psicologia: Viés cognitivo de confirmação em tomadores de decisão."
                              ],
                              "realWorldApplication": "Em bancos, modelos de concessão de crédito treinados em dados históricos podem negar empréstimos a minorias devido a padrões passados de exclusão, perpetuando ciclos de pobreza; reconhecimento permite auditorias e correções para compliance e equidade social."
                            },
                            "estimatedTime": "1.5 horas",
                            "difficulty": "intermediate",
                            "status": "not_started",
                            "prerequisites": []
                          },
                          {
                            "id": "10.1.6.3.1.3",
                            "name": "Diferenciar viés algorítmico de viés de representação",
                            "description": "Distinguir viés inerente aos dados de entrada do viés introduzido por escolhas algorítmicas, como funções de perda que priorizam grupos majoritários.",
                            "atomicExpansion": {
                              "steps": [
                                {
                                  "stepNumber": 1,
                                  "title": "Compreender o Viés de Representação",
                                  "subSteps": [
                                    "Leia definições de viés de representação: desequilíbrio nos dados de entrada que não reflete a população real.",
                                    "Analise exemplos: dataset de recrutamento com 90% de candidatos homens devido a fontes históricas enviesadas.",
                                    "Identifique fontes: coleta de dados não aleatória ou sub-representação de grupos minoritários.",
                                    "Registre características chave: origina-se dos dados, persiste mesmo com algoritmos neutros.",
                                    "Crie um diagrama simples mostrando fluxo de dados enviesados para o modelo."
                                  ],
                                  "verification": "Crie um resumo de 3 frases definindo viés de representação e dê um exemplo pessoal.",
                                  "estimatedTime": "20 minutos",
                                  "materials": [
                                    "Artigo sobre viés em dados (ex: 'FairML Book'), caderno para anotações, diagrama online (Draw.io)"
                                  ],
                                  "tips": "Foquem em exemplos reais de dados históricos para fixar o conceito.",
                                  "learningObjective": "Definir viés de representação e identificar suas fontes nos dados de entrada.",
                                  "commonMistakes": [
                                    "Confundir com viés algorítmico achando que é sempre culpa do código.",
                                    "Ignorar que dados limpos podem mitigar isso."
                                  ]
                                },
                                {
                                  "stepNumber": 2,
                                  "title": "Compreender o Viés Algorítmico",
                                  "subSteps": [
                                    "Defina viés algorítmico: viés introduzido pelas escolhas no design do algoritmo, independente dos dados.",
                                    "Estude exemplos: função de perda MSE que prioriza erros em grupos majoritários, amplificando desigualdades.",
                                    "Explore mecanismos: otimizadores que convergem para soluções que favorecem amostras frequentes.",
                                    "Compare com neutralidade: mesmo dados equilibrados podem gerar viés se o algoritmo for assimétrico.",
                                    "Anote funções comuns: cross-entropy vs. focal loss em cenários desbalanceados."
                                  ],
                                  "verification": "Liste 2 exemplos de escolhas algorítmicas que introduzem viés e explique por quê.",
                                  "estimatedTime": "25 minutos",
                                  "materials": [
                                    "Documentação de bibliotecas ML (Scikit-learn, TensorFlow), vídeo curto sobre funções de perda"
                                  ],
                                  "tips": "Pense em 'o algoritmo escolhe o que otimizar' para diferenciar dos dados.",
                                  "learningObjective": "Identificar como design algorítmico introduz viés, como em funções de perda.",
                                  "commonMistakes": [
                                    "Achar que viés algorítmico só ocorre com dados ruins.",
                                    "Subestimar impacto de hiperparâmetros como learning rate."
                                  ]
                                },
                                {
                                  "stepNumber": 3,
                                  "title": "Diferenciar os Dois Tipos de Viés",
                                  "subSteps": [
                                    "Crie uma tabela comparativa: colunas para origem, detecção, mitigação e exemplos.",
                                    "Simule cenários: dados enviesados + algoritmo neutro (viés de representação) vs. dados bons + algoritmo enviesado.",
                                    "Discuta interações: como viés de representação pode ser agravado por viés algorítmico.",
                                    "Pratique verbalizando: explique a diferença para um 'colega imaginário'.",
                                    "Identifique testes diagnósticos: re-treinar com dados balanceados vs. mudar função de perda."
                                  ],
                                  "verification": "Preencha tabela comparativa e grave áudio de 1 minuto explicando a diferença.",
                                  "estimatedTime": "30 minutos",
                                  "materials": [
                                    "Planilha Google Sheets ou Excel para tabela, gravador de voz no celular"
                                  ],
                                  "tips": "Use mnemônicos: 'Representação = dados ruins no início; Algorítmico = código ruim no meio'.",
                                  "learningObjective": "Comparar e contrastar viés de representação e algorítmico com critérios claros.",
                                  "commonMistakes": [
                                    "Misturar os dois como 'viés de dados' genérico.",
                                    "Não considerar cenários isolados."
                                  ]
                                },
                                {
                                  "stepNumber": 4,
                                  "title": "Aplicar a Diferenciação em um Caso Prático",
                                  "subSteps": [
                                    "Escolha um dataset real (ex: Adult UCI para renda).",
                                    "Analise viés de representação: cheque distribuição demográfica.",
                                    "Teste viés algorítmico: treine modelo com diferentes funções de perda e compare métricas por grupo.",
                                    "Conclua qual viés domina e sugira fixes específicos.",
                                    "Documente achados em relatório curto."
                                  ],
                                  "verification": "Produza relatório de 200 palavras identificando tipos de viés no dataset analisado.",
                                  "estimatedTime": "45 minutos",
                                  "materials": [
                                    "Dataset Adult UCI (Kaggle), Jupyter Notebook, bibliotecas Python (Pandas, Scikit-learn)"
                                  ],
                                  "tips": "Comece com análise exploratória de dados (EDA) para baseline.",
                                  "learningObjective": "Aplicar diferenciação prática para diagnosticar viés em um pipeline real.",
                                  "commonMistakes": [
                                    "Não isolar variáveis (ex: não balancear dados antes de testar algoritmo).",
                                    "Ignorar métricas de equidade como disparate impact."
                                  ]
                                }
                              ],
                              "practicalExample": "Em um sistema de aprovação de empréstimos, viés de representação ocorre se o dataset histórico tem apenas 10% de mulheres aplicantes, sub-representando-as. Viés algorítmico surge se a função de perda prioriza precisão em aprovados majoritários (homens), rejeitando mais mulheres mesmo com dados balanceados.",
                              "finalVerifications": [
                                "Explique a diferença em suas palavras sem consultar notas.",
                                "Identifique corretamente viés em 3 cenários hipotéticos dados.",
                                "Crie tabela comparativa precisa.",
                                "Sugira mitigação específica para cada tipo em um caso real.",
                                "Discuta como eles interagem em um pipeline de ML.",
                                "Avalie um paper curto sobre viés e classifique os tipos mencionados."
                              ],
                              "assessmentCriteria": [
                                "Precisão nas definições (90%+ correção).",
                                "Uso de exemplos concretos e relevantes.",
                                "Clareza na diferenciação (tabela ou diagrama bem estruturado).",
                                "Profundidade na análise de interações e mitigações.",
                                "Aplicação prática demonstrada em relatório.",
                                "Ausência de confusões comuns entre os tipos."
                              ],
                              "crossCurricularConnections": [
                                "Estatística: Distribuições e amostragem enviesada.",
                                "Ética e Filosofia: Justiça distributiva e impacto social.",
                                "Programação: Implementação de métricas de fairness em ML.",
                                "Sociologia: Desigualdades históricas refletidas em dados.",
                                "Matemática: Otimização e funções de perda."
                              ],
                              "realWorldApplication": "Em recrutamento automatizado na Amazon, viés de representação veio de currículos masculinos históricos; viés algorítmico de priorização de palavras-chave masculinas no algoritmo foi corrigido rebalanceando dados e alterando embeddings."
                            },
                            "estimatedTime": "2 horas",
                            "difficulty": "intermediate",
                            "status": "not_started",
                            "prerequisites": []
                          }
                        ]
                      },
                      {
                        "id": "10.1.6.3.2",
                        "name": "Identificação e Análise de Viés",
                        "description": "Aplicar métodos para detectar e quantificar viés em dados e modelos, utilizando métricas estatísticas e análises exploratórias para avaliar impactos na equidade.",
                        "specificSkills": [
                          {
                            "id": "10.1.6.3.2.1",
                            "name": "Realizar análise exploratória para detecção de viés",
                            "description": "Usar visualizações como histogramas de distribuição por grupos demográficos e testes estatísticos (ex.: teste qui-quadrado) para identificar desbalanceamentos em conjuntos de dados.",
                            "atomicExpansion": {
                              "steps": [
                                {
                                  "stepNumber": 1,
                                  "title": "Preparar e Explorar o Conjunto de Dados Inicial",
                                  "subSteps": [
                                    "Carregue o dataset usando pandas.read_csv() ou equivalente.",
                                    "Execute df.describe() e df.info() para overview estatístico e tipos de dados.",
                                    "Identifique colunas demográficas relevantes (ex.: gênero, raça, idade).",
                                    "Verifique valores ausentes com df.isnull().sum() e trate-os se necessário.",
                                    "Crie um resumo de contagens por grupos demográficos com value_counts()."
                                  ],
                                  "verification": "Dataset carregado sem erros, colunas demográficas identificadas e resumo de contagens gerado.",
                                  "estimatedTime": "30-45 minutos",
                                  "materials": [
                                    "Python com pandas instalado",
                                    "Notebook Jupyter",
                                    "Dataset de exemplo (ex.: UCI Adult Income)"
                                  ],
                                  "tips": [
                                    "Sempre salve uma cópia do dataset original.",
                                    "Use pd.set_option('display.max_columns', None) para visualizar todas as colunas."
                                  ],
                                  "learningObjective": "Compreender a estrutura do dataset e identificar variáveis proxy para viés demográfico.",
                                  "commonMistakes": [
                                    "Ignorar valores ausentes que podem mascarar viés.",
                                    "Não categorizar corretamente variáveis demográficas."
                                  ]
                                },
                                {
                                  "stepNumber": 2,
                                  "title": "Gerar Visualizações Exploratórias de Distribuições",
                                  "subSteps": [
                                    "Crie histogramas de variáveis alvo (ex.: aprovação) por grupos demográficos usando seaborn.histplot().",
                                    "Gere boxplots para variáveis numéricas (ex.: score) estratificadas por gênero/raça com sns.boxplot().",
                                    "Plote gráficos de barras para proporções com sns.countplot(hue='demografico').",
                                    "Adicione títulos, labels e salve as figuras com plt.savefig().",
                                    "Compare distribuições visualmente para desbalanceamentos óbvios."
                                  ],
                                  "verification": "Visualizações geradas mostram claramente distribuições por grupos, salvas em arquivos.",
                                  "estimatedTime": "45-60 minutos",
                                  "materials": [
                                    "Bibliotecas: matplotlib, seaborn",
                                    "Dataset preparado do Step 1"
                                  ],
                                  "tips": [
                                    "Use facet_grid() para múltiplas visualizações em uma grade.",
                                    "Escolha paletas de cores acessíveis para distinção clara."
                                  ],
                                  "learningObjective": "Visualizar desbalanceamentos demográficos para detecção intuitiva de viés.",
                                  "commonMistakes": [
                                    "Escalas inconsistentes nos eixos.",
                                    "Sobrecarregar gráficos com muitas variáveis de uma vez."
                                  ]
                                },
                                {
                                  "stepNumber": 3,
                                  "title": "Executar Testes Estatísticos para Viés",
                                  "subSteps": [
                                    "Prepare tabelas de contingência com pd.crosstab() para variáveis categóricas.",
                                    "Aplique teste qui-quadrado com scipy.stats.chi2_contingency() e interprete p-value.",
                                    "Para variáveis numéricas, use teste t de Student ou ANOVA com scipy.stats.",
                                    "Calcule medidas de associação como Cramer's V para força do viés.",
                                    "Registre resultados em um DataFrame de summary com p-values e conclusões."
                                  ],
                                  "verification": "Resultados de testes computados, com p-values <0.05 destacados indicando viés estatístico.",
                                  "estimatedTime": "45-60 minutos",
                                  "materials": [
                                    "Biblioteca scipy.stats",
                                    "Dataset e visualizações do Step anterior"
                                  ],
                                  "tips": [
                                    "Verifique premissas do teste (ex.: frequências esperadas >5 para qui-quadrado).",
                                    "Use alpha=0.05 como threshold padrão."
                                  ],
                                  "learningObjective": "Aplicar testes estatísticos rigorosos para quantificar viés além da visualização.",
                                  "commonMistakes": [
                                    "Confundir p-value com tamanho do efeito.",
                                    "Aplicar teste inadequado para o tipo de dados."
                                  ]
                                },
                                {
                                  "stepNumber": 4,
                                  "title": "Interpretar Resultados e Documentar Descobertas de Viés",
                                  "subSteps": [
                                    "Compile visualizações e resultados estatísticos em um relatório Markdown.",
                                    "Descreva viés detectados (ex.: 'Distribuição desigual em 20% para gênero').",
                                    "Priorize viés significativos baseado em p-value e impacto prático.",
                                    "Sugira próximos passos como reamostragem ou coleta de dados.",
                                    "Salve relatório e dataset processado para revisão."
                                  ],
                                  "verification": "Relatório completo gerado com evidências claras de viés identificado ou refutado.",
                                  "estimatedTime": "30-45 minutos",
                                  "materials": [
                                    "Notebook Jupyter para relatório",
                                    "Resultados dos steps anteriores"
                                  ],
                                  "tips": [
                                    "Use células Markdown para narrativa fluida.",
                                    "Inclua código reproduzível no relatório."
                                  ],
                                  "learningObjective": "Sintetizar análises em insights acionáveis sobre equidade em dados.",
                                  "commonMistakes": [
                                    "Ignorar viés não estatisticamente significativo mas visualmente evidente.",
                                    "Falta de contexto ético nas conclusões."
                                  ]
                                }
                              ],
                              "practicalExample": "Em um dataset de empréstimos bancários (ex.: LendingClub), crie histogramas de taxas de aprovação por gênero e raça, aplique teste qui-quadrado na tabela de contingência aprovação x gênero, detectando p-value <0.01 indicando viés de gênero.",
                              "finalVerifications": [
                                "Visualizações revelam desbalanceamentos claros em pelo menos uma variável demográfica.",
                                "Testes estatísticos executados corretamente com p-values reportados.",
                                "Relatório documenta viés com evidências quantitativas e qualitativas.",
                                "Medidas de associação calculadas para priorizar viés.",
                                "Sugestões de mitigação propostas baseadas nas descobertas.",
                                "Código reproduzível e dataset processado salvos."
                              ],
                              "assessmentCriteria": [
                                "Precisão nas visualizações (rótulos, escalas corretas).",
                                "Seleção e aplicação correta de testes estatísticos.",
                                "Interpretação adequada de p-values e limitações.",
                                "Clareza e completude do relatório final.",
                                "Uso eficiente de bibliotecas sem erros.",
                                "Identificação de pelo menos 2 potenciais fontes de viés."
                              ],
                              "crossCurricularConnections": [
                                "Estatística: Testes de hipótese e medidas de associação.",
                                "Ética e Sociedade: Implicações de viés em IA e decisões automatizadas.",
                                "Programação: Manipulação de dados com Python/pandas.",
                                "Visualização de Dados: Técnicas de storytelling com gráficos.",
                                "Ciência de Dados: EDA como base para modelagem ética."
                              ],
                              "realWorldApplication": "Detectar viés em datasets de recrutamento de RH para algoritmos de triagem, garantindo decisões de hiring equitativas e compliance com leis anti-discriminação como LGPD ou GDPR."
                            },
                            "estimatedTime": "3 horas",
                            "difficulty": "intermediate",
                            "status": "not_started",
                            "prerequisites": []
                          },
                          {
                            "id": "10.1.6.3.2.2",
                            "name": "Calcular métricas de equidade",
                            "description": "Implementar métricas como disparate de impacto, igualdade de odds e equalized odds para quantificar disparidades em previsões de modelos entre subgrupos protegidos.",
                            "atomicExpansion": {
                              "steps": [
                                {
                                  "stepNumber": 1,
                                  "title": "Compreender os Conceitos Fundamentais das Métricas de Equidade",
                                  "subSteps": [
                                    "Estude as definições de disparate impact (razão entre taxas de aprovação positivas entre grupos protegidos e não protegidos).",
                                    "Aprenda equalized odds: igualdade nas taxas de verdadeiro positivo e falso positivo entre grupos.",
                                    "Entenda equal opportunity: igualdade na taxa de verdadeiro positivo entre grupos.",
                                    "Revise conceitos de subgrupos protegidos (ex: gênero, raça) e taxas de confusão (TP, FP, TN, FN).",
                                    "Compare as métricas em uma tabela de referência."
                                  ],
                                  "verification": "Crie um resumo escrito explicando cada métrica com fórmulas matemáticas corretas.",
                                  "estimatedTime": "1 hora",
                                  "materials": [
                                    "Documentação do AIF360 (IBM AI Fairness 360)",
                                    "Artigos sobre fairness metrics (ex: Barocas et al.)",
                                    "Notebook Jupyter vazio"
                                  ],
                                  "tips": "Use diagramas de matriz de confusão para visualizar as taxas.",
                                  "learningObjective": "Dominar as definições e fórmulas das principais métricas de equidade.",
                                  "commonMistakes": [
                                    "Confundir disparate impact com equalized odds",
                                    "Ignorar a direção da disparidade (favorecendo qual grupo)",
                                    "Não normalizar por prevalência de classe"
                                  ]
                                },
                                {
                                  "stepNumber": 2,
                                  "title": "Preparar o Dataset com Subgrupos Protegidos",
                                  "subSteps": [
                                    "Carregue um dataset com predições, labels verdadeiras e atributos protegidos (ex: Adult UCI dataset).",
                                    "Separe os dados em subgrupos (ex: por gênero ou raça) usando pandas groupby.",
                                    "Calcule taxas básicas por grupo: taxa positiva prevista, taxa positiva verdadeira.",
                                    "Trate valores ausentes e codifique variáveis categóricas.",
                                    "Valide a distribuição dos subgrupos para equilíbrio."
                                  ],
                                  "verification": "Gere um relatório descrevendo as estatísticas descritivas por subgrupo.",
                                  "estimatedTime": "1.5 horas",
                                  "materials": [
                                    "Pandas, NumPy",
                                    "Dataset Adult UCI ou similar de Kaggle",
                                    "Jupyter Notebook"
                                  ],
                                  "tips": "Sempre verifique se os subgrupos têm amostras suficientes (>100 por grupo).",
                                  "learningObjective": "Preparar dados para análise de equidade, garantindo representatividade de subgrupos.",
                                  "commonMistakes": [
                                    "Não balancear ou verificar tamanhos de grupos",
                                    "Usar atributos protegidos como features no modelo",
                                    "Ignorar multicolinearidade entre atributos protegidos"
                                  ]
                                },
                                {
                                  "stepNumber": 3,
                                  "title": "Implementar o Cálculo das Métricas de Equidade",
                                  "subSteps": [
                                    "Implemente disparate impact: TP_rate_protected / TP_rate_unprotected.",
                                    "Calcule equalized odds: diferença absoluta em TPR e FPR entre grupos.",
                                    "Compute equal opportunity: diferença em TPR entre grupos.",
                                    "Use bibliotecas como AIF360 para validação: BinaryLabelDatasetMetric.",
                                    "Automatize em uma função que recebe y_true, y_pred, sensitive_feature."
                                  ],
                                  "verification": "Execute a função em um dataset de teste e compare com resultados manuais.",
                                  "estimatedTime": "2 horas",
                                  "materials": [
                                    "Scikit-learn metrics",
                                    "AIF360 library (pip install aif360)",
                                    "Exemplos de código GitHub fairness"
                                  ],
                                  "tips": "Defina thresholds: disparate impact <0.8 indica viés (regra dos 80%).",
                                  "learningObjective": "Codificar funções precisas para calcular métricas de equidade.",
                                  "commonMistakes": [
                                    "Dividir por zero em taxas zero",
                                    "Confundir y_pred binário com probabilidades",
                                    "Não usar métricas absolutas vs relativas corretamente"
                                  ]
                                },
                                {
                                  "stepNumber": 4,
                                  "title": "Interpretar Resultados e Visualizar Disparidades",
                                  "subSteps": [
                                    "Analise valores: identifique qual métrica viola mais (ex: disparate impact=0.6).",
                                    "Crie visualizações: bar charts de taxas por grupo, heatmaps de métricas.",
                                    "Compare múltiplas métricas em uma tabela de scores.",
                                    "Discuta implicações éticas: impacto em decisões reais.",
                                    "Salve relatório com recomendações de mitigação."
                                  ],
                                  "verification": "Gere um dashboard ou plot com métricas destacando disparidades.",
                                  "estimatedTime": "1 hora",
                                  "materials": [
                                    "Matplotlib, Seaborn",
                                    "Pandas para tabelas",
                                    "Exemplos de plots fairness"
                                  ],
                                  "tips": "Use cores distintas para grupos protegidos e inclua legendas claras.",
                                  "learningObjective": "Interpretar e comunicar resultados de equidade de forma visual e ética.",
                                  "commonMistakes": [
                                    "Interpretar disparidade sem contexto de prevalência",
                                    "Omitir confiança intervals nas métricas",
                                    "Ignorar trade-offs entre métricas"
                                  ]
                                }
                              ],
                              "practicalExample": "Em um modelo de aprovação de empréstimos, calcule disparate impact por gênero: se mulheres têm taxa de aprovação 20% menor que homens (0.75), indica viés. Use dataset com y_true (aprovado), y_pred (predição), sensitive_feature (gênero).",
                              "finalVerifications": [
                                "Calcula corretamente disparate impact, equalized odds e equal opportunity em dataset fornecido.",
                                "Identifica corretamente grupos com disparidade (>20% diferença).",
                                "Implementa função reutilizável sem erros de divisão por zero.",
                                "Gera visualizações claras destacando violações.",
                                "Explica implicações éticas em relatório escrito.",
                                "Valida resultados com AIF360 metrics."
                              ],
                              "assessmentCriteria": [
                                "Precisão das fórmulas implementadas (100% match com referências).",
                                "Qualidade do código: limpo, comentado, com testes unitários.",
                                "Profundidade da interpretação: discute thresholds e trade-offs.",
                                "Visualizações profissionais e informativas.",
                                "Tratamento de edge cases (grupos vazios, prevalência desbalanceada).",
                                "Relatório ético: recomendações de mitigação."
                              ],
                              "crossCurricularConnections": [
                                "Estatística: Taxas de confusão e testes de hipótese.",
                                "Programação: Manipulação de dados com Python/Pandas.",
                                "Ética e Filosofia: Justiça distributiva e discriminação algorítmica.",
                                "Machine Learning: Avaliação de modelos além de accuracy.",
                                "Visualização de Dados: Dashboards com Matplotlib/Seaborn."
                              ],
                              "realWorldApplication": "Em recrutamento (ex: Amazon hiring tool detectou viés de gênero), empréstimos bancários (Fair Lending laws), justiça criminal (COMPAS recidivism com viés racial), garantindo compliance com regulamentações como GDPR e AI Act."
                            },
                            "estimatedTime": "2.5 horas",
                            "difficulty": "advanced",
                            "status": "not_started",
                            "prerequisites": []
                          },
                          {
                            "id": "10.1.6.3.2.3",
                            "name": "Avaliar viés em modelos treinados",
                            "description": "Aplicar ferramentas como fairness audits em bibliotecas Python (ex.: AIF360) para analisar desempenho diferencial de modelos em validação cruzada por grupos sensíveis.",
                            "atomicExpansion": {
                              "steps": [
                                {
                                  "stepNumber": 1,
                                  "title": "Configurar ambiente e preparar dataset com atributos sensíveis",
                                  "subSteps": [
                                    "Instale bibliotecas necessárias: pip install aif360 scikit-learn pandas numpy",
                                    "Carregue um dataset padrão como Adult UCI (renda >50k) com atributos sensíveis (ex.: sexo, raça)",
                                    "Pré-processe os dados: trate missing values, codifique categóricas, defina protected attributes",
                                    "Divida em train/test sets estratificados por grupos protegidos",
                                    "Crie BinaryLabelDataset do AIF360 com privileged/protected groups definidos"
                                  ],
                                  "verification": "Confira se o dataset AIF360 é criado sem erros e atributos sensíveis estão corretamente mapeados (imprima dataset instance)",
                                  "estimatedTime": "30 minutos",
                                  "materials": "Python 3.8+, Jupyter Notebook, datasets UCI Adult (via fetch_openml ou download)",
                                  "tips": "Use StandardDataset do AIF360 para datasets prontos como Adult para agilizar",
                                  "learningObjective": "Entender preparação de dados para análise de fairness, garantindo estratificação por grupos sensíveis",
                                  "commonMistakes": "Ignorar desbalanceamento de classes nos grupos protegidos; não mapear corretamente privileged groups (ex.: branco=1 para raça)"
                                },
                                {
                                  "stepNumber": 2,
                                  "title": "Treinar modelo e gerar predições em validação cruzada",
                                  "subSteps": [
                                    "Escolha um modelo baseline como LogisticRegression do scikit-learn",
                                    "Implemente K-Fold cross-validation (k=5) estratificada por atributo sensível",
                                    "Treine o modelo em cada fold e gere probabilidades/predições out-of-fold",
                                    "Salve predições e labels verdadeiros para todos os folds em um dataset unificado",
                                    "Transforme predições em formato AIF360 (numpy arrays para scores/labels)"
                                  ],
                                  "verification": "Verifique métricas de acurácia geral via cross_val_score; confirme shape das predições (deve matching dataset size)",
                                  "estimatedTime": "45 minutos",
                                  "materials": "scikit-learn, AIF360, Jupyter Notebook",
                                  "tips": "Use StratifiedKFold para preservar distribuição de grupos sensíveis em folds",
                                  "learningObjective": "Aplicar validação cruzada robusta para predições confiáveis em análise de viés",
                                  "commonMistakes": "Usar validação simples train/test em vez de CV, levando a overfitting; esquecer de usar probabilidades em vez de labels hard"
                                },
                                {
                                  "stepNumber": 3,
                                  "title": "Aplicar métricas de fairness usando AIF360",
                                  "subSteps": [
                                    "Crie um classificador wrapper AIF360StandardClassifier com seu modelo",
                                    "Calcule métricas como Disparate Impact, Equalized Odds, Statistical Parity Difference por grupo protegido",
                                    "Use BinaryLabelDatasetMetric para computar métricas em predições CV",
                                    "Aplique fairness audits: crie AuditSession e rode auditor.run() em thresholds variados",
                                    "Gere relatório com métricas discriminatórias (ex.: >0.8 ideal para Disparate Impact)"
                                  ],
                                  "verification": "Imprima relatório do auditor mostrando métricas por grupo (ex.: disparate_impact > 0.8 ou <1.25 aceitável)",
                                  "estimatedTime": "40 minutos",
                                  "materials": "AIF360 (fairness/auditing modules), predições do step anterior",
                                  "tips": "Teste múltiplas métricas; foque em sensíveis como raça/gênero separadamente",
                                  "learningObjective": "Dominar toolkit AIF360 para quantificar viés diferencial em desempenho",
                                  "commonMistakes": "Confundir privileged/unprivileged groups; usar thresholds fixos sem sweep"
                                },
                                {
                                  "stepNumber": 4,
                                  "title": "Analisar resultados, visualizar e interpretar viés",
                                  "subSteps": [
                                    "Visualize métricas com plots: bar charts de disparate impact por fold/grupo (matplotlib/seaborn)",
                                    "Compare desempenho (ACC, TPR, FPR) entre grupos privilegiados vs. não privilegiados",
                                    "Identifique padrões de viés (ex.: FPR alto em minorias)",
                                    "Gere relatório final: tabela com métricas CV médias e recomendações de mitigação",
                                    "Salve resultados em JSON/CSV para reprodutibilidade"
                                  ],
                                  "verification": "Confira plots e relatório mostrando viés quantificado (ex.: disparate impact=0.75 indica viés)",
                                  "estimatedTime": "35 minutos",
                                  "materials": "matplotlib, seaborn, pandas para visualização/export",
                                  "tips": "Use groupby em DataFrame para métricas por grupo; documente thresholds usados",
                                  "learningObjective": "Interpretar métricas de fairness e comunicar achados de viés de forma acionável",
                                  "commonMistakes": "Ignorar variância entre folds; superinterpretar métricas isoladas sem contexto estatístico"
                                }
                              ],
                              "practicalExample": "Usando dataset Adult UCI para prever renda (>50k), com 'sex' como atributo sensível (male=privilegiado). Treine LogisticRegression via CV, aplique AIF360: descubra disparate_impact=0.65 para mulheres, indicando viés (mulheres menos promovidas). Visualize TPR mais baixo para mulheres.",
                              "finalVerifications": [
                                "Relatório AIF360 gerado com métricas por grupo sensível (Disparate Impact, Equal Opportunity Difference)",
                                "Validação cruzada realizada com pelo menos 5 folds estratificados",
                                "Visualizações mostram diferenças de desempenho entre grupos (plots salvos)",
                                "Recomendações de mitigação baseadas em achados (ex.: reweighting)",
                                "Código reprodutível em notebook com dataset sintético ou real",
                                "Métricas gerais (ACC) reportadas junto com fairness scores"
                              ],
                              "assessmentCriteria": [
                                "Correta instalação e uso de AIF360 (sem erros de import)",
                                "Dataset preparado com protected attributes explicitamente definidos",
                                "Métricas de fairness calculadas precisamente em CV (tolerância 1e-3)",
                                "Análise inclui múltiplos grupos sensíveis e thresholds",
                                "Interpretação discute implicações éticas e trade-offs com acurácia",
                                "Visualizações claras e relatório estruturado",
                                "Código limpo, comentado e modular"
                              ],
                              "crossCurricularConnections": [
                                "Estatística: Testes de hipótese para significância de diferenças (ex.: t-test em métricas por grupo)",
                                "Ética e Filosofia: Discussão de justiça distributiva em algoritmos",
                                "Programação: Manipulação avançada de dados com pandas e ML pipelines",
                                "Ciência de Dados: Integração com feature engineering para debiasing",
                                "Direito/Sociedade: Regulamentações como GDPR e AI Act sobre discriminação algorítmica"
                              ],
                              "realWorldApplication": "Em sistemas de RH para triagem de currículos (ex.: Amazon hiring tool biased contra mulheres), análise revela viés por gênero, permitindo mitigação via re-sampling antes de deploy em produção, evitando lawsuits e promovendo equidade em decisões automatizadas."
                            },
                            "estimatedTime": "3 horas",
                            "difficulty": "advanced",
                            "status": "not_started",
                            "prerequisites": []
                          }
                        ]
                      },
                      {
                        "id": "10.1.6.3.3",
                        "name": "Mitigação de Viés e Promoção de Equidade",
                        "description": "Explorar técnicas pré-processamento, em-processamento e pós-processamento para reduzir viés e garantir decisões justas em aplicações de ciência de dados.",
                        "specificSkills": [
                          {
                            "id": "10.1.6.3.3.1",
                            "name": "Aplicar técnicas de reamostragem e reponderação",
                            "description": "Implementar oversampling (ex.: SMOTE) ou undersampling para balancear dados, e reponderação de amostras para mitigar viés de seleção em treinamento de modelos.",
                            "atomicExpansion": {
                              "steps": [
                                {
                                  "stepNumber": 1,
                                  "title": "Análise inicial do desbalanceamento e viés nos dados",
                                  "subSteps": [
                                    "Carregue o dataset usando pandas e explore as distribuições das classes com value_counts() e gráficos de barras.",
                                    "Calcule métricas de desbalanceamento, como proporção entre classes majoritária e minoritária.",
                                    "Identifique viés de seleção verificando correlações entre features e target, e estatísticas descritivas por classe.",
                                    "Visualize distribuições com histogramas ou boxplots para detectar padrões de viés.",
                                    "Documente as evidências de desbalanceamento e viés em um relatório inicial."
                                  ],
                                  "verification": "Relatório gerado com gráficos e métricas mostrando proporções de classes (ex.: 90% vs 10%) e evidências de viés.",
                                  "estimatedTime": "1 hora",
                                  "materials": [
                                    "Python",
                                    "Pandas",
                                    "Matplotlib/Seaborn",
                                    "Jupyter Notebook"
                                  ],
                                  "tips": "Use seaborn para visualizações mais claras; sempre compare distribuições antes de qualquer resampling.",
                                  "learningObjective": "Identificar e quantificar desbalanceamento e viés de seleção em datasets.",
                                  "commonMistakes": [
                                    "Ignorar viés em features não-target",
                                    "Usar apenas value_counts() sem visualizações",
                                    "Confundir desbalanceamento com ruído nos dados"
                                  ]
                                },
                                {
                                  "stepNumber": 2,
                                  "title": "Implementação de Undersampling",
                                  "subSteps": [
                                    "Instale e importe imbalanced-learn; selecione uma técnica como RandomUnderSampler ou NearMiss.",
                                    "Aplique undersampling ao dataset de treinamento, ajustando parâmetros como sampling_strategy='auto'.",
                                    "Verifique o novo balanceamento com value_counts() e plote distribuições antes/depois.",
                                    "Divida os dados balanceados em train/test e treine um modelo baseline (ex.: LogisticRegression).",
                                    "Compare métricas como precision, recall e F1-score para a classe minoritária."
                                  ],
                                  "verification": "Dataset undersampled com classes balanceadas e relatório de métricas mostrando melhora no recall da classe minoritária.",
                                  "estimatedTime": "1.5 horas",
                                  "materials": [
                                    "imbalanced-learn",
                                    "Scikit-learn",
                                    "Pandas"
                                  ],
                                  "tips": "Aplique resampling apenas no train set para evitar data leakage; use cross-validation para robustez.",
                                  "learningObjective": "Aplicar undersampling para reduzir a classe majoritária de forma controlada.",
                                  "commonMistakes": [
                                    "Aplicar undersampling no dataset completo",
                                    "Escolher kNN sem dados suficientes",
                                    "Não avaliar impacto nas métricas"
                                  ]
                                },
                                {
                                  "stepNumber": 3,
                                  "title": "Implementação de Oversampling com SMOTE",
                                  "subSteps": [
                                    "Instale imbalanced-learn e importe SMOTE; configure parâmetros como k_neighbors=5 e sampling_strategy='auto'.",
                                    "Aplique SMOTE ao train set para gerar amostras sintéticas da classe minoritária.",
                                    "Visualize e valide as novas amostras com PCA ou t-SNE para garantir realismo.",
                                    "Treine o modelo com dados oversampled e compare métricas com baseline e undersampling.",
                                    "Ajuste hiperparâmetros de SMOTE via GridSearchCV para otimização."
                                  ],
                                  "verification": "Dataset oversampled com SMOTE, gráficos de validação e tabela comparativa de métricas superior ao baseline.",
                                  "estimatedTime": "2 horas",
                                  "materials": [
                                    "imbalanced-learn (SMOTE)",
                                    "Scikit-learn",
                                    "PCA/t-SNE de sklearn"
                                  ],
                                  "tips": "SMOTE funciona melhor com dados numéricos; normalize features antes; evite oversampling excessivo para não overfitting.",
                                  "learningObjective": "Gerar amostras sintéticas realistas para balancear classes minoritárias.",
                                  "commonMistakes": [
                                    "Usar SMOTE em datasets com alta dimensionalidade sem redução",
                                    "Não validar sinteticidade das amostras",
                                    "Aplicar em test set"
                                  ]
                                },
                                {
                                  "stepNumber": 4,
                                  "title": "Aplicação de reponderação de amostras e avaliação comparativa",
                                  "subSteps": [
                                    "Configure class_weight='balanced' em modelos como LogisticRegression ou use compute_class_weight do sklearn.",
                                    "Treine múltiplos modelos com diferentes técnicas (undersample, oversample, reponderação) usando o mesmo dataset original.",
                                    "Gere matriz de confusão, ROC-AUC e Precision-Recall curves para comparação.",
                                    "Analise trade-offs (ex.: recall vs precisão) e selecione a melhor técnica com base no contexto ético.",
                                    "Documente recomendações para mitigação de viés em produção."
                                  ],
                                  "verification": "Tabela comparativa de métricas de todos os métodos, com escolha justificada da melhor abordagem.",
                                  "estimatedTime": "1.5 horas",
                                  "materials": [
                                    "Scikit-learn (class_weight, metrics)",
                                    "Matplotlib para plots"
                                  ],
                                  "tips": "Reponderação é eficiente para datasets grandes; combine com validação cruzada estratificada.",
                                  "learningObjective": "Usar pesos de classe para mitigar viés sem alterar o dataset.",
                                  "commonMistakes": [
                                    "Confundir reponderação com resampling",
                                    "Não estratificar splits",
                                    "Ignorar custo computacional em datasets grandes"
                                  ]
                                }
                              ],
                              "practicalExample": "Em um dataset de aprovação de empréstimos (ex.: UCI Credit Approval, com 90% aprovações e 10% rejeições), aplique undersampling para reduzir aprovações, SMOTE para gerar rejeições sintéticas e reponderação em um RandomForestClassifier. Compare F1-score antes/depois, melhorando equidade na predição de rejeições minoritárias.",
                              "finalVerifications": [
                                "Distribuições de classes balanceadas ou pesos ajustados comprovados por value_counts() e gráficos.",
                                "Métricas de modelo (F1, recall minoritária) melhoradas em pelo menos 20% vs baseline.",
                                "Ausência de data leakage confirmada por splits corretos.",
                                "Relatório com trade-offs éticos e técnicos documentado.",
                                "Código reproduzível em Jupyter com todas as técnicas implementadas.",
                                "Validação visual de amostras SMOTE via redução dimensional."
                              ],
                              "assessmentCriteria": [
                                "Identificação precisa de desbalanceamento (>90% acurácia em métricas iniciais).",
                                "Implementação correta de pelo menos duas técnicas sem erros de sintaxe ou lógica.",
                                "Análise comparativa quantitativa com tabelas e gráficos claros.",
                                "Justificativa ética da escolha da técnica (equidade vs performance).",
                                "Código limpo, comentado e com boas práticas (ex.: seed para reprodutibilidade).",
                                "Interpretação correta de métricas, focando em classe minoritária."
                              ],
                              "crossCurricularConnections": [
                                "Estatística: Distribuições probabilísticas e testes de hipóteses para viés.",
                                "Ética e Filosofia: Princípios de justiça e equidade em algoritmos.",
                                "Programação: Manipulação de dados em Python com bibliotecas especializadas.",
                                "Machine Learning: Avaliação de modelos e otimização de hiperparâmetros.",
                                "Matemática Computacional: Álgebra linear em SMOTE e pesos."
                              ],
                              "realWorldApplication": "Em sistemas de detecção de fraudes bancárias (fraudes raras ~1%), onde undersampling/SMOTE/reponderação evitam viés que ignora fraudes, promovendo decisões justas e reduzindo perdas financeiras; ou em diagnósticos médicos raros, garantindo recall alto para doenças minoritárias."
                            },
                            "estimatedTime": "2 horas",
                            "difficulty": "intermediate",
                            "status": "not_started",
                            "prerequisites": []
                          },
                          {
                            "id": "10.1.6.3.3.2",
                            "name": "Usar algoritmos fair-aware",
                            "description": "Treinar modelos com restrições de equidade, como regularização de disparidade em árvores de decisão ou redes neurais, para otimizar precisão e fairness simultaneamente.",
                            "atomicExpansion": {
                              "steps": [
                                {
                                  "stepNumber": 1,
                                  "title": "Compreender Métricas de Equidade e Identificar Viés no Dataset",
                                  "subSteps": [
                                    "Estude métricas de fairness como Demographic Parity, Equalized Odds e Disparate Impact.",
                                    "Carregue um dataset com viés conhecido (ex: Adult UCI ou German Credit) usando pandas.",
                                    "Calcule métricas de viés baseline sem intervenções usando bibliotecas como AIF360.",
                                    "Visualize distribuições de features sensíveis (gênero, raça) com seaborn.",
                                    "Defina thresholds aceitáveis para fairness (ex: Disparate Impact > 0.8)."
                                  ],
                                  "verification": "Gere um relatório com métricas baseline mostrando viés presente (ex: Demographic Parity < 0.8).",
                                  "estimatedTime": "2 hours",
                                  "materials": "Python, pandas, AIF360, seaborn, Jupyter Notebook",
                                  "tips": "Sempre normalize features sensíveis antes de calcular métricas para evitar escalas distorcidas.",
                                  "learningObjective": "Identificar e quantificar viés em datasets para guiar mitigação.",
                                  "commonMistakes": "Ignorar correlações entre features proxy e sensíveis, levando a viés oculto."
                                },
                                {
                                  "stepNumber": 2,
                                  "title": "Implementar Regularização de Equidade em Árvores de Decisão",
                                  "subSteps": [
                                    "Instale e importe fairlearn ou scikit-fairlearn para reducers de viés.",
                                    "Treine uma árvore de decisão padrão com sklearn e meça precisão/fairness.",
                                    "Aplique regularização como Massaging ou Reweighing no dataset.",
                                    "Treine árvore fair-aware com constraints (ex: max depth limitada por grupo sensível).",
                                    "Compare métricas pré e pós-mitigação."
                                  ],
                                  "verification": "Modelo treinado atinge precisão > 80% e Demographic Parity > 0.85.",
                                  "estimatedTime": "3 hours",
                                  "materials": "Python, scikit-learn, fairlearn, AIF360",
                                  "tips": "Use GridSearchCV para hiperparâmetros que balanceiem accuracy e fairness.",
                                  "learningObjective": "Aplicar técnicas de regularização para árvores de decisão fair-aware.",
                                  "commonMistakes": "Sobrerregularizar, sacrificando precisão desnecessariamente."
                                },
                                {
                                  "stepNumber": 3,
                                  "title": "Treinar Redes Neurais com Restrições de Fairness",
                                  "subSteps": [
                                    "Prepare dataset para PyTorch/TensorFlow com split train/test.",
                                    "Defina loss function custom com termo de regularização de disparidade (ex: lambda * |DP_diff|).",
                                    "Implemente rede neural simples (MLP) com constraints em camadas finais.",
                                    "Treine com otimizador Adam, monitorando loss fairness via callbacks.",
                                    "Ajuste hiperparâmetros lambda via validação cruzada."
                                  ],
                                  "verification": "Rede converge com loss total < 0.5 e Equalized Odds < 0.1.",
                                  "estimatedTime": "4 hours",
                                  "materials": "Python, PyTorch ou TensorFlow, AIF360 metrics",
                                  "tips": "Monitore gradients de fairness loss para evitar vanishing gradients em constraints.",
                                  "learningObjective": "Integrar fairness em treinamento de deep learning via regularização.",
                                  "commonMistakes": "Não escalar lambda adequadamente, dominando ou ignorando o termo de fairness."
                                },
                                {
                                  "stepNumber": 4,
                                  "title": "Avaliar, Otimizar e Comparar Modelos Fair-Aware",
                                  "subSteps": [
                                    "Teste todos os modelos em conjunto de teste hold-out.",
                                    "Calcule trade-off curve (precisão vs. fairness) com múltiplos lambdas.",
                                    "Otimize Pareto frontier usando otimização multi-objetivo (ex: NSGA-II).",
                                    "Gere relatório comparativo com tabelas e ROC curves por grupo.",
                                    "Documente escolhas finais baseadas em requisitos de negócio."
                                  ],
                                  "verification": "Relatório final mostra melhoria em fairness sem perda >10% em precisão.",
                                  "estimatedTime": "2 hours",
                                  "materials": "Matplotlib, pandas, scikit-optimize",
                                  "tips": "Priorize métricas de grupo (group fairness) sobre overall para cenários sensíveis.",
                                  "learningObjective": "Otimizar modelos para balancear precisão e equidade simultaneamente.",
                                  "commonMistakes": "Avaliar apenas em métricas agregadas, mascarando viés em subgrupos."
                                }
                              ],
                              "practicalExample": "Em um sistema de aprovação de empréstimos, use um dataset como German Credit com feature sensível 'idade >50'. Treine uma árvore de decisão fair-aware com Reweighing para reduzir disparate impact de 0.6 para 0.9, mantendo accuracy em 85%, evitando discriminação etária.",
                              "finalVerifications": [
                                "Métricas de fairness melhoradas em pelo menos 20% vs. baseline.",
                                "Precisão mantida dentro de 5-10% do modelo não-fair.",
                                "Código reproduzível gera resultados consistentes em seeds fixas.",
                                "Trade-off analisado com curvas Pareto.",
                                "Relatório documenta decisões e limitações."
                              ],
                              "assessmentCriteria": [
                                "Compreensão demonstrada de pelo menos 3 métricas de fairness.",
                                "Implementação correta de regularização em árvores e NNs.",
                                "Análise quantitativa de trade-offs com evidências.",
                                "Código limpo, comentado e modular.",
                                "Aplicação ética discutida em relatório.",
                                "Reprodutibilidade e robustez testada."
                              ],
                              "crossCurricularConnections": [
                                "Ética em IA: Princípios de justiça algorítmica.",
                                "Direito e Políticas Públicas: Regulamentações anti-discriminação (GDPR, AI Act).",
                                "Estatística: Testes de hipóteses para disparidades.",
                                "Programação: Otimização multi-objetivo."
                              ],
                              "realWorldApplication": "Desenvolvimento de sistemas de RH para recrutamento justo, reduzindo viés de gênero/raça em seleções automáticas, ou em saúde para diagnósticos equitativos por etnia."
                            },
                            "estimatedTime": "3 horas",
                            "difficulty": "advanced",
                            "status": "not_started",
                            "prerequisites": []
                          },
                          {
                            "id": "10.1.6.3.3.3",
                            "name": "Implementar ajustes pós-processamento",
                            "description": "Ajustar thresholds de decisão ou previsões pós-treinamento para equalizar taxas de erro falso positivo/negativo entre grupos, garantindo conformidade com critérios de equidade.",
                            "atomicExpansion": {
                              "steps": [
                                {
                                  "stepNumber": 1,
                                  "title": "Avaliar Viés Atual no Modelo",
                                  "subSteps": [
                                    "Carregue o modelo treinado e o conjunto de teste com labels sensíveis (ex: gênero, raça).",
                                    "Calcule métricas de erro por grupo: False Positive Rate (FPR), False Negative Rate (FNR), True Positive Rate (TPR).",
                                    "Visualize disparidades usando gráficos de ROC por grupo e tabelas de confusão.",
                                    "Identifique o grupo com maior disparidade em FPR/FNR.",
                                    "Documente as taxas atuais para baseline."
                                  ],
                                  "verification": "Tabela de métricas mostrando FPR/FNR por grupo gerada e salva.",
                                  "estimatedTime": "1.5 horas",
                                  "materials": [
                                    "Python com scikit-learn, pandas, matplotlib",
                                    "Dataset com atributos sensíveis (ex: Adult UCI)"
                                  ],
                                  "tips": "Use groupby para segmentar métricas; normalize rates para comparações justas.",
                                  "learningObjective": "Compreender e quantificar viés de predição em subgrupos protegidos.",
                                  "commonMistakes": [
                                    "Ignorar normalização por prevalência de classe",
                                    "Confundir FPR com accuracy geral",
                                    "Não estratificar teste por grupos sensíveis"
                                  ]
                                },
                                {
                                  "stepNumber": 2,
                                  "title": "Definir Critérios de Equidade e Thresholds Iniciais",
                                  "subSteps": [
                                    "Escolha um critério de equidade: Equalized Odds (FPR e TPR iguais) ou Equal FPR/FNR.",
                                    "Defina thresholds iniciais: use 0.5 para todos os grupos como baseline.",
                                    "Estabeleça tolerâncias: ex: diferença máxima de 0.05 entre grupos.",
                                    "Gere curvas ROC separadas por grupo para identificar faixas viáveis de thresholds.",
                                    "Formalize a função objetivo: minimizar |FPR_g1 - FPR_g2| + |FNR_g1 - FNR_g2|."
                                  ],
                                  "verification": "Documento com critérios definidos, thresholds iniciais e tolerâncias anotados.",
                                  "estimatedTime": "1 hora",
                                  "materials": [
                                    "Jupyter Notebook",
                                    "Bibliotecas: scikit-learn.metrics, fairlearn"
                                  ],
                                  "tips": "Priorize Equalized Odds para tarefas de classificação binária crítica.",
                                  "learningObjective": "Selecionar e justificar critérios de equidade pós-processamento.",
                                  "commonMistakes": [
                                    "Escolher threshold único sem considerar trade-offs",
                                    "Definir tolerâncias muito rígidas sem contexto",
                                    "Ignorar impacto na accuracy geral"
                                  ]
                                },
                                {
                                  "stepNumber": 3,
                                  "title": "Implementar Ajustes Iterativos de Thresholds",
                                  "subSteps": [
                                    "Para cada grupo, otimize threshold independentemente usando grid search em ROC para target FPR.",
                                    "Calcule thresholds que equalizem FPR/FNR: resolva numericamente ou via otimização.",
                                    "Aplique thresholds ajustados nas predições e recalcule métricas.",
                                    "Itere: ajuste com base em disparidades residuais até convergência dentro da tolerância.",
                                    "Compare com baseline: plote before/after disparidades."
                                  ],
                                  "verification": "Código executado com thresholds otimizados e métricas equalizadas dentro da tolerância.",
                                  "estimatedTime": "2 horas",
                                  "materials": [
                                    "Python: numpy, scipy.optimize",
                                    "Fairlearn ou AIF360 para thresholds"
                                  ],
                                  "tips": "Use otimização bayesiana para eficiência em grandes espaços.",
                                  "learningObjective": "Executar otimização de thresholds para mitigar disparidades.",
                                  "commonMistakes": [
                                    "Overfitting thresholds ao teste",
                                    "Não validar em hold-out set",
                                    "Aplicar threshold global sem group-specific"
                                  ]
                                },
                                {
                                  "stepNumber": 4,
                                  "title": "Validar e Integrar no Pipeline",
                                  "subSteps": [
                                    "Teste em conjunto de validação independente para evitar leakage.",
                                    "Avalie trade-offs: accuracy, precision, recall pós-ajuste.",
                                    "Integre no pipeline de inferência: função que aplica threshold por grupo sensível.",
                                    "Monitore drift: defina alertas para recálculo periódico.",
                                    "Documente o processo para auditoria de conformidade."
                                  ],
                                  "verification": "Pipeline funcional com testes unitários passando e relatório de validação.",
                                  "estimatedTime": "1.5 horas",
                                  "materials": [
                                    "Git para versionamento",
                                    "Testes com pytest",
                                    "Docker para pipeline"
                                  ],
                                  "tips": "Mantenha thresholds como hiperparâmetros treináveis.",
                                  "learningObjective": "Integrar mitigação de viés de forma robusta e auditável.",
                                  "commonMistakes": [
                                    "Leakage de validação",
                                    "Ignorar custo computacional de group-specific",
                                    "Falta de documentação para compliance"
                                  ]
                                },
                                {
                                  "stepNumber": 5,
                                  "title": "Avaliar Impacto Final e Iterar",
                                  "subSteps": [
                                    "Compare métricas finais vs baseline em múltiplos conjuntos.",
                                    "Realize análise de sensibilidade: varie critérios e thresholds.",
                                    "Colete feedback ético: consulte stakeholders sobre trade-offs.",
                                    "Prepare relatório com visualizações e recomendações.",
                                    "Planeje monitoramento contínuo pós-deploy."
                                  ],
                                  "verification": "Relatório final gerado com todas as comparações e aprovações.",
                                  "estimatedTime": "1 hora",
                                  "materials": [
                                    "Relatório em Markdown/PDF",
                                    "Ferramentas de visualização: seaborn"
                                  ],
                                  "tips": "Use dashboards como Streamlit para relatórios interativos.",
                                  "learningObjective": "Avaliar holisticamente o impacto da mitigação de viés.",
                                  "commonMistakes": [
                                    "Focar só em métricas de equidade ignorando utility",
                                    "Não considerar adversarial attacks",
                                    "Parar sem plano de manutenção"
                                  ]
                                }
                              ],
                              "practicalExample": "Em um modelo de aprovação de empréstimos usando o dataset German Credit, calcule FPR por gênero (homens vs mulheres). Baseline: FPR_homens=0.15, FPR_mulheres=0.30. Otimize thresholds: ajuste para 0.45 (homens) e 0.35 (mulheres), equalizando FPR em ~0.20, reduzindo disparidade em 33% sem perda significativa de accuracy.",
                              "finalVerifications": [
                                "FPR e FNR equalizados dentro de 0.05 entre grupos.",
                                "Thresholds aplicados corretamente no pipeline de inferência.",
                                "Relatório de métricas before/after salvo e visualizado.",
                                "Testes unitários passando para group-specific thresholds.",
                                "Trade-off de accuracy <5% comparado ao baseline.",
                                "Documentação de critérios e processo completa."
                              ],
                              "assessmentCriteria": [
                                "Precisão na identificação e quantificação de viés inicial (90%+ accuracy em métricas).",
                                "Thresholds otimizados convergem para equidade dentro da tolerância definida.",
                                "Código limpo, modular e reproduzível com seeds fixas.",
                                "Análise de trade-offs documentada quantitativamente.",
                                "Integração robusta com validação cruzada.",
                                "Relatório reflete compreensão ética e prática."
                              ],
                              "crossCurricularConnections": [
                                "Estatística: Métricas de confusão e curvas ROC.",
                                "Ética e Filosofia: Princípios de justiça distributiva em IA.",
                                "Programação: Otimização numérica com SciPy.",
                                "Gestão de Projetos: Auditoria e monitoramento contínuo.",
                                "Negócios: Impacto regulatório (GDPR, AI Act)."
                              ],
                              "realWorldApplication": "Em sistemas de justiça criminal (ex: COMPAS), ajustar thresholds reduz sentenças injustas para minorias; em recrutamento (ex: Amazon hiring tool), equaliza callbacks por gênero; em saúde (ex: diagnósticos), mitiga erros em subgrupos étnicos para conformidade HIPAA."
                            },
                            "estimatedTime": "2.5 horas",
                            "difficulty": "advanced",
                            "status": "not_started",
                            "prerequisites": []
                          },
                          {
                            "id": "10.1.6.3.3.4",
                            "name": "Documentar e auditar processos éticos",
                            "description": "Criar relatórios de auditoria de viés, incluindo trade-offs entre utilidade e equidade, e boas práticas para transparência em projetos de ciência de dados.",
                            "atomicExpansion": {
                              "steps": [
                                {
                                  "stepNumber": 1,
                                  "title": "Definir o Escopo e Objetivos da Auditoria Ética",
                                  "subSteps": [
                                    "Identificar o projeto de ciência de dados a ser auditado, incluindo dataset, modelo e pipeline.",
                                    "Estabelecer objetivos claros: detectar viés, avaliar equidade e documentar trade-offs.",
                                    "Mapear stakeholders envolvidos (equipe de dados, liderança, usuários impactados).",
                                    "Definir métricas de viés (ex: disparate impact, equalized odds) e de utilidade (ex: accuracy, F1-score).",
                                    "Criar um checklist inicial de boas práticas éticas (transparência, reprodutibilidade)."
                                  ],
                                  "verification": "Checklist preenchido e aprovado por um revisor, com escopo documentado em um template inicial.",
                                  "estimatedTime": "2-4 horas",
                                  "materials": "Template de escopo de auditoria (Google Docs ou Jupyter Notebook), documentação do projeto original.",
                                  "tips": "Envolva stakeholders cedo para alinhar expectativas e evitar escopo creep.",
                                  "learningObjective": "Compreender os componentes essenciais de uma auditoria ética em projetos de dados.",
                                  "commonMistakes": "Definir escopo muito amplo, ignorando limitações de tempo/recursos."
                                },
                                {
                                  "stepNumber": 2,
                                  "title": "Coletar Evidências e Analisar Viés e Mitigações",
                                  "subSteps": [
                                    "Coletar logs, código e relatórios prévios do pipeline de dados e modelagem.",
                                    "Executar testes de viés usando ferramentas como AIF360 ou Fairlearn em subgrupos demográficos.",
                                    "Documentar mitigações aplicadas (ex: reamostragem, reweighting, adversarial debiasing).",
                                    "Registrar evidências fotográficas ou screenshots de análises quantitativas.",
                                    "Compilar dados brutos em uma tabela comparativa de antes/depois das mitigações."
                                  ],
                                  "verification": "Tabela de evidências completa com métricas calculadas e visualizações geradas.",
                                  "estimatedTime": "4-6 horas",
                                  "materials": "Bibliotecas Python (AIF360, Fairlearn), dataset de teste, Jupyter Notebook.",
                                  "tips": "Automatize cálculos de métricas com scripts reutilizáveis para eficiência.",
                                  "learningObjective": "Aplicar ferramentas padrão para quantificar viés e validar mitigações.",
                                  "commonMistakes": "Focar apenas em métricas de viés, negligenciando análise qualitativa de contexto."
                                },
                                {
                                  "stepNumber": 3,
                                  "title": "Avaliar Trade-offs entre Utilidade e Equidade",
                                  "subSteps": [
                                    "Calcular trade-offs quantitativos: plotar curvas de Pareto (utilidade vs equidade).",
                                    "Analisar impactos qualitativos: quem é afetado por reduções em performance geral?",
                                    "Documentar justificativas para escolhas de trade-off, baseadas em requisitos do negócio.",
                                    "Comparar com benchmarks éticos (ex: guidelines da ACM ou EU AI Act).",
                                    "Priorizar trade-offs com matriz de decisão (risco alto/baixo, impacto alto/baixo)."
                                  ],
                                  "verification": "Gráficos de trade-off e matriz de decisão incluídos no rascunho do relatório.",
                                  "estimatedTime": "3-5 horas",
                                  "materials": "Ferramentas de plotagem (Matplotlib, Seaborn), planilhas para matriz de decisão.",
                                  "tips": "Use visualizações claras para comunicar trade-offs complexos a não-técnicos.",
                                  "learningObjective": "Balancear objetivos conflitantes de utilidade e equidade de forma transparente.",
                                  "commonMistakes": "Ignorar trade-offs, assumindo que mitigações não afetam performance geral."
                                },
                                {
                                  "stepNumber": 4,
                                  "title": "Redigir o Relatório Final com Boas Práticas de Transparência",
                                  "subSteps": [
                                    "Estruturar o relatório: introdução, metodologia, achados, trade-offs, recomendações.",
                                    "Incluir seções de transparência: código-fonte linkado, dataset anonimizado, reprodutibilidade.",
                                    "Listar boas práticas: versionamento, auditorias regulares, disclosure de limitações.",
                                    "Adicionar apêndices com dados brutos, código e referências bibliográficas.",
                                    "Revisar linguagem para acessibilidade (evitar jargão excessivo)."
                                  ],
                                  "verification": "Relatório completo formatado (PDF/Markdown) com todas seções e links funcionais.",
                                  "estimatedTime": "4-6 horas",
                                  "materials": "Templates de relatório (LaTeX, Markdown to PDF), GitHub para versionamento.",
                                  "tips": "Use headings e sumários executivos para facilitar navegação.",
                                  "learningObjective": "Produzir documentação ética profissional e transparente.",
                                  "commonMistakes": "Omitir limitações ou fontes de dados sensíveis sem anonimização."
                                },
                                {
                                  "stepNumber": 5,
                                  "title": "Revisar, Validar e Publicar o Relatório",
                                  "subSteps": [
                                    "Solicitar feedback de pares e stakeholders via revisão colaborativa.",
                                    "Verificar conformidade com padrões éticos (ex: checklist de transparência).",
                                    "Testar reprodutibilidade executando pipeline documentado.",
                                    "Arquivar versão final em repositório acessível (interno ou público).",
                                    "Planejar follow-up: auditorias futuras e monitoramento contínuo."
                                  ],
                                  "verification": "Feedback incorporado, relatório assinado e arquivado com log de mudanças.",
                                  "estimatedTime": "2-4 horas",
                                  "materials": "Ferramentas de colaboração (Google Docs, GitHub Pull Requests), checklist de validação.",
                                  "tips": "Defina prazos curtos para feedback para manter momentum.",
                                  "learningObjective": "Garantir qualidade e accountability através de revisão iterativa.",
                                  "commonMistakes": "Pular revisão, resultando em erros factuais ou omissões."
                                }
                              ],
                              "practicalExample": "Em um projeto de scoring de crédito bancário, auditar um modelo de ML: identificar viés racial no dataset histórico, documentar mitigação via reamostragem (reduzindo accuracy de 92% para 88%, mas equalizando disparate impact de 0.65 para 0.95), e relatar trade-offs justificando a escolha pela equidade regulatória, com código no GitHub para transparência.",
                              "finalVerifications": [
                                "Relatório inclui análise quantitativa de viés pré e pós-mitigação.",
                                "Trade-offs entre utilidade e equidade são explicitamente plotados e justificados.",
                                "Boas práticas de transparência (código, dados anonimizados, reprodutibilidade) estão linkadas.",
                                "Checklist de conformidade ética está completo e assinado.",
                                "Recomendações acionáveis para iterações futuras estão listadas.",
                                "Feedback de stakeholders foi incorporado e documentado."
                              ],
                              "assessmentCriteria": [
                                "Completude: Todas seções do relatório presentes e detalhadas (30%).",
                                "Precisão: Métricas de viés e trade-offs calculadas corretamente (25%).",
                                "Transparência: Facilidade de reprodução e compreensão (20%).",
                                "Profundidade de análise: Trade-offs e contextos qualitativos bem explorados (15%).",
                                "Clareza e profissionalismo: Linguagem acessível e estrutura lógica (10%)."
                              ],
                              "crossCurricularConnections": [
                                "Ética Filosófica: Debates sobre utilitarismo vs. justiça distributiva.",
                                "Direito e Regulamentação: Conformidade com GDPR, AI Act e leis anti-discriminação.",
                                "Comunicação Técnica: Redação de relatórios para audiências mistas.",
                                "Gestão de Projetos: Planejamento de auditorias e gerenciamento de stakeholders."
                              ],
                              "realWorldApplication": "Em empresas como Google ou bancos (ex: JPMorgan), esses relatórios são mandatórios para compliance regulatório, evitam multas milionárias por discriminação algorítmica e constroem confiança pública em sistemas de IA, como em ferramentas de recrutamento ou diagnósticos médicos."
                            },
                            "estimatedTime": "1.5 horas",
                            "difficulty": "intermediate",
                            "status": "not_started",
                            "prerequisites": []
                          }
                        ]
                      }
                    ]
                  },
                  {
                    "id": "10.1.6.4",
                    "name": "Transparência na Manipulação de Dados",
                    "description": "Práticas para documentar e tornar transparentes os processos de limpeza, integração e análise de dados.",
                    "individualConcepts": [
                      {
                        "id": "10.1.6.4.1",
                        "name": "Documentação de Processos de Limpeza de Dados",
                        "description": "Práticas essenciais para registrar de forma detalhada todas as etapas de limpeza de dados, incluindo identificação de problemas como valores ausentes, duplicatas e outliers, garantindo que qualquer analista possa entender e reproduzir as transformações aplicadas.",
                        "specificSkills": [
                          {
                            "id": "10.1.6.4.1.1",
                            "name": "Identificar e registrar problemas de qualidade nos dados",
                            "description": "Aprender a inspecionar conjuntos de dados para detectar valores ausentes, duplicatas, outliers e inconsistências, e documentar essas descobertas em relatórios ou comentários no código, utilizando ferramentas como pandas em Python.",
                            "atomicExpansion": {
                              "steps": [
                                {
                                  "stepNumber": 1,
                                  "title": "Carregar e realizar inspeção inicial do dataset",
                                  "subSteps": [
                                    "Importar a biblioteca pandas com o alias pd.",
                                    "Carregar o dataset de um arquivo CSV ou similar usando pd.read_csv().",
                                    "Executar df.info() para verificar tipos de dados, valores não nulos e memória usada.",
                                    "Executar df.describe() para obter estatísticas descritivas básicas.",
                                    "Visualizar as primeiras linhas com df.head() e df.tail() para inspeção manual."
                                  ],
                                  "verification": "Verificar se o dataset foi carregado corretamente sem erros e se as estatísticas iniciais são exibidas no console.",
                                  "estimatedTime": "15 minutos",
                                  "materials": "Python com pandas instalado, arquivo CSV de exemplo com dados simulados (ex: dataset de vendas).",
                                  "tips": "Sempre defina o encoding='utf-8' ao carregar arquivos para evitar problemas com caracteres especiais.",
                                  "learningObjective": "Entender a estrutura básica do dataset e identificar potenciais problemas iniciais via inspeção.",
                                  "commonMistakes": "Esquecer de importar pandas ou usar caminho incorreto para o arquivo, causando erro de carregamento."
                                },
                                {
                                  "stepNumber": 2,
                                  "title": "Detectar valores ausentes e duplicatas",
                                  "subSteps": [
                                    "Identificar valores ausentes com df.isnull().sum() e df.isnull().sum() / len(df) * 100 para percentuais.",
                                    "Visualizar missing values com df.isnull().head(10) ou seaborn heatmap se disponível.",
                                    "Verificar duplicatas com df.duplicated().sum() e listar com df[df.duplicated()].",
                                    "Remover temporariamente duplicatas para análise com df.drop_duplicates(inplace=True) e comparar tamanhos.",
                                    "Documentar contagens e localizações em um dicionário ou lista para relatório."
                                  ],
                                  "verification": "Relatório parcial gerado listando número e percentual de valores ausentes por coluna, e total de duplicatas.",
                                  "estimatedTime": "20 minutos",
                                  "materials": "Notebook Jupyter ou script Python, dataset carregado do step anterior.",
                                  "tips": "Use df.fillna(0) apenas para teste; nunca altere dados permanentemente sem documentar.",
                                  "learningObjective": "Dominar funções específicas do pandas para detecção de ausentes e duplicatas.",
                                  "commonMistakes": "Confundir NaN com zeros ou strings vazias; sempre use isnull() para NaN."
                                },
                                {
                                  "stepNumber": 3,
                                  "title": "Identificar outliers e inconsistências",
                                  "subSteps": [
                                    "Calcular quartis e IQR para cada coluna numérica: Q1 = df[col].quantile(0.25), Q3 = df[col].quantile(0.75), IQR = Q3 - Q1.",
                                    "Definir limites: lower = Q1 - 1.5*IQR, upper = Q3 + 1.5*IQR; filtrar outliers com (df[col] < lower) | (df[col] > upper).",
                                    "Verificar inconsistências categóricas: df[col].value_counts() para detectar variações de case ou erros de digitação.",
                                    "Usar boxplots com matplotlib ou seaborn para visualização gráfica de outliers.",
                                    "Registrar outliers com suas posições e valores em uma lista ou DataFrame separado."
                                  ],
                                  "verification": "Lista de outliers e inconsistências identificadas, com pelo menos um boxplot gerado.",
                                  "estimatedTime": "25 minutos",
                                  "materials": "Pandas, matplotlib ou seaborn instalados, dataset do step anterior.",
                                  "tips": "Ajuste o multiplicador IQR (ex: 1.5 padrão, 3 para outliers extremos) baseado no domínio dos dados.",
                                  "learningObjective": "Aplicar métodos estatísticos para detecção de anomalias em dados numéricos e categóricos.",
                                  "commonMistakes": "Aplicar detecção de outliers em colunas categóricas; sempre filtre por tipo de dado."
                                },
                                {
                                  "stepNumber": 4,
                                  "title": "Documentar problemas em relatório ou comentários no código",
                                  "subSteps": [
                                    "Criar um DataFrame de resumo com colunas: Problema, Coluna, Quantidade, Percentual, Exemplos.",
                                    "Adicionar comentários no código com # PROBLEMA: descrição detalhada.",
                                    "Gerar relatório em Markdown ou TXT usando pd.to_markdown() ou print formatado.",
                                    "Salvar relatório como arquivo: resumo_df.to_csv('problemas_qualidade.csv').",
                                    "Revisar e validar o relatório comparando com inspeções manuais."
                                  ],
                                  "verification": "Arquivo de relatório gerado contendo todos os problemas identificados de forma clara e estruturada.",
                                  "estimatedTime": "15 minutos",
                                  "materials": "Código dos steps anteriores, editor de texto ou Jupyter para Markdown.",
                                  "tips": "Use templates de relatório para padronização em projetos futuros.",
                                  "learningObjective": "Desenvolver hábitos de documentação transparente para reprodutibilidade.",
                                  "commonMistakes": "Omitir exemplos concretos ou posições dos problemas, tornando o relatório genérico."
                                }
                              ],
                              "practicalExample": "Ao analisar um dataset de vendas de uma loja online (colunas: data, produto, preco, quantidade), detecta-se 15% de valores ausentes em 'preco', 50 duplicatas exatas, outliers em 'quantidade' (vendas de 1000 unidades em dia normal), e inconsistências como 'Produto' vs 'produto'. Documenta-se em relatório: 'Outliers em quantidade: índices 45, 120 com valores 1500, 2000'.",
                              "finalVerifications": [
                                "Relatório completo gerado listando todos os tipos de problemas com quantidades e exemplos.",
                                "Código comentado permite reproduzir todas as detecções sem erros.",
                                "Visualizações (ex: heatmap de missings, boxplots) salvas ou exibidas.",
                                "Percentuais de problemas calculados corretamente e condizem com df.shape.",
                                "Nenhum problema foi alterado sem documentação prévia.",
                                "Relatório salvo em formato acessível (CSV/MD)."
                              ],
                              "assessmentCriteria": [
                                "Precisão na detecção: 100% dos problemas reais identificados (verificado manualmente).",
                                "Completude da documentação: Todos os tipos de problemas cobertos com exemplos.",
                                "Clareza do relatório: Estrutura legível, com tabelas e explicações concisas.",
                                "Eficiência do código: Uso correto de funções pandas sem loops desnecessários.",
                                "Visualizações adequadas: Gráficos relevantes para suporte às detecções.",
                                "Tempo respeitado: Conclusão dentro do estimado total (75 min)."
                              ],
                              "crossCurricularConnections": [
                                "Estatística: Uso de quartis e IQR para análise descritiva.",
                                "Programação: Manipulação de dados com Python e bibliotecas.",
                                "Ética em Dados: Transparência na identificação de vieses iniciais.",
                                "Comunicação: Redação de relatórios técnicos claros."
                              ],
                              "realWorldApplication": "Em empresas de e-commerce, identificar problemas em dados de clientes previne decisões erradas em campanhas de marketing; em pesquisa científica, garante validade de conclusões publicadas."
                            },
                            "estimatedTime": "2 horas",
                            "difficulty": "beginner",
                            "status": "not_started",
                            "prerequisites": []
                          },
                          {
                            "id": "10.1.6.4.1.2",
                            "name": "Documentar transformações de limpeza em notebooks",
                            "description": "Utilizar Jupyter Notebooks para executar e anotar operações de limpeza, como imputação de valores, normalização e remoção de ruídos, incluindo células de markdown para explicar o racional de cada decisão.",
                            "atomicExpansion": {
                              "steps": [
                                {
                                  "stepNumber": 1,
                                  "title": "Configurar o Jupyter Notebook e Carregar os Dados",
                                  "subSteps": [
                                    "Instalar e abrir o Jupyter Notebook ou JupyterLab.",
                                    "Criar um novo notebook e nomeá-lo adequadamente (ex: 'Limpeza_Dados_Vendas.ipynb').",
                                    "Importar bibliotecas necessárias: pandas, numpy, matplotlib ou seaborn.",
                                    "Carregar o dataset usando pd.read_csv() ou similar.",
                                    "Executar a célula de carregamento e visualizar as primeiras linhas com df.head()."
                                  ],
                                  "verification": "Célula executada sem erros e dados carregados corretamente visíveis no output.",
                                  "estimatedTime": "15 minutos",
                                  "materials": [
                                    "Jupyter Notebook instalado",
                                    "Dataset de exemplo (ex: CSV com dados sujos)",
                                    "Bibliotecas: pandas, numpy"
                                  ],
                                  "tips": "Use %matplotlib inline para plots inline; salve o notebook frequentemente com Ctrl+S.",
                                  "learningObjective": "Configurar ambiente Jupyter e carregar dados para inspeção inicial.",
                                  "commonMistakes": [
                                    "Esquecer de importar pandas como pd",
                                    "Caminho incorreto do arquivo CSV",
                                    "Não executar a célula de imports"
                                  ]
                                },
                                {
                                  "stepNumber": 2,
                                  "title": "Inspecionar Dados e Identificar Problemas de Limpeza",
                                  "subSteps": [
                                    "Adicionar célula Markdown: '# Inspeção Inicial dos Dados' com descrição do dataset.",
                                    "Executar df.info() e df.describe() para estatísticas.",
                                    "Verificar missing values com df.isnull().sum().",
                                    "Identificar outliers com boxplots ou histogramas.",
                                    "Adicionar célula Markdown resumindo problemas encontrados (ex: 20% missing em 'preco')."
                                  ],
                                  "verification": "Relatório em Markdown listando problemas como missing values, outliers e ruídos identificados.",
                                  "estimatedTime": "20 minutos",
                                  "materials": [
                                    "Dataset carregado",
                                    "Bibliotecas de visualização: matplotlib, seaborn"
                                  ],
                                  "tips": "Use df.shape para dimensões; combine múltiplos comandos em uma célula para eficiência.",
                                  "learningObjective": "Diagnosticar issues de dados usando métodos de inspeção e documentar achados.",
                                  "commonMistakes": [
                                    "Ignorar missing values categóricos",
                                    "Não plotar visualizações para outliers",
                                    "Markdown vago sem números específicos"
                                  ]
                                },
                                {
                                  "stepNumber": 3,
                                  "title": "Executar e Documentar Transformações de Limpeza",
                                  "subSteps": [
                                    "Para imputação: Adicionar Markdown explicando método (ex: média para numéricos), então df['coluna'].fillna(df['coluna'].mean()).",
                                    "Para normalização: Markdown com racional (ex: MinMaxScaler), aplicar e mostrar before/after.",
                                    "Para remoção de ruídos/outliers: Markdown justificando threshold (ex: IQR), filtrar df = df[(df['coluna'] >= Q1 - 1.5*IQR) & ...].",
                                    "Executar cada transformação em células separadas com print(before/after).",
                                    "Adicionar Markdown após cada uma: 'Racional: ... Impacto: reduziu missing de 20% para 0%.'"
                                  ],
                                  "verification": "Cada transformação tem célula de código + Markdown com racional, before/after visível.",
                                  "estimatedTime": "30 minutos",
                                  "materials": [
                                    "Dataset inspecionado",
                                    "Scikit-learn para normalização (opcional)"
                                  ],
                                  "tips": "Use variáveis como df_clean = df.copy() para não sobrescrever original; inclua shapes em prints.",
                                  "learningObjective": "Aplicar operações de limpeza com documentação transparente do racional.",
                                  "commonMistakes": [
                                    "Não copiar df antes de modificar",
                                    "Imputar sem justificar método",
                                    "Esquecer before/after comparisons"
                                  ]
                                },
                                {
                                  "stepNumber": 4,
                                  "title": "Verificar Resultados Finais e Finalizar o Notebook",
                                  "subSteps": [
                                    "Re-executar df.info() e df.describe() no dataset limpo.",
                                    "Adicionar seção Markdown: '# Verificações Finais' com summary de mudanças.",
                                    "Salvar visualizações finais (ex: histogramas pós-limpeza).",
                                    "Adicionar célula com df.to_csv('dados_limpos.csv') para exportar.",
                                    "Incluir conclusão Markdown: lições aprendidas e próximos passos."
                                  ],
                                  "verification": "Notebook completo roda do início ao fim sem erros; dataset exportado limpo.",
                                  "estimatedTime": "15 minutos",
                                  "materials": [
                                    "Notebook com transformações",
                                    "Espaço para salvar CSV"
                                  ],
                                  "tips": "Use Kernel > Restart & Run All para teste final; versionar com nbconvert para HTML.",
                                  "learningObjective": "Validar limpeza e produzir notebook reproduzível e bem documentado.",
                                  "commonMistakes": [
                                    "Não re-rodar todo o notebook",
                                    "Exportar sem verificações finais",
                                    "Conclusão genérica sem métricas"
                                  ]
                                }
                              ],
                              "practicalExample": "Em um dataset de vendas de e-commerce (CSV com colunas: id, produto, preco, data_venda), identifique 15% missing em 'preco', outliers em 'quantidade' (>1000), e normalize 'preco' para 0-1. Documente: imputar média em preco (racional: dados normais distribuídos), remover outliers via IQR, aplicar MinMaxScaler, mostrando df.shape antes/depois e plots.",
                              "finalVerifications": [
                                "Notebook executa completamente sem erros (Restart & Run All).",
                                "Cada transformação tem Markdown com racional explícito e métricas (ex: % missing reduzido).",
                                "Before/after comparisons visíveis para todas operações.",
                                "Dataset final exportado como CSV limpo.",
                                "Visualizações confirmam ausência de ruídos/outliers.",
                                "Conclusão resume impacto e reproducibilidade."
                              ],
                              "assessmentCriteria": [
                                "Clareza e completude da documentação Markdown (racional por step).",
                                "Correção técnica das transformações (imputação, normalização, remoção de ruídos).",
                                "Uso efetivo de visualizações e métricas para inspeção/verificação.",
                                "Reprodutibilidade: notebook roda independentemente.",
                                "Estrutura lógica: seções claras de inspeção, limpeza, verificação.",
                                "Adequação do racional às boas práticas éticas de transparência."
                              ],
                              "crossCurricularConnections": [
                                "Ética em Dados: Transparência e auditabilidade de manipulações.",
                                "Programação Python: Uso avançado de pandas e Jupyter.",
                                "Estatística: Detecção de outliers via IQR, normalização.",
                                "Visualização de Dados: Plots para inspeção e validação.",
                                "Gestão de Projetos: Versionamento e exportação de resultados."
                              ],
                              "realWorldApplication": "Em pipelines de Data Science profissionais (ex: Kaggle, empresas como Google/Amazon), notebooks documentados garantem colaboração em equipe, auditorias regulatórias (GDPR), e reprodutibilidade em ML models, evitando erros caros em produção."
                            },
                            "estimatedTime": "3 horas",
                            "difficulty": "intermediate",
                            "status": "not_started",
                            "prerequisites": []
                          },
                          {
                            "id": "10.1.6.4.1.3",
                            "name": "Manter logs de versões das transformações de limpeza",
                            "description": "Implementar versionamento de scripts de limpeza com ferramentas como Git, registrando mudanças em arquivos de log que descrevam o impacto de cada alteração nos dados.",
                            "atomicExpansion": {
                              "steps": [
                                {
                                  "stepNumber": 1,
                                  "title": "Configurar Repositório Git e Estrutura de Logs Inicial",
                                  "subSteps": [
                                    "Instale o Git no ambiente de desenvolvimento se não estiver presente.",
                                    "Crie um novo repositório Git no diretório do projeto de limpeza de dados.",
                                    "Crie um arquivo de log inicial (ex: cleaning_log.md ou cleaning_versions.json) com cabeçalho descrevendo o dataset original e métricas iniciais (linhas, colunas, valores ausentes).",
                                    "Faça o primeiro commit com mensagem detalhando o estado inicial dos dados.",
                                    "Configure branches para desenvolvimento (ex: main para produção, dev para testes)."
                                  ],
                                  "verification": "Verifique com 'git log' que o commit inicial existe e o arquivo de log foi criado com métricas iniciais.",
                                  "estimatedTime": "1 hora",
                                  "materials": [
                                    "Git instalado",
                                    "Editor de texto (VS Code)",
                                    "Dataset de exemplo para limpeza"
                                  ],
                                  "tips": "Use 'git init' em um diretório limpo e adicione .gitignore para excluir dados sensíveis.",
                                  "learningObjective": "Entender a configuração básica de versionamento para rastrear mudanças em scripts de limpeza.",
                                  "commonMistakes": [
                                    "Esquecer de commitar o log inicial",
                                    "Não documentar métricas baseline",
                                    "Commitar dados reais sem anonimização"
                                  ]
                                },
                                {
                                  "stepNumber": 2,
                                  "title": "Implementar Primeira Transformação de Limpeza e Registrar",
                                  "subSteps": [
                                    "Escreva um script Python (ex: clean_step1.py) para uma transformação simples, como remoção de duplicatas.",
                                    "Execute o script e calcule métricas pós-limpeza (ex: linhas removidas, % de dados afetados).",
                                    "Atualize o arquivo de log com seção para esta transformação: descrição, código snippet chave, métricas antes/depois.",
                                    "Adicione o script e log ao Git, commit com mensagem: 'feat: remoção de duplicatas - removidas 5% das linhas'.",
                                    "Push para repositório remoto se aplicável."
                                  ],
                                  "verification": "Execute 'git diff HEAD~1' para confirmar mudanças e verifique log descrevendo impacto.",
                                  "estimatedTime": "1.5 horas",
                                  "materials": [
                                    "Python/Pandas",
                                    "Jupyter Notebook para testes",
                                    "Dataset sujo de exemplo"
                                  ],
                                  "tips": "Sempre rode df.info() antes/depois para métricas precisas e inclua no log.",
                                  "learningObjective": "Aprender a documentar o impacto quantitativo de transformações em logs versionados.",
                                  "commonMistakes": [
                                    "Não calcular impacto numérico",
                                    "Mensagens de commit vagas como 'update'",
                                    "Alterar dados sem backup versionado"
                                  ]
                                },
                                {
                                  "stepNumber": 3,
                                  "title": "Realizar Alterações Subsequentes e Versionar com Impacto",
                                  "subSteps": [
                                    "Crie uma nova transformação (ex: tratamento de valores ausentes com imputação média).",
                                    "Modifique o script existente ou crie novo, teste em cópia dos dados.",
                                    "Registre no log: comparação com versão anterior, impacto (ex: precisão melhorou 2%), potenciais vieses introduzidos.",
                                    "Commit com mensagem detalhada: 'fix: imputação de missing values - impacto: reduziu NaNs em 10%, avg imputation'.",
                                    "Use tags Git (ex: git tag v1.1-cleaning) para marcar versões estáveis."
                                  ],
                                  "verification": "Use 'git log --oneline' e 'git show <commit>' para inspecionar descrições de impacto em múltiplos commits.",
                                  "estimatedTime": "2 horas",
                                  "materials": [
                                    "Scripts Python anteriores",
                                    "Ferramentas de métricas como Pandas Profiling"
                                  ],
                                  "tips": "Inclua gráficos simples (ex: before/after plots) no log para visualização de impacto.",
                                  "learningObjective": "Dominar o versionamento iterativo com ênfase em rastreabilidade e impacto de mudanças.",
                                  "commonMistakes": [
                                    "Commits sem menção a métricas",
                                    "Não testar em dados de validação",
                                    "Sobrescrever logs sem versionamento"
                                  ]
                                },
                                {
                                  "stepNumber": 4,
                                  "title": "Revisar Histórico, Auditabilidade e Boas Práticas",
                                  "subSteps": [
                                    "Navegue pelo histórico com 'git log --graph' e revise logs para consistência.",
                                    "Simule auditoria: identifique uma transformação e reverta com 'git revert' ou checkout.",
                                    "Adicione script de automação para gerar relatório de logs (ex: git log parser).",
                                    "Documente lições aprendidas em novo commit no log.",
                                    "Configure hooks Git para validar mensagens de commit com padrões (impacto obrigatório)."
                                  ],
                                  "verification": "Gere um relatório consolidado do histórico e confirme que todos commits descrevem impactos.",
                                  "estimatedTime": "1 hora",
                                  "materials": [
                                    "Git CLI ou GitHub/GitLab interface",
                                    "Script de parsing opcional"
                                  ],
                                  "tips": "Use conventional commits (feat:, fix:) para padronizar mensagens.",
                                  "learningObjective": "Garantir transparência e auditabilidade através de revisão de versionamento.",
                                  "commonMistakes": [
                                    "Ignorar revisão de histórico",
                                    "Não planejar reversibilidade",
                                    "Mensagens inconsistentes"
                                  ]
                                }
                              ],
                              "practicalExample": "Em um dataset de vendas com 10k linhas, versão 1 remove duplicatas (reduz 500 linhas, log registra % duplicado=5%). Versão 2 imputa missing prices com média (impacto: NaNs de 8% para 0%, média price muda 2%), permitindo análise precisa de tendências sem perda de dados.",
                              "finalVerifications": [
                                "Todos commits têm mensagens descrevendo transformações e impactos quantitativos.",
                                "Arquivo de log centralizado resume todas versões com métricas before/after.",
                                "Histórico Git permite reverter a qualquer versão sem perda de rastreabilidade.",
                                "Relatório gerado mostra cadeia completa de transformações e efeitos cumulativos.",
                                "Nenhuma mudança afeta dados sem documentação de potenciais vieses.",
                                "Branches e tags organizam versões de produção vs. experimentais."
                              ],
                              "assessmentCriteria": [
                                "Precisão e completude das descrições de impacto em logs (métricas numéricas obrigatórias).",
                                "Consistência no uso de Git para todos scripts e logs relacionados.",
                                "Capacidade de auditoria: fácil identificar e reverter mudanças específicas.",
                                "Integração de boas práticas como tags, branches e conventional commits.",
                                "Documentação cobre vieses éticos e qualidade de dados pós-limpeza.",
                                "Automação mínima para relatórios de versionamento."
                              ],
                              "crossCurricularConnections": [
                                "Ética em Dados: Garante transparência e reprodutibilidade para auditorias.",
                                "Programação: Uso avançado de Git para colaboração em equipes de dados.",
                                "Matemática Computacional: Métricas estatísticas para quantificar impactos de limpeza.",
                                "Gestão de Projetos: Versionamento como prática de engenharia de software em pipelines de dados.",
                                "Ciência de Dados: Integração com MLflow ou DVC para versionamento de dados."
                              ],
                              "realWorldApplication": "Em empresas de saúde ou finanças, logs de versionamento de limpeza de dados são cruciais para compliance (GDPR, HIPAA), permitindo auditorias regulatórias que rastreiem como transformações afetaram análises, evitando multas por manipulação opaca de dados."
                            },
                            "estimatedTime": "2 horas",
                            "difficulty": "intermediate",
                            "status": "not_started",
                            "prerequisites": []
                          }
                        ]
                      },
                      {
                        "id": "10.1.6.4.2",
                        "name": "Documentação de Integração de Dados",
                        "description": "Métodos para tornar transparentes os processos de coleta e fusão de dados de múltiplas fontes, incluindo mapeamento de esquemas, resolução de conflitos e validação de integridade.",
                        "specificSkills": [
                          {
                            "id": "10.1.6.4.2.1",
                            "name": "Mapear e documentar esquemas de fontes de dados",
                            "description": "Criar diagramas e tabelas que descrevam os esquemas de dados de diferentes fontes (ex: CSV, SQL, APIs), destacando correspondências e discrepâncias antes da integração.",
                            "atomicExpansion": {
                              "steps": [
                                {
                                  "stepNumber": 1,
                                  "title": "Identificar e Coletar Informações sobre as Fontes de Dados",
                                  "subSteps": [
                                    "Liste todas as fontes de dados envolvidas (ex: arquivos CSV, bancos SQL, endpoints de API).",
                                    "Acesse cada fonte e extraia metadados básicos como nomes de colunas, tipos de dados e amostras de registros.",
                                    "Registre detalhes como tamanho da fonte, formato e proprietário/origem.",
                                    "Crie um inventário inicial em uma tabela simples (ex: Google Sheets ou Markdown).",
                                    "Documente restrições de acesso ou autenticação necessárias."
                                  ],
                                  "verification": "Verifique se o inventário lista todas as fontes com metadados básicos extraídos e sem erros de acesso.",
                                  "estimatedTime": "1 hora",
                                  "materials": [
                                    "Acesso às fontes de dados",
                                    "Ferramentas como Excel/Google Sheets, pandas (Python) ou SQL clients"
                                  ],
                                  "tips": "Comece com fontes mais simples (CSV) para ganhar confiança antes de APIs complexas.",
                                  "learningObjective": "Compreender as fontes de dados disponíveis e coletar metadados essenciais para análise.",
                                  "commonMistakes": [
                                    "Ignorar tipos de dados implícitos em CSVs",
                                    "Não registrar credenciais de acesso",
                                    "Pular amostras de dados reais"
                                  ]
                                },
                                {
                                  "stepNumber": 2,
                                  "title": "Analisar e Mapear Esquemas Individuais de Cada Fonte",
                                  "subSteps": [
                                    "Para CSV: Use pandas ou ferramentas como CSV Lint para inferir schema (colunas, tipos, nulls).",
                                    "Para SQL: Execute queries DESCRIBE/INFORMATION_SCHEMA para listar tabelas, colunas, tipos e constraints.",
                                    "Para APIs: Consulte documentação Swagger/OpenAPI ou faça chamadas de teste para inspecionar respostas JSON.",
                                    "Crie um diagrama ER (Entity-Relationship) ou tabela separada para cada fonte, incluindo chaves primárias e relacionamentos.",
                                    "Anote estatísticas básicas como % de valores nulos e distribuições de tipos."
                                  ],
                                  "verification": "Cada fonte tem um schema mapeado em formato tabular ou diagrama, com tipos de dados e amostras validadas.",
                                  "estimatedTime": "2 horas",
                                  "materials": [
                                    "Python com pandas/openpyxl",
                                    "Ferramentas de diagramação como Draw.io ou Lucidchart",
                                    "Postman para APIs",
                                    "Cliente SQL como DBeaver"
                                  ],
                                  "tips": "Padronize nomes de colunas para facilitar comparação futura (ex: snake_case).",
                                  "learningObjective": "Desenvolver habilidade em extrair e representar esquemas de dados de múltiplos formatos.",
                                  "commonMistakes": [
                                    "Assumir tipos de dados sem verificação (ex: string como data)",
                                    "Esquecer relacionamentos entre tabelas em SQL",
                                    "Não testar APIs com payloads reais"
                                  ]
                                },
                                {
                                  "stepNumber": 3,
                                  "title": "Comparar Esquemas e Identificar Correspondências e Discrepâncias",
                                  "subSteps": [
                                    "Alinhe colunas semanticamente semelhantes entre fontes (ex: 'nome' em CSV vs 'customer_name' em SQL).",
                                    "Crie uma tabela de mapeamento cruzado destacando matches exatos, sinônimos e conflitos de tipo.",
                                    "Identifique discrepâncias: diferenças de tipo, cardinalidade, valores nulos ou ausência de campos.",
                                    "Classifique discrepâncias por severidade (crítica, moderada, cosmética).",
                                    "Proponha resoluções iniciais, como casts de tipo ou campos derivados."
                                  ],
                                  "verification": "Tabela de mapeamento completa com todas correspondências e discrepâncias listadas e classificadas.",
                                  "estimatedTime": "1.5 horas",
                                  "materials": [
                                    "Planilhas comparativas",
                                    "Ferramentas como Excel com fórmulas VLOOKUP ou Python diff libraries"
                                  ],
                                  "tips": "Use cores na tabela para visualizar matches (verde) vs discrepâncias (vermelho).",
                                  "learningObjective": "Capacitar análise comparativa de esquemas para detectar issues pré-integração.",
                                  "commonMistakes": [
                                    "Mapeamentos baseados só em nomes iguais",
                                    "Ignorar diferenças culturais em formatos de data",
                                    "Subestimar impacto de nulls altos"
                                  ]
                                },
                                {
                                  "stepNumber": 4,
                                  "title": "Documentar em Diagramas, Tabelas e Validar a Documentação",
                                  "subSteps": [
                                    "Gere diagramas unificados (ex: ERD com sobreposições ou Sankey para fluxos de dados).",
                                    "Compile tudo em um relatório final: inventário, schemas individuais, mapeamento e resoluções.",
                                    "Valide com amostras reais: simule join/query entre fontes para testar mapeamentos.",
                                    "Inclua glossário de termos e versão do documento.",
                                    "Compartilhe draft para feedback de stakeholders."
                                  ],
                                  "verification": "Documento final gerado, validado com dados reais e aprovado por revisão.",
                                  "estimatedTime": "1 hora",
                                  "materials": [
                                    "Draw.io/Lucidchart para diagramas",
                                    "Markdown/PDF para relatório",
                                    "Acesso a dados de teste"
                                  ],
                                  "tips": "Use versionamento (Git) para rastrear mudanças na documentação.",
                                  "learningObjective": "Produzir documentação profissional e verificável para integração de dados.",
                                  "commonMistakes": [
                                    "Diagramas muito complexos sem legenda",
                                    "Omitir validação com dados reais",
                                    "Não versionar o documento"
                                  ]
                                }
                              ],
                              "practicalExample": "Em um projeto de integração para análise de vendas: mapeie schema de CSV de transações (colunas: id_venda:int, data:date, valor:float) com tabela SQL de clientes (id_cliente:int, nome:str, regiao:str) e API de produtos (product_id:int, preco:float). Documente que 'id_venda' não tem match direto, mas 'data' conflita (string vs date), propondo padronização.",
                              "finalVerifications": [
                                "Todos schemas estão mapeados com tipos, nulls e relacionamentos documentados.",
                                "Tabela de mapeamento cobre 100% das colunas com correspondências/discrepâncias.",
                                "Diagramas são legíveis e incluem amostras de dados.",
                                "Validação com query de teste roda sem erros de schema.",
                                "Relatório inclui glossário e propostas de resolução.",
                                "Documentação é versionada e compartilhável."
                              ],
                              "assessmentCriteria": [
                                "Precisão e completude dos schemas extraídos (sem omissões >5%).",
                                "Clareza na identificação de correspondências/discrepâncias (classificação lógica).",
                                "Qualidade visual e legibilidade de diagramas/tabelas.",
                                "Validade prática: testes com dados reais confirmam mapeamentos.",
                                "Profundidade de análise: inclui estatísticas como % nulls e propostas de resolução.",
                                "Profissionalismo: estrutura, versionamento e glossário presentes."
                              ],
                              "crossCurricularConnections": [
                                "Matemática: Modelagem estatística de distribuições de dados e inferência de tipos.",
                                "Programação: Uso de bibliotecas como pandas/SQL para extração automatizada.",
                                "Ética em Dados: Garantia de transparência em manipulações pré-integração.",
                                "Gestão de Projetos: Documentação para colaboração em equipes multidisciplinares."
                              ],
                              "realWorldApplication": "Em data engineering para ETL pipelines em empresas como bancos ou e-commerces, onde mapear schemas de legacy systems (SQL), arquivos de export (CSV) e serviços externos (APIs) previne erros caros em integrações, assegurando compliance e eficiência analítica."
                            },
                            "estimatedTime": "2 horas",
                            "difficulty": "beginner",
                            "status": "not_started",
                            "prerequisites": []
                          },
                          {
                            "id": "10.1.6.4.2.2",
                            "name": "Registrar junções e fusões de dados",
                            "description": "Documentar operações de merge, join e concatenação usando bibliotecas como pandas, incluindo critérios de junção (inner, outer, left) e tratamento de chaves duplicadas em relatórios executáveis.",
                            "atomicExpansion": {
                              "steps": [
                                {
                                  "stepNumber": 1,
                                  "title": "Preparar ambiente e dados de exemplo para junções",
                                  "subSteps": [
                                    "Instale pandas via pip se necessário.",
                                    "Crie dois DataFrames de exemplo: um com clientes (ID, nome, email) e outro com vendas (ID_cliente, produto, valor).",
                                    "Inclua dados com chaves duplicadas e ausentes para simular cenários reais.",
                                    "Salve os DataFrames em variáveis e visualize-os com .head() e .info().",
                                    "Adicione comentários iniciais no código explicando o propósito dos dados."
                                  ],
                                  "verification": "Execute o código e confirme que os DataFrames são exibidos corretamente com shapes e amostras visíveis.",
                                  "estimatedTime": "20 minutos",
                                  "materials": [
                                    "Python com pandas instalado",
                                    "Jupyter Notebook ou script .py",
                                    "Dados CSV de exemplo (clientes.csv, vendas.csv)"
                                  ],
                                  "tips": "Use pd.read_csv() para carregar dados reais e seed para reproducibilidade com np.random.seed(42).",
                                  "learningObjective": "Configurar um ambiente de teste com dados representativos para operações de junção.",
                                  "commonMistakes": [
                                    "Esquecer de importar pandas como pd.",
                                    "Criar DataFrames sem chaves que combinem.",
                                    "Não verificar tipos de dados com .dtypes."
                                  ]
                                },
                                {
                                  "stepNumber": 2,
                                  "title": "Executar e documentar operações de merge e join",
                                  "subSteps": [
                                    "Realize merge com how='inner' e documente o resultado em célula Markdown.",
                                    "Repita com how='left', 'right' e 'outer', comparando shapes e salvando cada resultado em variáveis nomeadas.",
                                    "Use on='ID_cliente' para especificar a chave e suffix para colunas conflitantes.",
                                    "Adicione docstrings ou comentários explicando critérios de junção e mudanças nos dados.",
                                    "Visualize diferenças com .merge(..., indicator=True) para rastrear origens."
                                  ],
                                  "verification": "Gere tabelas comparativas dos merges e confirme que todos os tipos produzem resultados esperados sem erros.",
                                  "estimatedTime": "30 minutos",
                                  "materials": [
                                    "Jupyter Notebook",
                                    "DataFrames do Step 1"
                                  ],
                                  "tips": "Sempre use validate='one_to_many' para detectar duplicatas precocemente.",
                                  "learningObjective": "Dominar merge/join com diferentes critérios e registrar o processo para transparência.",
                                  "commonMistakes": [
                                    "Confundir 'left' com 'right' join.",
                                    "Ignorar NaNs resultantes de outer joins.",
                                    "Não documentar o 'how' parameter."
                                  ]
                                },
                                {
                                  "stepNumber": 3,
                                  "title": "Documentar concatenação e tratamento de chaves duplicadas",
                                  "subSteps": [
                                    "Use pd.concat([df1, df2], axis=0, ignore_index=True) para junção vertical e documente.",
                                    "Trate duplicatas com .drop_duplicates(subset=['ID_cliente']) e explique no comentário.",
                                    "Para axis=1, concatene colunas e resolva overlaps com join='outer'.",
                                    "Registre antes/depois com prints de shape e amostras.",
                                    "Adicione seção Markdown resumindo impactos na integridade dos dados."
                                  ],
                                  "verification": "Confirme que o DataFrame final não tem duplicatas e mantém integridade com .duplicated().sum() == 0.",
                                  "estimatedTime": "25 minutos",
                                  "materials": [
                                    "DataFrames processados dos steps anteriores",
                                    "Jupyter Notebook"
                                  ],
                                  "tips": "Use sort=False em concat para preservar ordem original.",
                                  "learningObjective": "Aplicar concatenação segura e documentar resolução de duplicatas para auditoria.",
                                  "commonMistakes": [
                                    "Concatenar axis errado (0 vs 1).",
                                    "Não resetar index após concat.",
                                    "Esquecer de tratar duplicatas pós-merge."
                                  ]
                                },
                                {
                                  "stepNumber": 4,
                                  "title": "Compilar relatório executável com verificações finais",
                                  "subSteps": [
                                    "Estruture o notebook com headers: Introdução, Dados, Operações, Resultados, Conclusões.",
                                    "Inclua células de código executáveis com todos os merges/concats e outputs visíveis.",
                                    "Adicione tabela comparativa de shapes e métricas (ex: duplicatas removidas).",
                                    "Exporte como HTML ou PDF com nbconvert para compartilhamento.",
                                    "Teste execução célula por célula para garantir reprodutibilidade."
                                  ],
                                  "verification": "Execute todo o notebook do início e confirme outputs consistentes sem warnings.",
                                  "estimatedTime": "20 minutos",
                                  "materials": [
                                    "Jupyter Notebook completo",
                                    "nbconvert instalado"
                                  ],
                                  "tips": "Use %matplotlib inline para plots de verificação se aplicável.",
                                  "learningObjective": "Gerar um relatório executável que demonstre transparência total nas manipulações de dados.",
                                  "commonMistakes": [
                                    "Células fora de ordem.",
                                    "Faltam outputs em células de código.",
                                    "Não testar exportação."
                                  ]
                                }
                              ],
                              "practicalExample": "Carregue df_clientes (ID:1-10, nomes) e df_vendas (ID_cliente:5-15 com duplicata no 7). Faça left_merge para listar clientes com vendas totais, trate duplicata no ID 7 somando valores, concat com df_regioes no axis=1, documente tudo em notebook com antes/depois.",
                              "finalVerifications": [
                                "Explicar verbalmente diferenças entre inner/outer/left joins com exemplos dos dados.",
                                "Executar notebook sem erros e mostrar relatório HTML funcional.",
                                "Identificar e corrigir manualmente uma duplicata em dados de teste.",
                                "Comparar shapes pré/pós-operações e justificar mudanças.",
                                "Demonstrar uso de indicator=True em merge."
                              ],
                              "assessmentCriteria": [
                                "Documentação cobre todos os critérios de junção e tratamentos explicitamente.",
                                "Código é executável, limpo e com comentários inline.",
                                "Relatório inclui visualizações comparativas (shapes, amostras).",
                                "Tratamento de duplicatas é proativo e documentado.",
                                "Transparência total: qualquer auditor pode reproduzir resultados.",
                                "Tempo de execução otimizado sem loops desnecessários."
                              ],
                              "crossCurricularConnections": [
                                "Programação em Python (pandas avançado)",
                                "Ética em Dados (transparência e reprodutibilidade)",
                                "Estatística (manipulação de datasets para análise)",
                                "Gestão de Projetos (relatórios executáveis para equipes)"
                              ],
                              "realWorldApplication": "Em análises empresariais, documentar merges de CRM e ERP garante compliance regulatório (ex: LGPD), permitindo auditorias transparentes em relatórios de vendas consolidadas ou fusões de datasets de marketing."
                            },
                            "estimatedTime": "3 horas",
                            "difficulty": "intermediate",
                            "status": "not_started",
                            "prerequisites": []
                          },
                          {
                            "id": "10.1.6.4.2.3",
                            "name": "Validar integridade pós-integração",
                            "description": "Desenvolver checklists e scripts automatizados para verificar a consistência dos dados integrados, documentando métricas como número de registros preservados e detecção de perdas.",
                            "atomicExpansion": {
                              "steps": [
                                {
                                  "stepNumber": 1,
                                  "title": "Definir Critérios de Integridade dos Dados",
                                  "subSteps": [
                                    "Analisar as fontes de dados pré-integração para identificar métricas chave como contagem de registros, chaves primárias únicas e agregações (somas, médias).",
                                    "Listar potenciais problemas de integridade: duplicatas, valores nulos, desvios em distribuições estatísticas e inconsistências em joins.",
                                    "Mapear expectativas pré e pós-integração, considerando o contexto ético de transparência.",
                                    "Priorizar critérios críticos, como preservação de registros sensíveis.",
                                    "Documentar em formato tabular para clareza."
                                  ],
                                  "verification": "Lista de critérios documentada em tabela ou lista numerada, com pelo menos 5 métricas definidas e justificadas.",
                                  "estimatedTime": "1 hora",
                                  "materials": [
                                    "Documentação das fontes de dados",
                                    "Editor de texto (ex: VS Code, Google Docs)",
                                    "Exemplos de datasets de teste"
                                  ],
                                  "tips": "Inclua métricas estatísticas como desvios padrão para detectar anomalias sutis.",
                                  "learningObjective": "Identificar e documentar métricas essenciais para validar a integridade ética de dados integrados.",
                                  "commonMistakes": [
                                    "Focar apenas em contagens básicas, ignorando agregações e distribuições.",
                                    "Não considerar o impacto ético de perdas em dados sensíveis."
                                  ]
                                },
                                {
                                  "stepNumber": 2,
                                  "title": "Criar Checklist Manual de Validação",
                                  "subSteps": [
                                    "Converter critérios em itens de checklist acionáveis, com checkboxes para pré/pós-integração.",
                                    "Incluir verificações manuais como contagem de linhas via ferramentas básicas (ex: wc no terminal ou Excel).",
                                    "Adicionar seções para anotações de discrepâncias e evidências.",
                                    "Testar checklist em um subconjunto pequeno de dados.",
                                    "Revisar checklist com foco em completude e usabilidade."
                                  ],
                                  "verification": "Checklist funcional testado em dados de amostra, com todos os itens marcados e discrepâncias anotadas.",
                                  "estimatedTime": "1.5 horas",
                                  "materials": [
                                    "Datasets de teste pré e pós-integração",
                                    "Ferramentas como Excel, Google Sheets ou Markdown"
                                  ],
                                  "tips": "Use templates de checklist prontos (ex: GitHub checklists) para agilizar.",
                                  "learningObjective": "Desenvolver uma ferramenta manual prática para validação inicial de integridade.",
                                  "commonMistakes": [
                                    "Checklist muito genérica, sem itens específicos para o dataset.",
                                    "Omitir espaço para evidências fotográficas ou capturas de tela."
                                  ]
                                },
                                {
                                  "stepNumber": 3,
                                  "title": "Desenvolver Scripts Automatizados de Validação",
                                  "subSteps": [
                                    "Configurar ambiente Python com pandas e numpy para carregar dados pré/pós.",
                                    "Implementar funções para métricas: contagem, unicidade, agregações e detecção de perdas.",
                                    "Criar script principal que compara métricas e gera relatório em JSON/CSV.",
                                    "Adicionar alertas para discrepâncias (ex: if abs(pre - post) > threshold).",
                                    "Testar script com dados de amostra e refatorar erros."
                                  ],
                                  "verification": "Script executa sem erros, gera relatório com métricas comparadas e detecta discrepâncias intencionalmente inseridas.",
                                  "estimatedTime": "2 horas",
                                  "materials": [
                                    "Python 3+, bibliotecas pandas, numpy",
                                    "Datasets de teste",
                                    "Jupyter Notebook ou IDE"
                                  ],
                                  "tips": "Use asserts para validações rígidas e logging para rastreabilidade.",
                                  "learningObjective": "Criar automação escalável para verificação de integridade, promovendo eficiência e transparência.",
                                  "commonMistakes": [
                                    "Não tratar tipos de dados inconsistentes (ex: strings vs ints).",
                                    "Esquecer de versionar o script no Git para rastreabilidade ética."
                                  ]
                                },
                                {
                                  "stepNumber": 4,
                                  "title": "Executar Validação, Documentar e Relatar Métricas",
                                  "subSteps": [
                                    "Executar checklist manual e script em dataset completo.",
                                    "Compilar resultados em relatório unificado, destacando métricas preservadas e perdas detectadas.",
                                    "Documentar processo, incluindo código fonte e thresholds usados.",
                                    "Analisar causas de qualquer perda e propor mitigações.",
                                    "Arquivar relatório em repositório acessível para auditoria ética."
                                  ],
                                  "verification": "Relatório final gerado, com evidências de execução e zero discrepâncias não explicadas.",
                                  "estimatedTime": "1 hora",
                                  "materials": [
                                    "Script desenvolvido",
                                    "Checklist",
                                    "Ferramenta de relatórios (ex: Jupyter ou Markdown to PDF)"
                                  ],
                                  "tips": "Inclua visualizações (gráficos de barras para métricas) para maior transparência.",
                                  "learningObjective": "Integrar validações em um fluxo documentado, garantindo accountability na manipulação de dados.",
                                  "commonMistakes": [
                                    "Não quantificar perdas com evidências (ex: % de registros perdidos).",
                                    "Relatório incompleto, sem links para código ou dados."
                                  ]
                                }
                              ],
                              "practicalExample": "Integre um CSV de vendas (10.000 registros, colunas: id_venda, valor, data) com um CSV de clientes (id_cliente, nome). Use checklist para verificar contagem de vendas pós-join (deve ser 10.000), ausência de duplicatas em id_venda e soma de valores preservada (±0.1%). Script automatizado gera relatório: 'Registros preservados: 100%, Perdas detectadas: 0, Desvio em soma: 0.05%'.",
                              "finalVerifications": [
                                "Checklist cobre todas as métricas definidas no Step 1.",
                                "Script automatizado executa em <5 minutos e gera relatório padronizado.",
                                "Relatório documenta 100% das métricas com evidências comparativas.",
                                "Nenhuma perda de dados >1% sem explicação ética.",
                                "Processo todo versionado e reproduzível.",
                                "Análise de causas para discrepâncias incluída."
                              ],
                              "assessmentCriteria": [
                                "Completude: Todos os campos obrigatórios preenchidos nos steps (100%).",
                                "Precisão: Métricas validam integridade com <0.5% de erro.",
                                "Automação: Script detecta e relata todas as 5+ métricas chave.",
                                "Documentação: Relatório claro, com visuals e justificativas éticas.",
                                "Eficiência: Tempo total <6 horas, escalável para grandes datasets.",
                                "Inovação: Inclusão de thresholds dinâmicos ou alertas extras."
                              ],
                              "crossCurricularConnections": [
                                "Programação: Desenvolvimento de scripts em Python/pandas para automação.",
                                "Estatística: Uso de métricas agregadas e testes de consistência.",
                                "Ética e Governança: Ênfase em transparência e auditoria de dados.",
                                "Gestão de Projetos: Criação de checklists e relatórios padronizados.",
                                "Banco de Dados: Conceitos de joins, integridade referencial e SQL."
                              ],
                              "realWorldApplication": "Em pipelines ETL de empresas como bancos (integração de transações e contas) ou e-commerces (vendas + estoque), esses checklists e scripts previnem perdas financeiras por erros de integração, garantindo relatórios precisos para decisões estratégicas e conformidade regulatória (ex: LGPD, GDPR)."
                            },
                            "estimatedTime": "2 horas",
                            "difficulty": "intermediate",
                            "status": "not_started",
                            "prerequisites": []
                          }
                        ]
                      },
                      {
                        "id": "10.1.6.4.3",
                        "name": "Transparência e Reprodutibilidade na Análise de Dados",
                        "description": "Técnicas para documentar análises exploratórias e modelagem, assegurando que resultados sejam reproduzíveis por meio de seeds aleatórias, ambientes controlados e relatórios completos.",
                        "specificSkills": [
                          {
                            "id": "10.1.6.4.3.1",
                            "name": "Usar seeds e ambientes determinísticos em análises",
                            "description": "Configurar sementes aleatórias (random seeds) em bibliotecas como NumPy e scikit-learn, e documentar o ambiente de execução (versões de pacotes) para garantir resultados idênticos em execuções futuras.",
                            "atomicExpansion": {
                              "steps": [
                                {
                                  "stepNumber": 1,
                                  "title": "Configurar sementes aleatórias em NumPy",
                                  "subSteps": [
                                    "Instale NumPy se necessário: pip install numpy",
                                    "Importe NumPy no script Python: import numpy as np",
                                    "Defina a semente no início do script: np.random.seed(42)",
                                    "Gere uma sequência de números aleatórios: random_numbers = np.random.rand(5)",
                                    "Imprima os números para visualização: print(random_numbers)"
                                  ],
                                  "verification": "Execute o script duas vezes e confirme que a sequência de números aleatórios é idêntica em ambas as execuções.",
                                  "estimatedTime": "15-20 minutos",
                                  "materials": [
                                    "Python 3.8+",
                                    "NumPy (versão 1.21+)",
                                    "Editor de código como VS Code ou Jupyter Notebook"
                                  ],
                                  "tips": "Defina a seed o mais cedo possível no script, antes de qualquer operação aleatória.",
                                  "learningObjective": "Dominar o uso de np.random.seed() para garantir reprodutibilidade em geração de dados aleatórios com NumPy.",
                                  "commonMistakes": "Definir a seed após a primeira geração aleatória; esquecer de importar NumPy."
                                },
                                {
                                  "stepNumber": 2,
                                  "title": "Configurar sementes aleatórias em scikit-learn",
                                  "subSteps": [
                                    "Instale scikit-learn: pip install scikit-learn",
                                    "Importe as bibliotecas necessárias: import numpy as np; from sklearn.model_selection import train_test_split; from sklearn.neighbors import KNeighborsClassifier",
                                    "Defina np.random.seed(42) antes de gerar dados",
                                    "Gere dados de exemplo e divida com random_state: X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)",
                                    "Treine um modelo com random_state: knn = KNeighborsClassifier(); knn.fit(X_train, y_train) com random_state=42 se aplicável"
                                  ],
                                  "verification": "Execute o pipeline duas vezes e verifique se a divisão de dados e métricas do modelo (ex: acurácia) são idênticas.",
                                  "estimatedTime": "20-25 minutos",
                                  "materials": [
                                    "Python 3.8+",
                                    "NumPy 1.21+",
                                    "scikit-learn 1.0+"
                                  ],
                                  "tips": "Use random_state em todas as funções e modelos do scikit-learn que envolvam aleatoriedade.",
                                  "learningObjective": "Aplicar random_state em funções e modelos do scikit-learn para reprodutibilidade em machine learning.",
                                  "commonMistakes": "Não definir np.random.seed global antes do train_test_split; omitir random_state em modelos como RandomForest."
                                },
                                {
                                  "stepNumber": 3,
                                  "title": "Documentar o ambiente de execução",
                                  "subSteps": [
                                    "Instale pipreqs ou use pip freeze: pip install pipreqs",
                                    "Gere requirements.txt: pipreqs . --force ou pip freeze > requirements.txt",
                                    "Adicione versão do Python: use sys.version em um script para registrar",
                                    "Registre versões específicas: print(np.__version__); print(sklearn.__version__)",
                                    "Salve em um arquivo README.md com instruções de instalação: pip install -r requirements.txt"
                                  ],
                                  "verification": "Crie um ambiente virtual novo, instale via requirements.txt e execute o script original para confirmar compatibilidade.",
                                  "estimatedTime": "15 minutos",
                                  "materials": [
                                    "pipreqs",
                                    "Ambiente virtual (venv)"
                                  ],
                                  "tips": "Use ambientes virtuais (venv) para isolar dependências e evitar conflitos.",
                                  "learningObjective": "Criar documentação completa de dependências para permitir replicação exata do ambiente.",
                                  "commonMistakes": "Incluir pacotes desnecessários no requirements.txt; não especificar versões pinned (ex: numpy==1.21.0)."
                                },
                                {
                                  "stepNumber": 4,
                                  "title": "Testar reprodutibilidade completa",
                                  "subSteps": [
                                    "Combine seeds de NumPy/scikit-learn e ambiente documentado em um script único",
                                    "Execute o script em dois terminais/ambientes separados",
                                    "Compare saídas linha a linha (use difflib ou print hash dos resultados)",
                                    "Salve outputs em arquivos e use diff para comparação",
                                    "Teste em máquina diferente ou container Docker se possível"
                                  ],
                                  "verification": "Todas as execuções produzem outputs bit-identicos, sem variações aleatórias.",
                                  "estimatedTime": "20-30 minutos",
                                  "materials": [
                                    "Script completo do exercício anterior",
                                    "requirements.txt",
                                    "Ferramenta diff (git diff ou fc em Windows)"
                                  ],
                                  "tips": "Use hashlib para hash de arrays NumPy e comparar deterministicamente.",
                                  "learningObjective": "Validar end-to-end a reprodutibilidade do pipeline de análise de dados.",
                                  "commonMistakes": "Ignorar variações sutis como ordem de imports; testar apenas em uma máquina."
                                }
                              ],
                              "practicalExample": "Crie um script que gera 100 amostras aleatórias com np.random.seed(42), divide em train/test com train_test_split(random_state=42), treina um KNN e imprime acurácia. Gere requirements.txt. Execute duas vezes: outputs exatos iguais, acurácia idêntica (ex: 0.95).",
                              "finalVerifications": [
                                "Script roda identicamente em múltiplas execuções sem seed alterada.",
                                "requirements.txt instala ambiente reproduzível em nova máquina.",
                                "Hashes de dados gerados e métricas de modelo coincidem.",
                                "Nenhuma warning sobre depreciação ou versão incompatível.",
                                "README explica passos para replicação.",
                                "Teste em venv limpo confirma reprodutibilidade."
                              ],
                              "assessmentCriteria": [
                                "Uso correto e consistente de np.random.seed() e random_state em todas operações aleatórias.",
                                "requirements.txt inclui versões pinned de NumPy e scikit-learn.",
                                "Outputs de execuções múltiplas são 100% idênticos.",
                                "Documentação clara em README.md com instruções.",
                                "Tratamento de erros comuns como conflitos de versão.",
                                "Código limpo, comentado e modular."
                              ],
                              "crossCurricularConnections": [
                                "Programação: Gerenciamento de dependências e versionamento (ex: Git, Poetry).",
                                "Estatística: Reprodutibilidade em experimentos e controle de variáveis aleatórias.",
                                "Ética em Pesquisa: Transparência e verificabilidade científica.",
                                "Engenharia de Software: Pipelines CI/CD determinísticos.",
                                "Ciência de Dados: Boas práticas em notebooks Jupyter."
                              ],
                              "realWorldApplication": "Em pesquisas acadêmicas e indústria de ML, garante que colegas ou revisores reproduzam resultados exatos, facilitando debugging, auditorias éticas e colaboração em equipes remotas, evitando 'funciona na minha máquina'."
                            },
                            "estimatedTime": "1 hora",
                            "difficulty": "beginner",
                            "status": "not_started",
                            "prerequisites": []
                          },
                          {
                            "id": "10.1.6.4.3.2",
                            "name": "Criar relatórios automatizados de análise",
                            "description": "Gerar relatórios em PDF ou HTML com ferramentas como Jupyter ou R Markdown, incluindo resumos estatísticos, visualizações e interpretações das análises exploratórias.",
                            "atomicExpansion": {
                              "steps": [
                                {
                                  "stepNumber": 1,
                                  "title": "Configurar o ambiente e preparar dados",
                                  "subSteps": [
                                    "Instalar e abrir Jupyter Notebook ou RStudio.",
                                    "Criar um novo notebook (.ipynb) ou arquivo R Markdown (.Rmd).",
                                    "Importar bibliotecas essenciais: pandas, numpy, matplotlib, seaborn (Python) ou tidyverse, ggplot2 (R).",
                                    "Carregar um conjunto de dados de exemplo em CSV ou built-in (ex: iris ou mtcars).",
                                    "Executar célula inicial para verificar carregamento dos dados."
                                  ],
                                  "verification": "Dados carregados corretamente e exibidos via head() ou print(df.head()).",
                                  "estimatedTime": "30 minutos",
                                  "materials": [
                                    "Jupyter Notebook/RStudio",
                                    "Dataset CSV de exemplo",
                                    "Internet para instalação de pacotes"
                                  ],
                                  "tips": "Use conda ou renv para gerenciar dependências e garantir reprodutibilidade.",
                                  "learningObjective": "Preparar um ambiente reprodutível para análise de dados.",
                                  "commonMistakes": [
                                    "Esquecer de instalar pacotes",
                                    "Usar caminhos absolutos para arquivos",
                                    "Não testar execução inicial"
                                  ]
                                },
                                {
                                  "stepNumber": 2,
                                  "title": "Realizar análises exploratórias e estatísticas",
                                  "subSteps": [
                                    "Calcular estatísticas descritivas (média, mediana, desvio padrão) com describe() ou summary().",
                                    "Identificar outliers e padrões iniciais nos dados.",
                                    "Executar correlações e testes básicos (ex: cor() em R ou df.corr() em Python).",
                                    "Documentar achados em células de markdown.",
                                    "Salvar resultados intermediários em variáveis."
                                  ],
                                  "verification": "Tabela de resumos estatísticos gerada e exibida sem erros.",
                                  "estimatedTime": "45 minutos",
                                  "materials": [
                                    "Notebook configurado",
                                    "Bibliotecas de análise"
                                  ],
                                  "tips": "Sempre inclua seeds para números aleatórios (set.seed() ou np.random.seed()).",
                                  "learningObjective": "Gerar resumos estatísticos transparentes e reprodutíveis.",
                                  "commonMistakes": [
                                    "Ignorar valores ausentes",
                                    "Não documentar suposições",
                                    "Sobrepor variáveis sem backup"
                                  ]
                                },
                                {
                                  "stepNumber": 3,
                                  "title": "Criar visualizações e interpretações",
                                  "subSteps": [
                                    "Gerar gráficos: histogramas, boxplots e scatterplots com matplotlib/seaborn ou ggplot2.",
                                    "Personalizar títulos, labels e temas para clareza.",
                                    "Adicionar interpretações em texto markdown abaixo de cada visualização.",
                                    "Integrar múltiplos gráficos em uma figura composta.",
                                    "Verificar acessibilidade (cores, legibilidade)."
                                  ],
                                  "verification": "Gráficos renderizados corretamente e interpretações associadas.",
                                  "estimatedTime": "60 minutos",
                                  "materials": [
                                    "Bibliotecas de visualização",
                                    "Dados preparados"
                                  ],
                                  "tips": "Use facetas ou subplots para comparar variáveis múltiplas.",
                                  "learningObjective": "Produzir visualizações informativas com narrativas claras.",
                                  "commonMistakes": [
                                    "Eixos não rotulados",
                                    "Cores indistinguíveis para daltônicos",
                                    "Gráficos sem contexto"
                                  ]
                                },
                                {
                                  "stepNumber": 4,
                                  "title": "Gerar e exportar o relatório automatizado",
                                  "subSteps": [
                                    "Estruturar o notebook com seções: introdução, análise, conclusões.",
                                    "Executar todas as células (Kernel > Restart & Run All).",
                                    "Exportar para PDF/HTML via nbconvert (Python) ou knit (R).",
                                    "Incluir versão do código e pacotes usados (ex: sessionInfo() ou !pip list).",
                                    "Compartilhar arquivo fonte para reprodutibilidade."
                                  ],
                                  "verification": "Relatório exportado abre corretamente com todos elementos intactos.",
                                  "estimatedTime": "30 minutos",
                                  "materials": [
                                    "Notebook completo",
                                    "Ferramentas de exportação"
                                  ],
                                  "tips": "Teste em máquina limpa para validar dependências.",
                                  "learningObjective": "Automatizar relatórios reprodutíveis e profissionais.",
                                  "commonMistakes": [
                                    "Não reiniciar kernel antes de exportar",
                                    "Omitir metadados de ambiente",
                                    "Quebras em exportação"
                                  ]
                                }
                              ],
                              "practicalExample": "Usando dataset Iris, gere um relatório com: resumo estatístico das sépalas/pétalas, boxplots por espécie, scatterplot de comprimento vs largura, interpretação de clusters e exporte em PDF destacando padrões reprodutíveis.",
                              "finalVerifications": [
                                "Relatório em PDF/HTML gerado sem erros de renderização.",
                                "Código executa end-to-end em <5 minutos após restart.",
                                "Visualizações e textos integrados coerentemente.",
                                "Metadados de ambiente e pacotes incluídos.",
                                "Arquivo fonte permite reprodução exata por terceiros.",
                                "Interpretações alinhadas aos dados explorados."
                              ],
                              "assessmentCriteria": [
                                "Clareza e precisão dos resumos estatísticos (20%)",
                                "Qualidade e relevância das visualizações (25%)",
                                "Profundidade das interpretações (20%)",
                                "Reprodutibilidade total do relatório (20%)",
                                "Profissionalismo do layout e estrutura (10%)",
                                "Inclusão de metadados e boas práticas (5%)"
                              ],
                              "crossCurricularConnections": [
                                "Estatística: Resumos descritivos e testes.",
                                "Programação: Scripts automatizados em Python/R.",
                                "Comunicação: Narrativas técnicas claras.",
                                "Ética em Dados: Transparência e reprodutibilidade.",
                                "Design Gráfico: Visualizações acessíveis."
                              ],
                              "realWorldApplication": "Cientistas de dados em empresas como bancos ou saúde geram relatórios automatizados para stakeholders, garantindo transparência em análises de risco ou epidemias, facilitando auditorias e decisões baseadas em evidências reprodutíveis."
                            },
                            "estimatedTime": "3 horas",
                            "difficulty": "intermediate",
                            "status": "not_started",
                            "prerequisites": []
                          },
                          {
                            "id": "10.1.6.4.3.3",
                            "name": "Implementar versionamento de pipelines de análise",
                            "description": "Estruturar pipelines de análise com DVC (Data Version Control) ou MLflow, registrando experimentos, hiperparâmetros e métricas para total transparência e rastreabilidade.",
                            "atomicExpansion": {
                              "steps": [
                                {
                                  "stepNumber": 1,
                                  "title": "Configurar o ambiente de versionamento com DVC ou MLflow",
                                  "subSteps": [
                                    "Instale DVC via pip: `pip install dvc` e inicialize no repositório Git com `dvc init`.",
                                    "Configure MLflow como alternativa: `pip install mlflow` e inicie o servidor com `mlflow ui`.",
                                    "Crie um arquivo params.yaml para hiperparâmetros iniciais, como learning_rate: 0.01.",
                                    "Integre Git: garanta que o repo esteja versionado com `git init` e primeiro commit.",
                                    "Teste configuração rodando `dvc pull` ou iniciando um run no MLflow."
                                  ],
                                  "verification": "Verifique com `dvc status` mostrando clean e `mlflow experiments list` listando experimentos.",
                                  "estimatedTime": "1 hora",
                                  "materials": [
                                    "Python 3.8+",
                                    "Git instalado",
                                    "DVC ou MLflow via pip"
                                  ],
                                  "tips": "Use ambientes virtuais (venv) para isolar dependências e evitar conflitos.",
                                  "learningObjective": "Entender e configurar ferramentas de versionamento para pipelines de dados.",
                                  "commonMistakes": [
                                    "Não inicializar DVC dentro de um repo Git",
                                    "Ignorar configuração de cache remoto (ex: S3)"
                                  ]
                                },
                                {
                                  "stepNumber": 2,
                                  "title": "Estruturar o pipeline de análise em stages",
                                  "subSteps": [
                                    "Defina stages no dvc.yaml: raw data -> processed data -> model training.",
                                    "Crie scripts Python para cada stage, como process_data.py e train_model.py.",
                                    "Adicione dependências entre stages: `deps: [-data/raw.csv]` e `outs: [data/processed.csv]`.",
                                    "Inclua métricas no pipeline: use `dvc metrics show` ou MLflow logging.",
                                    "Execute o pipeline com `dvc repro` para testar fluxo end-to-end."
                                  ],
                                  "verification": "Rode `dvc dag` para visualizar o grafo de dependências sem erros.",
                                  "estimatedTime": "1.5 horas",
                                  "materials": [
                                    "Editor de código (VS Code)",
                                    "dvc.yaml template",
                                    "Dados de exemplo CSV"
                                  ],
                                  "tips": "Mantenha stages modulares e independentes para facilitar debugging.",
                                  "learningObjective": "Estruturar pipelines modulares com versionamento automático de ins/outs.",
                                  "commonMistakes": [
                                    "Definir dependências circulares",
                                    "Não versionar arquivos grandes com DVC add"
                                  ]
                                },
                                {
                                  "stepNumber": 3,
                                  "title": "Versionar dados, código e experimentos",
                                  "subSteps": [
                                    "Tracke dados com `dvc add data/raw.csv` e commit `dvc push`.",
                                    "Registre hiperparâmetros em params.yaml e logue com `mlflow.log_param('lr', 0.01)`.",
                                    "Logue métricas: `mlflow.log_metric('accuracy', 0.85)` ou `dvc metrics add -m metrics.json`.",
                                    "Execute experimentos variando params: `dvc exp run --set-param learning_rate=0.05`.",
                                    "Compare experimentos com `dvc exp show` ou MLflow UI."
                                  ],
                                  "verification": "Confira histórico com `dvc exp list` mostrando múltiplos runs com métricas diferentes.",
                                  "estimatedTime": "2 horas",
                                  "materials": [
                                    "Dados reais ou sintéticos",
                                    "MLflow tracking server rodando localmente"
                                  ],
                                  "tips": "Sempre use `dvc exp branch` para criar branches experimentais sem poluir main.",
                                  "learningObjective": "Implementar rastreamento completo de experimentos para reprodutibilidade.",
                                  "commonMistakes": [
                                    "Não commitar .dvc files no Git",
                                    "Logar métricas inconsistentes entre runs"
                                  ]
                                },
                                {
                                  "stepNumber": 4,
                                  "title": "Testar reprodutibilidade e transparência total",
                                  "subSteps": [
                                    "Clone o repo em ambiente limpo e rode `dvc repro` para reproduzir pipeline.",
                                    "Verifique métricas iguais com `dvc metrics diff`.",
                                    "Documente o pipeline em README.md com comandos de setup e repro.",
                                    "Simule colaboração: push/pull changes e teste em outro dir.",
                                    "Gere relatório final com `dvc exp show --sort-by metrics.json:accuracy`."
                                  ],
                                  "verification": "Pipeline reproduz métricas exatas (diff=0) em novo ambiente após `dvc pull`.",
                                  "estimatedTime": "1 hora",
                                  "materials": [
                                    "Repo Git clonado em novo diretório",
                                    "Cache DVC remoto opcional"
                                  ],
                                  "tips": "Use seeds fixos em random states para garantir determinismo em ML.",
                                  "learningObjective": "Validar transparência e reprodutibilidade em cenários reais de equipe.",
                                  "commonMistakes": [
                                    "Dependências de ambiente local não versionadas",
                                    "Ignorar floats em métricas por precisão"
                                  ]
                                }
                              ],
                              "practicalExample": "Em um pipeline de ML para prever churn de clientes: stage1 processa dados de 1M registros CSV com DVC add; stage2 treina XGBoost logando hiperparâmetros (max_depth=6) e métricas (AUC=0.92) no MLflow; `dvc repro` reproduz o modelo exato em qualquer máquina.",
                              "finalVerifications": [
                                "Pipeline executa end-to-end sem intervenção manual via `dvc repro`.",
                                "Dados e modelos versionados acessíveis via `dvc pull`.",
                                "Experimentos comparáveis com histórico de params/métricas idêntico.",
                                "Reprodutibilidade confirmada em ambiente isolado (Docker/VM).",
                                "README documenta passos para setup e run.",
                                "Métricas diff mostram zero variação entre runs."
                              ],
                              "assessmentCriteria": [
                                "Pipeline possui pelo menos 3 stages interdependentes no dvc.yaml.",
                                "Todos ins/outs trackeados corretamente com DVC (no Git e cache).",
                                "Hiperparâmetros e métricas logados em pelo menos 3 experimentos.",
                                "Reprodutibilidade 100% testada em novo clone do repo.",
                                "Documentação clara cobre setup, execução e troubleshooting.",
                                "Uso correto de branches experimentais sem commits sujos."
                              ],
                              "crossCurricularConnections": [
                                "Programação em Python: scripts modulares e logging.",
                                "Controle de Versão com Git: integração nativa com DVC.",
                                "Ética em Dados: garante transparência e auditoria.",
                                "Engenharia de Software: pipelines CI/CD like.",
                                "Estatística Computacional: rastreio de experimentos A/B."
                              ],
                              "realWorldApplication": "Em equipes de data science de empresas como Google ou startups de fintech, DVC/MLflow permite colaboração em projetos ML sensíveis, como detecção de fraude, onde auditores exigem reprodutibilidade total para compliance regulatório (GDPR, HIPAA)."
                            },
                            "estimatedTime": "4 horas",
                            "difficulty": "advanced",
                            "status": "not_started",
                            "prerequisites": []
                          }
                        ]
                      }
                    ]
                  },
                  {
                    "id": "10.1.6.5",
                    "name": "Responsabilidade Ética",
                    "description": "Definição de accountability para cientistas de dados em casos de mau uso ou impactos negativos de dados e modelos.",
                    "individualConcepts": [
                      {
                        "id": "10.1.6.5.1",
                        "name": "Definição de Accountability",
                        "description": "Conceito fundamental de accountability, que se refere à obrigação dos cientistas de dados de responderem por suas ações, decisões e resultados de modelos, especialmente em cenários de mau uso ou impactos negativos.",
                        "specificSkills": [
                          {
                            "id": "10.1.6.5.1.1",
                            "name": "Identificar a definição de accountability",
                            "description": "Explicar o significado de accountability como a capacidade de prestar contas por ações envolvendo dados e modelos, incluindo rastreabilidade de decisões e transparência nos processos.",
                            "atomicExpansion": {
                              "steps": [
                                {
                                  "stepNumber": 1,
                                  "title": "Compreender o conceito geral de accountability",
                                  "subSteps": [
                                    "Pesquise a definição de 'accountability' em fontes confiáveis como dicionários ou enciclopédias online.",
                                    "Identifique sinônimos e antônimos para fixar o significado.",
                                    "Leia exemplos cotidianos, como responsabilidade de um funcionário por suas ações no trabalho.",
                                    "Anote a essência: capacidade de prestar contas por ações e decisões.",
                                    "Compare com conceitos semelhantes, como 'responsibility'."
                                  ],
                                  "verification": "Escreva uma definição em suas próprias palavras e compare com fontes originais.",
                                  "estimatedTime": "20 minutos",
                                  "materials": [
                                    "Dicionário online (ex: Oxford ou Priberam)",
                                    "Enciclopédia Wikipedia (página de Accountability)"
                                  ],
                                  "tips": "Foquem em exemplos pessoais para tornar o conceito relatable.",
                                  "learningObjective": "Dominar o significado básico de accountability fora do contexto técnico.",
                                  "commonMistakes": [
                                    "Confundir com 'responsibility' (responsabilidade sem prestação de contas)",
                                    "Ignorar o aspecto de transparência"
                                  ]
                                },
                                {
                                  "stepNumber": 2,
                                  "title": "Contextualizar accountability em ciência de dados",
                                  "subSteps": [
                                    "Leia sobre ética em dados e IA, focando em ações com dados e modelos.",
                                    "Identifique cenários onde decisões afetam pessoas (ex: modelos preditivos).",
                                    "Analise como falta de accountability causa problemas éticos.",
                                    "Registre exemplos de violações em notícias reais (ex: vieses em algoritmos).",
                                    "Ligue ao contexto educacional: MC-13 Ética no Uso de Dados."
                                  ],
                                  "verification": "Liste 3 exemplos de accountability em projetos de dados.",
                                  "estimatedTime": "25 minutos",
                                  "materials": [
                                    "Artigos sobre ética em IA (ex: site da UNESCO)",
                                    "Vídeos curtos no YouTube sobre ética em dados"
                                  ],
                                  "tips": "Use analogias: como um médico explica decisões de diagnóstico.",
                                  "learningObjective": "Entender accountability como prestação de contas em ações com dados e modelos.",
                                  "commonMistakes": [
                                    "Limitar a contextos não técnicos",
                                    "Subestimar impactos sociais"
                                  ]
                                },
                                {
                                  "stepNumber": 3,
                                  "title": "Identificar elementos chave: rastreabilidade e transparência",
                                  "subSteps": [
                                    "Defina rastreabilidade: capacidade de traçar decisões de volta às fontes.",
                                    "Defina transparência: processos claros e auditáveis.",
                                    "Estude exemplos: logs em pipelines de dados ou explainable AI (XAI).",
                                    "Crie um diagrama simples mostrando fluxo de decisões rastreáveis.",
                                    "Discuta como esses elementos garantem accountability."
                                  ],
                                  "verification": "Desenhe um fluxograma com rastreabilidade em um modelo de ML.",
                                  "estimatedTime": "30 minutos",
                                  "materials": [
                                    "Ferramenta de desenho online (ex: Draw.io)",
                                    "Documentação de bibliotecas como SHAP para XAI"
                                  ],
                                  "tips": "Pense em 'black box' vs. 'white box' modelos.",
                                  "learningObjective": "Reconhecer rastreabilidade e transparência como pilares da accountability.",
                                  "commonMistakes": [
                                    "Confundir rastreabilidade com apenas logging",
                                    "Ignorar transparência para stakeholders"
                                  ]
                                },
                                {
                                  "stepNumber": 4,
                                  "title": "Sintetizar e verificar a definição completa",
                                  "subSteps": [
                                    "Combine todos os elementos em uma definição unificada.",
                                    "Teste a definição com cenários hipotéticos.",
                                    "Compare com definições acadêmicas ou regulatórias (ex: GDPR).",
                                    "Escreva um parágrafo explicativo.",
                                    "Autoavalie usando critérios de precisão."
                                  ],
                                  "verification": "Explique a definição para um colega ou grave um vídeo de 1 minuto.",
                                  "estimatedTime": "15 minutos",
                                  "materials": [
                                    "Papel e caneta ou editor de texto",
                                    "Referências regulatórias (ex: AI Act da UE)"
                                  ],
                                  "tips": "Use linguagem simples e evite jargões desnecessários.",
                                  "learningObjective": "Formular uma definição precisa e acionável de accountability.",
                                  "commonMistakes": [
                                    "Omitir rastreabilidade ou transparência",
                                    "Fazer definição genérica sem contexto de dados"
                                  ]
                                }
                              ],
                              "practicalExample": "Em um projeto de análise de dados para previsão de fraudes bancárias, accountability envolve documentar a origem dos dados de treinamento, registrar escolhas de features no modelo e fornecer relatórios transparentes sobre como o modelo chegou a uma decisão de 'fraude', permitindo auditoria por reguladores.",
                              "finalVerifications": [
                                "Pode definir accountability em próprias palavras, incluindo dados e modelos.",
                                "Explica rastreabilidade com um exemplo concreto.",
                                "Identifica transparência como prestação de contas.",
                                "Diferencia de conceitos como privacy ou fairness.",
                                "Aplica a definição a um cenário real de ciência de dados.",
                                "Lista riscos de falta de accountability."
                              ],
                              "assessmentCriteria": [
                                "Precisão na inclusão de rastreabilidade e transparência (peso: alto).",
                                "Clareza e concisão da definição fornecida.",
                                "Uso correto de exemplos contextualizados em dados/IA.",
                                "Capacidade de diferenciar de termos relacionados.",
                                "Profundidade na explicação de impactos éticos.",
                                "Criatividade em aplicações práticas."
                              ],
                              "crossCurricularConnections": [
                                "Ética Filosófica: Conceitos de responsabilidade moral (Aristóteles).",
                                "Direito: Responsabilidade civil e compliance (GDPR, LGPD).",
                                "Gestão de Projetos: Auditoria e governança de TI.",
                                "Filosofia da Ciência: Transparência em experimentos.",
                                "Educação Cívica: Accountability governamental."
                              ],
                              "realWorldApplication": "Na indústria de tecnologia, como no Google ou Meta, accountability é implementada via 'AI Principles' para auditar modelos de recomendação, garantindo rastreabilidade de vieses e transparência para usuários e reguladores, evitando multas milionárias por violações éticas."
                            },
                            "estimatedTime": "1 hora",
                            "difficulty": "beginner",
                            "status": "not_started",
                            "prerequisites": []
                          },
                          {
                            "id": "10.1.6.5.1.2",
                            "name": "Diferenciar accountability de responsabilidade",
                            "description": "Comparar accountability (prestar contas com mecanismos de verificação) versus responsabilidade geral (obrigação moral), com exemplos no contexto de ciência de dados.",
                            "atomicExpansion": {
                              "steps": [
                                {
                                  "stepNumber": 1,
                                  "title": "Definir o conceito de responsabilidade geral",
                                  "subSteps": [
                                    "Pesquise definições padrão de 'responsabilidade' em dicionários éticos e contextos profissionais.",
                                    "Identifique componentes chave: obrigação moral, dever interno sem necessidade de relatórios externos.",
                                    "Anote exemplos cotidianos fora de ciência de dados, como um aluno responsável por entregar tarefas.",
                                    "Registre 3-5 sinônimos e contextos onde é usada (ex: responsabilidade parental).",
                                    "Crie um mapa mental resumindo a essência moral e voluntária."
                                  ],
                                  "verification": "Você pode explicar responsabilidade em suas próprias palavras sem consultar notas, com pelo menos 3 exemplos.",
                                  "estimatedTime": "20 minutos",
                                  "materials": [
                                    "Dicionário online (ex: Oxford ou Merriam-Webster)",
                                    "Papel e caneta para mapa mental",
                                    "Notebook para anotações"
                                  ],
                                  "tips": "Foquem na ausência de mecanismos formais de enforcement; é mais sobre consciência interna.",
                                  "learningObjective": "Compreender responsabilidade como uma obrigação moral autoimposta sem verificações externas.",
                                  "commonMistakes": [
                                    "Confundir com accountability prematuramente",
                                    "Ignorar o aspecto moral/voluntário"
                                  ]
                                },
                                {
                                  "stepNumber": 2,
                                  "title": "Definir o conceito de accountability",
                                  "subSteps": [
                                    "Pesquise definições de 'accountability' em contextos éticos e profissionais, enfatizando prestação de contas.",
                                    "Destaque elementos chave: mecanismos de verificação, relatórios, consequências mensuráveis.",
                                    "Anote exemplos iniciais, como auditorias em empresas.",
                                    "Liste 3-5 componentes essenciais (ex: transparência, rastreabilidade, sanções).",
                                    "Crie um diagrama comparando com responsabilidade (preparando para o próximo step)."
                                  ],
                                  "verification": "Descreva accountability incluindo pelo menos 4 mecanismos de verificação em um parágrafo curto.",
                                  "estimatedTime": "25 minutos",
                                  "materials": [
                                    "Artigos sobre ética em governança (ex: Wikipedia Accountability, sites de compliance)",
                                    "Ferramenta de diagramação (ex: Draw.io ou papel)",
                                    "Notebook"
                                  ],
                                  "tips": "Enfatize que accountability é externa e mensurável, não apenas intenção.",
                                  "learningObjective": "Dominar accountability como prestação de contas com sistemas de verificação e consequências.",
                                  "commonMistakes": [
                                    "Reduzir a apenas 'responsabilidade com punição'",
                                    "Omitir a necessidade de transparência e rastreio"
                                  ]
                                },
                                {
                                  "stepNumber": 3,
                                  "title": "Comparar e contrastar os dois conceitos",
                                  "subSteps": [
                                    "Crie uma tabela de comparação: colunas para semelhanças, diferenças, exemplos.",
                                    "Identifique 5 diferenças chave (ex: interna vs externa, moral vs legal, auto vs auditada).",
                                    "Discuta overlaps: responsabilidade pode levar a accountability em estruturas formais.",
                                    "Escreva um resumo de 200 palavras diferenciando os termos.",
                                    "Revise com um peer ou autoavaliação para clareza."
                                  ],
                                  "verification": "Preencha uma tabela de comparação corretamente sem erros conceituais.",
                                  "estimatedTime": "30 minutos",
                                  "materials": [
                                    "Planilha ou tabela em Google Sheets/Excel",
                                    "Resumos dos steps anteriores",
                                    "Timer para escrita"
                                  ],
                                  "tips": "Use Venn diagram para visual; foque em accountability como 'responsabilidade + verificação'.",
                                  "learningObjective": "Discernir diferenças e semelhanças entre responsabilidade e accountability de forma estruturada.",
                                  "commonMistakes": [
                                    "Tratar como sinônimos",
                                    "Ignorar nuances contextuais"
                                  ]
                                },
                                {
                                  "stepNumber": 4,
                                  "title": "Aplicar conceitos no contexto de ciência de dados",
                                  "subSteps": [
                                    "Identifique cenários em ciência de dados: ex: limpeza de dados (responsabilidade) vs relatório auditável de bias (accountability).",
                                    "Crie 3 exemplos específicos: um onde só responsabilidade basta, outro exigindo accountability.",
                                    "Analise um case real (ex: Cambridge Analytica: falha em accountability).",
                                    "Desenvolva um checklist para implementar accountability em projetos de dados.",
                                    "Simule uma discussão: defenda por que um data scientist precisa de ambos."
                                  ],
                                  "verification": "Produza 3 exemplos contextualizados corretamente avaliados por rubrica simples.",
                                  "estimatedTime": "35 minutos",
                                  "materials": [
                                    "Casos de estudo em ética de dados (ex: artigos sobre GDPR, bias em ML)",
                                    "Notebook ou blog para exemplos",
                                    "Vídeos curtos sobre ética em DS (ex: YouTube TED talks)"
                                  ],
                                  "tips": "Sempre pergunte: 'Há mecanismos de verificação?'; ligue a dados sensíveis/públicos.",
                                  "learningObjective": "Aplicar distinção em cenários reais de ciência de dados, enfatizando ética.",
                                  "commonMistakes": [
                                    "Exemplos genéricos sem vínculo a DS",
                                    "Subestimar accountability em dados"
                                  ]
                                }
                              ],
                              "practicalExample": "Em um projeto de machine learning para previsão de crédito, o data scientist tem responsabilidade moral em não introduzir bias (obrigação interna). Accountability surge quando o modelo é auditado por reguladores, exigindo logs de limpeza de dados, relatórios de fairness metrics e sanções por não conformidade (ex: multa sob GDPR).",
                              "finalVerifications": [
                                "Defina ambos termos com precisão em <1 minuto.",
                                "Explique 4 diferenças chave sem hesitação.",
                                "Forneça 2 exemplos de DS onde accountability é crítica.",
                                "Diferencie em uma conversa simulada de 2 minutos.",
                                "Crie tabela de comparação sem erros.",
                                "Identifique quando responsabilidade basta vs quando precisa accountability."
                              ],
                              "assessmentCriteria": [
                                "Precisão conceitual: definições corretas e distintas (30%)",
                                "Profundidade de comparação: ≥5 diferenças claras (25%)",
                                "Relevância a ciência de dados: exemplos contextualizados (20%)",
                                "Clareza e estrutura: uso de tabelas/diagramas (15%)",
                                "Aplicação prática: checklist viável (10%)"
                              ],
                              "crossCurricularConnections": [
                                "Ética em Programação: accountability em code reviews e CI/CD pipelines.",
                                "Governança de Dados: GDPR e frameworks como FAIR principles.",
                                "Gestão de Projetos Ágeis: responsibility em sprints vs accountability em retrospectives.",
                                "Filosofia Ética: distinções kantianas de dever vs utilitarismo em verificações.",
                                "Direito Digital: liability legal em manipulação de dados pessoais."
                              ],
                              "realWorldApplication": "Em empresas como Google ou bancos, data scientists são responsáveis por ética em modelos de IA, mas accountability é enforced via Data Stewardship Committees, auditorias anuais e relatórios públicos, prevenindo escândalos como o de discriminação algorítmica em hiring tools."
                            },
                            "estimatedTime": "1.5 horas",
                            "difficulty": "intermediate",
                            "status": "not_started",
                            "prerequisites": []
                          },
                          {
                            "id": "10.1.6.5.1.3",
                            "name": "Relacionar accountability às etapas da ciência de dados",
                            "description": "Mapear accountability às fases como coleta, limpeza e avaliação de modelos, destacando pontos de risco ético em cada etapa.",
                            "atomicExpansion": {
                              "steps": [
                                {
                                  "stepNumber": 1,
                                  "title": "Revisar as etapas principais do ciclo de vida da ciência de dados",
                                  "subSteps": [
                                    "Liste as fases padrão: coleta, limpeza/pré-processamento, exploração/análise, modelagem, avaliação e implantação.",
                                    "Descreva brevemente o que acontece em cada fase, focando em atividades humanas e decisões.",
                                    "Identifique onde intervenções humanas introduzem accountability, como seleção de fontes ou algoritmos.",
                                    "Crie um diagrama simples ou tabela resumindo as fases.",
                                    "Pesquise exemplos reais de falhas em cada fase relacionadas a ética."
                                  ],
                                  "verification": "Conferir se o aluno pode recitar e explicar todas as 6 fases com exemplos éticos iniciais.",
                                  "estimatedTime": "45 minutos",
                                  "materials": "Notebook, acesso à internet para artigos sobre ciclo de CRISP-DM ou KDD, papel e caneta para diagrama.",
                                  "tips": "Use o modelo CRISP-DM como referência padrão para consistência.",
                                  "learningObjective": "Compreender o fluxo sequencial da ciência de dados para mapear accountability adequadamente.",
                                  "commonMistakes": "Confundir fases (ex: limpeza com modelagem) ou ignorar implantação como fase crítica."
                                },
                                {
                                  "stepNumber": 2,
                                  "title": "Mapear accountability na coleta e limpeza de dados",
                                  "subSteps": [
                                    "Analise riscos na coleta: viés de seleção, privacidade (GDPR), consentimento informado.",
                                    "Discuta accountability na limpeza: perda de dados sensíveis, manipulação inadvertida para resultados desejados.",
                                    "Identifique responsáveis: cientista de dados, dono de dados, equipe legal.",
                                    "Crie uma matriz de riscos x accountability para essas fases.",
                                    "Exemplifique com caso de coleta de dados demográficos enviesados."
                                  ],
                                  "verification": "O aluno apresenta uma matriz completa com pelo menos 3 riscos por fase e donos de accountability.",
                                  "estimatedTime": "60 minutos",
                                  "materials": "Planilha Google Sheets ou Excel para matriz, casos de estudo como Cambridge Analytica.",
                                  "tips": "Sempre pergunte: 'Quem é responsabilizado se isso falhar?' para cada risco.",
                                  "learningObjective": "Associar pontos de risco ético específicos às fases iniciais de dados.",
                                  "commonMistakes": "Subestimar limpeza como 'técnica apenas', ignorando impactos éticos em dados minoritários."
                                },
                                {
                                  "stepNumber": 3,
                                  "title": "Mapear accountability na modelagem e avaliação de modelos",
                                  "subSteps": [
                                    "Examine modelagem: escolha de algoritmos enviesados, overfitting para populações não representativas.",
                                    "Aborde avaliação: métricas que mascaram viés (ex: accuracy vs. fairness metrics).",
                                    "Defina accountability para validação cruzada e testes éticos.",
                                    "Desenvolva checklist de accountability para desenvolvedores e revisores.",
                                    "Simule um cenário de modelo de crédito com viés racial."
                                  ],
                                  "verification": "Checklist aprovada com critérios éticos integrados e exemplo simulado mapeado.",
                                  "estimatedTime": "60 minutos",
                                  "materials": "Ferramentas como Python/Jupyter com bibliotecas fairlearn ou AIF360, exemplos de datasets UCI.",
                                  "tips": "Inclua métricas de equidade como disparate impact em toda avaliação.",
                                  "learningObjective": "Reconhecer como escolhas técnicas carregam responsabilidade ética.",
                                  "commonMistakes": "Focar só em performance técnica, negligenciando viés algorítmico inerente."
                                },
                                {
                                  "stepNumber": 4,
                                  "title": "Sintetizar mapeamento completo e destacar riscos éticos transversais",
                                  "subSteps": [
                                    "Integre todos os mapeamentos em um framework unificado (ex: tabela ou fluxograma).",
                                    "Identifique riscos transversais: transparência, auditabilidade em todas as fases.",
                                    "Proponha mecanismos de accountability: auditorias, documentação reproducibility.",
                                    "Crie um plano de mitigação para um projeto hipotético de ciência de dados.",
                                    "Revise com pares para feedback."
                                  ],
                                  "verification": "Framework completo apresentado, com plano de mitigação viável e feedback incorporado.",
                                  "estimatedTime": "45 minutos",
                                  "materials": "Ferramenta de diagrama como Lucidchart ou Draw.io, template de framework ético.",
                                  "tips": "Use cores para diferenciar fases e riscos no fluxograma para clareza visual.",
                                  "learningObjective": "Criar um mapeamento holístico de accountability ao ciclo de dados.",
                                  "commonMistakes": "Fragmentar mapeamentos sem síntese, perdendo visão sistêmica."
                                },
                                {
                                  "stepNumber": 5,
                                  "title": "Aplicar e validar o mapeamento em um caso prático",
                                  "subSteps": [
                                    "Escolha um dataset real (ex: saúde ou recrutamento).",
                                    "Aplique o framework fase por fase, documentando accountability.",
                                    "Simule uma auditoria ética e ajuste gaps.",
                                    "Registre lições aprendidas em relatório curto.",
                                    "Compartilhe para revisão final."
                                  ],
                                  "verification": "Relatório submetido com aplicação completa e auto-avaliação de gaps éticos.",
                                  "estimatedTime": "60 minutos",
                                  "materials": "Dataset público (Kaggle), relatório template em Markdown ou Word.",
                                  "tips": "Comece com dataset pequeno para evitar sobrecarga; foque em 3-4 fases chave.",
                                  "learningObjective": "Demonstrar aplicação prática do mapeamento de accountability.",
                                  "commonMistakes": "Escolher dataset muito complexo, perdendo foco na accountability ética."
                                }
                              ],
                              "practicalExample": "Em um projeto de ML para triagem de currículos em uma empresa de tech, mapeie: Coleta (viés em fontes de LinkedIn), Limpeza (remoção de dados de minorias), Modelagem (algoritmo que penaliza nomes estrangeiros), Avaliação (métrica de accuracy ignora fairness), com accountability atribuída ao cientista de dados e compliance officer.",
                              "finalVerifications": [
                                "Pode listar pelo menos 2 riscos éticos e donos de accountability por fase principal.",
                                "Constrói um fluxograma ou tabela mapeando accountability ao ciclo completo.",
                                "Identifica mecanismos de mitigação para riscos transversais como viés e privacidade.",
                                "Aplica o mapeamento a um caso real com relatório documentado.",
                                "Explica diferenças entre accountability técnica e ética em exemplos.",
                                "Realiza auto-auditoria ética em um pipeline simulado."
                              ],
                              "assessmentCriteria": [
                                "Precisão e completude do mapeamento (cobertura de todas as fases: 30%)",
                                "Profundidade na identificação de riscos éticos e pontos de accountability (25%)",
                                "Qualidade do framework visual/sintetizado (20%)",
                                "Aplicação prática e viabilidade de mitigação (15%)",
                                "Clareza na comunicação e exemplos reais (10%)"
                              ],
                              "crossCurricularConnections": [
                                "Ética Filosófica: Conceitos de responsabilidade moral em decisões algorítmicas.",
                                "Direito e Regulamentação: GDPR, LGPD e leis de IA explicável.",
                                "Gestão de Projetos: Accountability em equipes ágeis e DevOps.",
                                "Estatística: Viés em amostragem e inferência causal."
                              ],
                              "realWorldApplication": "Em empresas como Google ou bancos, cientistas de dados usam esse mapeamento para auditorias éticas em modelos de recomendação ou scoring de crédito, evitando multas regulatórias e danos à reputação, como no caso do COMPAS nos EUA onde viés racial levou a escrutínio público."
                            },
                            "estimatedTime": "2 horas",
                            "difficulty": "intermediate",
                            "status": "not_started",
                            "prerequisites": []
                          }
                        ]
                      },
                      {
                        "id": "10.1.6.5.2",
                        "name": "Casos de Mau Uso e Impactos Negativos",
                        "description": "Análise de situações reais ou hipotéticas onde o mau uso de dados ou modelos causa danos, enfatizando a responsabilidade do cientista de dados.",
                        "specificSkills": [
                          {
                            "id": "10.1.6.5.2.1",
                            "name": "Reconhecer exemplos de mau uso de dados",
                            "description": "Identificar casos como viés em algoritmos de recrutamento ou vazamento de dados sensíveis, com base em estudos de caso da bibliografia como Witten e Frank.",
                            "atomicExpansion": {
                              "steps": [
                                {
                                  "stepNumber": 1,
                                  "title": "Compreender Conceitos Básicos de Mau Uso de Dados",
                                  "subSteps": [
                                    "Definir mau uso de dados como qualquer aplicação que viole princípios éticos, como privacidade, equidade ou transparência.",
                                    "Identificar categorias principais: viés algorítmico, vazamento de dados sensíveis e manipulação intencional.",
                                    "Ler seção introdutória sobre ética em data mining no livro de Witten e Frank.",
                                    "Anotar exemplos iniciais de cada categoria.",
                                    "Discutir com um colega ou em fórum online um exemplo pessoal."
                                  ],
                                  "verification": "Criar um glossário com 5 termos chave e suas definições, revisado por si mesmo.",
                                  "estimatedTime": "25 minutos",
                                  "materials": [
                                    "Livro 'Data Mining: Practical Machine Learning Tools and Techniques' de Witten e Frank (capítulo sobre ética)",
                                    "Caderno de notas ou documento digital"
                                  ],
                                  "tips": "Use mind maps para conectar conceitos e facilitar retenção.",
                                  "learningObjective": "Dominar definições fundamentais para reconhecer padrões de mau uso.",
                                  "commonMistakes": [
                                    "Confundir viés com erros técnicos aleatórios; ignorar contexto ético além do técnico."
                                  ]
                                },
                                {
                                  "stepNumber": 2,
                                  "title": "Analisar Casos de Viés em Algoritmos de Recrutamento",
                                  "subSteps": [
                                    "Pesquisar o caso da Amazon (2018): algoritmo treinado em currículos masculinos que discriminava mulheres.",
                                    "Identificar fontes do viés: dados de treinamento enviesados e falta de correção.",
                                    "Comparar com estudo de Witten e Frank sobre viés em machine learning.",
                                    "Mapear impactos: exclusão de candidatos qualificados e perpetuação de desigualdades.",
                                    "Simular análise: dado um dataset fictício, destacar potenciais vieses."
                                  ],
                                  "verification": "Escrever um parágrafo resumindo causas, impactos e lições do caso Amazon.",
                                  "estimatedTime": "35 minutos",
                                  "materials": [
                                    "Artigos online sobre caso Amazon",
                                    "Livro Witten e Frank",
                                    "Ferramenta de busca como Google Scholar"
                                  ],
                                  "tips": "Busque fontes primárias como relatórios da empresa para maior credibilidade.",
                                  "learningObjective": "Reconhecer mecanismos de viés em algoritmos e seus impactos sociais.",
                                  "commonMistakes": [
                                    "Focar apenas no resultado final sem analisar a origem nos dados; subestimar efeitos a longo prazo."
                                  ]
                                },
                                {
                                  "stepNumber": 3,
                                  "title": "Estudar Exemplos de Vazamento de Dados Sensíveis",
                                  "subSteps": [
                                    "Revisar caso Cambridge Analytica: uso indevido de dados do Facebook para manipulação eleitoral.",
                                    "Examinar violações: coleta sem consentimento e compartilhamento inadequado.",
                                    "Relacionar com discussões em Witten e Frank sobre privacidade em análise de dados.",
                                    "Listar consequências: multas regulatórias, perda de confiança pública e danos individuais.",
                                    "Criar fluxograma do vazamento desde coleta até exposição."
                                  ],
                                  "verification": "Produzir um fluxograma ou diagrama do caso, com 5 pontos chave marcados.",
                                  "estimatedTime": "30 minutos",
                                  "materials": [
                                    "Documentários ou artigos sobre Cambridge Analytica",
                                    "Livro Witten e Frank",
                                    "Ferramenta de diagramação como Draw.io"
                                  ],
                                  "tips": "Visualize o fluxo de dados para entender pontos de falha ética.",
                                  "learningObjective": "Identificar riscos de privacidade em pipelines de dados reais.",
                                  "commonMistakes": [
                                    "Ignorar responsabilidade compartilhada entre empresas e usuários; confundir com hacks cibernéticos simples."
                                  ]
                                },
                                {
                                  "stepNumber": 4,
                                  "title": "Praticar Reconhecimento em Estudos de Caso Integrados",
                                  "subSteps": [
                                    "Selecionar 3 casos adicionais da bibliografia de Witten e Frank.",
                                    "Para cada um: classificar tipo de mau uso, causas e impactos.",
                                    "Criar tabela comparativa entre viés e vazamentos.",
                                    "Debater soluções éticas: auditorias, anonymização e regulamentações.",
                                    "Testar em cenário hipotético: analisar notícia recente de mau uso."
                                  ],
                                  "verification": "Preencher tabela comparativa com 3 casos e autoavaliar completude.",
                                  "estimatedTime": "40 minutos",
                                  "materials": [
                                    "Livro Witten e Frank (estudos de caso)",
                                    "Notícias recentes via busca web",
                                    "Planilha Excel ou Google Sheets"
                                  ],
                                  "tips": "Priorize casos recentes para relevância; discuta em grupo para perspectivas múltiplas.",
                                  "learningObjective": "Aplicar reconhecimento de padrões éticos em múltiplos contextos.",
                                  "commonMistakes": [
                                    "Generalizar demais sem evidências específicas; negligenciar soluções preventivas."
                                  ]
                                }
                              ],
                              "practicalExample": "Em 2018, a Amazon desenvolveu um algoritmo de recrutamento que rejeitava currículos com termos como 'mulheres liderança' devido a treinamento em dados históricos dominados por homens, ilustrando viés algorítmico que perpetuou discriminação de gênero, conforme discutido em contextos éticos de Witten e Frank.",
                              "finalVerifications": [
                                "Listar corretamente 3 exemplos de viés e 2 de vazamento de dados sensíveis.",
                                "Explicar causas raiz em pelo menos 2 casos reais da bibliografia.",
                                "Identificar impactos éticos e sociais em um estudo de caso dado.",
                                "Propor 2 medidas preventivas para mau uso em cenários hipotéticos.",
                                "Mapear conexões entre casos usando tabela comparativa.",
                                "Autoavaliar compreensão com quiz de 5 perguntas sobre conceitos chave."
                              ],
                              "assessmentCriteria": [
                                "Precisão na identificação de tipos de mau uso (90% acurácia).",
                                "Profundidade na análise de causas e impactos (detalhes específicos de casos).",
                                "Uso correto de referências bibliográficas como Witten e Frank.",
                                "Criatividade em exemplos práticos e conexões reais.",
                                "Completude da tabela ou fluxogramas (todos elementos presentes).",
                                "Clareza e estrutura na comunicação escrita ou visual."
                              ],
                              "crossCurricularConnections": [
                                "Ética Filosófica: Debates sobre utilitarismo vs. direitos individuais em dados.",
                                "Direito e Regulamentação: LGPD/GDPR e responsabilidades legais em privacidade.",
                                "Programação e ML: Técnicas de debiasing em algoritmos (Python scikit-learn).",
                                "Sociologia: Impactos em desigualdades sociais e discriminação sistêmica.",
                                "Gestão de Projetos: Auditorias éticas em pipelines de dados empresariais."
                              ],
                              "realWorldApplication": "Essa habilidade permite que profissionais de dados atuem como auditores éticos em empresas, identificando riscos em hiring tools ou campanhas de marketing, evitando multas milionárias (ex: GDPR) e construindo confiança pública, essencial para carreiras em Data Science responsável."
                            },
                            "estimatedTime": "2 horas",
                            "difficulty": "intermediate",
                            "status": "not_started",
                            "prerequisites": [
                              "10.1.6.5.1.1"
                            ]
                          },
                          {
                            "id": "10.1.6.5.2.2",
                            "name": "Avaliar impactos negativos de modelos",
                            "description": "Analisar consequências sociais, econômicas e legais de modelos mal utilizados, como discriminação em IA ou privacidade violada.",
                            "atomicExpansion": {
                              "steps": [
                                {
                                  "stepNumber": 1,
                                  "title": "Identificar os Principais Tipos de Impactos Negativos",
                                  "subSteps": [
                                    "Classifique os impactos em categorias: sociais (ex: discriminação), econômicos (ex: perda de empregos) e legais (ex: violações de privacidade).",
                                    "Liste exemplos iniciais para cada categoria relacionados a modelos de IA ou dados.",
                                    "Crie um mapa mental conectando modelos mal utilizados a esses impactos.",
                                    "Pesquise definições formais de termos como 'bias algorítmico' e 'invasão de privacidade'.",
                                    "Discuta com um parceiro ou anote potenciais interseções entre categorias."
                                  ],
                                  "verification": "Mapa mental completo com pelo menos 3 exemplos por categoria e definições anotadas.",
                                  "estimatedTime": "45 minutos",
                                  "materials": "Papel e caneta ou ferramenta digital como MindMeister; artigos introdutórios sobre ética em IA (ex: 'Weapons of Math Destruction' de Cathy O'Neil).",
                                  "tips": "Use cores diferentes para cada categoria para visualização clara.",
                                  "learningObjective": "Compreender e categorizar os tipos fundamentais de impactos negativos de modelos.",
                                  "commonMistakes": "Confundir impactos sociais com econômicos sem exemplos concretos; ignorar aspectos legais."
                                },
                                {
                                  "stepNumber": 2,
                                  "title": "Analisar Casos Reais de Mau Uso de Modelos",
                                  "subSteps": [
                                    "Selecione 2-3 casos famosos, como COMPAS (discriminação racial em sentenças judiciais) ou Cambridge Analytica (violação de privacidade).",
                                    "Descreva o modelo usado, o problema técnico e os impactos observados em cada categoria.",
                                    "Colete evidências de fontes confiáveis: relatórios, artigos acadêmicos ou notícias.",
                                    "Compare os casos: o que eles têm em comum e o que é único?",
                                    "Registre métricas quantitativas, como taxas de erro discriminatórias."
                                  ],
                                  "verification": "Relatório de 1 página por caso com evidências citadas e comparações.",
                                  "estimatedTime": "1 hora",
                                  "materials": "Acesso à internet para ProPublica (COMPAS), relatórios FTC (Cambridge Analytica); caderno de anotações.",
                                  "tips": "Priorize fontes primárias como estudos acadêmicos para evitar viés midiático.",
                                  "learningObjective": "Aplicar conceitos teóricos a casos históricos para reconhecer padrões de mau uso.",
                                  "commonMistakes": "Focar apenas em um tipo de impacto; usar fontes não verificadas."
                                },
                                {
                                  "stepNumber": 3,
                                  "title": "Avaliar Impactos de um Modelo Específico",
                                  "subSteps": [
                                    "Escolha um modelo hipotético ou real (ex: algoritmo de recrutamento que ignora gênero).",
                                    "Simule cenários: descreva dados enviesados, saída do modelo e consequências sociais/econômicas/legais.",
                                    "Quantifique impactos: estime números afetados, custos ou violações legais.",
                                    "Identifique partes interessadas (stakeholders) e seus prejuízos.",
                                    "Crie um fluxograma mostrando causalidade: dados → modelo → impacto."
                                  ],
                                  "verification": "Fluxograma e tabela de impactos com quantificações e stakeholders listados.",
                                  "estimatedTime": "50 minutos",
                                  "materials": "Ferramenta de fluxograma como Lucidchart ou Draw.io; exemplos de datasets enviesados (ex: Kaggle).",
                                  "tips": "Use 'e se' para explorar cenários extremos e realistas.",
                                  "learningObjective": "Desenvolver habilidades analíticas para avaliar causalidade em impactos negativos.",
                                  "commonMistakes": "Subestimar efeitos em cadeia; ignorar stakeholders indiretos como sociedade."
                                },
                                {
                                  "stepNumber": 4,
                                  "title": "Sintetizar Avaliação e Propor Reflexões Éticas",
                                  "subSteps": [
                                    "Resuma os impactos identificados em um relatório final integrando todos os steps.",
                                    "Avalie a severidade: priorize por escala (local vs global) e probabilidade.",
                                    "Discuta lições aprendidas e sugestões preventivas (ex: auditorias éticas).",
                                    "Autoavalie sua análise: o que faltou?",
                                    "Apresente oralmente ou por escrito para feedback."
                                  ],
                                  "verification": "Relatório final de 2 páginas com resumo, priorização e sugestões.",
                                  "estimatedTime": "40 minutos",
                                  "materials": "Processador de texto; gravação para autoapresentação.",
                                  "tips": "Use escalas numéricas (1-10) para severidade para objetividade.",
                                  "learningObjective": "Integrar análises em uma avaliação holística com recomendações práticas.",
                                  "commonMistakes": "Ser muito genérico nas sugestões; não priorizar impactos."
                                }
                              ],
                              "practicalExample": "Analise o algoritmo COMPAS usado em tribunais dos EUA: dados de treinamento enviesados levaram a taxas de reincidência superestimadas para afro-americanos (impacto social: discriminação; econômico: sentenças mais longas custando milhões; legal: violações de direitos civis). Simule correção com dados balanceados e meça redução de bias.",
                              "finalVerifications": [
                                "Pode listar e exemplificar 3 impactos por categoria (social, econômico, legal).",
                                "Analisa corretamente pelo menos 2 casos reais com evidências.",
                                "Cria fluxograma causal preciso para um modelo específico.",
                                "Prioriza impactos por severidade com justificativa.",
                                "Propõe 3 mitigações éticas viáveis.",
                                "Identifica stakeholders e seus prejuízos em cenários simulados."
                              ],
                              "assessmentCriteria": [
                                "Profundidade da categorização de impactos (clareza e exemplos).",
                                "Precisão na análise de casos reais (evidências e comparações).",
                                "Criatividade e lógica no fluxograma causal.",
                                "Objetividade na quantificação e priorização de impactos.",
                                "Relevância e praticidade das sugestões preventivas.",
                                "Integração holística de todos os steps no relatório final.",
                                "Clareza na comunicação oral/escrita."
                              ],
                              "crossCurricularConnections": [
                                "Filosofia: Debates éticos sobre utilitarismo vs direitos individuais.",
                                "Direito: Estudo de leis de proteção de dados (GDPR, LGPD).",
                                "Economia: Análise de custos-benefícios de modelos falhos.",
                                "Sociologia: Impactos em desigualdades sociais e minorias.",
                                "Ciência da Computação: Técnicas de mitigação de bias em ML."
                              ],
                              "realWorldApplication": "Em empresas de IA, desenvolvedores usam essa avaliação para auditar modelos antes do deploy, evitando multas milionárias (ex: Google com Gemini) e construindo confiança pública; em políticas públicas, governos regulam algoritmos em saúde e justiça para equidade social."
                            },
                            "estimatedTime": "2.5 horas",
                            "difficulty": "advanced",
                            "status": "not_started",
                            "prerequisites": [
                              "10.1.6.5.1.2"
                            ]
                          },
                          {
                            "id": "10.1.6.5.2.3",
                            "name": "Estudar casos reais de falhas éticas",
                            "description": "Revisar estudos de caso de livros como Raschka e Cielen, discutindo lições aprendidas sobre accountability em projetos de ciência de dados.",
                            "atomicExpansion": {
                              "steps": [
                                {
                                  "stepNumber": 1,
                                  "title": "Selecionar e Coletar Casos Reais de Falhas Éticas",
                                  "subSteps": [
                                    "Pesquise casos famosos em ciência de dados, como Cambridge Analytica, COMPAS (preconceito algorítmico) e Tay (chatbot do Twitter).",
                                    "Consulte livros como 'Python Machine Learning' de Sebastian Raschka e 'Predictive Analytics and Data Mining' de Joris Cielen para estudos de caso relevantes.",
                                    "Selecione 2-3 casos que ilustrem falhas em accountability, anotando fontes confiáveis (artigos acadêmicos, relatórios oficiais).",
                                    "Crie uma tabela resumida com colunas: Caso, Data, Envolvidos, Impacto Inicial.",
                                    "Priorize casos recentes (pós-2010) para relevância com regulamentações atuais como GDPR."
                                  ],
                                  "verification": "Lista de 2-3 casos selecionados com fontes e tabela resumida completa.",
                                  "estimatedTime": "1.5 horas",
                                  "materials": "Livros de Raschka e Cielen (PDF ou físico), acesso à internet (Google Scholar, ProPublica), caderno ou Google Docs.",
                                  "tips": "Comece com buscas específicas como 'ethical failures in data science case studies' para eficiência.",
                                  "learningObjective": "Identificar fontes confiáveis de estudos de caso éticos em ciência de dados.",
                                  "commonMistakes": "Selecionar fontes não verificadas (blogs sensacionalistas) ou casos irrelevantes para accountability."
                                },
                                {
                                  "stepNumber": 2,
                                  "title": "Analisar Detalhes e Violações Éticas dos Casos",
                                  "subSteps": [
                                    "Leia descrições detalhadas de cada caso, identificando: coleta de dados, manipulação, modelo usado e deployment.",
                                    "Mapeie violações éticas: falta de consentimento, bias não mitigado, ausência de auditoria.",
                                    "Avalie impactos negativos: sociais (discriminação), econômicos (multas) e reputacionais.",
                                    "Crie um diagrama de fluxo mostrando onde a accountability falhou (ex: dados → modelo → decisão).",
                                    "Registre citações diretas de livros ou relatórios sobre lições iniciais."
                                  ],
                                  "verification": "Diagramas de fluxo e mapas de violações completos para cada caso selecionado.",
                                  "estimatedTime": "2 horas",
                                  "materials": "Fontes selecionadas do Step 1, ferramentas de diagramação (Draw.io ou papel), highlighter para anotações.",
                                  "tips": "Use a estrutura 'O quê? Por quê? Como? Consequências?' para análise estruturada.",
                                  "learningObjective": "Desconstruir falhas éticas para entender mecanismos de accountability em projetos de dados.",
                                  "commonMistakes": "Focar apenas em fatos superficiais sem conectar a etapas do pipeline de dados."
                                },
                                {
                                  "stepNumber": 3,
                                  "title": "Extrair e Discutir Lições Aprendidas sobre Accountability",
                                  "subSteps": [
                                    "Para cada caso, liste 3-5 lições chave: ex. necessidade de transparência em modelos, auditorias independentes.",
                                    "Discuta accountability: papéis de data scientists, gerentes e stakeholders; compare com frameworks como FAT/ML.",
                                    "Relacione lições aos livros de Raschka/Cielen, destacando capítulos sobre ética prática.",
                                    "Simule uma discussão em grupo: anote prós/contras de soluções propostas (ex: anonimização vs. explicabilidade).",
                                    "Formule princípios pessoais de accountability para projetos futuros."
                                  ],
                                  "verification": "Lista consolidada de lições com referências e princípios pessoais documentados.",
                                  "estimatedTime": "2 horas",
                                  "materials": "Análises do Step 2, quadro branco ou Miro para brainstorming, resumos dos livros.",
                                  "tips": "Pergunte 'O que eu faria diferente?' para tornar a discussão ativa e reflexiva.",
                                  "learningObjective": "Sintetizar lições acionáveis para promover responsabilidade ética em ciência de dados.",
                                  "commonMistakes": "Generalizar lições sem ancorar em evidências específicas dos casos."
                                },
                                {
                                  "stepNumber": 4,
                                  "title": "Aplicar Lições a Cenários Hipotéticos e Refletir",
                                  "subSteps": [
                                    "Crie 2 cenários hipotéticos em data science (ex: scoring de crédito com dados enviesados).",
                                    "Aplique lições dos casos para propor soluções éticas com accountability.",
                                    "Avalie riscos residuais e métricas de sucesso (ex: taxa de bias <5%).",
                                    "Escreva um relatório final de 1 página resumindo insights e compromissos pessoais.",
                                    "Compartilhe com um par para feedback inicial."
                                  ],
                                  "verification": "Cenários hipotéticos resolvidos, relatório final e feedback recebido.",
                                  "estimatedTime": "1.5 horas",
                                  "materials": "Insights do Step 3, processador de texto, parceiro de discussão (opcional).",
                                  "tips": "Use templates de relatório para manter foco e brevidade.",
                                  "learningObjective": "Transferir conhecimento de casos reais para prática ética proativa.",
                                  "commonMistakes": "Ignorar aplicação prática, ficando apenas na teoria."
                                }
                              ],
                              "practicalExample": "No caso Cambridge Analytica (2018), dados do Facebook foram usados sem consentimento para manipular eleições. Análise revela falha em accountability: falta de verificação de uso de dados por terceiros. Lição: Implementar logs de auditoria em APIs de dados.",
                              "finalVerifications": [
                                "Capacidade de citar 3+ casos reais com detalhes precisos de falhas éticas.",
                                "Lista de pelo menos 5 lições sobre accountability extraídas e justificadas.",
                                "Diagrama ou mapa de um caso mostrando pontos de falha no pipeline de dados.",
                                "Princípios pessoais de ética aplicados a um cenário hipotético.",
                                "Relatório final de 1 página com referências aos livros de Raschka e Cielen.",
                                "Feedback positivo de par sobre compreensão demonstrada."
                              ],
                              "assessmentCriteria": [
                                "Profundidade da análise: Identificação precisa de violações e impactos (30%).",
                                "Relevância das lições: Foco em accountability com conexões aos livros (25%).",
                                "Criatividade na aplicação: Soluções práticas para cenários hipotéticos (20%).",
                                "Organização e clareza: Diagramas, tabelas e relatório bem estruturados (15%).",
                                "Evidências de reflexão: Princípios pessoais e feedback incorporado (10%)."
                              ],
                              "crossCurricularConnections": [
                                "Filosofia: Debates sobre utilitarismo vs. deontologia em decisões de dados.",
                                "Direito: Estudo de leis como LGPD/GDPR e responsabilidades legais em IA.",
                                "Sociologia: Impactos sociais de bias algorítmico em minorias.",
                                "Gestão de Projetos: Frameworks de governança ética em equipes ágeis."
                              ],
                              "realWorldApplication": "Em um projeto de ML para previsão de churn em uma empresa, aplicar lições para auditar dados de entrada por bias, documentar decisões éticas e incluir cláusulas de accountability em contratos com stakeholders, evitando multas regulatórias e danos à reputação."
                            },
                            "estimatedTime": "3 horas",
                            "difficulty": "advanced",
                            "status": "not_started",
                            "prerequisites": [
                              "10.1.6.5.2.1"
                            ]
                          }
                        ]
                      },
                      {
                        "id": "10.1.6.5.3",
                        "name": "Mecanismos e Práticas de Responsabilidade Ética",
                        "description": "Estratégias e ferramentas para implementar accountability, garantindo que cientistas de dados mitiguem riscos e demonstrem conformidade ética.",
                        "specificSkills": [
                          {
                            "id": "10.1.6.5.3.1",
                            "name": "Aplicar auditoria em projetos de dados",
                            "description": "Desenvolver checklists para auditoria ética em coleta, modelagem e deployment, incluindo rastreamento de linhagem de dados.",
                            "atomicExpansion": {
                              "steps": [
                                {
                                  "stepNumber": 1,
                                  "title": "Entender Princípios Éticos e Componentes de Auditoria em Projetos de Dados",
                                  "subSteps": [
                                    "Estude os princípios éticos fundamentais em ciência de dados, como privacidade, consentimento, viés e transparência.",
                                    "Identifique os estágios chave de um projeto de dados: coleta, modelagem e deployment.",
                                    "Revise regulamentações relevantes, como GDPR, LGPD ou CCPA, e seu impacto em auditorias.",
                                    "Analise exemplos de checklists de auditoria ética de fontes confiáveis (ex: OWASP para dados).",
                                    "Mapeie riscos éticos comuns em cada estágio do ciclo de vida dos dados."
                                  ],
                                  "verification": "Criar um mapa mental ou documento resumindo princípios, estágios e riscos identificados.",
                                  "estimatedTime": "4 horas",
                                  "materials": "Documentos de princípios éticos (GDPR, livros como 'Weapons of Math Destruction'), templates de checklists online, software de mind mapping (ex: XMind).",
                                  "tips": "Use fontes acadêmicas e oficiais para garantir precisão; anote citações para rastreabilidade.",
                                  "learningObjective": "Compreender o framework ético completo para auditorias em projetos de dados.",
                                  "commonMistakes": "Ignorar regulamentações locais ou focar apenas em viés, negligenciando transparência."
                                },
                                {
                                  "stepNumber": 2,
                                  "title": "Desenvolver Checklist para Auditoria Ética na Coleta de Dados",
                                  "subSteps": [
                                    "Liste itens de verificação para consentimento informado e anonimização de dados.",
                                    "Inclua checks para fontes de dados legítimas e detecção de viés na coleta.",
                                    "Defina métricas para volume e qualidade de dados coletados eticamente.",
                                    "Adicione perguntas sobre documentação de origens e aprovações necessárias.",
                                    "Teste a checklist com um dataset simulado pequeno."
                                  ],
                                  "verification": "Checklist com pelo menos 10 itens testada em um exemplo de coleta hipotética.",
                                  "estimatedTime": "3 horas",
                                  "materials": "Templates de checklists (Google Sheets ou Excel), datasets públicos (Kaggle), guias de privacidade (ex: NIST Privacy Framework).",
                                  "tips": "Torne itens acionáveis com 'Sim/Não' e espaço para evidências.",
                                  "learningObjective": "Criar uma checklist robusta para garantir ética na fase de coleta.",
                                  "commonMistakes": "Esquecer de checks para dados sensíveis ou subestimar viés em fontes não auditadas."
                                },
                                {
                                  "stepNumber": 3,
                                  "title": "Criar Checklist para Modelagem e Processamento Ético de Dados",
                                  "subSteps": [
                                    "Desenvolva itens para detecção e mitigação de viés em algoritmos de modelagem.",
                                    "Inclua verificações de interpretabilidade e explicabilidade dos modelos.",
                                    "Adicione checks para validação cruzada ética e reprodutibilidade.",
                                    "Liste requisitos para logging de transformações de dados.",
                                    "Integre avaliações de impacto ético em subgrupos afetados."
                                  ],
                                  "verification": "Checklist aplicada a um modelo simples (ex: regressão linear) com relatório de conformidade.",
                                  "estimatedTime": "4 horas",
                                  "materials": "Bibliotecas Python (Pandas, Scikit-learn, Fairlearn), Jupyter Notebook, frameworks de viés (AIF360).",
                                  "tips": "Use ferramentas automatizadas para checks de viés para eficiência.",
                                  "learningObjective": "Garantir que a modelagem respeite padrões éticos de equidade e transparência.",
                                  "commonMistakes": "Focar só em performance, ignorando viés ou falta de documentação de hiperparâmetros."
                                },
                                {
                                  "stepNumber": 4,
                                  "title": "Elaborar Checklist para Deployment e Rastreamento de Linhagem de Dados",
                                  "subSteps": [
                                    "Crie itens para auditoria de segurança no deployment (ex: criptografia, acesso controlado).",
                                    "Implemente checks para monitoramento contínuo de drift ético e performance.",
                                    "Desenvolva seção para rastreamento de linhagem: origens, transformações e versões.",
                                    "Inclua planos de resposta a incidentes éticos e auditorias pós-deployment.",
                                    "Teste a checklist em um pipeline simulado (ex: MLflow para linhagem)."
                                  ],
                                  "verification": "Checklist integrada com tool de linhagem gerando um relatório completo de um fluxo simulado.",
                                  "estimatedTime": "5 horas",
                                  "materials": "Ferramentas de MLOps (MLflow, DVC), diagramas de fluxo de dados (Draw.io), guias de deployment ético.",
                                  "tips": "Visualize a linhagem com gráficos para facilitar rastreamento.",
                                  "learningObjective": "Assegurar responsabilidade total no deployment com rastreabilidade completa.",
                                  "commonMistakes": "Não planejar monitoramento pós-deployment ou usar ferramentas inadequadas para linhagem."
                                },
                                {
                                  "stepNumber": 5,
                                  "title": "Integrar Checklists, Revisar e Finalizar a Auditoria Completa",
                                  "subSteps": [
                                    "Combine todas as checklists em um documento unificado com seções claras.",
                                    "Realize revisão por pares ou auto-revisão usando rubrica ética.",
                                    "Adicione índices de pontuação e thresholds para aprovação.",
                                    "Simule uma auditoria completa em um projeto de dados end-to-end.",
                                    "Documente lições aprendidas e atualizações futuras."
                                  ],
                                  "verification": "Checklist final aplicada com sucesso a um case study, gerando relatório aprovado.",
                                  "estimatedTime": "3 horas",
                                  "materials": "Documento mestre (Google Docs ou Markdown), rubrica de revisão ética, case studies reais.",
                                  "tips": "Use versionamento (Git) para checklists para manter histórico.",
                                  "learningObjective": "Produzir uma ferramenta de auditoria ética acionável e escalável.",
                                  "commonMistakes": "Sobrecarregar com itens irrelevantes ou pular simulações reais."
                                }
                              ],
                              "practicalExample": "Em um projeto de análise preditiva para empréstimos bancários, use a checklist para auditar: coleta (verificar consentimento de dados de clientes), modelagem (detectar viés racial via Fairlearn), deployment (rastrear linhagem com MLflow) e gerar relatório confirmando conformidade GDPR.",
                              "finalVerifications": [
                                "Checklist cobre todos os estágios: coleta, modelagem, deployment e linhagem.",
                                "Cada item é acionável com critérios Sim/Não e evidências.",
                                "Simulação completa resulta em pelo menos 90% de conformidade.",
                                "Riscos éticos identificados têm planos de mitigação.",
                                "Documentação inclui fontes e versionamento.",
                                "Checklist é escalável para projetos maiores."
                              ],
                              "assessmentCriteria": [
                                "Completude: Todos os campos éticos principais abordados (20%).",
                                "Detalhamento: Sub-itens claros e mensuráveis (25%).",
                                "Praticidade: Aplicável em cenários reais com tempo viável (20%).",
                                "Rastreabilidade: Linhagem de dados totalmente mapeada (15%).",
                                "Inovação: Inclusão de checks avançados como drift ético (10%).",
                                "Clareza: Linguagem acessível e estrutura lógica (10%)."
                              ],
                              "crossCurricularConnections": [
                                "Ética Filosófica: Análise de dilemas morais em dados.",
                                "Direito e Compliance: Aplicação de leis de proteção de dados.",
                                "Programação e MLOps: Uso de tools para automação de auditorias.",
                                "Gestão de Projetos: Integração em ciclos ágeis.",
                                "Estatística: Detecção quantitativa de viés."
                              ],
                              "realWorldApplication": "Empresas como Google e bancos usam auditorias semelhantes para compliance regulatório, evitando multas milionárias (ex: multas GDPR) e construindo confiança pública em IA, como no framework de Responsible AI da Microsoft."
                            },
                            "estimatedTime": "2 horas",
                            "difficulty": "intermediate",
                            "status": "not_started",
                            "prerequisites": [
                              "10.1.6.5.1.3"
                            ]
                          },
                          {
                            "id": "10.1.6.5.3.2",
                            "name": "Documentar decisões éticas",
                            "description": "Criar relatórios de impacto ético e logs de decisões em projetos, alinhados a frameworks como os de ética em IA de Russell e Norvig.",
                            "atomicExpansion": {
                              "steps": [
                                {
                                  "stepNumber": 1,
                                  "title": "Compreender Frameworks Éticos Relevantes",
                                  "subSteps": [
                                    "Ler seções sobre ética em IA no livro 'Artificial Intelligence: A Modern Approach' de Russell e Norvig.",
                                    "Identificar princípios chave como alinhamento de valores humanos, transparência e equidade.",
                                    "Pesquisar frameworks complementares, como os Princípios de Asilomar para IA.",
                                    "Mapear princípios aplicáveis ao contexto de ciência de dados e manipulação de dados.",
                                    "Criar um glossário pessoal com definições de termos éticos chave."
                                  ],
                                  "verification": "Glossário com pelo menos 5 princípios éticos resumidos e exemplos iniciais.",
                                  "estimatedTime": "1.5 horas",
                                  "materials": [
                                    "Livro 'Artificial Intelligence: A Modern Approach' (capítulos sobre ética)",
                                    "Site oficial dos Princípios de Asilomar para IA",
                                    "Artigos acadêmicos sobre ética em dados (ex: Google Scholar)"
                                  ],
                                  "tips": "Priorize princípios relevantes ao seu projeto específico para evitar sobrecarga informacional.",
                                  "learningObjective": "Dominar os fundamentos de frameworks éticos em IA para embasar decisões futuras.",
                                  "commonMistakes": [
                                    "Confundir ética com conformidade legal apenas",
                                    "Ignorar variações culturais nos princípios éticos",
                                    "Não anotar fontes para referência posterior"
                                  ]
                                },
                                {
                                  "stepNumber": 2,
                                  "title": "Identificar Decisões Éticas no Projeto",
                                  "subSteps": [
                                    "Mapear o pipeline completo do projeto de dados (coleta, limpeza, modelagem, deployment).",
                                    "Marcar pontos potenciais de dilema ético, como privacidade, viés ou impacto social.",
                                    "Avaliar riscos éticos usando uma checklist baseada em Russell e Norvig (ex: alinhamento com valores humanos).",
                                    "Consultar stakeholders para perspectivas adicionais sobre impactos.",
                                    "Priorizar decisões de alto impacto em uma lista numerada."
                                  ],
                                  "verification": "Diagrama ou lista com pelo menos 3-5 decisões éticas identificadas e classificadas por risco.",
                                  "estimatedTime": "2 horas",
                                  "materials": [
                                    "Diagrama do pipeline de dados do projeto",
                                    "Checklist ética personalizada (template Google Docs ou Notion)",
                                    "Ferramentas de mapeamento como Lucidchart ou Draw.io"
                                  ],
                                  "tips": "Use perguntas guiadas: 'Quem pode ser afetado?' e 'Quais valores humanos estão em jogo?'",
                                  "learningObjective": "Desenvolver habilidade para detectar dilemas éticos proativamente em fluxos de trabalho de dados.",
                                  "commonMistakes": [
                                    "Subestimar impactos indiretos de decisões técnicas",
                                    "Focar apenas em dados pessoais, ignorando viés algorítmico",
                                    "Não envolver equipe multidisciplinar cedo"
                                  ]
                                },
                                {
                                  "stepNumber": 3,
                                  "title": "Criar Log Detalhado de Decisões Éticas",
                                  "subSteps": [
                                    "Estruturar o log com colunas: Data, Decisão, Alternativas Consideradas, Justificativa Ética, Referência a Framework.",
                                    "Registrar cada decisão identificada com evidências e raciocínio alinhado a Russell e Norvig.",
                                    "Incluir métricas quantificáveis onde possível (ex: redução de viés em 20%).",
                                    "Versionar o log para rastrear evoluções.",
                                    "Adicionar hyperlinks para dados ou códigos relevantes."
                                  ],
                                  "verification": "Log preenchido com todas as decisões, formatado em tabela legível e versionado.",
                                  "estimatedTime": "2.5 horas",
                                  "materials": [
                                    "Planilha Google Sheets ou Excel para log estruturado",
                                    "Templates de log ético (ex: GitHub repos de ética em IA)",
                                    "Ferramentas de versionamento como Git"
                                  ],
                                  "tips": "Mantenha o log conciso mas completo: uma página por decisão principal.",
                                  "learningObjective": "Aprender a registrar decisões de forma auditável e reproduzível.",
                                  "commonMistakes": [
                                    "Omitir alternativas rejeitadas e por quê",
                                    "Usar jargão técnico sem explicação ética",
                                    "Não datar entradas para rastreabilidade"
                                  ]
                                },
                                {
                                  "stepNumber": 4,
                                  "title": "Elaborar Relatório de Impacto Ético e Validar",
                                  "subSteps": [
                                    "Sintetizar o log em um relatório narrativo com seções: Introdução, Decisões Chave, Impactos Projetados, Lições Aprendidas.",
                                    "Avaliar impactos potenciais usando matriz de risco (probabilidade x severidade).",
                                    "Cruzar com frameworks éticos e citar fontes.",
                                    "Revisar com pares ou mentor para feedback.",
                                    "Finalizar com recomendações para iterações futuras."
                                  ],
                                  "verification": "Relatório completo (3-5 páginas) com matriz de impacto e feedback incorporado.",
                                  "estimatedTime": "3 horas",
                                  "materials": [
                                    "Template de relatório ético (Word ou LaTeX)",
                                    "Ferramentas de análise de risco como Risk Matrix templates",
                                    "Ferramenta de revisão colaborativa como Google Docs"
                                  ],
                                  "tips": "Use visualizações como gráficos para ilustrar impactos éticos.",
                                  "learningObjective": "Produzir relatórios profissionais que comuniquem ética a não-especialistas.",
                                  "commonMistakes": [
                                    "Fazer relatório muito técnico sem foco em impactos humanos",
                                    "Ignorar feedback externo",
                                    "Não quantificar impactos onde possível"
                                  ]
                                }
                              ],
                              "practicalExample": "Em um projeto de modelo preditivo para empréstimos bancários, documente a decisão de implementar anonimização de dados sensíveis (ex: gênero, etnia) citando princípios de equidade de Russell e Norvig; logue alternativas como remoção total vs. técnicas de fairML; relatórios mostram redução de viés discriminatório em 15%.",
                              "finalVerifications": [
                                "Log contém todas decisões identificadas com justificativas alinhadas a frameworks.",
                                "Relatório sintetiza impactos éticos de forma clara e acionável.",
                                "Matriz de risco ética está completa e priorizada.",
                                "Documentos versionados e compartilháveis.",
                                "Feedback de revisão incorporado com resumo de mudanças.",
                                "Glossário de termos éticos incluído para acessibilidade."
                              ],
                              "assessmentCriteria": [
                                "Completude: Todas decisões éticas mapeadas e documentadas.",
                                "Alinhamento: Evidências claras de referência a frameworks como Russell e Norvig.",
                                "Clareza: Linguagem acessível a stakeholders não-técnicos.",
                                "Profundidade: Análise de alternativas e impactos quantificados.",
                                "Reprodutibilidade: Estrutura auditável com versionamento.",
                                "Inovação: Recomendações práticas para melhorias éticas futuras."
                              ],
                              "crossCurricularConnections": [
                                "Filosofia: Aplicação de teorias éticas (utilitarismo, deontologia).",
                                "Direito: Conformidade com regulamentações como LGPD e GDPR.",
                                "Ciência da Computação: Integração ética em pipelines de ML.",
                                "Gestão de Projetos: Auditoria e governança em equipes ágeis.",
                                "Comunicação: Redação de relatórios para públicos variados."
                              ],
                              "realWorldApplication": "Em empresas como Google ou bancos, logs e relatórios éticos são mandatórios para auditorias regulatórias, aprovação de modelos de IA e relatórios anuais de responsabilidade corporativa, evitando multas e construindo confiança pública."
                            },
                            "estimatedTime": "1.5 horas",
                            "difficulty": "intermediate",
                            "status": "not_started",
                            "prerequisites": [
                              "10.1.6.5.2.2"
                            ]
                          },
                          {
                            "id": "10.1.6.5.3.3",
                            "name": "Implementar transparência em modelos",
                            "description": "Utilizar técnicas como explainable AI para tornar modelos auditáveis, com exemplos de scikit-learn e TensorFlow citados na bibliografia.",
                            "atomicExpansion": {
                              "steps": [
                                {
                                  "stepNumber": 1,
                                  "title": "Compreender Fundamentos de Explainable AI (XAI)",
                                  "subSteps": [
                                    "Estude conceitos chave como interpretabilidade local vs global, métodos model-agnostic (LIME, SHAP) e model-specific.",
                                    "Revise exemplos de bibliotecas: SHAP para feature importance, LIME para predições individuais.",
                                    "Analise casos onde opacidade em modelos leva a vieses éticos, como em empréstimos bancários.",
                                    "Identifique requisitos de auditabilidade: reprodutibilidade, clareza e neutralidade.",
                                    "Crie um glossário pessoal com 5 termos XAI essenciais."
                                  ],
                                  "verification": "Resuma em um documento os 3 principais benefícios da XAI para transparência ética.",
                                  "estimatedTime": "45 minutos",
                                  "materials": "Documentação SHAP (shap.readthedocs.io), LIME (lime-ml.readthedocs.io), Artigo 'Why Should I Trust You? (Ribeiro et al.)'",
                                  "tips": "Use diagramas para visualizar local vs global interpretabilidade.",
                                  "learningObjective": "Dominar conceitos básicos de XAI e sua relevância ética.",
                                  "commonMistakes": "Confundir interpretabilidade com acurácia; ignorar contexto ético."
                                },
                                {
                                  "stepNumber": 2,
                                  "title": "Implementar XAI em Modelos Scikit-learn",
                                  "subSteps": [
                                    "Carregue um dataset simples (ex: Iris ou Boston Housing) e treine um modelo Random Forest.",
                                    "Instale e importe SHAP: pip install shap; shap.initjs() para plots.",
                                    "Gere explicações globais com summary_plot e locais com force_plot para uma predição.",
                                    "Extraia feature importances e valide com permutation_importance do scikit-learn.",
                                    "Salve visualizações como HTML para auditoria."
                                  ],
                                  "verification": "Produza um SHAP summary plot e explique as top 3 features em um relatório curto.",
                                  "estimatedTime": "1 hora",
                                  "materials": "Python/Jupyter, scikit-learn, pandas, shap library, Dataset Iris (sklearn.datasets)",
                                  "tips": "Sempre normalize dados antes para evitar distorções em importâncias.",
                                  "learningObjective": "Aplicar SHAP/LIME em modelos tree-based para transparência.",
                                  "commonMistakes": "Não seedar random_state, levando a resultados irreprodutíveis."
                                },
                                {
                                  "stepNumber": 3,
                                  "title": "Implementar XAI em Modelos TensorFlow/Keras",
                                  "subSteps": [
                                    "Construa uma rede neural simples para classificação binária (ex: dataset de crédito).",
                                    "Use tf-explain ou SHAP KernelExplainer para deep learning.",
                                    "Gere Grad-CAM ou Integrated Gradients para visualizações de camadas.",
                                    "Compare explicações com baseline scikit-learn no mesmo dataset.",
                                    "Integre callbacks para logging de explicações durante treinamento."
                                  ],
                                  "verification": "Gere um mapa de calor Grad-CAM e interprete impactos em predições.",
                                  "estimatedTime": "1 hora 30 minutos",
                                  "materials": "TensorFlow/Keras, tf-explain (pip install tf-explain), shap, Dataset UCI Credit (archive.ics.uci.edu)",
                                  "tips": "Use GPU se disponível para acelerar KernelExplainer em grandes modelos.",
                                  "learningObjective": "Estender transparência a modelos deep learning complexos.",
                                  "commonMistakes": "Aplicar explainers incompatíveis sem adaptação para black-box models."
                                },
                                {
                                  "stepNumber": 4,
                                  "title": "Avaliar e Documentar Transparência para Auditoria",
                                  "subSteps": [
                                    "Teste robustez das explicações variando inputs (adversarial testing).",
                                    "Crie um relatório com métricas: faithfulness score, stability index.",
                                    "Documente pipeline completo em Markdown/Jupyter com código, plots e interpretações.",
                                    "Simule auditoria: responda perguntas como 'Por que esta predição foi negada?'.",
                                    "Versione o código com Git e adicione licenças open-source para reprodutibilidade."
                                  ],
                                  "verification": "Produza um relatório auditável de 2 páginas com evidências visuais e métricas.",
                                  "estimatedTime": "45 minutos",
                                  "materials": "Jupyter Notebook, Git, Métricas de XAI (biblioteca alibi para faithfulness)",
                                  "tips": "Estruture relatório com seções: Modelo, Explicações, Interpretações, Limitações.",
                                  "learningObjective": "Garantir que implementações sejam auditáveis e éticas.",
                                  "commonMistakes": "Omitir limitações das explicações, como aproximações em SHAP."
                                }
                              ],
                              "practicalExample": "Em um modelo de aprovação de empréstimos com scikit-learn Random Forest, use SHAP para explicar por que um cliente foi rejeitado: feature 'renda baixa' contribui +0.3 para risco, visualizado em force_plot, permitindo auditoria ética.",
                              "finalVerifications": [
                                "Explicações são reprodutíveis com seed fixo.",
                                "Top features alinhadas com domínio ético (sem vieses proxy).",
                                "Relatório cobre local/global interpretabilidade.",
                                "Visualizações salvas e interpretáveis sem código.",
                                "Métricas de qualidade XAI acima de 0.8 (ex: faithfulness).",
                                "Simulação de auditoria responde 5 perguntas chave."
                              ],
                              "assessmentCriteria": [
                                "Precisão das explicações (alinhamento com feature importances nativas).",
                                "Clareza visual e textual no relatório.",
                                "Cobertura de ambos scikit-learn e TensorFlow.",
                                "Tratamento de vieses éticos identificados.",
                                "Reprodutibilidade total do pipeline.",
                                "Profundidade de sub-steps implementados."
                              ],
                              "crossCurricularConnections": [
                                "Ética em IA: Alinhamento com princípios FAIR (Findable, Accessible, Interoperable, Reusable).",
                                "Direito e Governança: Conformidade com GDPR Artigo 22 (direito a explicação).",
                                "Matemática: Estatística inferencial para validar importâncias.",
                                "Programação: Boas práticas de ML pipelines (MLOps)."
                              ],
                              "realWorldApplication": "Em bancos, implementar XAI em modelos de scoring de crédito permite auditorias regulatórias (ex: BCBS 239), reduzindo multas por discriminação e aumentando confiança pública em decisões automatizadas."
                            },
                            "estimatedTime": "3 horas",
                            "difficulty": "advanced",
                            "status": "not_started",
                            "prerequisites": [
                              "10.1.6.5.2.3"
                            ]
                          },
                          {
                            "id": "10.1.6.5.3.4",
                            "name": "Planejar respostas a incidentes éticos",
                            "description": "Elaborar protocolos para lidar com impactos negativos detectados pós-deploy, incluindo notificações e correções.",
                            "atomicExpansion": {
                              "steps": [
                                {
                                  "stepNumber": 1,
                                  "title": "Identificar e Documentar o Incidente Ético",
                                  "subSteps": [
                                    "Monitore logs e feedback pós-deploy para detectar anomalias éticas, como viés em previsões.",
                                    "Registre detalhes do incidente: data, sistema afetado, dados envolvidos e evidências iniciais.",
                                    "Classifique o incidente por gravidade (baixa, média, alta) usando critérios éticos pré-definidos.",
                                    "Consulte stakeholders iniciais para validar a detecção.",
                                    "Crie um relatório inicial com capturas de tela ou dados de exemplo."
                                  ],
                                  "verification": "Relatório inicial documentado e compartilhado com equipe ética para aprovação.",
                                  "estimatedTime": "2-4 horas",
                                  "materials": "Ferramentas de monitoramento (ex: Prometheus, ELK Stack), templates de relatório ético, acesso a logs.",
                                  "tips": "Use checklists padronizadas para agilizar a documentação e evitar omissões.",
                                  "learningObjective": "Aprender a detectar e registrar incidentes éticos de forma sistemática.",
                                  "commonMistakes": "Ignorar incidentes menores que podem escalar; documentar de forma vaga sem evidências."
                                },
                                {
                                  "stepNumber": 2,
                                  "title": "Avaliar Impacto e Riscos Associados",
                                  "subSteps": [
                                    "Quantifique o impacto: número de usuários afetados, tipo de dano (privacidade, discriminação, etc.).",
                                    "Analise riscos legais, reputacionais e operacionais usando matriz de risco.",
                                    "Consulte frameworks éticos como GDPR ou AI Ethics Guidelines para contextualizar.",
                                    "Entreviste usuários afetados ou simule cenários de impacto.",
                                    "Priorize ações baseadas na severidade do risco identificado."
                                  ],
                                  "verification": "Matriz de risco preenchida e riscos priorizados em relatório de avaliação.",
                                  "estimatedTime": "4-6 horas",
                                  "materials": "Matriz de risco em Excel/Google Sheets, guidelines éticas (ex: IEEE Ethically Aligned Design), dados de impacto.",
                                  "tips": "Envolva especialistas multidisciplinares para uma avaliação imparcial.",
                                  "learningObjective": "Desenvolver habilidades para quantificar e priorizar riscos éticos.",
                                  "commonMistakes": "Subestimar impactos indiretos como perda de confiança pública."
                                },
                                {
                                  "stepNumber": 3,
                                  "title": "Elaborar o Protocolo de Resposta",
                                  "subSteps": [
                                    "Defina ações corretivas: rollback, correção de modelo, mitigação de viés.",
                                    "Estabeleça procedimentos de notificação: quem, quando e como notificar (usuários, reguladores).",
                                    "Inclua timelines: prazos para correção e follow-up.",
                                    "Crie plano de comunicação transparente, incluindo pedido de desculpas se aplicável.",
                                    "Incorpore lições aprendidas para prevenção futura."
                                  ],
                                  "verification": "Protocolo escrito com todas as seções completas e revisado por pares.",
                                  "estimatedTime": "6-8 horas",
                                  "materials": "Templates de protocolo (Word/Google Docs), checklists de conformidade legal.",
                                  "tips": "Use linguagem clara e acessível para garantir adesão por toda a equipe.",
                                  "learningObjective": "Criar protocolos acionáveis e completos para respostas éticas.",
                                  "commonMistakes": "Omitir notificações obrigatórias por lei; criar planos irrealistas sem recursos."
                                },
                                {
                                  "stepNumber": 4,
                                  "title": "Testar, Implementar e Monitorar o Protocolo",
                                  "subSteps": [
                                    "Simule o protocolo em ambiente de teste para validar eficácia.",
                                    "Execute notificações e correções em produção com monitoramento em tempo real.",
                                    "Colete feedback pós-implementação de stakeholders.",
                                    "Monitore métricas de sucesso por 1-2 semanas (ex: resolução de queixas).",
                                    "Atualize o protocolo com lições aprendidas e arquive para auditoria."
                                  ],
                                  "verification": "Relatório de implementação com evidências de execução e métricas de monitoramento.",
                                  "estimatedTime": "8-12 horas iniciais + monitoramento contínuo",
                                  "materials": "Ambiente de staging, ferramentas de comunicação (email/Slack), dashboards de monitoramento.",
                                  "tips": "Automatize partes do monitoramento para eficiência.",
                                  "learningObjective": "Executar e refinar protocolos éticos em cenários reais.",
                                  "commonMistakes": "Não testar adequadamente, levando a falhas na implementação."
                                }
                              ],
                              "practicalExample": "Em um sistema de recomendação de empréstimos deployado, detecta-se viés racial em aprovações pós-lançamento. O protocolo inclui: notificar clientes afetados em 24h, pausar o modelo, retrainar com dados balanceados e reportar à autoridade reguladora.",
                              "finalVerifications": [
                                "Protocolo cobre detecção, avaliação, resposta e monitoramento.",
                                "Inclui notificações claras para todos os afetados.",
                                "Timelines realistas e mensuráveis definidos.",
                                "Lições aprendidas documentadas para iterações futuras.",
                                "Conformidade com leis como LGPD/GDPR verificada.",
                                "Simulação de teste realizada com sucesso."
                              ],
                              "assessmentCriteria": [
                                "Completude: Todos os elementos do protocolo presentes (20%).",
                                "Clareza e Acessibilidade: Linguagem simples e estrutura lógica (20%).",
                                "Realismo: Timelines e recursos viáveis (20%).",
                                "Abrangência de Riscos: Cobertura de impactos éticos, legais e sociais (20%).",
                                "Inovação Preventiva: Medidas para evitar recorrências (10%).",
                                "Evidências de Teste: Documentação de simulações (10%)."
                              ],
                              "crossCurricularConnections": [
                                "Direito Digital: Conformidade com regulamentações de dados.",
                                "Gestão de Projetos: Planejamento de crises e timelines.",
                                "Psicologia Organizacional: Comunicação em situações de crise.",
                                "Engenharia de Software: Rollback e monitoramento de sistemas."
                              ],
                              "realWorldApplication": "Empresas como Google usam protocolos semelhantes no 'Responsible AI Practices' para responder a incidentes como o Gemini image generator bias em 2024, notificando usuários, pausando features e publicando relatórios de transparência."
                            },
                            "estimatedTime": "2 horas",
                            "difficulty": "advanced",
                            "status": "not_started",
                            "prerequisites": [
                              "10.1.6.5.3.1"
                            ]
                          }
                        ]
                      }
                    ]
                  }
                ]
              }
            ],
            "totalSkills": 319
          }
        ],
        "totalSkills": 319,
        "percentage": 0
      }
    ]
  }
}